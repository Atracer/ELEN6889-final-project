<html>
<head>
<title>mesh_utils.py</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<style type="text/css">
.s0 { color: #808080;}
.s1 { color: #a9b7c6;}
.s2 { color: #629755; font-style: italic;}
.s3 { color: #cc7832;}
.s4 { color: #6a8759;}
.s5 { color: #6897bb;}
</style>
</head>
<body bgcolor="#2b2b2b">
<table CELLSPACING=0 CELLPADDING=5 COLS=1 WIDTH="100%" BGCOLOR="#606060" >
<tr><td><center>
<font face="Arial, Helvetica" color="#000000">
mesh_utils.py</font>
</center></td></tr></table>
<pre><span class="s0"># Copyright 2021 The TensorFlow Authors. All Rights Reserved.</span>
<span class="s0">#</span>
<span class="s0"># Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);</span>
<span class="s0"># you may not use this file except in compliance with the License.</span>
<span class="s0"># You may obtain a copy of the License at</span>
<span class="s0">#</span>
<span class="s0">#     http://www.apache.org/licenses/LICENSE-2.0</span>
<span class="s0">#</span>
<span class="s0"># Unless required by applicable law or agreed to in writing, software</span>
<span class="s0"># distributed under the License is distributed on an &quot;AS IS&quot; BASIS,</span>
<span class="s0"># WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span>
<span class="s0"># See the License for the specific language governing permissions and</span>
<span class="s0"># limitations under the License.</span>
<span class="s0"># ==============================================================================</span>
<span class="s2">&quot;&quot;&quot;Utils for building a device mesh.&quot;&quot;&quot;</span>

<span class="s3">import </span><span class="s1">itertools</span>
<span class="s3">import </span><span class="s1">logging</span>
<span class="s3">from </span><span class="s1">typing </span><span class="s3">import </span><span class="s1">Any</span><span class="s3">, </span><span class="s1">Dict</span><span class="s3">, </span><span class="s1">List</span><span class="s3">, </span><span class="s1">Optional</span><span class="s3">, </span><span class="s1">Sequence</span><span class="s3">, </span><span class="s1">Tuple</span>

<span class="s3">import </span><span class="s1">jax</span>
<span class="s3">import </span><span class="s1">numpy </span><span class="s3">as </span><span class="s1">np</span>

<span class="s1">logger = logging.getLogger(__name__)</span>

<span class="s1">_TPU_V2 = </span><span class="s4">'TPU v2'</span>
<span class="s1">_TPU_V3 = </span><span class="s4">'TPU v3'</span>
<span class="s1">_TPU_V4 = </span><span class="s4">'TPU v4'</span>

<span class="s0"># Maps physical topology -&gt; mesh shape -&gt; transpose to use for jekbradbury's</span>
<span class="s0"># famous contiguous mesh trick.</span>
<span class="s0">#</span>
<span class="s0"># The trick only works for certain topologies and mesh shapes. Trivial dims of</span>
<span class="s0"># size 1 can be added to the shapes listed, and they are also supported.</span>
<span class="s1">_TRANSPOSE_TRICKS: Dict[Tuple[int</span><span class="s3">, </span><span class="s1">...]</span><span class="s3">,</span>
                        <span class="s1">Dict[Tuple[int</span><span class="s3">, </span><span class="s1">...]</span><span class="s3">, </span><span class="s1">Tuple[int</span><span class="s3">, </span><span class="s1">...]]] = {</span>
    <span class="s1">(</span><span class="s5">2</span><span class="s3">, </span><span class="s5">2</span><span class="s3">, </span><span class="s5">1</span><span class="s1">): {</span>
        <span class="s1">(</span><span class="s5">2</span><span class="s3">, </span><span class="s5">2</span><span class="s1">): (</span><span class="s5">0</span><span class="s3">, </span><span class="s5">1</span><span class="s3">, </span><span class="s5">2</span><span class="s1">)</span><span class="s3">,</span>
    <span class="s1">}</span><span class="s3">,</span>
    <span class="s1">(</span><span class="s5">2</span><span class="s3">, </span><span class="s5">2</span><span class="s3">, </span><span class="s5">4</span><span class="s1">): {</span>
        <span class="s1">(</span><span class="s5">4</span><span class="s3">, </span><span class="s5">4</span><span class="s1">): (</span><span class="s5">0</span><span class="s3">, </span><span class="s5">1</span><span class="s3">, </span><span class="s5">2</span><span class="s1">)</span><span class="s3">,</span>
    <span class="s1">}</span><span class="s3">,</span>
    <span class="s1">(</span><span class="s5">4</span><span class="s3">, </span><span class="s5">4</span><span class="s3">, </span><span class="s5">4</span><span class="s1">): {</span>
        <span class="s1">(</span><span class="s5">16</span><span class="s3">, </span><span class="s5">4</span><span class="s1">): (</span><span class="s5">0</span><span class="s3">, </span><span class="s5">2</span><span class="s3">, </span><span class="s5">1</span><span class="s1">)</span><span class="s3">,</span>
    <span class="s1">}</span><span class="s3">,</span>
    <span class="s1">(</span><span class="s5">4</span><span class="s3">, </span><span class="s5">8</span><span class="s3">, </span><span class="s5">8</span><span class="s1">): {</span>
        <span class="s1">(</span><span class="s5">64</span><span class="s3">, </span><span class="s5">4</span><span class="s1">): (</span><span class="s5">0</span><span class="s3">, </span><span class="s5">2</span><span class="s3">, </span><span class="s5">1</span><span class="s1">)</span><span class="s3">,</span>
        <span class="s1">(</span><span class="s5">4</span><span class="s3">, </span><span class="s5">64</span><span class="s1">): (</span><span class="s5">0</span><span class="s3">, </span><span class="s5">2</span><span class="s3">, </span><span class="s5">1</span><span class="s1">)</span><span class="s3">,</span>
    <span class="s1">}</span><span class="s3">,</span>
    <span class="s1">(</span><span class="s5">8</span><span class="s3">, </span><span class="s5">8</span><span class="s3">, </span><span class="s5">8</span><span class="s1">): {</span>
        <span class="s1">(</span><span class="s5">64</span><span class="s3">, </span><span class="s5">8</span><span class="s1">): (</span><span class="s5">0</span><span class="s3">, </span><span class="s5">2</span><span class="s3">, </span><span class="s5">1</span><span class="s1">)</span><span class="s3">,</span>
    <span class="s1">}</span><span class="s3">,</span>
    <span class="s1">(</span><span class="s5">8</span><span class="s3">, </span><span class="s5">16</span><span class="s3">, </span><span class="s5">16</span><span class="s1">): {</span>
        <span class="s1">(</span><span class="s5">256</span><span class="s3">, </span><span class="s5">8</span><span class="s1">): (</span><span class="s5">0</span><span class="s3">, </span><span class="s5">2</span><span class="s3">, </span><span class="s5">1</span><span class="s1">)</span><span class="s3">,</span>
        <span class="s1">(</span><span class="s5">8</span><span class="s3">, </span><span class="s5">256</span><span class="s1">): (</span><span class="s5">0</span><span class="s3">, </span><span class="s5">2</span><span class="s3">, </span><span class="s5">1</span><span class="s1">)</span><span class="s3">,</span>
    <span class="s1">}</span><span class="s3">,</span>
<span class="s1">}</span>

<span class="s0"># Physical ordering of core IDs in a tray that creates a ring</span>
<span class="s1">_TRAY_RING_ORDER = (</span><span class="s5">0</span><span class="s3">, </span><span class="s5">1</span><span class="s3">, </span><span class="s5">2</span><span class="s3">, </span><span class="s5">3</span><span class="s3">, </span><span class="s5">6</span><span class="s3">, </span><span class="s5">7</span><span class="s3">, </span><span class="s5">4</span><span class="s3">, </span><span class="s5">5</span><span class="s1">)</span>


<span class="s3">def </span><span class="s1">_create_device_mesh_for_nd_torus(</span>
    <span class="s1">physical_mesh: np.ndarray</span><span class="s3">, </span><span class="s1">mesh_shape: Sequence[int]</span><span class="s3">,</span>
<span class="s1">) -&gt; Tuple[np.ndarray</span><span class="s3">, </span><span class="s1">List[Tuple[int</span><span class="s3">, </span><span class="s1">...]]]:</span>
  <span class="s2">&quot;&quot;&quot;Assigns logical parallelism axes to physical axes of an N-D torus network. 
 
  Given logical parallelism axes with sizes in `mesh_shape` and devices in an 
  N-dimensional torus network represented by `physical_mesh`, maps each logical 
  axis to one or more physical axes. Prefer to map more-performance-sensitive 
  logical axes to larger numbers of physical axes to maximize the bandwidth 
  available to them. Also prefer to assign logical axes to multiple physical 
  axes of the same size (e.g., a 2D square) rather than multiple physical axes 
  of different sizes when possible. 
 
  Note that this routine will never split a physical axis over more than one 
  logical axis (which would reduce total usable bandwidth but may sometimes be 
  desired anyway). As a result, it will error out in cases where this is 
  necessary to produce a valid mapping. 
 
  Let's use a concrete example to explain the concepts and considerations. 
 
  As an example, suppose the logical mesh is [data, model], for data and model 
  parallelism respectively. Also suppose that data parallelism is less 
  performance sensitive than model parallelism. Consider a 3D TPU pod slice of 
  shape 4x4x16, represented by a physical mesh of shape (4, 4, 16). 
 
  A TPU pod slice has equal bandwidth along all axes with wraparound links, but 
  a 2D plane of size 4x4 may have faster XLA collective implementations than a 
  non-square plane or a 1D subgroup. If the mesh_shape is [16, 16], we may want 
  the more performance sensitive `model` axis to be mapped to the 4x4 XY plane. 
 
  Args: 
    physical_mesh: a np.ndarray of devices in the shape of the N-D torus 
      physical topology. 
    mesh_shape: shape of the logical mesh (size of the various logical 
      parallelism axes), with axes ordered by increasing network intensity. 
    prefer_symmetric: whether to prefer to assign a logical axis to multiple 
      physical axes of the same size rather than axes of different sizes. 
 
  Returns: 
    An np.ndarray of devices in the shape of the logical mesh (mesh_shape), with 
      each logical parallelism axis mapped to one or more physical mesh axes. 
    The axis assignment (a list of length num_logical_axes, whose elements 
      are tuples representing physical axis indices). 
  &quot;&quot;&quot;</span>
  <span class="s0"># Remaining physical axes to be assigned to logical axes.</span>
  <span class="s1">assignable_physical_mesh = list(physical_mesh.shape)</span>
  <span class="s0"># Map each logical axis to a subset of physical axes.</span>
  <span class="s1">assignment: List[Tuple[int</span><span class="s3">, </span><span class="s1">...]] = [() </span><span class="s3">for </span><span class="s1">_ </span><span class="s3">in </span><span class="s1">mesh_shape]</span>

  <span class="s3">def </span><span class="s1">sort_key(i):</span>
    <span class="s0"># Sort from smaller dims to larger dims. Because smaller dims have fewer</span>
    <span class="s0"># choices. Use the index to break ties: `mesh_shape` is assumed to ordered</span>
    <span class="s0"># by lowest network intensity first, so larger i comes earlier.</span>
    <span class="s3">return </span><span class="s1">(mesh_shape[i]</span><span class="s3">, </span><span class="s1">-i)</span>

  <span class="s1">sorted_dims = sorted(list(range(len(mesh_shape)))</span><span class="s3">, </span><span class="s1">key=sort_key)</span>

  <span class="s3">for </span><span class="s1">logical_axis_index </span><span class="s3">in </span><span class="s1">sorted_dims:</span>
    <span class="s1">logical_axis_size = mesh_shape[logical_axis_index]</span>
    <span class="s0"># Preferentially map to more physical axes first for higher bandwidth.</span>
    <span class="s3">for </span><span class="s1">num_axes </span><span class="s3">in </span><span class="s1">range(</span><span class="s5">3</span><span class="s3">, </span><span class="s5">0</span><span class="s3">, </span><span class="s1">-</span><span class="s5">1</span><span class="s1">):</span>
      <span class="s0"># Try assign to any subset of size num_axes. Generate all candidates.</span>
      <span class="s1">axes = itertools.combinations(assignable_physical_mesh</span><span class="s3">, </span><span class="s1">num_axes)</span>
      <span class="s1">indices = itertools.combinations(</span>
          <span class="s1">range(len(assignable_physical_mesh))</span><span class="s3">, </span><span class="s1">num_axes)</span>
      <span class="s3">for </span><span class="s1">c_axes</span><span class="s3">, </span><span class="s1">c_indices </span><span class="s3">in </span><span class="s1">zip(axes</span><span class="s3">, </span><span class="s1">indices):</span>
        <span class="s0"># TODO(zhangqiaorjc): Due to limitations in XLA, 2D collectives only</span>
        <span class="s0"># implemented for square 2D plane. Mapping a physical axis to two</span>
        <span class="s0"># logical axes might be slower for non-square 2D plane, e.g., map 32 to</span>
        <span class="s0"># 4x8 or a single axis. If XLA 2D collectives support non-square plane</span>
        <span class="s0"># soon, we can continue to preferentially map to 2D plane in general,</span>
        <span class="s0"># otherwise, we should treat non-square 2D plane and 1D submesh equally.</span>
        <span class="s3">if </span><span class="s1">np.product(c_axes) == logical_axis_size:</span>
          <span class="s1">assignment[logical_axis_index] = c_indices</span>
          <span class="s0"># Zero the assigned physical axes.</span>
          <span class="s1">assignable_physical_mesh = [</span>
              <span class="s5">0 </span><span class="s3">if </span><span class="s1">i </span><span class="s3">in </span><span class="s1">c_indices </span><span class="s3">else </span><span class="s1">v</span>
              <span class="s3">for </span><span class="s1">i</span><span class="s3">, </span><span class="s1">v </span><span class="s3">in </span><span class="s1">enumerate(assignable_physical_mesh)</span>
          <span class="s1">]</span>
          <span class="s3">break</span>
      <span class="s3">if </span><span class="s1">assignment[logical_axis_index]:</span>
        <span class="s0"># We already found an assignment from one candidate above.</span>
        <span class="s3">break</span>
    <span class="s3">else</span><span class="s1">:</span>
      <span class="s0"># If the num_axes for loop did not break, i.e. none of the candidates work</span>
      <span class="s0"># goto here with this while-else construct.</span>
      <span class="s3">if </span><span class="s1">logical_axis_size &gt; </span><span class="s5">1</span><span class="s1">:</span>
        <span class="s3">raise </span><span class="s1">NotImplementedError(</span><span class="s4">f'Failed to find assignment for '</span>
                                  <span class="s4">f'logical_axis_index </span><span class="s3">{</span><span class="s1">logical_axis_index</span><span class="s3">}</span><span class="s4">'</span><span class="s1">)</span>
  <span class="s0"># Flatten the assignment, e.g., [(), (2,), (0, 1)] -&gt; (2, 0, 1).</span>
  <span class="s1">transpose: List[int] = []</span>
  <span class="s3">for </span><span class="s1">x </span><span class="s3">in </span><span class="s1">assignment:</span>
    <span class="s3">for </span><span class="s1">y </span><span class="s3">in </span><span class="s1">x:</span>
      <span class="s1">transpose.append(int(y))</span>
  <span class="s3">return </span><span class="s1">physical_mesh.transpose(transpose).reshape(mesh_shape)</span><span class="s3">, </span><span class="s1">assignment</span>


<span class="s3">def </span><span class="s1">_bounds_from_last_device(last_device) -&gt; Sequence[int]:</span>
  <span class="s2">&quot;&quot;&quot;Gets the bound from the given last device.&quot;&quot;&quot;</span>
  <span class="s0"># Must be passed the device at the highest-coordinate corner of the</span>
  <span class="s0"># relevant mesh, which is a requirement we know is satisfied by the last</span>
  <span class="s0"># device in jax.devices().</span>
  <span class="s3">assert </span><span class="s1">hasattr(last_device</span><span class="s3">, </span><span class="s4">'coords'</span><span class="s1">)</span><span class="s3">, </span><span class="s4">'Only TPU supported'</span>
  <span class="s1">x</span><span class="s3">, </span><span class="s1">y</span><span class="s3">, </span><span class="s1">z = last_device.coords</span>
  <span class="s3">return </span><span class="s1">x + </span><span class="s5">1</span><span class="s3">, </span><span class="s1">y + </span><span class="s5">1</span><span class="s3">, </span><span class="s1">z + </span><span class="s5">1</span><span class="s3">, </span><span class="s1">last_device.core_on_chip + </span><span class="s5">1</span>


<span class="s3">def </span><span class="s1">_get_physical_tpu_mesh(jax_devices: Sequence[Any]) -&gt; np.ndarray:</span>
  <span class="s2">r&quot;&quot;&quot;Rearrange TPU devices in a slice into a physical mesh. 
 
  Args: 
    jax_devices: A list of JAX devices in a TPU slice in process-tiled z, y, x, 
      core order, e.g. from jax.devices(). 
 
  Returns: 
    A np.ndarray of JAX devices with shape [global_x, global_y, global_z]. On 
      v2 and v3, global_z is instead cores_per_chip (i.e., 2). 
  &quot;&quot;&quot;</span>
  <span class="s1">device_kind = jax_devices[</span><span class="s5">0</span><span class="s1">].device_kind</span>
  <span class="s3">def </span><span class="s1">sort_key(device):</span>
    <span class="s1">x</span><span class="s3">, </span><span class="s1">y</span><span class="s3">, </span><span class="s1">z = device.coords</span>
    <span class="s1">core = device.core_on_chip</span>
    <span class="s3">if </span><span class="s1">device_kind </span><span class="s3">in </span><span class="s1">(_TPU_V2</span><span class="s3">, </span><span class="s1">_TPU_V3):</span>
      <span class="s3">assert </span><span class="s1">z == </span><span class="s5">0</span>
      <span class="s3">return </span><span class="s1">(x</span><span class="s3">, </span><span class="s1">y</span><span class="s3">, </span><span class="s1">core)</span>
    <span class="s3">else</span><span class="s1">:</span>
      <span class="s3">if </span><span class="s1">core != </span><span class="s5">0</span><span class="s1">:</span>
        <span class="s3">raise </span><span class="s1">ValueError(</span>
            <span class="s4">'Creating meshes for TPU &gt;v3 requires one device per chip'</span><span class="s1">)</span>
      <span class="s3">return </span><span class="s1">(x</span><span class="s3">, </span><span class="s1">y</span><span class="s3">, </span><span class="s1">z)</span>
  <span class="s1">sorted_devices = sorted(jax_devices</span><span class="s3">, </span><span class="s1">key=sort_key)</span>
  <span class="s1">x</span><span class="s3">, </span><span class="s1">y</span><span class="s3">, </span><span class="s1">*_ = _bounds_from_last_device(sorted_devices[-</span><span class="s5">1</span><span class="s1">])</span>
  <span class="s3">return </span><span class="s1">np.array(sorted_devices).reshape((x</span><span class="s3">, </span><span class="s1">y</span><span class="s3">, </span><span class="s1">-</span><span class="s5">1</span><span class="s1">))</span>


<span class="s0"># jekbradbury's famous trick for creating contiguous submeshes (where available)</span>
<span class="s3">def </span><span class="s1">_transpose_trick(physical_mesh: np.ndarray</span><span class="s3">,</span>
                     <span class="s1">mesh_shape: Sequence[int]) -&gt; np.ndarray:</span>
  <span class="s1">mesh_shape = tuple(mesh_shape)</span>
  <span class="s1">topology = physical_mesh.shape</span>
  <span class="s3">if </span><span class="s1">topology </span><span class="s3">not in </span><span class="s1">_TRANSPOSE_TRICKS:</span>
    <span class="s3">raise </span><span class="s1">ValueError(</span>
        <span class="s4">f&quot;create_device_mesh cannot create contiguous submeshes for &quot;</span>
        <span class="s4">f&quot;physical mesh topology </span><span class="s3">{</span><span class="s1">topology</span><span class="s3">}</span><span class="s4">&quot;</span><span class="s1">)</span>

  <span class="s1">mesh_shape_no_trivial_dims: Tuple[int</span><span class="s3">, </span><span class="s1">...] = ()</span>
  <span class="s3">for </span><span class="s1">dim_size </span><span class="s3">in </span><span class="s1">mesh_shape:</span>
    <span class="s3">if </span><span class="s1">dim_size != </span><span class="s5">1</span><span class="s1">:</span>
      <span class="s1">mesh_shape_no_trivial_dims += (dim_size</span><span class="s3">,</span><span class="s1">)</span>

  <span class="s3">if </span><span class="s1">mesh_shape_no_trivial_dims </span><span class="s3">not in </span><span class="s1">_TRANSPOSE_TRICKS[topology]:</span>
    <span class="s3">raise </span><span class="s1">ValueError(</span>
        <span class="s4">f&quot;create_device_mesh cannot create contiguous submeshes for &quot;</span>
        <span class="s4">f&quot;mesh_shape </span><span class="s3">{</span><span class="s1">mesh_shape</span><span class="s3">} </span><span class="s4">and physical mesh topology </span><span class="s3">{</span><span class="s1">topology</span><span class="s3">}</span><span class="s4">. &quot;</span>
        <span class="s4">f&quot;Available mesh_shapes: </span><span class="s3">{</span><span class="s1">list(_TRANSPOSE_TRICKS[topology].keys())</span><span class="s3">}</span><span class="s4">&quot;</span><span class="s1">)</span>

  <span class="s3">return </span><span class="s1">physical_mesh.transpose(</span>
      <span class="s1">*_TRANSPOSE_TRICKS[topology][mesh_shape_no_trivial_dims])</span>


<span class="s3">def </span><span class="s1">create_device_mesh(</span>
    <span class="s1">mesh_shape: Sequence[int]</span><span class="s3">,</span>
    <span class="s1">devices: Optional[Sequence[Any]] = </span><span class="s3">None, </span><span class="s1">*</span><span class="s3">,</span>
    <span class="s1">contiguous_submeshes: bool = </span><span class="s3">False</span><span class="s1">) -&gt; np.ndarray:</span>
  <span class="s2">&quot;&quot;&quot;Creates a performant device mesh for jax.sharding.Mesh. 
 
  Args: 
    mesh_shape: shape of logical mesh, ordered by increasing network-intensity 
      e.g. [replica, data, mdl] where mdl has the most network communication 
      requirements. 
    devices: optionally, the devices to construct a mesh for. Defaults to 
      jax.devices(). 
    contiguous_submeshes: if True, this function will attempt to create a mesh 
      where each process's local devices form a contiguous submesh. This is 
      required when passing host local inputs to `pjit`. A ValueError will be 
      raised if this function can't produce a suitable mesh. 
 
  Returns: 
    A np.ndarray of JAX devices with mesh_shape as its shape that can be fed 
    into jax.sharding.Mesh with good collective performance. 
  &quot;&quot;&quot;</span>
  <span class="s3">if </span><span class="s1">devices </span><span class="s3">is None</span><span class="s1">:</span>
    <span class="s1">devices = jax.devices()</span>
  <span class="s3">if </span><span class="s1">np.prod(mesh_shape) != len(devices):</span>
    <span class="s3">raise </span><span class="s1">ValueError(</span><span class="s4">f'Number of devices </span><span class="s3">{</span><span class="s1">len(devices)</span><span class="s3">} </span><span class="s4">must equal the product '</span>
                     <span class="s4">f'of mesh_shape </span><span class="s3">{</span><span class="s1">mesh_shape</span><span class="s3">}</span><span class="s4">'</span><span class="s1">)</span>
  <span class="s1">last_device = devices[-</span><span class="s5">1</span><span class="s1">]</span>
  <span class="s3">if </span><span class="s1">last_device.device_kind </span><span class="s3">in </span><span class="s1">(_TPU_V2</span><span class="s3">, </span><span class="s1">_TPU_V3):</span>
    <span class="s3">if </span><span class="s1">len(devices) == </span><span class="s5">8</span><span class="s1">:</span>
      <span class="s1">logger.info(</span><span class="s4">'Reordering mesh to physical ring order on single-tray TPU v2/v3.'</span><span class="s1">)</span>
      <span class="s1">device_mesh = np.asarray(devices)</span>
      <span class="s1">device_mesh = device_mesh[np.array(_TRAY_RING_ORDER)]</span>
      <span class="s1">device_mesh = device_mesh.reshape(mesh_shape)</span>
      <span class="s3">return </span><span class="s1">device_mesh</span>
    <span class="s3">elif </span><span class="s1">mesh_shape[-</span><span class="s5">1</span><span class="s1">] == </span><span class="s5">8</span><span class="s1">:</span>
      <span class="s1">device_mesh = np.asarray(devices).reshape(mesh_shape)</span>
      <span class="s1">logger.info(</span><span class="s4">'Reordering mesh to physical ring order on each TPU v2/v3 tray.'</span><span class="s1">)</span>
      <span class="s1">perm = np.array(_TRAY_RING_ORDER)</span>
      <span class="s1">device_mesh = device_mesh[...</span><span class="s3">, </span><span class="s1">perm]</span>
      <span class="s3">return </span><span class="s1">device_mesh</span>
    <span class="s3">else</span><span class="s1">:</span>
      <span class="s0"># TODO(skye): implement 2D mesh_shape logic here:</span>
      <span class="s0"># https://github.com/tensorflow/lingvo/blob/0df40cf604dfcd14e28f7087d73687a0bd2fe5c6/lingvo/core/gshard_utils.py#L187</span>
      <span class="s0"># (possibly replaces above mesh_shape[-1] == 8 case)</span>
      <span class="s3">return </span><span class="s1">np.asarray(devices).reshape(mesh_shape)</span>
  <span class="s3">elif </span><span class="s1">last_device.platform == </span><span class="s4">'tpu'</span><span class="s1">:</span>
    <span class="s1">physical_mesh = _get_physical_tpu_mesh(devices)</span>
    <span class="s3">if </span><span class="s1">contiguous_submeshes:</span>
      <span class="s1">physical_mesh = _transpose_trick(physical_mesh</span><span class="s3">, </span><span class="s1">mesh_shape)</span>
    <span class="s1">device_mesh</span><span class="s3">, </span><span class="s1">assignment = _create_device_mesh_for_nd_torus(</span>
        <span class="s1">physical_mesh</span><span class="s3">, </span><span class="s1">mesh_shape)</span>
    <span class="s1">logger.info(</span><span class="s4">'_create_device_mesh_for_nd_torus assignment: %s'</span><span class="s3">, </span><span class="s1">assignment)</span>
    <span class="s3">return </span><span class="s1">device_mesh</span>
  <span class="s3">else</span><span class="s1">:</span>
    <span class="s1">device_mesh = np.asarray(devices).reshape(mesh_shape)</span>
    <span class="s3">return </span><span class="s1">device_mesh</span>

<span class="s3">def </span><span class="s1">create_hybrid_device_mesh(mesh_shape: Sequence[int]</span><span class="s3">,</span>
                              <span class="s1">dcn_mesh_shape: Sequence[int]</span><span class="s3">,</span>
                              <span class="s1">devices: Optional[Sequence[Any]] = </span><span class="s3">None, </span><span class="s1">*</span><span class="s3">,</span>
                              <span class="s1">process_is_granule: bool = </span><span class="s3">False</span><span class="s1">) -&gt; np.ndarray:</span>
  <span class="s2">&quot;&quot;&quot;Creates a device mesh for hybrid (e.g., ICI and DCN) parallelism. 
 
  Args: 
    mesh_shape: shape of the logical mesh for the faster/inner network, ordered 
      by increasing network intensity, e.g. [replica, data, mdl] where mdl has 
      the most network communication requirements. 
    dcn_mesh_shape: shape of the logical mesh for the slower/outer network, 
      in the same order as mesh_shape. 
    devices: optionally, the devices to construct a mesh for. Defaults to 
      jax.devices(). 
    process_is_granule: if True, this function will treat processes as the units 
      of the slower/outer network. Otherwise it will look for slice_index 
      attributes on devices and use slices as the units. Enabling this is meant 
      as a fallback for platforms (e.g., GPU) that don't set slice_index. 
 
  Returns: 
    A np.ndarray of JAX devices with mesh_shape * dcn_mesh_shape as its shape 
    that can be fed into jax.sharding.Mesh for hybrid parallelism. 
  &quot;&quot;&quot;</span>
  <span class="s3">if </span><span class="s1">devices </span><span class="s3">is None</span><span class="s1">:</span>
    <span class="s1">devices = jax.devices()</span>
  <span class="s1">attr = </span><span class="s4">'process_index' </span><span class="s3">if </span><span class="s1">process_is_granule </span><span class="s3">else </span><span class="s4">'slice_index'</span>
  <span class="s3">assert </span><span class="s1">hasattr(devices[</span><span class="s5">0</span><span class="s1">]</span><span class="s3">, </span><span class="s1">attr)</span>
  <span class="s1">granule_id</span><span class="s3">, </span><span class="s1">granules = </span><span class="s5">0</span><span class="s3">, </span><span class="s1">[]</span>
  <span class="s3">while True</span><span class="s1">:</span>
    <span class="s1">granule = [dev </span><span class="s3">for </span><span class="s1">dev </span><span class="s3">in </span><span class="s1">devices </span><span class="s3">if </span><span class="s1">getattr(dev</span><span class="s3">, </span><span class="s1">attr) == granule_id]</span>
    <span class="s3">if </span><span class="s1">granule:</span>
      <span class="s1">granules.append(granule)</span>
      <span class="s1">granule_id += </span><span class="s5">1</span>
    <span class="s3">else</span><span class="s1">:</span>
      <span class="s3">break</span>
  <span class="s3">if </span><span class="s1">np.prod(dcn_mesh_shape) != len(granules):</span>
    <span class="s3">raise </span><span class="s1">ValueError(</span>
        <span class="s4">'Number of slices must equal the product of dcn_mesh_shape'</span><span class="s1">)</span>
  <span class="s1">per_granule_meshes = [create_device_mesh(mesh_shape</span><span class="s3">, </span><span class="s1">granule)</span>
                        <span class="s3">for </span><span class="s1">granule </span><span class="s3">in </span><span class="s1">granules]</span>
  <span class="s0"># TODO(jekbradbury): handle non-uniform DCN topologies</span>
  <span class="s1">granule_mesh = np.arange(len(granules)).reshape(dcn_mesh_shape)</span>
  <span class="s1">blocks = np.vectorize(</span>
    <span class="s3">lambda </span><span class="s1">i: per_granule_meshes[i]</span><span class="s3">, </span><span class="s1">otypes=[object])(granule_mesh)</span>
  <span class="s1">device_mesh = np.block(blocks.tolist())</span>
  <span class="s3">return </span><span class="s1">device_mesh</span>
</pre>
</body>
</html>