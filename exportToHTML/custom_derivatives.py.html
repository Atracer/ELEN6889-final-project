<html>
<head>
<title>custom_derivatives.py</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<style type="text/css">
.s0 { color: #808080;}
.s1 { color: #a9b7c6;}
.s2 { color: #cc7832;}
.s3 { color: #6a8759;}
.s4 { color: #629755; font-style: italic;}
.s5 { color: #6897bb;}
</style>
</head>
<body bgcolor="#2b2b2b">
<table CELLSPACING=0 CELLPADDING=5 COLS=1 WIDTH="100%" BGCOLOR="#606060" >
<tr><td><center>
<font face="Arial, Helvetica" color="#000000">
custom_derivatives.py</font>
</center></td></tr></table>
<pre><span class="s0"># Copyright 2020 The JAX Authors.</span>
<span class="s0">#</span>
<span class="s0"># Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);</span>
<span class="s0"># you may not use this file except in compliance with the License.</span>
<span class="s0"># You may obtain a copy of the License at</span>
<span class="s0">#</span>
<span class="s0">#     https://www.apache.org/licenses/LICENSE-2.0</span>
<span class="s0">#</span>
<span class="s0"># Unless required by applicable law or agreed to in writing, software</span>
<span class="s0"># distributed under the License is distributed on an &quot;AS IS&quot; BASIS,</span>
<span class="s0"># WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span>
<span class="s0"># See the License for the specific language governing permissions and</span>
<span class="s0"># limitations under the License.</span>

<span class="s2">from </span><span class="s1">functools </span><span class="s2">import </span><span class="s1">update_wrapper</span><span class="s2">, </span><span class="s1">reduce</span><span class="s2">, </span><span class="s1">partial</span>
<span class="s2">import </span><span class="s1">inspect</span>
<span class="s2">from </span><span class="s1">typing </span><span class="s2">import </span><span class="s1">(Callable</span><span class="s2">, </span><span class="s1">Generic</span><span class="s2">, </span><span class="s1">List</span><span class="s2">, </span><span class="s1">Optional</span><span class="s2">, </span><span class="s1">Sequence</span><span class="s2">, </span><span class="s1">Tuple</span><span class="s2">, </span><span class="s1">TypeVar</span><span class="s2">, </span><span class="s1">Any)</span>

<span class="s2">from </span><span class="s1">jax.custom_transpose </span><span class="s2">import </span><span class="s1">custom_transpose</span>
<span class="s2">from </span><span class="s1">jax.tree_util </span><span class="s2">import </span><span class="s1">(tree_flatten</span><span class="s2">, </span><span class="s1">tree_unflatten</span><span class="s2">, </span><span class="s1">tree_map</span><span class="s2">,</span>
                           <span class="s1">treedef_is_leaf</span><span class="s2">, </span><span class="s1">treedef_tuple</span><span class="s2">,</span>
                           <span class="s1">register_pytree_node_class</span><span class="s2">, </span><span class="s1">tree_leaves)</span>
<span class="s2">from </span><span class="s1">jax.errors </span><span class="s2">import </span><span class="s1">UnexpectedTracerError</span>
<span class="s2">from </span><span class="s1">jax.config </span><span class="s2">import </span><span class="s1">config</span>

<span class="s2">from </span><span class="s1">jax._src </span><span class="s2">import </span><span class="s1">core</span>
<span class="s2">from </span><span class="s1">jax._src </span><span class="s2">import </span><span class="s1">custom_api_util</span>
<span class="s2">from </span><span class="s1">jax._src </span><span class="s2">import </span><span class="s1">dtypes</span>
<span class="s2">from </span><span class="s1">jax._src </span><span class="s2">import </span><span class="s1">effects</span>
<span class="s2">from </span><span class="s1">jax._src </span><span class="s2">import </span><span class="s1">linear_util </span><span class="s2">as </span><span class="s1">lu</span>
<span class="s2">from </span><span class="s1">jax._src </span><span class="s2">import </span><span class="s1">traceback_util</span>
<span class="s2">from </span><span class="s1">jax._src.ad_util </span><span class="s2">import </span><span class="s1">(Zero</span><span class="s2">, </span><span class="s1">SymbolicZero</span><span class="s2">, </span><span class="s1">zeros_like_aval</span><span class="s2">,</span>
                              <span class="s1">stop_gradient_p)</span>
<span class="s2">from </span><span class="s1">jax._src.api_util </span><span class="s2">import </span><span class="s1">argnums_partial</span><span class="s2">, </span><span class="s1">flatten_fun_nokwargs</span>
<span class="s2">from </span><span class="s1">jax._src.core </span><span class="s2">import </span><span class="s1">raise_to_shaped</span>
<span class="s2">from </span><span class="s1">jax._src.interpreters </span><span class="s2">import </span><span class="s1">ad</span>
<span class="s2">from </span><span class="s1">jax._src.interpreters </span><span class="s2">import </span><span class="s1">batching</span>
<span class="s2">from </span><span class="s1">jax._src.interpreters </span><span class="s2">import </span><span class="s1">mlir</span>
<span class="s2">from </span><span class="s1">jax._src.interpreters </span><span class="s2">import </span><span class="s1">partial_eval </span><span class="s2">as </span><span class="s1">pe</span>
<span class="s2">from </span><span class="s1">jax._src.interpreters </span><span class="s2">import </span><span class="s1">xla</span>
<span class="s2">from </span><span class="s1">jax._src.interpreters.batching </span><span class="s2">import </span><span class="s1">not_mapped</span>
<span class="s2">from </span><span class="s1">jax._src.lax </span><span class="s2">import </span><span class="s1">lax</span>
<span class="s2">from </span><span class="s1">jax._src.util </span><span class="s2">import </span><span class="s1">cache</span><span class="s2">, </span><span class="s1">safe_zip</span><span class="s2">, </span><span class="s1">safe_map</span><span class="s2">, </span><span class="s1">split_list</span><span class="s2">, </span><span class="s1">Unhashable</span>


<span class="s1">traceback_util.register_exclusion(__file__)</span>

<span class="s1">map = safe_map</span>
<span class="s1">zip = safe_zip</span>

<span class="s1">allowed_effects: effects.EffectTypeSet = (</span>
    <span class="s1">effects.custom_derivatives_allowed_effects)</span>

<span class="s0">### util</span>

<span class="s2">def </span><span class="s1">_resolve_kwargs(fun</span><span class="s2">, </span><span class="s1">args</span><span class="s2">, </span><span class="s1">kwargs):</span>
  <span class="s1">ba = inspect.signature(fun).bind(*args</span><span class="s2">, </span><span class="s1">**kwargs)</span>
  <span class="s1">ba.apply_defaults()</span>
  <span class="s2">if </span><span class="s1">ba.kwargs:</span>
    <span class="s2">raise </span><span class="s1">TypeError(</span><span class="s3">&quot;keyword arguments could not be resolved to positions&quot;</span><span class="s1">)</span>
  <span class="s2">else</span><span class="s1">:</span>
    <span class="s2">return </span><span class="s1">ba.args</span>

<span class="s2">def </span><span class="s1">_initial_style_jaxpr(fun</span><span class="s2">, </span><span class="s1">in_avals):</span>
  <span class="s1">jaxpr</span><span class="s2">, </span><span class="s1">_</span><span class="s2">, </span><span class="s1">consts = pe.trace_to_jaxpr_dynamic(fun</span><span class="s2">, </span><span class="s1">in_avals)</span>
  <span class="s2">return </span><span class="s1">jaxpr</span><span class="s2">, </span><span class="s1">consts</span>

<span class="s2">def </span><span class="s1">_close_jaxpr(jaxpr):</span>
  <span class="s2">return </span><span class="s1">core.ClosedJaxpr(pe.convert_constvars_jaxpr(jaxpr)</span><span class="s2">, </span><span class="s1">())</span>

<span class="s2">def </span><span class="s1">_sum_tangents(_</span><span class="s2">, </span><span class="s1">x</span><span class="s2">, </span><span class="s1">*xs):</span>
  <span class="s2">return </span><span class="s1">reduce(ad.add_tangents</span><span class="s2">, </span><span class="s1">xs</span><span class="s2">, </span><span class="s1">x)</span>

<span class="s2">def </span><span class="s1">_zeros_like_pytree(x):</span>
  <span class="s2">return </span><span class="s1">tree_map(Zero.from_value</span><span class="s2">, </span><span class="s1">x)</span>

<span class="s1">@partial(partial</span><span class="s2">, </span><span class="s1">tree_map)</span>
<span class="s2">def </span><span class="s1">_stop_gradient(x):</span>
  <span class="s2">if </span><span class="s1">isinstance(x</span><span class="s2">, </span><span class="s1">core.Tracer):</span>
    <span class="s2">return </span><span class="s1">stop_gradient_p.bind(x)</span>
  <span class="s2">else</span><span class="s1">:</span>
    <span class="s2">return </span><span class="s1">x</span>

<span class="s0"># like the api_util.py function, but also grabs output avals for error checking</span>
<span class="s1">@lu.transformation_with_aux</span>
<span class="s2">def </span><span class="s1">_flatten_fun_nokwargs(in_tree</span><span class="s2">, </span><span class="s1">*args_flat):</span>
  <span class="s1">py_args = tree_unflatten(in_tree</span><span class="s2">, </span><span class="s1">args_flat)</span>
  <span class="s1">ans = </span><span class="s2">yield </span><span class="s1">py_args</span><span class="s2">, </span><span class="s1">{}</span>
  <span class="s1">ans_flat</span><span class="s2">, </span><span class="s1">ans_tree = tree_flatten(ans)</span>
  <span class="s1">ans_avals = [core.raise_to_shaped(core.get_aval(x)) </span><span class="s2">for </span><span class="s1">x </span><span class="s2">in </span><span class="s1">ans_flat]</span>
  <span class="s2">yield </span><span class="s1">ans_flat</span><span class="s2">, </span><span class="s1">(ans_tree</span><span class="s2">, </span><span class="s1">ans_avals)</span>


<span class="s0">### JVPs</span>
<span class="s1">ReturnValue = TypeVar(</span><span class="s3">'ReturnValue'</span><span class="s1">)</span>

<span class="s1">@custom_api_util.register_custom_decorator_type</span>
<span class="s2">class </span><span class="s1">custom_jvp(Generic[ReturnValue]):</span>
  <span class="s4">&quot;&quot;&quot;Set up a JAX-transformable function for a custom JVP rule definition. 
 
  This class is meant to be used as a function decorator. Instances are 
  callables that behave similarly to the underlying function to which the 
  decorator was applied, except when a differentiation transformation (like 
  :py:func:`jax.jvp` or :py:func:`jax.grad`) is applied, in which case a custom 
  user-supplied JVP rule function is used instead of tracing into and 
  performing automatic differentiation of the underlying function's 
  implementation. 
 
  There are two instance methods available for defining the custom JVP rule: 
  :py:func:`~jax.custom_jvp.defjvp` for defining a *single* custom JVP rule for 
  all the function's inputs, and for convenience 
  :py:func:`~jax.custom_jvp.defjvps`, which wraps 
  :py:func:`~jax.custom_jvp.defjvp`, and allows you to provide separate 
  definitions for the partial derivatives of the function w.r.t. each of its 
  arguments. 
 
  For example:: 
 
    @jax.custom_jvp 
    def f(x, y): 
      return jnp.sin(x) * y 
 
    @f.defjvp 
    def f_jvp(primals, tangents): 
      x, y = primals 
      x_dot, y_dot = tangents 
      primal_out = f(x, y) 
      tangent_out = jnp.cos(x) * x_dot * y + jnp.sin(x) * y_dot 
      return primal_out, tangent_out 
 
  For a more detailed introduction, see the tutorial_. 
 
  .. _tutorial: https://jax.readthedocs.io/en/latest/notebooks/Custom_derivative_rules_for_Python_code.html 
  &quot;&quot;&quot;</span>
  <span class="s1">fun: Callable[...</span><span class="s2">, </span><span class="s1">ReturnValue]</span>
  <span class="s1">nondiff_argnums: Tuple[int</span><span class="s2">, </span><span class="s1">...]</span>
  <span class="s1">jvp: Optional[Callable[...</span><span class="s2">, </span><span class="s1">Tuple[ReturnValue</span><span class="s2">, </span><span class="s1">ReturnValue]]] = </span><span class="s2">None</span>
  <span class="s1">symbolic_zeros: bool = </span><span class="s2">False</span>

  <span class="s2">def </span><span class="s1">__init__(self</span><span class="s2">,</span>
               <span class="s1">fun: Callable[...</span><span class="s2">, </span><span class="s1">ReturnValue]</span><span class="s2">,</span>
               <span class="s1">nondiff_argnums: Tuple[int</span><span class="s2">, </span><span class="s1">...] = ()</span><span class="s2">,</span>
               <span class="s1">):</span>
    <span class="s1">update_wrapper(self</span><span class="s2">, </span><span class="s1">fun)</span>
    <span class="s1">self.fun = fun</span>
    <span class="s1">self.nondiff_argnums = nondiff_argnums</span>

  <span class="s1">__getattr__ = custom_api_util.forward_attr</span>

  <span class="s2">def </span><span class="s1">defjvp(self</span><span class="s2">,</span>
             <span class="s1">jvp: Callable[...</span><span class="s2">, </span><span class="s1">Tuple[ReturnValue</span><span class="s2">, </span><span class="s1">ReturnValue]]</span><span class="s2">,</span>
             <span class="s1">symbolic_zeros: bool = </span><span class="s2">False,</span>
             <span class="s1">) -&gt; Callable[...</span><span class="s2">, </span><span class="s1">Tuple[ReturnValue</span><span class="s2">, </span><span class="s1">ReturnValue]]:</span>
    <span class="s4">&quot;&quot;&quot;Define a custom JVP rule for the function represented by this instance. 
 
    Args: 
      jvp: a Python callable representing the custom JVP rule. When there are no 
        ``nondiff_argnums``, the ``jvp`` function should accept two arguments, 
        where the first is a tuple of primal inputs and the second is a tuple of 
        tangent inputs. The lengths of both tuples are equal to the number of 
        parameters of the ``custom_jvp`` function. The ``jvp`` function should 
        produce as output a pair where the first element is the primal output 
        and the second element is the tangent output. Elements of the input and 
        output tuples may be arrays or any nested tuples/lists/dicts thereof. 
      symbolic_zeros: boolean, indicating whether the rule should be passed 
        objects representing static symbolic zeros in its tangent tuple 
        argument; otherwise, only standard JAX types (e.g. array-likes) are 
        passed. Setting this option to True allows a JVP rule to detect whether 
        certain inputs are not involved in differentiation, but at the cost of 
        needing special handling for these objects (which e.g. can't be passed 
        into jax.numpy functions). Default False. 
 
    Returns: 
      None. 
 
    Example:: 
 
      @jax.custom_jvp 
      def f(x, y): 
        return jnp.sin(x) * y 
 
      @f.defjvp 
      def f_jvp(primals, tangents): 
        x, y = primals 
        x_dot, y_dot = tangents 
        primal_out = f(x, y) 
        tangent_out = jnp.cos(x) * x_dot * y + jnp.sin(x) * y_dot 
        return primal_out, tangent_out 
    &quot;&quot;&quot;</span>
    <span class="s1">self.jvp = jvp</span>
    <span class="s1">self.symbolic_zeros = symbolic_zeros</span>
    <span class="s2">return </span><span class="s1">jvp</span>

  <span class="s2">def </span><span class="s1">defjvps(self</span><span class="s2">, </span><span class="s1">*jvps: Optional[Callable[...</span><span class="s2">, </span><span class="s1">ReturnValue]]):</span>
    <span class="s4">&quot;&quot;&quot;Convenience wrapper for defining JVPs for each argument separately. 
 
    This convenience wrapper cannot be used together with ``nondiff_argnums``. 
 
    Args: 
      *jvps: a sequence of functions, one for each positional argument of the 
        ``custom_jvp`` function. Each function takes as arguments the tangent 
        value for the corresponding primal input, the primal output, and the 
        primal inputs. See the example below. 
 
    Returns: 
      None. 
 
    Example:: 
 
      @jax.custom_jvp 
      def f(x, y): 
        return jnp.sin(x) * y 
 
      f.defjvps(lambda x_dot, primal_out, x, y: jnp.cos(x) * x_dot * y, 
                lambda y_dot, primal_out, x, y: jnp.sin(x) * y_dot) 
    &quot;&quot;&quot;</span>
    <span class="s2">if </span><span class="s1">self.nondiff_argnums:</span>
      <span class="s2">raise </span><span class="s1">TypeError(</span><span class="s3">&quot;Can't use ``defjvps`` with ``nondiff_argnums``.&quot;</span><span class="s1">)</span>

    <span class="s2">def </span><span class="s1">jvp(primals</span><span class="s2">, </span><span class="s1">tangents):</span>
      <span class="s1">primal_out = self(*primals)</span>
      <span class="s1">zeros = _zeros_like_pytree(primal_out)</span>
      <span class="s1">all_tangents_out = [jvp(t</span><span class="s2">, </span><span class="s1">primal_out</span><span class="s2">, </span><span class="s1">*primals) </span><span class="s2">if </span><span class="s1">jvp </span><span class="s2">else </span><span class="s1">zeros</span>
                          <span class="s2">for </span><span class="s1">t</span><span class="s2">, </span><span class="s1">jvp </span><span class="s2">in </span><span class="s1">zip(tangents</span><span class="s2">, </span><span class="s1">jvps)]</span>
      <span class="s1">tangent_out = tree_map(_sum_tangents</span><span class="s2">, </span><span class="s1">primal_out</span><span class="s2">, </span><span class="s1">*all_tangents_out)</span>
      <span class="s2">return </span><span class="s1">primal_out</span><span class="s2">, </span><span class="s1">tangent_out</span>

    <span class="s1">self.defjvp(jvp)</span>

  <span class="s1">@traceback_util.api_boundary</span>
  <span class="s2">def </span><span class="s1">__call__(self</span><span class="s2">, </span><span class="s1">*args: Any</span><span class="s2">, </span><span class="s1">**kwargs: Any) -&gt; ReturnValue:  </span><span class="s0"># pytype: disable=invalid-annotation</span>
    <span class="s1">primal_name = getattr(self.fun</span><span class="s2">, </span><span class="s3">'__name__'</span><span class="s2">, </span><span class="s1">str(self.fun))</span>
    <span class="s2">if not </span><span class="s1">self.jvp:</span>
      <span class="s1">msg = </span><span class="s3">f&quot;No JVP defined for custom_jvp function </span><span class="s2">{</span><span class="s1">primal_name</span><span class="s2">} </span><span class="s3">using defjvp.&quot;</span>
      <span class="s2">raise </span><span class="s1">AttributeError(msg)</span>
    <span class="s1">jvp_name    = getattr(self.jvp</span><span class="s2">, </span><span class="s3">'__name__'</span><span class="s2">, </span><span class="s1">str(self.jvp))</span>
    <span class="s1">args = _resolve_kwargs(self.fun</span><span class="s2">, </span><span class="s1">args</span><span class="s2">, </span><span class="s1">kwargs)</span>
    <span class="s2">if </span><span class="s1">self.nondiff_argnums:</span>
      <span class="s1">nondiff_argnums = set(self.nondiff_argnums)</span>
      <span class="s1">args = tuple(_stop_gradient(x) </span><span class="s2">if </span><span class="s1">i </span><span class="s2">in </span><span class="s1">nondiff_argnums </span><span class="s2">else </span><span class="s1">x</span>
                   <span class="s2">for </span><span class="s1">i</span><span class="s2">, </span><span class="s1">x </span><span class="s2">in </span><span class="s1">enumerate(args))</span>
      <span class="s1">diff_argnums = [i </span><span class="s2">for </span><span class="s1">i </span><span class="s2">in </span><span class="s1">range(len(args)) </span><span class="s2">if </span><span class="s1">i </span><span class="s2">not in </span><span class="s1">nondiff_argnums]</span>
      <span class="s1">f_</span><span class="s2">, </span><span class="s1">dyn_args = argnums_partial(lu.wrap_init(self.fun)</span><span class="s2">, </span><span class="s1">diff_argnums</span><span class="s2">, </span><span class="s1">args</span><span class="s2">,</span>
                                     <span class="s1">require_static_args_hashable=</span><span class="s2">False</span><span class="s1">)</span>
      <span class="s1">static_args = [args[i] </span><span class="s2">for </span><span class="s1">i </span><span class="s2">in </span><span class="s1">self.nondiff_argnums]</span>
      <span class="s1">jvp = _add_args(lu.wrap_init(self.jvp)</span><span class="s2">, </span><span class="s1">static_args)</span>
    <span class="s2">else</span><span class="s1">:</span>
      <span class="s1">f_</span><span class="s2">, </span><span class="s1">dyn_args = lu.wrap_init(self.fun)</span><span class="s2">, </span><span class="s1">args  </span><span class="s0"># type: ignore</span>
      <span class="s1">jvp = lu.wrap_init(self.jvp)</span>
    <span class="s1">args_flat</span><span class="s2">, </span><span class="s1">in_tree = tree_flatten(dyn_args)</span>
    <span class="s1">flat_fun</span><span class="s2">, </span><span class="s1">out_type1 = _flatten_fun_nokwargs(f_</span><span class="s2">, </span><span class="s1">in_tree)</span>
    <span class="s1">flat_jvp</span><span class="s2">, </span><span class="s1">out_type2 = _flatten_jvp(jvp</span><span class="s2">, </span><span class="s1">primal_name</span><span class="s2">, </span><span class="s1">jvp_name</span><span class="s2">, </span><span class="s1">in_tree</span><span class="s2">,</span>
                                       <span class="s1">out_type1)</span>
    <span class="s1">out_flat = custom_jvp_call_p.bind(flat_fun</span><span class="s2">, </span><span class="s1">flat_jvp</span><span class="s2">, </span><span class="s1">*args_flat</span><span class="s2">,</span>
                                      <span class="s1">symbolic_zeros=self.symbolic_zeros)</span>
    <span class="s1">_</span><span class="s2">, </span><span class="s1">(out_tree</span><span class="s2">, </span><span class="s1">_) = lu.merge_linear_aux(out_type1</span><span class="s2">, </span><span class="s1">out_type2)</span>
    <span class="s2">return </span><span class="s1">tree_unflatten(out_tree</span><span class="s2">, </span><span class="s1">out_flat)</span>

<span class="s2">def </span><span class="s1">_add_args(f</span><span class="s2">, </span><span class="s1">extra_args):</span>
  <span class="s2">return </span><span class="s1">_add_args_(f</span><span class="s2">, </span><span class="s1">tuple(Unhashable(arg) </span><span class="s2">for </span><span class="s1">arg </span><span class="s2">in </span><span class="s1">extra_args))</span>

<span class="s1">@lu.transformation</span>
<span class="s2">def </span><span class="s1">_add_args_(extra_args</span><span class="s2">, </span><span class="s1">*args</span><span class="s2">, </span><span class="s1">**kwargs):</span>
  <span class="s1">extra_args = tuple(arg.val </span><span class="s2">for </span><span class="s1">arg </span><span class="s2">in </span><span class="s1">extra_args)</span>
  <span class="s1">all_args = (extra_args + args)</span>
  <span class="s2">yield </span><span class="s1">(</span><span class="s2">yield </span><span class="s1">all_args</span><span class="s2">, </span><span class="s1">kwargs)</span>

<span class="s1">@lu.transformation_with_aux</span>
<span class="s2">def </span><span class="s1">_flatten_jvp(primal_name</span><span class="s2">, </span><span class="s1">jvp_name</span><span class="s2">, </span><span class="s1">in_tree</span><span class="s2">, </span><span class="s1">maybe_out_type</span><span class="s2">, </span><span class="s1">*args):</span>
  <span class="s1">primals_in</span><span class="s2">, </span><span class="s1">tangents_in = split_list(args</span><span class="s2">, </span><span class="s1">[len(args) // </span><span class="s5">2</span><span class="s1">])</span>
  <span class="s1">py_primals = tree_unflatten(in_tree</span><span class="s2">, </span><span class="s1">primals_in)</span>
  <span class="s1">py_tangents = tree_unflatten(in_tree</span><span class="s2">, </span><span class="s1">tangents_in)</span>
  <span class="s1">pair_out = </span><span class="s2">yield </span><span class="s1">(py_primals</span><span class="s2">, </span><span class="s1">py_tangents)</span><span class="s2">, </span><span class="s1">{}</span>
  <span class="s2">if not </span><span class="s1">isinstance(pair_out</span><span class="s2">, </span><span class="s1">(list</span><span class="s2">, </span><span class="s1">tuple)) </span><span class="s2">or </span><span class="s1">len(pair_out) != </span><span class="s5">2</span><span class="s1">:</span>
    <span class="s1">msg = (</span><span class="s3">f&quot;Custom JVP rule </span><span class="s2">{</span><span class="s1">jvp_name</span><span class="s2">} </span><span class="s3">for function </span><span class="s2">{</span><span class="s1">primal_name</span><span class="s2">} </span><span class="s3">&quot;</span>
           <span class="s3">&quot;must produce a pair (list or tuple of length two) representing &quot;</span>
           <span class="s3">f&quot;primal and tangent outputs, but got </span><span class="s2">{</span><span class="s1">pair_out</span><span class="s2">}</span><span class="s3">.&quot;</span><span class="s1">)</span>
    <span class="s2">raise </span><span class="s1">TypeError(msg)</span>
  <span class="s1">py_primals_out</span><span class="s2">, </span><span class="s1">py_tangents_out = pair_out</span>
  <span class="s1">primals_out</span><span class="s2">, </span><span class="s1">out_tree = tree_flatten(py_primals_out)</span>
  <span class="s1">tangents_out</span><span class="s2">, </span><span class="s1">out_tree2 = tree_flatten(py_tangents_out)</span>
  <span class="s1">primal_avals = [core.raise_to_shaped(core.get_aval(x)) </span><span class="s2">for </span><span class="s1">x </span><span class="s2">in </span><span class="s1">primals_out]</span>
  <span class="s2">if </span><span class="s1">out_tree != out_tree2:</span>
    <span class="s1">msg = (</span><span class="s3">f&quot;Custom JVP rule </span><span class="s2">{</span><span class="s1">jvp_name</span><span class="s2">} </span><span class="s3">for function </span><span class="s2">{</span><span class="s1">primal_name</span><span class="s2">} </span><span class="s3">must &quot;</span>
           <span class="s3">&quot;produce primal and tangent outputs with equal container (pytree) &quot;</span>
           <span class="s3">f&quot;structures, but got </span><span class="s2">{</span><span class="s1">out_tree</span><span class="s2">} </span><span class="s3">and </span><span class="s2">{</span><span class="s1">out_tree2</span><span class="s2">} </span><span class="s3">respectively.&quot;</span><span class="s1">)</span>
    <span class="s2">raise </span><span class="s1">TypeError(msg)</span>
  <span class="s0"># If the primal function already ran, check out_tree agreement.</span>
  <span class="s2">try</span><span class="s1">: out_type_ = maybe_out_type()</span>
  <span class="s2">except </span><span class="s1">lu.StoreException: out_type_ = </span><span class="s2">None</span>
  <span class="s2">if </span><span class="s1">out_type_ </span><span class="s2">is not None</span><span class="s1">:</span>
    <span class="s1">out_tree_</span><span class="s2">, </span><span class="s1">primal_avals_ = out_type_</span>
    <span class="s1">ty_tree  = tree_unflatten(out_tree </span><span class="s2">, </span><span class="s1">[a.str_short() </span><span class="s2">for </span><span class="s1">a </span><span class="s2">in </span><span class="s1">primal_avals])</span>
    <span class="s1">ty_tree_ = tree_unflatten(out_tree_</span><span class="s2">, </span><span class="s1">[a.str_short() </span><span class="s2">for </span><span class="s1">a </span><span class="s2">in </span><span class="s1">primal_avals_])</span>
    <span class="s2">if </span><span class="s1">out_tree_ != out_tree:</span>
      <span class="s1">m = (</span><span class="s3">f&quot;Custom JVP rule </span><span class="s2">{</span><span class="s1">jvp_name</span><span class="s2">} </span><span class="s3">for function </span><span class="s2">{</span><span class="s1">primal_name</span><span class="s2">} </span><span class="s3">must &quot;</span>
           <span class="s3">&quot;produce a pair (list or tuple of length two) &quot;</span>
           <span class="s3">&quot;where the first element represents the primal output &quot;</span>
           <span class="s3">&quot;(equal in value to the output of the custom_jvp-decorated function &quot;</span>
           <span class="s3">f&quot;</span><span class="s2">{</span><span class="s1">primal_name</span><span class="s2">}</span><span class="s3">, &quot;</span>
           <span class="s3">&quot;and in particular of the same container/pytree structure), but &quot;</span>
           <span class="s3">&quot;instead the JVP rule output's first element had container/pytree &quot;</span>
           <span class="s3">&quot;structure:</span><span class="s2">\n</span><span class="s3">&quot;</span>
           <span class="s3">f&quot;&quot;&quot;    </span><span class="s2">{</span><span class="s1">str(ty_tree ).replace(</span><span class="s3">&quot;'&quot;</span><span class="s2">, </span><span class="s3">&quot;&quot;</span><span class="s1">)</span><span class="s2">}\n</span><span class="s3">&quot;&quot;&quot;</span>
           <span class="s3">f&quot;while the custom_jvp-decorated function </span><span class="s2">{</span><span class="s1">primal_name</span><span class="s2">} </span><span class="s3">had output &quot;</span>
           <span class="s3">&quot;container/pytree structure:</span><span class="s2">\n</span><span class="s3">&quot;</span>
           <span class="s3">f&quot;&quot;&quot;    </span><span class="s2">{</span><span class="s1">str(ty_tree_).replace(</span><span class="s3">&quot;'&quot;</span><span class="s2">, </span><span class="s3">&quot;&quot;</span><span class="s1">)</span><span class="s2">}</span><span class="s3">.&quot;&quot;&quot;</span><span class="s1">)</span>
      <span class="s2">raise </span><span class="s1">TypeError(m)</span>
    <span class="s2">if not </span><span class="s1">all(map(core.typematch</span><span class="s2">, </span><span class="s1">primal_avals</span><span class="s2">, </span><span class="s1">primal_avals_)):</span>
      <span class="s1">m = (</span><span class="s3">f&quot;Custom JVP rule </span><span class="s2">{</span><span class="s1">jvp_name</span><span class="s2">} </span><span class="s3">for function </span><span class="s2">{</span><span class="s1">primal_name</span><span class="s2">} </span><span class="s3">must &quot;</span>
           <span class="s3">&quot;produce a pair (list or tuple of length two) &quot;</span>
           <span class="s3">&quot;where the first element represents the primal output &quot;</span>
           <span class="s3">&quot;(equal in value to the output of the custom_jvp-decorated function &quot;</span>
           <span class="s3">f&quot;</span><span class="s2">{</span><span class="s1">primal_name</span><span class="s2">}</span><span class="s3">, &quot;</span>
           <span class="s3">&quot;and in particular with leaves of the same shape/dtype), but &quot;</span>
           <span class="s3">&quot;instead the JVP rule output's first element had shapes/dtypes of:</span><span class="s2">\n</span><span class="s3">&quot;</span>
           <span class="s3">f&quot;&quot;&quot;    </span><span class="s2">{</span><span class="s1">str(ty_tree ).replace(</span><span class="s3">&quot;'&quot;</span><span class="s2">, </span><span class="s3">&quot;&quot;</span><span class="s1">)</span><span class="s2">}\n</span><span class="s3">&quot;&quot;&quot;</span>
           <span class="s3">f&quot;while the custom_jvp-decorated function </span><span class="s2">{</span><span class="s1">primal_name</span><span class="s2">} </span><span class="s3">had output &quot;</span>
           <span class="s3">&quot;shapes/dtypes of:</span><span class="s2">\n</span><span class="s3">&quot;</span>
           <span class="s3">f&quot;&quot;&quot;    </span><span class="s2">{</span><span class="s1">str(ty_tree_).replace(</span><span class="s3">&quot;'&quot;</span><span class="s2">, </span><span class="s3">&quot;&quot;</span><span class="s1">)</span><span class="s2">}</span><span class="s3">&quot;&quot;&quot;</span><span class="s1">)</span>
      <span class="s2">raise </span><span class="s1">TypeError(m)</span>
  <span class="s0"># TODO(mattjj): compare primals' tangent types to tangent objects' types</span>
  <span class="s1">primal_avals_out = [</span>
      <span class="s1">raise_to_shaped(core.get_aval(x)</span><span class="s2">, </span><span class="s1">weak_type=</span><span class="s2">False</span><span class="s1">).strip_named_shape()</span>
      <span class="s2">for </span><span class="s1">x </span><span class="s2">in </span><span class="s1">primals_out]</span>
  <span class="s1">tangent_avals_out = [</span>
      <span class="s1">raise_to_shaped(core.get_aval(t)</span><span class="s2">, </span><span class="s1">weak_type=</span><span class="s2">False</span><span class="s1">).strip_named_shape()</span>
      <span class="s2">if </span><span class="s1">type(t) </span><span class="s2">is not </span><span class="s1">SymbolicZero </span><span class="s2">else </span><span class="s1">t.aval </span><span class="s2">for </span><span class="s1">t </span><span class="s2">in </span><span class="s1">tangents_out]</span>
  <span class="s2">if </span><span class="s1">primal_avals_out != tangent_avals_out:</span>
    <span class="s2">if </span><span class="s1">len(primal_avals_out) == </span><span class="s5">1</span><span class="s1">:</span>
      <span class="s1">(av1</span><span class="s2">,</span><span class="s1">)</span><span class="s2">, </span><span class="s1">(av2</span><span class="s2">,</span><span class="s1">) = primal_avals_out</span><span class="s2">, </span><span class="s1">tangent_avals_out</span>
      <span class="s1">msg = (</span><span class="s3">&quot;Custom JVP rule must produce primal and tangent outputs with &quot;</span>
             <span class="s3">&quot;equal shapes and dtypes, but got {} and {} respectively.&quot;</span><span class="s1">)</span>
      <span class="s2">raise </span><span class="s1">TypeError(msg.format(av1.str_short()</span><span class="s2">, </span><span class="s1">av2.str_short()))</span>
    <span class="s2">else</span><span class="s1">:</span>
      <span class="s1">msg = (</span><span class="s3">&quot;Custom JVP rule must produce primal and tangent outputs with &quot;</span>
             <span class="s3">&quot;equal shapes and dtypes, but got:</span><span class="s2">\n</span><span class="s3">{}&quot;</span><span class="s1">)</span>
      <span class="s1">disagreements = (</span>
          <span class="s3">f&quot;  primal </span><span class="s2">{</span><span class="s1">av1.str_short()</span><span class="s2">} </span><span class="s3">for tangent </span><span class="s2">{</span><span class="s1">av2.str_short()</span><span class="s2">}</span><span class="s3">&quot;</span>
          <span class="s2">for </span><span class="s1">av1</span><span class="s2">, </span><span class="s1">av2 </span><span class="s2">in </span><span class="s1">zip(primal_avals_out</span><span class="s2">, </span><span class="s1">tangent_avals_out) </span><span class="s2">if </span><span class="s1">av1 != av2)</span>
      <span class="s2">raise </span><span class="s1">TypeError(msg.format(</span><span class="s3">'</span><span class="s2">\n</span><span class="s3">'</span><span class="s1">.join(disagreements)))</span>
  <span class="s2">yield </span><span class="s1">primals_out + tangents_out</span><span class="s2">, </span><span class="s1">(out_tree</span><span class="s2">, </span><span class="s1">primal_avals)</span>

<span class="s2">class </span><span class="s1">CustomJVPCallPrimitive(core.Primitive):</span>
  <span class="s1">multiple_results = </span><span class="s2">True</span>

  <span class="s2">def </span><span class="s1">bind(self</span><span class="s2">, </span><span class="s1">fun</span><span class="s2">, </span><span class="s1">jvp</span><span class="s2">, </span><span class="s1">*args</span><span class="s2">, </span><span class="s1">symbolic_zeros):</span>
    <span class="s1">args = map(core.full_lower</span><span class="s2">, </span><span class="s1">args)</span>
    <span class="s1">top_trace = core.find_top_trace(args)</span>
    <span class="s1">fun</span><span class="s2">, </span><span class="s1">env_trace_todo1 = process_env_traces(</span>
        <span class="s1">fun</span><span class="s2">, </span><span class="s1">self</span><span class="s2">, </span><span class="s1">top_trace </span><span class="s2">and </span><span class="s1">top_trace.level</span><span class="s2">, False</span><span class="s1">)</span>
    <span class="s1">jvp</span><span class="s2">, </span><span class="s1">env_trace_todo2 = process_env_traces(</span>
        <span class="s1">jvp</span><span class="s2">, </span><span class="s1">self</span><span class="s2">, </span><span class="s1">top_trace </span><span class="s2">and </span><span class="s1">top_trace.level</span><span class="s2">, True</span><span class="s1">)</span>
    <span class="s1">tracers = map(top_trace.full_raise</span><span class="s2">, </span><span class="s1">args)  </span><span class="s0"># type: ignore</span>
    <span class="s1">outs = top_trace.process_custom_jvp_call(self</span><span class="s2">, </span><span class="s1">fun</span><span class="s2">, </span><span class="s1">jvp</span><span class="s2">, </span><span class="s1">tracers</span><span class="s2">,  </span><span class="s0"># type: ignore</span>
                                             <span class="s1">symbolic_zeros=symbolic_zeros)  </span><span class="s0"># type: ignore</span>
    <span class="s1">_</span><span class="s2">, </span><span class="s1">env_trace_todo = lu.merge_linear_aux(env_trace_todo1</span><span class="s2">, </span><span class="s1">env_trace_todo2)</span>
    <span class="s2">return </span><span class="s1">core.apply_todos(env_trace_todo</span><span class="s2">, </span><span class="s1">map(core.full_lower</span><span class="s2">, </span><span class="s1">outs))</span>

  <span class="s2">def </span><span class="s1">impl(self</span><span class="s2">, </span><span class="s1">fun</span><span class="s2">, </span><span class="s1">_</span><span class="s2">, </span><span class="s1">*args):</span>
    <span class="s2">with </span><span class="s1">core.new_sublevel():</span>
      <span class="s2">return </span><span class="s1">fun.call_wrapped(*args)</span>

  <span class="s2">def </span><span class="s1">post_process(self</span><span class="s2">, </span><span class="s1">trace</span><span class="s2">, </span><span class="s1">out_tracers</span><span class="s2">, </span><span class="s1">jvp_was_run: bool):</span>
    <span class="s2">return </span><span class="s1">trace.post_process_custom_jvp_call(out_tracers</span><span class="s2">, </span><span class="s1">jvp_was_run)</span>

  <span class="s2">def </span><span class="s1">get_bind_params(self</span><span class="s2">, </span><span class="s1">params):</span>
    <span class="s1">new_params = dict(params)</span>
    <span class="s1">call_jaxpr = new_params.pop(</span><span class="s3">'call_jaxpr'</span><span class="s1">)</span>
    <span class="s1">num_consts = new_params.pop(</span><span class="s3">'num_consts'</span><span class="s1">)</span>
    <span class="s1">jvp_jaxpr_thunk = new_params.pop(</span><span class="s3">'jvp_jaxpr_thunk'</span><span class="s1">)</span>
    <span class="s1">fun = lu.wrap_init(core.jaxpr_as_fun(call_jaxpr))</span>

    <span class="s1">@lu.wrap_init</span>
    <span class="s2">def </span><span class="s1">jvp(*xs):</span>
      <span class="s1">jvp_jaxpr</span><span class="s2">, </span><span class="s1">jvp_consts = jvp_jaxpr_thunk()</span>
      <span class="s1">n</span><span class="s2">, </span><span class="s1">ragged = divmod(len(xs)</span><span class="s2">, </span><span class="s5">2</span><span class="s1">)</span>
      <span class="s2">assert not </span><span class="s1">ragged</span>
      <span class="s1">primals</span><span class="s2">, </span><span class="s1">tangents = xs[num_consts:n]</span><span class="s2">, </span><span class="s1">xs[n+num_consts:]</span>
      <span class="s2">return </span><span class="s1">core.eval_jaxpr(jvp_jaxpr</span><span class="s2">, </span><span class="s1">jvp_consts</span><span class="s2">, </span><span class="s1">*primals</span><span class="s2">, </span><span class="s1">*tangents)</span>

    <span class="s2">return </span><span class="s1">[fun</span><span class="s2">, </span><span class="s1">jvp]</span><span class="s2">, </span><span class="s1">new_params</span>

<span class="s1">@lu.transformation_with_aux</span>
<span class="s2">def </span><span class="s1">process_env_traces(primitive</span><span class="s2">, </span><span class="s1">level: int</span><span class="s2">, </span><span class="s1">jvp_was_run: bool</span><span class="s2">, </span><span class="s1">*args):</span>
  <span class="s1">outs = </span><span class="s2">yield </span><span class="s1">args</span><span class="s2">, </span><span class="s1">{}</span>
  <span class="s1">todo = []</span>
  <span class="s2">while True</span><span class="s1">:</span>
    <span class="s1">tracers = [x </span><span class="s2">for </span><span class="s1">x </span><span class="s2">in </span><span class="s1">outs </span><span class="s2">if </span><span class="s1">isinstance(x</span><span class="s2">, </span><span class="s1">core.Tracer)</span>
               <span class="s2">and </span><span class="s1">(level </span><span class="s2">is None or </span><span class="s1">x._trace.level &gt; level)]</span>
    <span class="s2">if </span><span class="s1">tracers:</span>
      <span class="s1">ans = max(tracers</span><span class="s2">, </span><span class="s1">key=</span><span class="s2">lambda </span><span class="s1">x: x._trace.level)</span>
    <span class="s2">else</span><span class="s1">:</span>
      <span class="s2">break</span>
    <span class="s1">trace = ans._trace.main.with_cur_sublevel()</span>
    <span class="s1">outs = map(trace.full_raise</span><span class="s2">, </span><span class="s1">outs)</span>
    <span class="s1">outs</span><span class="s2">, </span><span class="s1">cur_todo = primitive.post_process(trace</span><span class="s2">, </span><span class="s1">outs</span><span class="s2">, </span><span class="s1">jvp_was_run)</span>
    <span class="s1">todo.append(cur_todo)</span>
  <span class="s2">yield </span><span class="s1">outs</span><span class="s2">, </span><span class="s1">tuple(todo)  </span><span class="s0"># Ensure the aux output is immutable</span>


<span class="s1">allowed_effects.add_type(lax.InOutFeedEffect)</span>

<span class="s1">custom_jvp_call_p = CustomJVPCallPrimitive(</span><span class="s3">'custom_jvp_call'</span><span class="s1">)</span>

<span class="s2">def </span><span class="s1">_custom_jvp_call_typecheck(_</span><span class="s2">, </span><span class="s1">*in_avals</span><span class="s2">, </span><span class="s1">call_jaxpr</span><span class="s2">, </span><span class="s1">jvp_jaxpr_thunk</span><span class="s2">,</span>
                               <span class="s1">num_consts</span><span class="s2">, </span><span class="s1">symbolic_zeros):</span>
  <span class="s0"># TODO(mattjj): could do more checking here...</span>
  <span class="s2">del </span><span class="s1">in_avals</span><span class="s2">, </span><span class="s1">jvp_jaxpr_thunk</span><span class="s2">, </span><span class="s1">num_consts</span>
  <span class="s1">disallowed_effects = allowed_effects.filter_not_in(call_jaxpr.effects)</span>
  <span class="s2">if </span><span class="s1">disallowed_effects:</span>
    <span class="s2">raise </span><span class="s1">NotImplementedError(</span>
        <span class="s3">f'Effects not supported in `custom_jvp`: </span><span class="s2">{</span><span class="s1">disallowed_effects</span><span class="s2">}</span><span class="s3">'</span><span class="s1">)</span>
  <span class="s2">return </span><span class="s1">call_jaxpr.out_avals</span><span class="s2">, </span><span class="s1">call_jaxpr.effects</span>
<span class="s1">core.custom_typechecks[custom_jvp_call_p] = _custom_jvp_call_typecheck</span>

<span class="s2">def </span><span class="s1">_custom_jvp_call_mlir_translation(ctx</span><span class="s2">, </span><span class="s1">*args</span><span class="s2">, </span><span class="s1">call_jaxpr</span><span class="s2">, </span><span class="s1">jvp_jaxpr_thunk</span><span class="s2">,</span>
                                      <span class="s1">num_consts</span><span class="s2">, </span><span class="s1">symbolic_zeros):</span>
  <span class="s2">del </span><span class="s1">jvp_jaxpr_thunk</span><span class="s2">, </span><span class="s1">num_consts</span><span class="s2">, </span><span class="s1">symbolic_zeros</span>
  <span class="s1">args_ = map(mlir.wrap_singleton_ir_values</span><span class="s2">, </span><span class="s1">args)</span>
  <span class="s1">consts = mlir._ir_consts(call_jaxpr.consts)</span>
  <span class="s1">out</span><span class="s2">, </span><span class="s1">tokens = mlir.jaxpr_subcomp(ctx.module_context</span><span class="s2">, </span><span class="s1">call_jaxpr.jaxpr</span><span class="s2">,</span>
                                   <span class="s1">ctx.tokens_in</span><span class="s2">, </span><span class="s1">consts</span><span class="s2">, </span><span class="s1">*args_</span><span class="s2">,</span>
                                   <span class="s1">dim_var_values=ctx.dim_var_values)</span>
  <span class="s1">ctx.set_tokens_out(tokens)</span>
  <span class="s2">return </span><span class="s1">out</span>
<span class="s1">mlir.register_lowering(custom_jvp_call_p</span><span class="s2">, </span><span class="s1">_custom_jvp_call_mlir_translation)</span>

<span class="s0"># If a (multi)linear function is defined with a custom jvp, then</span>
<span class="s0"># custom_jvp_call_ can appear in jaxprs to be transposed. Since it's already</span>
<span class="s0"># been linearized, we can drop the jvp rule.</span>
<span class="s2">def </span><span class="s1">_custom_jvp_call_transpose(params</span><span class="s2">, </span><span class="s1">jaxpr</span><span class="s2">, </span><span class="s1">args</span><span class="s2">, </span><span class="s1">ct</span><span class="s2">, </span><span class="s1">_</span><span class="s2">, </span><span class="s1">reduce_axes):</span>
  <span class="s2">del </span><span class="s1">params</span>
  <span class="s2">return </span><span class="s1">ad.backward_pass(jaxpr.jaxpr</span><span class="s2">, </span><span class="s1">reduce_axes</span><span class="s2">, None, </span><span class="s1">jaxpr.consts</span><span class="s2">, </span><span class="s1">args</span><span class="s2">, </span><span class="s1">ct)</span>
<span class="s1">ad.primitive_transposes[custom_jvp_call_p] = _custom_jvp_call_transpose</span>


<span class="s0">### VJPs</span>

<span class="s1">@custom_api_util.register_custom_decorator_type</span>
<span class="s2">class </span><span class="s1">custom_vjp(Generic[ReturnValue]):</span>
  <span class="s4">&quot;&quot;&quot;Set up a JAX-transformable function for a custom VJP rule definition. 
 
  This class is meant to be used as a function decorator. Instances are 
  callables that behave similarly to the underlying function to which the 
  decorator was applied, except when a reverse-mode differentiation 
  transformation (like :py:func:`jax.grad`) is applied, in which case a custom 
  user-supplied VJP rule function is used instead of tracing into and performing 
  automatic differentiation of the underlying function's implementation. There 
  is a single instance method, :py:func:`~jax.custom_vjp.defvjp`, which may be 
  used to define the custom VJP rule. 
 
  This decorator precludes the use of forward-mode automatic differentiation. 
 
  For example:: 
 
    @jax.custom_vjp 
    def f(x, y): 
      return jnp.sin(x) * y 
 
    def f_fwd(x, y): 
      return f(x, y), (jnp.cos(x), jnp.sin(x), y) 
 
    def f_bwd(res, g): 
      cos_x, sin_x, y = res 
      return (cos_x * g * y, sin_x * g) 
 
    f.defvjp(f_fwd, f_bwd) 
 
  For a more detailed introduction, see the tutorial_. 
 
  .. _tutorial: https://jax.readthedocs.io/en/latest/notebooks/Custom_derivative_rules_for_Python_code.html 
  &quot;&quot;&quot;</span>

  <span class="s2">def </span><span class="s1">__init__(self</span><span class="s2">,</span>
               <span class="s1">fun: Callable[...</span><span class="s2">, </span><span class="s1">ReturnValue]</span><span class="s2">,</span>
               <span class="s1">nondiff_argnums: Tuple[int</span><span class="s2">, </span><span class="s1">...] = ()):</span>
    <span class="s1">update_wrapper(self</span><span class="s2">, </span><span class="s1">fun)</span>
    <span class="s1">self.fun = fun</span>
    <span class="s1">self.nondiff_argnums = nondiff_argnums</span>
    <span class="s1">self.fwd: Optional[Callable[...</span><span class="s2">, </span><span class="s1">Tuple[ReturnValue</span><span class="s2">, </span><span class="s1">Any]]] = </span><span class="s2">None</span>
    <span class="s1">self.bwd: Optional[Callable[...</span><span class="s2">, </span><span class="s1">Tuple[Any</span><span class="s2">, </span><span class="s1">...]]] = </span><span class="s2">None</span>

  <span class="s1">__getattr__ = custom_api_util.forward_attr</span>

  <span class="s2">def </span><span class="s1">defvjp(self</span><span class="s2">,</span>
             <span class="s1">fwd: Callable[...</span><span class="s2">, </span><span class="s1">Tuple[ReturnValue</span><span class="s2">, </span><span class="s1">Any]]</span><span class="s2">,</span>
             <span class="s1">bwd: Callable[...</span><span class="s2">, </span><span class="s1">Tuple[Any</span><span class="s2">, </span><span class="s1">...]]) -&gt; </span><span class="s2">None</span><span class="s1">:</span>
    <span class="s4">&quot;&quot;&quot;Define a custom VJP rule for the function represented by this instance. 
 
    Args: 
      fwd: a Python callable representing the forward pass of the custom VJP 
        rule. When there are no ``nondiff_argnums``, the ``fwd`` function has 
        the same input signature as the underlying primal function. It should 
        return as output a pair, where the first element represents the primal 
        output and the second element represents any &quot;residual&quot; values to store 
        from the forward pass for use on the backward pass by the function 
        ``bwd``. Input arguments and elements of the output pair may be arrays 
        or nested tuples/lists/dicts thereof. 
      bwd: a Python callable representing the backward pass of the custom VJP 
        rule. When there are no ``nondiff_argnums``, the ``bwd`` function takes 
        two arguments, where the first is the &quot;residual&quot; values produced on the 
        forward pass by ``fwd``, and the second is the output cotangent with the 
        same structure as the primal function output. The output of ``bwd`` must 
        be a tuple of length equal to the number of arguments of the primal 
        function, and the tuple elements may be arrays or nested 
        tuples/lists/dicts thereof so as to match the structure of the primal 
        input arguments. 
 
    Returns: 
      None. 
 
    Example:: 
 
      @jax.custom_vjp 
      def f(x, y): 
        return jnp.sin(x) * y 
 
      def f_fwd(x, y): 
        return f(x, y), (jnp.cos(x), jnp.sin(x), y) 
 
      def f_bwd(res, g): 
        cos_x, sin_x, y = res 
        return (cos_x * g * y, sin_x * g) 
 
      f.defvjp(f_fwd, f_bwd) 
    &quot;&quot;&quot;</span>
    <span class="s1">self.fwd = fwd</span>
    <span class="s1">self.bwd = bwd</span>

  <span class="s1">@traceback_util.api_boundary</span>
  <span class="s2">def </span><span class="s1">__call__(self</span><span class="s2">, </span><span class="s1">*args: Any</span><span class="s2">, </span><span class="s1">**kwargs: Any) -&gt; ReturnValue:  </span><span class="s0"># pytype: disable=invalid-annotation</span>
    <span class="s1">primal_name = getattr(self.fun</span><span class="s2">, </span><span class="s3">'__name__'</span><span class="s2">, </span><span class="s1">str(self.fun))</span>
    <span class="s2">if not </span><span class="s1">self.fwd </span><span class="s2">or not </span><span class="s1">self.bwd:</span>
      <span class="s1">msg = </span><span class="s3">f&quot;No VJP defined for custom_vjp function </span><span class="s2">{</span><span class="s1">primal_name</span><span class="s2">} </span><span class="s3">using defvjp.&quot;</span>
      <span class="s2">raise </span><span class="s1">AttributeError(msg)</span>
    <span class="s1">fwd_name    = getattr(self.fwd</span><span class="s2">, </span><span class="s3">'__name__'</span><span class="s2">, </span><span class="s1">str(self.fwd))</span>
    <span class="s1">args = _resolve_kwargs(self.fun</span><span class="s2">, </span><span class="s1">args</span><span class="s2">, </span><span class="s1">kwargs)</span>
    <span class="s2">if </span><span class="s1">config.jax_enable_custom_vjp_by_custom_transpose:</span>
      <span class="s2">if </span><span class="s1">self.nondiff_argnums:</span>
        <span class="s2">raise </span><span class="s1">NotImplementedError(</span>
            <span class="s3">'nondiff_argnums not implemented for new custom_vjp'</span><span class="s1">)</span>
      <span class="s2">return </span><span class="s1">custom_vjp_by_custom_transpose(self.fun</span><span class="s2">, </span><span class="s1">self.fwd</span><span class="s2">, </span><span class="s1">self.bwd)(*args)</span>
    <span class="s2">else</span><span class="s1">:</span>
      <span class="s2">if </span><span class="s1">self.nondiff_argnums:</span>
        <span class="s2">for </span><span class="s1">i </span><span class="s2">in </span><span class="s1">self.nondiff_argnums: _check_for_tracers(args[i])</span>
        <span class="s1">nondiff_argnums = set(self.nondiff_argnums)</span>
        <span class="s1">dyn_argnums = [i </span><span class="s2">for </span><span class="s1">i </span><span class="s2">in </span><span class="s1">range(len(args)) </span><span class="s2">if </span><span class="s1">i </span><span class="s2">not in </span><span class="s1">nondiff_argnums]</span>
        <span class="s1">f_</span><span class="s2">, </span><span class="s1">dyn_args = argnums_partial(lu.wrap_init(self.fun)</span><span class="s2">, </span><span class="s1">dyn_argnums</span><span class="s2">,</span>
                                       <span class="s1">args</span><span class="s2">, </span><span class="s1">require_static_args_hashable=</span><span class="s2">False</span><span class="s1">)</span>
        <span class="s1">static_args = [args[i] </span><span class="s2">for </span><span class="s1">i </span><span class="s2">in </span><span class="s1">self.nondiff_argnums]</span>
        <span class="s1">fwd</span><span class="s2">, </span><span class="s1">_ = argnums_partial(lu.wrap_init(self.fwd)</span><span class="s2">, </span><span class="s1">dyn_argnums</span><span class="s2">, </span><span class="s1">args</span><span class="s2">,</span>
                                 <span class="s1">require_static_args_hashable=</span><span class="s2">False</span><span class="s1">)</span>
        <span class="s1">bwd = _add_args(lu.wrap_init(self.bwd)</span><span class="s2">, </span><span class="s1">static_args)</span>
      <span class="s2">else</span><span class="s1">:</span>
        <span class="s1">f_</span><span class="s2">, </span><span class="s1">dyn_args = lu.wrap_init(self.fun)</span><span class="s2">, </span><span class="s1">args</span>
        <span class="s1">fwd</span><span class="s2">, </span><span class="s1">bwd = lu.wrap_init(self.fwd)</span><span class="s2">, </span><span class="s1">lu.wrap_init(self.bwd)</span>
      <span class="s1">args_flat</span><span class="s2">, </span><span class="s1">in_tree = tree_flatten(dyn_args)</span>
      <span class="s1">in_avals = [core.raise_to_shaped(core.get_aval(x)) </span><span class="s2">for </span><span class="s1">x </span><span class="s2">in </span><span class="s1">args_flat]</span>
      <span class="s1">flat_fun</span><span class="s2">, </span><span class="s1">out_type = _flatten_fun_nokwargs(f_</span><span class="s2">, </span><span class="s1">in_tree)</span>
      <span class="s1">flat_fwd</span><span class="s2">, </span><span class="s1">out_trees = _flatten_fwd(fwd</span><span class="s2">, </span><span class="s1">primal_name</span><span class="s2">, </span><span class="s1">fwd_name</span><span class="s2">, </span><span class="s1">in_tree</span><span class="s2">,</span>
                                         <span class="s1">out_type)</span>
      <span class="s1">flat_bwd = _flatten_bwd(bwd</span><span class="s2">, </span><span class="s1">in_tree</span><span class="s2">, </span><span class="s1">in_avals</span><span class="s2">, </span><span class="s1">out_trees).call_wrapped</span>
      <span class="s1">out_flat = custom_vjp_call_p.bind(flat_fun</span><span class="s2">, </span><span class="s1">flat_fwd</span><span class="s2">, </span><span class="s1">flat_bwd</span><span class="s2">,</span>
                                        <span class="s1">*args_flat</span><span class="s2">, </span><span class="s1">out_trees=out_trees)</span>
      <span class="s1">_</span><span class="s2">, </span><span class="s1">(out_tree</span><span class="s2">, </span><span class="s1">_) = lu.merge_linear_aux(out_type</span><span class="s2">, </span><span class="s1">out_trees)</span>
      <span class="s2">return </span><span class="s1">tree_unflatten(out_tree</span><span class="s2">, </span><span class="s1">out_flat)</span>

<span class="s2">def </span><span class="s1">_check_for_tracers(x):</span>
  <span class="s2">for </span><span class="s1">leaf </span><span class="s2">in </span><span class="s1">tree_leaves(x):</span>
    <span class="s2">if </span><span class="s1">isinstance(x</span><span class="s2">, </span><span class="s1">core.Tracer):</span>
      <span class="s1">msg = (</span><span class="s3">&quot;Found a JAX Tracer object passed as an argument to a custom_vjp &quot;</span>
            <span class="s3">&quot;function in a position indicated by nondiff_argnums as &quot;</span>
            <span class="s3">&quot;non-differentiable. Tracers cannot be passed as non-differentiable &quot;</span>
            <span class="s3">&quot;arguments to custom_vjp functions; instead, nondiff_argnums should &quot;</span>
            <span class="s3">&quot;only be used for arguments that can't be or contain JAX tracers, &quot;</span>
            <span class="s3">&quot;e.g. function-valued arguments. In particular, array-valued &quot;</span>
            <span class="s3">&quot;arguments should typically not be indicated as nondiff_argnums.&quot;</span><span class="s1">)</span>
      <span class="s2">raise </span><span class="s1">UnexpectedTracerError(msg)</span>

<span class="s1">@lu.transformation_with_aux</span>
<span class="s2">def </span><span class="s1">_flatten_fwd(primal_name</span><span class="s2">, </span><span class="s1">fwd_name</span><span class="s2">, </span><span class="s1">in_tree</span><span class="s2">, </span><span class="s1">maybe_out_type</span><span class="s2">, </span><span class="s1">*args):</span>
  <span class="s1">py_args = tree_unflatten(in_tree</span><span class="s2">, </span><span class="s1">args)</span>
  <span class="s1">pair_out = </span><span class="s2">yield </span><span class="s1">py_args</span><span class="s2">, </span><span class="s1">{}</span>
  <span class="s2">if not </span><span class="s1">isinstance(pair_out</span><span class="s2">, </span><span class="s1">(list</span><span class="s2">, </span><span class="s1">tuple)) </span><span class="s2">or </span><span class="s1">len(pair_out) != </span><span class="s5">2</span><span class="s1">:</span>
    <span class="s1">msg = (</span><span class="s3">f&quot;Custom VJP fwd rule </span><span class="s2">{</span><span class="s1">fwd_name</span><span class="s2">} </span><span class="s3">for function </span><span class="s2">{</span><span class="s1">primal_name</span><span class="s2">} </span><span class="s3">&quot;</span>
           <span class="s3">&quot;must produce a pair (list or tuple of length two) where the first &quot;</span>
           <span class="s3">&quot;element represents the primal output (equal to those of the &quot;</span>
           <span class="s3">f&quot;custom_vjp-decorated function </span><span class="s2">{</span><span class="s1">primal_name</span><span class="s2">}</span><span class="s3">) and the &quot;</span>
           <span class="s3">&quot;second element represents residuals (i.e. values stored from the &quot;</span>
           <span class="s3">&quot;forward pass for use on the backward pass), but &quot;</span>
           <span class="s3">f&quot;instead of a pair the fwd rule </span><span class="s2">{</span><span class="s1">fwd_name</span><span class="s2">} </span><span class="s3">produced </span><span class="s2">{</span><span class="s1">pair_out</span><span class="s2">}</span><span class="s3">.&quot;</span><span class="s1">)</span>
    <span class="s2">raise </span><span class="s1">TypeError(msg)</span>
  <span class="s1">py_primals_out</span><span class="s2">, </span><span class="s1">res = pair_out</span>
  <span class="s1">primals_out</span><span class="s2">, </span><span class="s1">out_tree = tree_flatten(py_primals_out)</span>
  <span class="s1">res</span><span class="s2">, </span><span class="s1">res_tree = tree_flatten(res)</span>
  <span class="s1">primal_avals = [core.raise_to_shaped(core.get_aval(x)) </span><span class="s2">for </span><span class="s1">x </span><span class="s2">in </span><span class="s1">primals_out]</span>
  <span class="s0"># If the primal function already ran, check out_tree agreement.</span>
  <span class="s2">try</span><span class="s1">: out_type_ = maybe_out_type()</span>
  <span class="s2">except </span><span class="s1">lu.StoreException: out_type_ = </span><span class="s2">None</span>
  <span class="s2">if </span><span class="s1">out_type_ </span><span class="s2">is not None</span><span class="s1">:</span>
    <span class="s1">out_tree_</span><span class="s2">, </span><span class="s1">primal_avals_ = out_type_</span>
    <span class="s1">ty_tree  = tree_unflatten(out_tree </span><span class="s2">, </span><span class="s1">[a.str_short() </span><span class="s2">for </span><span class="s1">a </span><span class="s2">in </span><span class="s1">primal_avals])</span>
    <span class="s1">ty_tree_ = tree_unflatten(out_tree_</span><span class="s2">, </span><span class="s1">[a.str_short() </span><span class="s2">for </span><span class="s1">a </span><span class="s2">in </span><span class="s1">primal_avals_])</span>
    <span class="s2">if </span><span class="s1">out_tree_ != out_tree:</span>
      <span class="s1">m = (</span><span class="s3">f&quot;Custom VJP fwd rule </span><span class="s2">{</span><span class="s1">fwd_name</span><span class="s2">} </span><span class="s3">for function </span><span class="s2">{</span><span class="s1">primal_name</span><span class="s2">} </span><span class="s3">&quot;</span>
           <span class="s3">&quot;must produce a pair (list or tuple of length two) where the first &quot;</span>
           <span class="s3">&quot;element represents the primal output &quot;</span>
           <span class="s3">&quot;(equal to the output of the custom_vjp-decorated function &quot;</span>
           <span class="s3">f&quot;</span><span class="s2">{</span><span class="s1">primal_name</span><span class="s2">}</span><span class="s3">) and the &quot;</span>
           <span class="s3">&quot;second element represents residuals (i.e. values stored from the &quot;</span>
           <span class="s3">&quot;forward pass for use on the backward pass), but &quot;</span>
           <span class="s3">&quot;instead the fwd rule output's first element had container/pytree &quot;</span>
           <span class="s3">&quot;structure:</span><span class="s2">\n</span><span class="s3">&quot;</span>
           <span class="s3">f&quot;&quot;&quot;    </span><span class="s2">{</span><span class="s1">str(ty_tree ).replace(</span><span class="s3">&quot;'&quot;</span><span class="s2">, </span><span class="s3">&quot;&quot;</span><span class="s1">)</span><span class="s2">}\n</span><span class="s3">&quot;&quot;&quot;</span>
           <span class="s3">f&quot;while the custom_vjp-decorated function </span><span class="s2">{</span><span class="s1">primal_name</span><span class="s2">} </span><span class="s3">had output &quot;</span>
           <span class="s3">&quot;container/pytree structure:</span><span class="s2">\n</span><span class="s3">&quot;</span>
           <span class="s3">f&quot;&quot;&quot;    </span><span class="s2">{</span><span class="s1">str(ty_tree_).replace(</span><span class="s3">&quot;'&quot;</span><span class="s2">, </span><span class="s3">&quot;&quot;</span><span class="s1">)</span><span class="s2">}</span><span class="s3">.&quot;&quot;&quot;</span><span class="s1">)</span>
      <span class="s2">raise </span><span class="s1">TypeError(m)</span>
    <span class="s2">if not </span><span class="s1">all(map(core.typematch</span><span class="s2">, </span><span class="s1">primal_avals</span><span class="s2">, </span><span class="s1">primal_avals_)):</span>
      <span class="s1">m = (</span><span class="s3">f&quot;Custom VJP fwd rule </span><span class="s2">{</span><span class="s1">fwd_name</span><span class="s2">} </span><span class="s3">for function </span><span class="s2">{</span><span class="s1">primal_name</span><span class="s2">} </span><span class="s3">must &quot;</span>
           <span class="s3">&quot;produce a pair (list or tuple of length two) &quot;</span>
           <span class="s3">&quot;where the first element represents the primal output &quot;</span>
           <span class="s3">&quot;(equal to the output of the custom_vjp-decorated function &quot;</span>
           <span class="s3">f&quot;</span><span class="s2">{</span><span class="s1">primal_name</span><span class="s2">}</span><span class="s3">) and the second element represents residuals &quot;</span>
           <span class="s3">&quot;(i.e. values stored from the forward pass for use on the &quot;</span>
           <span class="s3">&quot;backward pass), but &quot;</span>
           <span class="s3">&quot;instead the fwd rule output's first element had shapes/dtypes of:</span><span class="s2">\n</span><span class="s3">&quot;</span>
           <span class="s3">f&quot;&quot;&quot;    </span><span class="s2">{</span><span class="s1">str(ty_tree ).replace(</span><span class="s3">&quot;'&quot;</span><span class="s2">, </span><span class="s3">&quot;&quot;</span><span class="s1">)</span><span class="s2">}\n</span><span class="s3">&quot;&quot;&quot;</span>
           <span class="s3">f&quot;while the custom_vjp-decorated function </span><span class="s2">{</span><span class="s1">primal_name</span><span class="s2">} </span><span class="s3">had output &quot;</span>
           <span class="s3">&quot;shapes/dtypes of:</span><span class="s2">\n</span><span class="s3">&quot;</span>
           <span class="s3">f&quot;&quot;&quot;    </span><span class="s2">{</span><span class="s1">str(ty_tree_).replace(</span><span class="s3">&quot;'&quot;</span><span class="s2">, </span><span class="s3">&quot;&quot;</span><span class="s1">)</span><span class="s2">}</span><span class="s3">&quot;&quot;&quot;</span><span class="s1">)</span>
      <span class="s2">raise </span><span class="s1">TypeError(m)</span>
  <span class="s2">yield </span><span class="s1">(*res</span><span class="s2">, </span><span class="s1">*primals_out)</span><span class="s2">, </span><span class="s1">(out_tree</span><span class="s2">, </span><span class="s1">res_tree)</span>

<span class="s1">@lu.transformation</span>
<span class="s2">def </span><span class="s1">_flatten_bwd(in_tree</span><span class="s2">, </span><span class="s1">in_avals</span><span class="s2">, </span><span class="s1">out_trees</span><span class="s2">, </span><span class="s1">*args):</span>
  <span class="s1">out_tree</span><span class="s2">, </span><span class="s1">res_tree = out_trees()</span>
  <span class="s2">assert </span><span class="s1">len(args) == res_tree.num_leaves + out_tree.num_leaves</span>
  <span class="s1">res</span><span class="s2">, </span><span class="s1">cts_out = split_list(args</span><span class="s2">, </span><span class="s1">[res_tree.num_leaves])</span>
  <span class="s1">py_res = tree_unflatten(res_tree</span><span class="s2">, </span><span class="s1">res)</span>
  <span class="s1">py_cts_out = tree_unflatten(out_tree</span><span class="s2">, </span><span class="s1">cts_out)</span>
  <span class="s1">py_cts_in = </span><span class="s2">yield </span><span class="s1">(py_res</span><span class="s2">, </span><span class="s1">py_cts_out)</span><span class="s2">, </span><span class="s1">{}</span>
  <span class="s0"># For each None in py_cts_in, indicating an argument for which the rule</span>
  <span class="s0"># produces no cotangent, we replace it with a pytree with the structure of the</span>
  <span class="s0"># corresponding subtree of in_tree and with leaves of a non-pytree sentinel</span>
  <span class="s0"># object, to be replaced with Nones in the final returned result.</span>
  <span class="s1">zero = object()  </span><span class="s0"># non-pytree sentinel to replace Nones in py_cts_in</span>
  <span class="s1">dummy = tree_unflatten(in_tree</span><span class="s2">, </span><span class="s1">[object()] * in_tree.num_leaves)</span>
  <span class="s1">cts_in_flat = []</span>
  <span class="s1">append = </span><span class="s2">lambda </span><span class="s1">x</span><span class="s2">, </span><span class="s1">d: cts_in_flat.extend([x] * len(tree_flatten(d)[</span><span class="s5">0</span><span class="s1">])) </span><span class="s2">or </span><span class="s1">x</span>
  <span class="s2">try</span><span class="s1">:</span>
    <span class="s2">if not </span><span class="s1">isinstance(py_cts_in</span><span class="s2">, </span><span class="s1">tuple):</span>
      <span class="s2">raise </span><span class="s1">ValueError</span>
    <span class="s1">tree_map(append</span><span class="s2">,</span>
             <span class="s1">tuple(zero </span><span class="s2">if </span><span class="s1">ct </span><span class="s2">is None else </span><span class="s1">ct </span><span class="s2">for </span><span class="s1">ct </span><span class="s2">in </span><span class="s1">py_cts_in)</span><span class="s2">, </span><span class="s1">dummy)</span>
  <span class="s2">except </span><span class="s1">ValueError:</span>
    <span class="s1">_</span><span class="s2">, </span><span class="s1">in_tree2 = tree_flatten(py_cts_in)</span>
    <span class="s1">msg = (</span><span class="s3">&quot;Custom VJP rule must produce an output with the same container &quot;</span>
           <span class="s3">&quot;(pytree) structure as the args tuple of the primal function, &quot;</span>
           <span class="s3">&quot;and in particular must produce a tuple of length equal to the &quot;</span>
           <span class="s3">&quot;number of arguments to the primal function, but got VJP output &quot;</span>
           <span class="s3">&quot;structure {} for primal input structure {}.&quot;</span><span class="s1">)</span>
    <span class="s2">raise </span><span class="s1">TypeError(msg.format(in_tree2</span><span class="s2">, </span><span class="s1">in_tree)) </span><span class="s2">from None</span>
  <span class="s0"># Ignore any None cotangents, and any corresponding to inputs for which the</span>
  <span class="s0"># type doesn't equal the tangent type (i.e. float0s)</span>
  <span class="s0"># TODO(mattjj): change this to check if tangent type represents 0dim vspace</span>
  <span class="s2">yield </span><span class="s1">[Zero(a.at_least_vspace()) </span><span class="s2">if </span><span class="s1">ct </span><span class="s2">is </span><span class="s1">zero </span><span class="s2">or </span><span class="s1">a != a.at_least_vspace()</span>
         <span class="s2">else </span><span class="s1">ct </span><span class="s2">for </span><span class="s1">a</span><span class="s2">, </span><span class="s1">ct </span><span class="s2">in </span><span class="s1">zip(in_avals</span><span class="s2">, </span><span class="s1">cts_in_flat)]</span>


<span class="s2">class </span><span class="s1">CustomVJPCallPrimitive(core.CallPrimitive):</span>
  <span class="s1">initial_style: core.Primitive</span>

  <span class="s2">def </span><span class="s1">bind(self</span><span class="s2">, </span><span class="s1">fun</span><span class="s2">, </span><span class="s1">fwd</span><span class="s2">, </span><span class="s1">bwd</span><span class="s2">, </span><span class="s1">*args</span><span class="s2">, </span><span class="s1">out_trees):</span>
    <span class="s1">args = map(core.full_lower</span><span class="s2">, </span><span class="s1">args)</span>
    <span class="s1">top_trace = core.find_top_trace(args)</span>
    <span class="s1">fun</span><span class="s2">, </span><span class="s1">env_trace_todo1 = process_env_traces(</span>
        <span class="s1">fun</span><span class="s2">, </span><span class="s1">self</span><span class="s2">, </span><span class="s1">top_trace </span><span class="s2">and </span><span class="s1">top_trace.level</span><span class="s2">, False</span><span class="s1">)</span>
    <span class="s1">fwd</span><span class="s2">, </span><span class="s1">env_trace_todo2 = process_env_traces_fwd(</span>
      <span class="s1">fwd</span><span class="s2">, </span><span class="s1">top_trace </span><span class="s2">and </span><span class="s1">top_trace.level</span><span class="s2">, </span><span class="s1">out_trees)</span>
    <span class="s1">tracers = map(top_trace.full_raise</span><span class="s2">, </span><span class="s1">args)  </span><span class="s0"># type: ignore</span>
    <span class="s1">bwd_ = </span><span class="s2">lambda </span><span class="s1">*args: bwd(*args)</span>
    <span class="s1">outs = top_trace.process_custom_vjp_call(self</span><span class="s2">, </span><span class="s1">fun</span><span class="s2">, </span><span class="s1">fwd</span><span class="s2">, </span><span class="s1">bwd_</span><span class="s2">, </span><span class="s1">tracers</span><span class="s2">,</span>
                                             <span class="s1">out_trees=out_trees)</span>
    <span class="s1">fst</span><span class="s2">, </span><span class="s1">env_trace_todo = lu.merge_linear_aux(env_trace_todo1</span><span class="s2">, </span><span class="s1">env_trace_todo2)</span>
    <span class="s2">if </span><span class="s1">fst:</span>
      <span class="s2">return </span><span class="s1">core.apply_todos(env_trace_todo</span><span class="s2">, </span><span class="s1">map(core.full_lower</span><span class="s2">, </span><span class="s1">outs))</span>
    <span class="s2">else</span><span class="s1">:</span>
      <span class="s1">env_trace_todo</span><span class="s2">, </span><span class="s1">bwd_transform = env_trace_todo</span>
      <span class="s1">bwd = _apply_bwd_transform(bwd_transform</span><span class="s2">, </span><span class="s1">bwd)</span>
      <span class="s2">return </span><span class="s1">core.apply_todos(env_trace_todo</span><span class="s2">, </span><span class="s1">map(core.full_lower</span><span class="s2">, </span><span class="s1">outs))</span>

  <span class="s2">def </span><span class="s1">impl(self</span><span class="s2">, </span><span class="s1">fun</span><span class="s2">, </span><span class="s1">fwd</span><span class="s2">, </span><span class="s1">bwd</span><span class="s2">, </span><span class="s1">*args</span><span class="s2">, </span><span class="s1">out_trees):</span>
    <span class="s2">del </span><span class="s1">fwd</span><span class="s2">, </span><span class="s1">bwd</span><span class="s2">, </span><span class="s1">out_trees</span>
    <span class="s2">with </span><span class="s1">core.new_sublevel():</span>
      <span class="s2">return </span><span class="s1">fun.call_wrapped(*args)</span>

  <span class="s2">def </span><span class="s1">post_process(self</span><span class="s2">, </span><span class="s1">trace</span><span class="s2">, </span><span class="s1">out_tracers</span><span class="s2">, </span><span class="s1">params):</span>
    <span class="s2">return </span><span class="s1">trace.post_process_custom_vjp_call(out_tracers</span><span class="s2">, </span><span class="s1">params)</span>
<span class="s1">custom_vjp_call_p = CustomVJPCallPrimitive(</span><span class="s3">'custom_vjp_call'</span><span class="s1">)</span>

<span class="s1">@lu.transformation_with_aux</span>
<span class="s2">def </span><span class="s1">process_env_traces_fwd(level: int</span><span class="s2">, </span><span class="s1">out_trees</span><span class="s2">, </span><span class="s1">*args):</span>
  <span class="s1">outs = </span><span class="s2">yield </span><span class="s1">args</span><span class="s2">, </span><span class="s1">{}</span>
  <span class="s1">todo = []</span>
  <span class="s1">bwd_transforms = []</span>
  <span class="s2">while True</span><span class="s1">:</span>
    <span class="s1">tracers = [x </span><span class="s2">for </span><span class="s1">x </span><span class="s2">in </span><span class="s1">outs </span><span class="s2">if </span><span class="s1">isinstance(x</span><span class="s2">, </span><span class="s1">core.Tracer)</span>
               <span class="s2">and </span><span class="s1">(level </span><span class="s2">is None or </span><span class="s1">x._trace.level &gt; level)]</span>
    <span class="s2">if </span><span class="s1">tracers:</span>
      <span class="s1">ans = max(tracers</span><span class="s2">, </span><span class="s1">key=</span><span class="s2">lambda </span><span class="s1">x: x._trace.level)</span>
    <span class="s2">else</span><span class="s1">:</span>
      <span class="s2">break</span>
    <span class="s1">trace = ans._trace.main.with_cur_sublevel()</span>
    <span class="s1">outs = map(trace.full_raise</span><span class="s2">, </span><span class="s1">outs)</span>
    <span class="s1">outs</span><span class="s2">, </span><span class="s1">cur_todo</span><span class="s2">, </span><span class="s1">bwd_xform = trace.post_process_custom_vjp_call_fwd(outs</span><span class="s2">, </span><span class="s1">out_trees)</span>
    <span class="s1">todo.append(cur_todo)</span>
    <span class="s1">bwd_transforms.append(bwd_xform)</span>
  <span class="s2">yield </span><span class="s1">outs</span><span class="s2">, </span><span class="s1">(tuple(todo)</span><span class="s2">, </span><span class="s1">tuple(bwd_transforms))</span>


<span class="s2">def </span><span class="s1">_apply_bwd_transform(todos</span><span class="s2">, </span><span class="s1">bwd):</span>
  <span class="s1">todos_list = list(todos)</span>
  <span class="s2">while </span><span class="s1">todos_list:</span>
    <span class="s1">bwd = todos_list.pop()(bwd)</span>
  <span class="s2">return </span><span class="s1">bwd</span>

<span class="s2">def </span><span class="s1">_custom_vjp_call_jaxpr_impl(*args</span><span class="s2">, </span><span class="s1">fun_jaxpr</span><span class="s2">, </span><span class="s1">**_):</span>
  <span class="s2">return </span><span class="s1">core.jaxpr_as_fun(fun_jaxpr)(*args)</span>

<span class="s2">def </span><span class="s1">_custom_vjp_call_jaxpr_abstract_eval(*_</span><span class="s2">, </span><span class="s1">fun_jaxpr</span><span class="s2">, </span><span class="s1">**__):</span>
  <span class="s1">disallowed_effects = allowed_effects.filter_not_in(fun_jaxpr.effects)</span>
  <span class="s2">if </span><span class="s1">disallowed_effects:</span>
    <span class="s2">raise </span><span class="s1">NotImplementedError(</span>
        <span class="s3">f'Effects not supported in `custom_vjp`: </span><span class="s2">{</span><span class="s1">disallowed_effects</span><span class="s2">}</span><span class="s3">'</span><span class="s1">)</span>
  <span class="s2">return </span><span class="s1">fun_jaxpr.out_avals</span><span class="s2">, </span><span class="s1">fun_jaxpr.effects</span>

<span class="s1">custom_vjp_call_jaxpr_p = core.AxisPrimitive(</span><span class="s3">'custom_vjp_call_jaxpr'</span><span class="s1">)</span>
<span class="s1">custom_vjp_call_jaxpr_p.multiple_results = </span><span class="s2">True</span>
<span class="s1">custom_vjp_call_jaxpr_p.def_impl(_custom_vjp_call_jaxpr_impl)</span>
<span class="s1">custom_vjp_call_jaxpr_p.def_effectful_abstract_eval(_custom_vjp_call_jaxpr_abstract_eval)</span>
<span class="s1">CustomVJPCallPrimitive.initial_style = custom_vjp_call_jaxpr_p</span>

<span class="s1">mlir.register_lowering(custom_vjp_call_jaxpr_p</span><span class="s2">, </span><span class="s1">mlir.lower_fun(</span>
    <span class="s1">_custom_vjp_call_jaxpr_impl</span><span class="s2">, </span><span class="s1">multiple_results=</span><span class="s2">True</span><span class="s1">))</span>


<span class="s2">def </span><span class="s1">_custom_vjp_call_jaxpr_jvp(</span>
    <span class="s1">primals</span><span class="s2">, </span><span class="s1">tangents</span><span class="s2">, </span><span class="s1">*</span><span class="s2">, </span><span class="s1">fun_jaxpr: core.ClosedJaxpr</span><span class="s2">,</span>
    <span class="s1">fwd_jaxpr_thunk: Callable[[]</span><span class="s2">, </span><span class="s1">Tuple[core.Jaxpr</span><span class="s2">, </span><span class="s1">Sequence[Any]]]</span><span class="s2">,</span>
    <span class="s1">bwd: Callable</span><span class="s2">, </span><span class="s1">out_trees: Callable</span><span class="s2">, </span><span class="s1">num_consts: int):</span>
  <span class="s1">_</span><span class="s2">, </span><span class="s1">args = split_list(primals</span><span class="s2">, </span><span class="s1">[num_consts])</span>
  <span class="s1">consts_dot</span><span class="s2">, </span><span class="s1">args_dot = split_list(tangents</span><span class="s2">, </span><span class="s1">[num_consts])</span>
  <span class="s2">if </span><span class="s1">any(type(t) </span><span class="s2">is not </span><span class="s1">Zero </span><span class="s2">for </span><span class="s1">t </span><span class="s2">in </span><span class="s1">consts_dot):</span>
    <span class="s2">raise </span><span class="s1">ad.CustomVJPException()</span>
  <span class="s1">fwd_jaxpr</span><span class="s2">, </span><span class="s1">fwd_consts = fwd_jaxpr_thunk()  </span><span class="s0"># consts can be tracers!</span>
  <span class="s1">out_tree</span><span class="s2">, </span><span class="s1">res_tree = out_trees()</span>
  <span class="s1">args_dot = map(ad.instantiate_zeros</span><span class="s2">, </span><span class="s1">args_dot)</span>
  <span class="s0"># Cast float0 to zeros with the primal dtype because custom vjp rules don't</span>
  <span class="s0"># currently handle float0s</span>
  <span class="s1">args_dot = map(ad.replace_float0s</span><span class="s2">, </span><span class="s1">args</span><span class="s2">, </span><span class="s1">args_dot)</span>
  <span class="s1">res_and_primals_out = core.eval_jaxpr(fwd_jaxpr</span><span class="s2">, </span><span class="s1">fwd_consts</span><span class="s2">, </span><span class="s1">*args)</span>
  <span class="s1">res</span><span class="s2">, </span><span class="s1">primals_out = split_list(res_and_primals_out</span><span class="s2">, </span><span class="s1">[res_tree.num_leaves])</span>
  <span class="s1">avals_out = [raise_to_shaped(core.get_aval(x)) </span><span class="s2">for </span><span class="s1">x </span><span class="s2">in </span><span class="s1">primals_out]</span>
  <span class="s1">tangents_out = ad.custom_lin_p.bind(</span>
      <span class="s1">*res</span><span class="s2">, </span><span class="s1">*args_dot</span><span class="s2">, </span><span class="s1">num_res=res_tree.num_leaves</span><span class="s2">, </span><span class="s1">bwd=bwd</span><span class="s2">, </span><span class="s1">out_avals=avals_out)</span>
  <span class="s1">tangents_out = map(ad.recast_to_float0</span><span class="s2">, </span><span class="s1">primals_out</span><span class="s2">, </span><span class="s1">tangents_out)</span>
  <span class="s2">return </span><span class="s1">primals_out</span><span class="s2">, </span><span class="s1">tangents_out</span>
<span class="s1">ad.primitive_jvps[custom_vjp_call_jaxpr_p] = _custom_vjp_call_jaxpr_jvp</span>

<span class="s2">def </span><span class="s1">_custom_vjp_call_jaxpr_vmap(spmd_axis_name</span><span class="s2">,</span>
    <span class="s1">axis_size</span><span class="s2">, </span><span class="s1">axis_name</span><span class="s2">, </span><span class="s1">main_type</span><span class="s2">, </span><span class="s1">args</span><span class="s2">, </span><span class="s1">in_dims</span><span class="s2">, </span><span class="s1">*</span><span class="s2">, </span><span class="s1">fun_jaxpr: core.ClosedJaxpr</span><span class="s2">,</span>
    <span class="s1">fwd_jaxpr_thunk: Callable[[]</span><span class="s2">, </span><span class="s1">Tuple[core.Jaxpr</span><span class="s2">, </span><span class="s1">Sequence[Any]]]</span><span class="s2">,</span>
    <span class="s1">bwd: Callable</span><span class="s2">, </span><span class="s1">out_trees: Callable</span><span class="s2">, </span><span class="s1">num_consts: int):</span>
  <span class="s1">args = [batching.moveaxis(x</span><span class="s2">, </span><span class="s1">d</span><span class="s2">, </span><span class="s5">0</span><span class="s1">) </span><span class="s2">if </span><span class="s1">d </span><span class="s2">is not </span><span class="s1">not_mapped </span><span class="s2">and </span><span class="s1">d != </span><span class="s5">0</span>
          <span class="s2">else </span><span class="s1">x </span><span class="s2">for </span><span class="s1">x</span><span class="s2">, </span><span class="s1">d </span><span class="s2">in </span><span class="s1">zip(args</span><span class="s2">, </span><span class="s1">in_dims)]</span>

  <span class="s1">in_batched = [d </span><span class="s2">is not </span><span class="s1">not_mapped </span><span class="s2">for </span><span class="s1">d </span><span class="s2">in </span><span class="s1">in_dims]</span>
  <span class="s1">_</span><span class="s2">, </span><span class="s1">args_batched = split_list(in_batched</span><span class="s2">, </span><span class="s1">[num_consts])</span>
  <span class="s1">batched_fun_jaxpr</span><span class="s2">, </span><span class="s1">out_batched = batching.batch_jaxpr(</span>
      <span class="s1">fun_jaxpr</span><span class="s2">, </span><span class="s1">axis_size</span><span class="s2">, </span><span class="s1">in_batched</span><span class="s2">, False, </span><span class="s1">axis_name</span><span class="s2">, </span><span class="s1">spmd_axis_name</span><span class="s2">,</span>
      <span class="s1">main_type)</span>
  <span class="s1">out_dims1 = [</span><span class="s5">0 </span><span class="s2">if </span><span class="s1">b </span><span class="s2">else </span><span class="s1">not_mapped </span><span class="s2">for </span><span class="s1">b </span><span class="s2">in </span><span class="s1">out_batched]</span>
  <span class="s1">out_dims2 = []</span>

  <span class="s1">@pe._memoize</span>
  <span class="s2">def </span><span class="s1">batched_fwd_jaxpr_thunk():</span>
    <span class="s1">fwd_jaxpr = core.ClosedJaxpr(*fwd_jaxpr_thunk())  </span><span class="s0"># consts can be tracers</span>
    <span class="s1">batched_fwd_jaxpr</span><span class="s2">, </span><span class="s1">out_batched = batching.batch_jaxpr(</span>
        <span class="s1">fwd_jaxpr</span><span class="s2">, </span><span class="s1">axis_size</span><span class="s2">, </span><span class="s1">args_batched</span><span class="s2">, False, </span><span class="s1">axis_name</span><span class="s2">, </span><span class="s1">spmd_axis_name</span><span class="s2">,</span>
        <span class="s1">main_type)</span>
    <span class="s1">out_dims2.append([</span><span class="s5">0 </span><span class="s2">if </span><span class="s1">b </span><span class="s2">else </span><span class="s1">not_mapped </span><span class="s2">for </span><span class="s1">b </span><span class="s2">in </span><span class="s1">out_batched])</span>
    <span class="s2">return </span><span class="s1">batched_fwd_jaxpr.jaxpr</span><span class="s2">, </span><span class="s1">batched_fwd_jaxpr.consts</span>

  <span class="s1">fwd_args_batched = [</span><span class="s5">0 </span><span class="s2">if </span><span class="s1">b </span><span class="s2">else </span><span class="s1">not_mapped </span><span class="s2">for </span><span class="s1">b </span><span class="s2">in </span><span class="s1">args_batched]</span>
  <span class="s1">fwd_out_dims = </span><span class="s2">lambda</span><span class="s1">: out_dims2[</span><span class="s5">0</span><span class="s1">]</span>
  <span class="s1">batched_bwd = batching.batch_custom_vjp_bwd(bwd</span><span class="s2">, </span><span class="s1">axis_name</span><span class="s2">, </span><span class="s1">axis_size</span><span class="s2">, </span><span class="s1">fwd_out_dims</span><span class="s2">,</span>
                                              <span class="s1">fwd_args_batched</span><span class="s2">, </span><span class="s1">main_type</span><span class="s2">, </span><span class="s1">spmd_axis_name)</span>

  <span class="s1">batched_outs = custom_vjp_call_jaxpr_p.bind(</span>
      <span class="s1">*args</span><span class="s2">, </span><span class="s1">fun_jaxpr=batched_fun_jaxpr</span><span class="s2">,</span>
      <span class="s1">fwd_jaxpr_thunk=batched_fwd_jaxpr_thunk</span><span class="s2">, </span><span class="s1">bwd=batched_bwd</span><span class="s2">,</span>
      <span class="s1">out_trees=out_trees</span><span class="s2">, </span><span class="s1">num_consts=num_consts)</span>
  <span class="s1">out_dims = out_dims2[</span><span class="s5">0</span><span class="s1">] </span><span class="s2">if </span><span class="s1">out_dims2 </span><span class="s2">else </span><span class="s1">out_dims1</span>
  <span class="s2">return </span><span class="s1">batched_outs</span><span class="s2">, </span><span class="s1">out_dims</span>
<span class="s1">batching.spmd_axis_primitive_batchers[custom_vjp_call_jaxpr_p] = _custom_vjp_call_jaxpr_vmap</span>
<span class="s1">batching.axis_primitive_batchers[custom_vjp_call_jaxpr_p] = partial(_custom_vjp_call_jaxpr_vmap</span><span class="s2">, None</span><span class="s1">)</span>

<span class="s1">xla.register_initial_style_primitive(custom_vjp_call_jaxpr_p)</span>

<span class="s1">batching.primitive_batchers[ad.custom_lin_p] = ad.raise_custom_vjp_error_on_jvp</span>
<span class="s1">mlir.register_lowering(ad.custom_lin_p</span><span class="s2">, </span><span class="s1">ad.raise_custom_vjp_error_on_jvp)</span>


<span class="s2">def </span><span class="s1">custom_gradient(fun):</span>
  <span class="s4">&quot;&quot;&quot;Convenience function for defining custom VJP rules (aka custom gradients). 
 
  While the canonical way to define custom VJP rules is via ``jax.custom_vjp``, 
  the ``custom_gradient`` convenience wrapper follows TensorFlow's 
  ``tf.custom_gradient`` API. The difference here is that ``custom_gradient`` 
  can be used as a decorator on one function that returns both the primal value 
  (representing the output of the mathematical function to be differentiated) 
  and the VJP (gradient) function. See 
  https://www.tensorflow.org/api_docs/python/tf/custom_gradient. 
 
  If the mathematical function to be differentiated has Haskell-like signature 
  ``a -&gt; b``, then the Python callable ``fun`` should have the signature 
  ``a -&gt; (b, CT b --o CT a)`` where we use ``CT x`` to denote a cotangent type 
  for ``x`` and the ``--o`` arrow to denote a linear function. See the example 
  below. That is, ``fun`` should return a pair where the first element 
  represents the value of the mathematical function to be differentiated and the 
  second element is a function to be called on the backward pass of reverse-mode 
  automatic differentiation (i.e. the &quot;custom gradient&quot; function). 
 
  The function returned as the second element of the output of ``fun`` can close 
  over intermediate values computed when evaluating the function to be 
  differentiated. That is, use lexical closure to share work between the forward 
  pass and the backward pass of reverse-mode automatic differentiation. However, 
  it cannot perform Python control flow which depends on the values of the 
  closed-over intermediate values or its cotangent arguments; if the function 
  includes such control flow, an error is raised. 
 
  Args: 
    fun: a Python callable specifying both the mathematical function to be 
      differentiated and its reverse-mode differentiation rule. It should return 
      a pair consisting of an output value and a Python callable that represents 
      the custom gradient function. 
 
  Returns: 
    A Python callable that accepts the same arguments as ``fun`` and returns the 
    output value specified by the first element of ``fun``'s output pair. 
 
  For example: 
 
  &gt;&gt;&gt; @jax.custom_gradient 
  ... def f(x): 
  ...   return x ** 2, lambda g: (g * x,) 
  ... 
  &gt;&gt;&gt; print(f(3.)) 
  9.0 
  &gt;&gt;&gt; print(jax.grad(f)(3.)) 
  3.0 
 
  An example with a function on two arguments, so that the VJP function must 
  return a tuple of length two: 
 
  &gt;&gt;&gt; @jax.custom_gradient 
  ... def f(x, y): 
  ...   return x * y, lambda g: (g * y, g * x) 
  ... 
  &gt;&gt;&gt; print(f(3., 4.)) 
  12.0 
  &gt;&gt;&gt; print(jax.grad(f, argnums=(0, 1))(3., 4.)) 
  (Array(4., dtype=float32, weak_type=True), Array(3., dtype=float32, weak_type=True)) 
  &quot;&quot;&quot;</span>
  <span class="s1">@custom_vjp</span>
  <span class="s2">def </span><span class="s1">wrapped_fun(*args</span><span class="s2">, </span><span class="s1">**kwargs):</span>
    <span class="s1">ans</span><span class="s2">, </span><span class="s1">_ = fun(*args</span><span class="s2">, </span><span class="s1">**kwargs)</span>
    <span class="s2">return </span><span class="s1">ans</span>

  <span class="s2">def </span><span class="s1">fwd(*args</span><span class="s2">, </span><span class="s1">**kwargs):</span>
    <span class="s1">ans</span><span class="s2">, </span><span class="s1">rule = fun(*args</span><span class="s2">, </span><span class="s1">**kwargs)</span>
    <span class="s1">ans_flat</span><span class="s2">, </span><span class="s1">out_tree = tree_flatten((ans</span><span class="s2">,</span><span class="s1">))</span>
    <span class="s1">rule</span><span class="s2">, </span><span class="s1">in_tree = flatten_fun_nokwargs(lu.wrap_init(rule)</span><span class="s2">, </span><span class="s1">out_tree)</span>
    <span class="s1">ans_avals = [core.get_aval(x).at_least_vspace() </span><span class="s2">for </span><span class="s1">x </span><span class="s2">in </span><span class="s1">ans_flat]</span>
    <span class="s1">jaxpr</span><span class="s2">, </span><span class="s1">_</span><span class="s2">, </span><span class="s1">consts = pe.trace_to_jaxpr_dynamic(rule</span><span class="s2">, </span><span class="s1">ans_avals)</span>
    <span class="s2">return </span><span class="s1">ans</span><span class="s2">, </span><span class="s1">Residuals(jaxpr</span><span class="s2">, </span><span class="s1">in_tree()</span><span class="s2">, </span><span class="s1">out_tree</span><span class="s2">, </span><span class="s1">consts)</span>

  <span class="s2">def </span><span class="s1">bwd(res</span><span class="s2">, </span><span class="s1">cts):</span>
    <span class="s1">jaxpr</span><span class="s2">, </span><span class="s1">in_tree</span><span class="s2">, </span><span class="s1">out_tree</span><span class="s2">, </span><span class="s1">consts = res</span>
    <span class="s1">cts_flat</span><span class="s2">, </span><span class="s1">out_tree_ = tree_flatten((cts</span><span class="s2">,</span><span class="s1">))</span>
    <span class="s2">if </span><span class="s1">out_tree != out_tree_: </span><span class="s2">raise </span><span class="s1">TypeError(</span><span class="s3">f'</span><span class="s2">{</span><span class="s1">out_tree</span><span class="s2">}\n</span><span class="s3">!=</span><span class="s2">\n{</span><span class="s1">out_tree_</span><span class="s2">}</span><span class="s3">'</span><span class="s1">)</span>
    <span class="s1">cts_out = core.eval_jaxpr(jaxpr</span><span class="s2">, </span><span class="s1">consts</span><span class="s2">, </span><span class="s1">*cts_flat)</span>
    <span class="s1">cts_out = tree_unflatten(in_tree</span><span class="s2">, </span><span class="s1">cts_out)</span>
    <span class="s2">if </span><span class="s1">treedef_is_leaf(in_tree):</span>
      <span class="s1">cts_out = (cts_out</span><span class="s2">,</span><span class="s1">)</span>
    <span class="s2">return </span><span class="s1">cts_out</span>

  <span class="s1">wrapped_fun.defvjp(fwd</span><span class="s2">, </span><span class="s1">bwd)</span>
  <span class="s2">return </span><span class="s1">wrapped_fun</span>

<span class="s1">@register_pytree_node_class</span>
<span class="s2">class </span><span class="s1">Residuals:</span>
  <span class="s2">def </span><span class="s1">__init__(self</span><span class="s2">, </span><span class="s1">jaxpr</span><span class="s2">, </span><span class="s1">in_tree</span><span class="s2">, </span><span class="s1">out_tree</span><span class="s2">, </span><span class="s1">consts):</span>
    <span class="s1">self.jaxpr = jaxpr</span>
    <span class="s1">self.in_tree = in_tree</span>
    <span class="s1">self.out_tree = out_tree</span>
    <span class="s1">self.consts = consts</span>
  <span class="s2">def </span><span class="s1">__iter__(self):</span>
    <span class="s2">return </span><span class="s1">iter((self.jaxpr</span><span class="s2">, </span><span class="s1">self.in_tree</span><span class="s2">, </span><span class="s1">self.out_tree</span><span class="s2">, </span><span class="s1">self.consts))</span>
  <span class="s2">def </span><span class="s1">tree_flatten(self):</span>
    <span class="s2">return </span><span class="s1">self.consts</span><span class="s2">, </span><span class="s1">(self.jaxpr</span><span class="s2">, </span><span class="s1">self.in_tree</span><span class="s2">, </span><span class="s1">self.out_tree)</span>
  <span class="s1">@classmethod</span>
  <span class="s2">def </span><span class="s1">tree_unflatten(cls</span><span class="s2">, </span><span class="s1">aux</span><span class="s2">, </span><span class="s1">consts):</span>
    <span class="s1">jaxpr</span><span class="s2">, </span><span class="s1">in_tree</span><span class="s2">, </span><span class="s1">out_tree = aux</span>
    <span class="s2">return </span><span class="s1">cls(jaxpr</span><span class="s2">, </span><span class="s1">in_tree</span><span class="s2">, </span><span class="s1">out_tree</span><span class="s2">, </span><span class="s1">consts)</span>


<span class="s2">def </span><span class="s1">closure_convert(fun: Callable</span><span class="s2">, </span><span class="s1">*example_args) -&gt; Tuple[Callable</span><span class="s2">, </span><span class="s1">List[Any]]:</span>
  <span class="s4">&quot;&quot;&quot;Closure conversion utility, for use with higher-order custom derivatives. 
 
  To define custom derivatives such as with ``jax.custom_vjp(f)``, the target 
  function ``f`` must take, as formal arguments, all values involved in 
  differentiation. If ``f`` is a higher-order function, in that it accepts as an 
  argument a Python function ``g``, then values stored away in ``g``'s closure 
  will not be visible to the custom derivative rules, and attempts at AD 
  involving these values will fail. One way around this is to convert the 
  closure by extracting these values, and to pass them as explicit formal 
  arguments across the custom derivative boundary. This utility carries out that 
  conversion. More precisely, it closure-converts the function ``fun`` 
  specialized to the types of the arguments given in ``example_args``. 
 
  When we refer here to &quot;values in the closure&quot; of ``fun``, we do not mean the 
  values that are captured by Python directly when ``fun`` is defined (e.g. the 
  Python objects in ``fun.__closure__``, if the attribute exists). Rather, we 
  mean values encountered during the execution of ``fun`` on ``example_args`` 
  that determine its output. This may include, for instance, arrays captured 
  transitively in Python closures, i.e. in the Python closure of functions 
  called by ``fun``, the closures of the functions that they call, and so forth. 
 
  The function ``fun`` must be a pure function. 
 
  Example usage:: 
 
    def minimize(objective_fn, x0): 
      converted_fn, aux_args = closure_convert(objective_fn, x0) 
      return _minimize(converted_fn, x0, *aux_args) 
 
    @partial(custom_vjp, nondiff_argnums=(0,)) 
    def _minimize(objective_fn, x0, *args): 
      z = objective_fn(x0, *args) 
      # ... find minimizer x_opt ... 
      return x_opt 
 
    def fwd(objective_fn, x0, *args): 
      y = _minimize(objective_fn, x0, *args) 
      return y, (y, args) 
 
    def rev(objective_fn, res, g): 
      y, args = res 
      y_bar = g 
      # ... custom reverse-mode AD ... 
      return x0_bar, *args_bars 
 
    _minimize.defvjp(fwd, rev) 
 
  Args: 
    fun: Python callable to be converted. Must be a pure function. 
    example_args: Arrays, scalars, or (nested) standard Python 
      containers (tuples, lists, dicts, namedtuples, i.e., pytrees) 
      thereof, used to determine the types of the formal arguments to 
      ``fun``. This type-specialized form of ``fun`` is the function 
      that will be closure converted. 
 
  Returns: 
    A pair comprising (i) a Python callable, accepting the same 
    arguments as ``fun`` followed by arguments corresponding to the 
    values hoisted from its closure, and (ii) a list of values hoisted 
    from the closure. 
  &quot;&quot;&quot;</span>
  <span class="s1">flat_args</span><span class="s2">, </span><span class="s1">in_tree = tree_flatten(example_args)</span>
  <span class="s1">in_avals = tuple(map(abstractify</span><span class="s2">, </span><span class="s1">flat_args))</span>
  <span class="s2">if </span><span class="s1">config.jax_check_tracer_leaks:</span>
    <span class="s2">return </span><span class="s1">_closure_convert_for_avals.__wrapped__(fun</span><span class="s2">, </span><span class="s1">in_tree</span><span class="s2">, </span><span class="s1">in_avals)</span>
  <span class="s2">else</span><span class="s1">:</span>
    <span class="s2">return </span><span class="s1">_closure_convert_for_avals(fun</span><span class="s2">, </span><span class="s1">in_tree</span><span class="s2">, </span><span class="s1">in_avals)</span>

<span class="s2">def </span><span class="s1">_maybe_perturbed(x: Any) -&gt; bool:</span>
  <span class="s0"># False if x can't represent an AD-perturbed value (i.e. a value</span>
  <span class="s0"># with a nontrivial tangent attached), up to heuristics, and True otherwise.</span>
  <span class="s0"># See https://github.com/google/jax/issues/6415 for motivation.</span>
  <span class="s1">x = core.full_lower(x)</span>
  <span class="s2">if not </span><span class="s1">isinstance(x</span><span class="s2">, </span><span class="s1">core.Tracer):</span>
    <span class="s0"># If x is not a Tracer, it can't be perturbed.</span>
    <span class="s2">return False</span>
  <span class="s2">elif </span><span class="s1">isinstance(x</span><span class="s2">, </span><span class="s1">pe.DynamicJaxprTracer):</span>
    <span class="s0"># If x is a DynamicJaxprTracer then we're staging out; differentiation could</span>
    <span class="s0"># happen later, but some types always have trivial tangents.</span>
    <span class="s1">vspace = x.aval.at_least_vspace()</span>
    <span class="s2">return not </span><span class="s1">(vspace </span><span class="s2">is </span><span class="s1">core.abstract_token </span><span class="s2">or</span>
                <span class="s1">getattr(vspace</span><span class="s2">, </span><span class="s3">'dtype'</span><span class="s2">, None</span><span class="s1">) == dtypes.float0)</span>
  <span class="s2">elif not </span><span class="s1">isinstance(x</span><span class="s2">, </span><span class="s1">ad.JVPTracer):</span>
    <span class="s0"># If x is not a JVPTracer, recursively check its contents.</span>
    <span class="s2">return </span><span class="s1">any(_maybe_perturbed(attr) </span><span class="s2">for </span><span class="s1">name</span><span class="s2">, </span><span class="s1">attr </span><span class="s2">in </span><span class="s1">x._contents())</span>
  <span class="s2">else</span><span class="s1">:</span>
    <span class="s2">return True  </span><span class="s0"># We can't be sure!</span>

<span class="s1">@cache()</span>
<span class="s2">def </span><span class="s1">_closure_convert_for_avals(fun</span><span class="s2">, </span><span class="s1">in_tree</span><span class="s2">, </span><span class="s1">in_avals):</span>
  <span class="s1">wrapped_fun</span><span class="s2">, </span><span class="s1">out_tree = flatten_fun_nokwargs(lu.wrap_init(fun)</span><span class="s2">, </span><span class="s1">in_tree)</span>
  <span class="s1">jaxpr</span><span class="s2">, </span><span class="s1">out_pvals</span><span class="s2">, </span><span class="s1">consts = pe.trace_to_jaxpr_dynamic(wrapped_fun</span><span class="s2">, </span><span class="s1">in_avals)</span>
  <span class="s1">out_tree = out_tree()</span>

  <span class="s1">(closure_consts</span><span class="s2">, </span><span class="s1">hoisted_consts)</span><span class="s2">, </span><span class="s1">merge = partition_list(_maybe_perturbed</span><span class="s2">, </span><span class="s1">consts)</span>
  <span class="s1">num_consts = len(hoisted_consts)</span>

  <span class="s2">def </span><span class="s1">converted_fun(*args_hconsts):</span>
    <span class="s1">num_args = len(args_hconsts) - num_consts</span>
    <span class="s1">args</span><span class="s2">, </span><span class="s1">hoisted_consts = split_list(args_hconsts</span><span class="s2">, </span><span class="s1">[num_args])</span>
    <span class="s1">consts = merge(closure_consts</span><span class="s2">, </span><span class="s1">hoisted_consts)</span>
    <span class="s1">all_args</span><span class="s2">, </span><span class="s1">in_tree2 = tree_flatten(tuple(args))</span>
    <span class="s2">assert </span><span class="s1">in_tree == in_tree2</span>
    <span class="s1">out_flat = core.eval_jaxpr(jaxpr</span><span class="s2">, </span><span class="s1">consts</span><span class="s2">, </span><span class="s1">*all_args)</span>
    <span class="s2">return </span><span class="s1">tree_unflatten(out_tree</span><span class="s2">, </span><span class="s1">out_flat)</span>

  <span class="s2">return </span><span class="s1">converted_fun</span><span class="s2">, </span><span class="s1">hoisted_consts</span>

<span class="s2">def </span><span class="s1">partition_list(choice</span><span class="s2">, </span><span class="s1">lst):</span>
  <span class="s1">out = []</span><span class="s2">, </span><span class="s1">[]</span>
  <span class="s1">which = [out[choice(elt)].append(elt) </span><span class="s2">or </span><span class="s1">choice(elt) </span><span class="s2">for </span><span class="s1">elt </span><span class="s2">in </span><span class="s1">lst]</span>
  <span class="s2">def </span><span class="s1">merge(l1</span><span class="s2">, </span><span class="s1">l2):</span>
    <span class="s1">i1</span><span class="s2">, </span><span class="s1">i2 = iter(l1)</span><span class="s2">, </span><span class="s1">iter(l2)</span>
    <span class="s2">return </span><span class="s1">[next(i2 </span><span class="s2">if </span><span class="s1">snd </span><span class="s2">else </span><span class="s1">i1) </span><span class="s2">for </span><span class="s1">snd </span><span class="s2">in </span><span class="s1">which]</span>
  <span class="s2">return </span><span class="s1">out</span><span class="s2">, </span><span class="s1">merge</span>

<span class="s2">def </span><span class="s1">abstractify(x):</span>
  <span class="s2">return </span><span class="s1">core.raise_to_shaped(core.get_aval(x))</span>


<span class="s0">### Custom transposition</span>

<span class="s2">def </span><span class="s1">linear_call(fun: Callable</span><span class="s2">, </span><span class="s1">fun_transpose: Callable</span><span class="s2">, </span><span class="s1">residual_args</span><span class="s2">,</span>
                <span class="s1">linear_args):</span>
  <span class="s4">&quot;&quot;&quot;Call a linear function, with a custom implementation for its transpose. 
 
  The `Haskell-like type signatures`_ of ``fun`` and ``fun_transpose`` are: 
 
  .. code-block:: haskell 
 
    fun           :: r -&gt; a -o b 
    fun_transpose :: r -&gt; b -o a 
 
  where the ``-o`` arrow indicates a linear function, ``r`` is the 
  residual input type and ``a`` is the linear input type. 
 
  The functions ``fun`` and ``fun_transpose`` are coupled as 
  transposes of one another. Specifically, the transpose of a 
  ``linear_call`` primitive is another ``linear_call`` to 
  ``fun_transpose``, with ``fun`` as its custom transposition. 
 
  For example: 
 
  &gt;&gt;&gt; def f(r, x): 
  ...   return x / r 
 
  &gt;&gt;&gt; def t(r, t): 
  ...   return t / r 
 
  &gt;&gt;&gt; def div_add(x, denom): 
  ...   return x + linear_call(f, t, denom, x) 
 
  &gt;&gt;&gt; def transpose(f, x_example): 
  ...   def transposed(y): 
  ...     x, = jax.linear_transpose(f, x_example)(y) 
  ...     return x 
  ...   return transposed 
 
  &gt;&gt;&gt; div_add(9., 3.) 
  Array(12., dtype=float32, weak_type=True) 
 
  &gt;&gt;&gt; transpose(partial(div_add, denom=3.), 1.)(18.)  # custom 
  Array(24., dtype=float32, weak_type=True) 
 
  &gt;&gt;&gt; transpose(lambda x: x + x / 3., 1.)(18.)  # reference 
  Array(24., dtype=float32, weak_type=True) 
 
  The above definition of ``f`` illustrates the purpose of a residual 
  argument: division is linear in one of its inputs (the dividend 
  ``x``) but not the other (the divisor ``r``). 
 
  As another example: 
 
  &gt;&gt;&gt; def custom_id(x): 
  ...   def f(_, x): return x 
  ...   def t(_, t): return 7. 
  ...   return linear_call(f, t, (), x) 
  &gt;&gt;&gt; custom_id(1.) 
  1.0 
  &gt;&gt;&gt; transpose(custom_id, 1.)(1.) 
  7.0 
  &gt;&gt;&gt; transpose(transpose(custom_id, 1.), 1.)(1.) 
  1.0 
  &gt;&gt;&gt; transpose(transpose(transpose(custom_id, 1.), 1.), 1.)(1.) 
  7.0 
 
  Args: 
    fun: a Python callable specifying a linear function. It should 
      take two arguments: one of &quot;residual&quot; inputs (type ``r``), 
      i.e. inputs in which the function is not necessarly linear, and 
      one of &quot;linear&quot; inputs (type ``a``).  It should return output 
      whose components are linear in the linear input (type ``b``). 
    fun_transpose: a Python callable specifying a structurally linear 
      function that is the transpose of ``fun`` with respect to its 
      linear inputs. Its first argument is the same residual inputs 
      (``r``) as ``fun``. Its second argument is of type 
      ``b``. Finally, its output is of type ``a`` and each of its 
      component are linear in its second argument (the ``b`` inputs). 
    residual_args: Argument in which ``fun`` and ``fun_transpose`` are 
      not necessarily linear. Not involved in transposition. 
    linear_args: Argument in which ``fun`` and ``fun_transpose`` are 
      linear and with respect to which the two are transposes. 
 
  Returns: 
    The call result, i.e. ``fun(residual_args, linear_args)``. 
 
  .. _Haskell-like type signatures: https://wiki.haskell.org/Type_signature 
  &quot;&quot;&quot;</span>
  <span class="s1">operands_res</span><span class="s2">, </span><span class="s1">res_tree = tree_flatten(residual_args)</span>
  <span class="s1">operands_lin</span><span class="s2">, </span><span class="s1">lin_tree = tree_flatten(linear_args)</span>

  <span class="s1">f_in_tree = treedef_tuple((res_tree</span><span class="s2">, </span><span class="s1">lin_tree))</span>
  <span class="s1">f</span><span class="s2">, </span><span class="s1">out_tree = flatten_fun_nokwargs(lu.wrap_init(fun)</span><span class="s2">, </span><span class="s1">f_in_tree)</span>

  <span class="s1">res_avals = map(abstractify</span><span class="s2">, </span><span class="s1">operands_res)</span>
  <span class="s1">lin_avals = map(abstractify</span><span class="s2">, </span><span class="s1">operands_lin)</span>
  <span class="s1">f_jaxpr</span><span class="s2">, </span><span class="s1">f_consts = _initial_style_jaxpr(f</span><span class="s2">, </span><span class="s1">(*res_avals</span><span class="s2">, </span><span class="s1">*lin_avals))</span>
  <span class="s1">f_jaxpr = _close_jaxpr(f_jaxpr)</span>
  <span class="s1">out_avals = map(core.raise_to_shaped</span><span class="s2">, </span><span class="s1">f_jaxpr.out_avals)</span>

  <span class="s1">t_in_tree = treedef_tuple((res_tree</span><span class="s2">, </span><span class="s1">out_tree()))</span>
  <span class="s1">t</span><span class="s2">, </span><span class="s1">t_out_tree = flatten_fun_nokwargs(lu.wrap_init(fun_transpose)</span><span class="s2">, </span><span class="s1">t_in_tree)</span>

  <span class="s1">t_jaxpr</span><span class="s2">, </span><span class="s1">t_consts = _initial_style_jaxpr(t</span><span class="s2">, </span><span class="s1">(*res_avals</span><span class="s2">, </span><span class="s1">*out_avals))</span>
  <span class="s1">t_jaxpr = _close_jaxpr(t_jaxpr)</span>

  <span class="s2">if </span><span class="s1">t_out_tree() != lin_tree:</span>
    <span class="s2">raise </span><span class="s1">TypeError(</span>
        <span class="s3">'transpose output pytree structure must match that of linear inputs, '</span>
        <span class="s3">f'got output structure </span><span class="s2">{</span><span class="s1">t_out_tree()</span><span class="s2">} </span><span class="s3">'</span>
        <span class="s3">f'and input structure </span><span class="s2">{</span><span class="s1">lin_tree</span><span class="s2">}</span><span class="s3">.'</span><span class="s1">)</span>

  <span class="s1">out = linear_call_p.bind(*f_consts</span><span class="s2">, </span><span class="s1">*t_consts</span><span class="s2">, </span><span class="s1">*operands_res</span><span class="s2">, </span><span class="s1">*operands_lin</span><span class="s2">,</span>
                           <span class="s1">callee=f_jaxpr</span><span class="s2">,</span>
                           <span class="s1">transpose=t_jaxpr</span><span class="s2">,</span>
                           <span class="s1">num_callee_consts=len(f_consts)</span><span class="s2">,</span>
                           <span class="s1">num_transpose_consts=len(t_consts)</span><span class="s2">,</span>
                           <span class="s1">num_res=len(operands_res))</span>

  <span class="s2">return </span><span class="s1">tree_unflatten(out_tree()</span><span class="s2">, </span><span class="s1">out)</span>

<span class="s2">def </span><span class="s1">_linear_call_impl(*args</span><span class="s2">, </span><span class="s1">callee</span><span class="s2">, </span><span class="s1">transpose</span><span class="s2">, </span><span class="s1">num_callee_consts</span><span class="s2">,</span>
                      <span class="s1">num_transpose_consts</span><span class="s2">, </span><span class="s1">num_res):</span>
  <span class="s2">del </span><span class="s1">transpose</span>
  <span class="s1">consts</span><span class="s2">, </span><span class="s1">_</span><span class="s2">, </span><span class="s1">operands_res</span><span class="s2">, </span><span class="s1">operands_lin = split_list(</span>
      <span class="s1">args</span><span class="s2">, </span><span class="s1">[num_callee_consts</span><span class="s2">, </span><span class="s1">num_transpose_consts</span><span class="s2">, </span><span class="s1">num_res])</span>
  <span class="s2">return </span><span class="s1">core.eval_jaxpr(callee.jaxpr</span><span class="s2">, </span><span class="s1">()</span><span class="s2">, </span><span class="s1">*consts</span><span class="s2">, </span><span class="s1">*operands_res</span><span class="s2">, </span><span class="s1">*operands_lin)</span>

<span class="s2">def </span><span class="s1">_linear_call_transpose_rule(cts</span><span class="s2">, </span><span class="s1">*args</span><span class="s2">, </span><span class="s1">callee</span><span class="s2">, </span><span class="s1">transpose</span><span class="s2">,</span>
                                <span class="s1">num_callee_consts</span><span class="s2">,</span>
                                <span class="s1">num_transpose_consts</span><span class="s2">, </span><span class="s1">num_res):</span>
  <span class="s1">f_consts</span><span class="s2">, </span><span class="s1">t_consts</span><span class="s2">, </span><span class="s1">operands_res</span><span class="s2">, </span><span class="s1">operands_lin = split_list(</span>
      <span class="s1">args</span><span class="s2">, </span><span class="s1">[num_callee_consts</span><span class="s2">, </span><span class="s1">num_transpose_consts</span><span class="s2">, </span><span class="s1">num_res])</span>
  <span class="s1">_</span><span class="s2">, </span><span class="s1">_</span><span class="s2">, </span><span class="s1">cts_avals = split_list(</span>
      <span class="s1">transpose.in_avals</span><span class="s2">, </span><span class="s1">[num_transpose_consts</span><span class="s2">, </span><span class="s1">num_res])</span>

  <span class="s2">assert </span><span class="s1">all(ad.is_undefined_primal(x)     </span><span class="s2">for </span><span class="s1">x </span><span class="s2">in </span><span class="s1">operands_lin)</span>
  <span class="s2">assert </span><span class="s1">all(</span><span class="s2">not </span><span class="s1">ad.is_undefined_primal(x) </span><span class="s2">for </span><span class="s1">x </span><span class="s2">in </span><span class="s1">operands_res)</span>

  <span class="s1">cts = [zeros_like_aval(a) </span><span class="s2">if </span><span class="s1">type(ct) </span><span class="s2">is </span><span class="s1">Zero </span><span class="s2">else </span><span class="s1">ct</span>
         <span class="s2">for </span><span class="s1">ct</span><span class="s2">, </span><span class="s1">a </span><span class="s2">in </span><span class="s1">zip(cts</span><span class="s2">, </span><span class="s1">cts_avals)]</span>

  <span class="s1">cts_out = linear_call_p.bind(*t_consts</span><span class="s2">, </span><span class="s1">*f_consts</span><span class="s2">, </span><span class="s1">*operands_res</span><span class="s2">, </span><span class="s1">*cts</span><span class="s2">,</span>
                               <span class="s1">callee=transpose</span><span class="s2">,</span>
                               <span class="s1">transpose=callee</span><span class="s2">,</span>
                               <span class="s1">num_callee_consts=len(t_consts)</span><span class="s2">,</span>
                               <span class="s1">num_transpose_consts=len(f_consts)</span><span class="s2">,</span>
                               <span class="s1">num_res=len(operands_res))</span>

  <span class="s2">return </span><span class="s1">[</span><span class="s2">None</span><span class="s1">] * (num_callee_consts + num_transpose_consts + num_res) + cts_out</span>

<span class="s2">def </span><span class="s1">_linear_call_abstract_eval(*args</span><span class="s2">, </span><span class="s1">**kwargs):</span>
  <span class="s2">return </span><span class="s1">map(core.raise_to_shaped</span><span class="s2">, </span><span class="s1">kwargs[</span><span class="s3">'callee'</span><span class="s1">].out_avals)</span>

<span class="s1">linear_call_p = core.Primitive(</span><span class="s3">'linear_call'</span><span class="s1">)</span>
<span class="s1">linear_call_p.multiple_results = </span><span class="s2">True</span>
<span class="s1">linear_call_p.def_impl(_linear_call_impl)</span>
<span class="s1">linear_call_p.def_abstract_eval(_linear_call_abstract_eval)</span>
<span class="s1">ad.primitive_transposes[linear_call_p] = _linear_call_transpose_rule</span>
<span class="s1">xla.register_initial_style_primitive(linear_call_p)</span>
<span class="s1">mlir.register_lowering(linear_call_p</span><span class="s2">, </span><span class="s1">mlir.lower_fun(</span>
    <span class="s1">_linear_call_impl</span><span class="s2">, </span><span class="s1">multiple_results=</span><span class="s2">True</span><span class="s1">))</span>


<span class="s0"># A stageable primitive that fails when evaluated</span>
<span class="s1">unreachable_p: core.Primitive = core.Primitive(</span><span class="s3">'unreachable'</span><span class="s1">)</span>
<span class="s1">unreachable_p.multiple_results = </span><span class="s2">True</span>

<span class="s2">def </span><span class="s1">unreachable_impl(*_</span><span class="s2">, </span><span class="s1">out_avals</span><span class="s2">, </span><span class="s1">exc_type</span><span class="s2">, </span><span class="s1">message):</span>
  <span class="s2">del </span><span class="s1">out_avals</span>
  <span class="s2">raise </span><span class="s1">exc_type(message)</span>

<span class="s0"># Evaluation raises an exception</span>
<span class="s1">unreachable_p.def_impl(unreachable_impl)</span>

<span class="s0"># Translation raises an exception</span>
<span class="s0"># TODO(frostig,mattjj): We have no good way to translate a function</span>
<span class="s0"># that errs. Since MLIR lowering over-approximates concrete evaluation,</span>
<span class="s0"># we err on MLIR lowering for the time being.</span>
<span class="s1">mlir.register_lowering(unreachable_p</span><span class="s2">, </span><span class="s1">unreachable_impl)</span>

<span class="s0"># Abstract evaluation proceeds without issue, to allow for staging</span>
<span class="s1">unreachable_p.def_abstract_eval(</span><span class="s2">lambda </span><span class="s1">*_</span><span class="s2">, </span><span class="s1">out_avals</span><span class="s2">, </span><span class="s1">**__: out_avals)</span>

<span class="s2">def </span><span class="s1">unreachable(*args</span><span class="s2">, </span><span class="s1">out_avals=</span><span class="s2">None, </span><span class="s1">exc_type=TypeError</span><span class="s2">,</span>
                <span class="s1">message=</span><span class="s3">'unreachable'</span><span class="s1">):</span>
  <span class="s4">&quot;&quot;&quot;Fail when evaluated concretely (but allow for staging). 
 
  This function allows one to assert an impossibility of 
  evaluation. It can be used to guarantee that evaluation does not 
  &quot;reach&quot; a certain point in the sense that it does not execute, but 
  it can nonetheless be staged out by JAX without error. 
 
  Args: 
    *args: The arbitrary pytree of arguments to the function. 
    out_avals: Optional specification of the output types of this 
     function invocation from the point of view of staging. If 
     ``None``, these are chosen as equal to types of input arguments. 
    exc_type: Optional constructor for the Python exception raised if 
      evaluated. 
    message: Optional string message for the Python exception raised 
      if evaluated. 
 
  &quot;&quot;&quot;</span>
  <span class="s2">if </span><span class="s1">out_avals </span><span class="s2">is None</span><span class="s1">:</span>
    <span class="s1">out_avals = tree_map(core.get_aval</span><span class="s2">, </span><span class="s1">args)</span>

  <span class="s1">args_flat</span><span class="s2">, </span><span class="s1">in_tree = tree_flatten(args)</span>
  <span class="s1">out_avals_flat</span><span class="s2">, </span><span class="s1">out_tree = tree_flatten(out_avals)</span>
  <span class="s1">out = unreachable_p.bind(*args_flat</span><span class="s2">, </span><span class="s1">out_avals=out_avals_flat</span><span class="s2">,</span>
                           <span class="s1">exc_type=exc_type</span><span class="s2">, </span><span class="s1">message=message)</span>
  <span class="s2">return </span><span class="s1">tree_unflatten(out_tree</span><span class="s2">, </span><span class="s1">out)</span>


<span class="s1">disallow_jvp = partial(</span>
    <span class="s1">unreachable</span><span class="s2">,</span>
    <span class="s1">exc_type=TypeError</span><span class="s2">,</span>
    <span class="s1">message=</span><span class="s3">&quot;can't apply forward-mode autodiff (jvp) to a custom_vjp function.&quot;</span><span class="s1">)</span>


<span class="s2">def </span><span class="s1">custom_vjp_by_custom_transpose(fun</span><span class="s2">, </span><span class="s1">fwd</span><span class="s2">, </span><span class="s1">bwd):</span>
  <span class="s1">fun = custom_jvp(fun)</span>

  <span class="s1">@fun.defjvp</span>
  <span class="s2">def </span><span class="s1">jvp(primals</span><span class="s2">, </span><span class="s1">tangents):</span>
    <span class="s1">outs</span><span class="s2">, </span><span class="s1">residuals = fwd(*primals)</span>
    <span class="s1">tan_out_types = tree_map(</span><span class="s2">lambda </span><span class="s1">o: core.get_aval(o).at_least_vspace()</span><span class="s2">, </span><span class="s1">outs)</span>
    <span class="s1">tan_fn = custom_transpose(partial(disallow_jvp</span><span class="s2">, </span><span class="s1">out_avals=tan_out_types))</span>
    <span class="s1">tan_fn.def_transpose(bwd)</span>
    <span class="s2">return </span><span class="s1">outs</span><span class="s2">, </span><span class="s1">tan_fn(tan_out_types</span><span class="s2">, </span><span class="s1">residuals</span><span class="s2">, </span><span class="s1">tangents)</span>

  <span class="s2">return </span><span class="s1">fun</span>


<span class="s0"># TODO(mattjj): remove these stubs, which exist to avoid breaking internal users</span>
<span class="s1">custom_jvp_call_jaxpr_p = core.Primitive(</span><span class="s3">&quot;custom_jvp_call_jaxpr&quot;</span><span class="s1">)</span>
</pre>
</body>
</html>