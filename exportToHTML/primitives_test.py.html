<html>
<head>
<title>primitives_test.py</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<style type="text/css">
.s0 { color: #808080;}
.s1 { color: #a9b7c6;}
.s2 { color: #629755; font-style: italic;}
.s3 { color: #cc7832;}
.s4 { color: #6a8759;}
.s5 { color: #6897bb;}
</style>
</head>
<body bgcolor="#2b2b2b">
<table CELLSPACING=0 CELLPADDING=5 COLS=1 WIDTH="100%" BGCOLOR="#606060" >
<tr><td><center>
<font face="Arial, Helvetica" color="#000000">
primitives_test.py</font>
</center></td></tr></table>
<pre><span class="s0"># Copyright 2020 The JAX Authors.</span>
<span class="s0">#</span>
<span class="s0"># Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);</span>
<span class="s0"># you may not use this file except in compliance with the License.</span>
<span class="s0"># You may obtain a copy of the License at</span>
<span class="s0">#</span>
<span class="s0">#     https://www.apache.org/licenses/LICENSE-2.0</span>
<span class="s0">#</span>
<span class="s0"># Unless required by applicable law or agreed to in writing, software</span>
<span class="s0"># distributed under the License is distributed on an &quot;AS IS&quot; BASIS,</span>
<span class="s0"># WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span>
<span class="s0"># See the License for the specific language governing permissions and</span>
<span class="s0"># limitations under the License.</span>
<span class="s2">&quot;&quot;&quot;Tests for JAX primitive coverage. 
 
The bulk of the testing is done by `test_prim`, which is parameterized by 
about 2000+ test harnesses. See `primitive_harness.py` docstring for a 
description of test harnesses. That module contains also the definitions 
of all the test harnesses, and a specification of which are only partially 
implemented for JAX. 
 
For each harness, we convert the JAX function to Tensorflow and then we run 
it on the same inputs in &quot;eager&quot;, &quot;graph&quot;, or &quot;compiled&quot; mode and we check 
that we get the same result as in JAX 
(see `tf_test_util.ConvertAndCompare`). 
 
Some harnesses need specific tolerances, or even custom equality assertions. 
Also, for some harnesses we need to specify some data types that result 
in Tensorflow errors (for some devices and compilation modes). These limitations 
are captured as jax2tf_limitations.Jax2TfLimitation objects. 
 
From the limitations objects, we generate a 
[report](https://github.com/google/jax/blob/main/jax/experimental/jax2tf/g3doc/primitives_with_limited_support.md). 
The report has instructions for how to re-generate it. 
 
If a harness run fails with error, and a limitation that matches the device 
and data types is found, 
the error is logged but does not abort the test. If a harness run succeeds 
and there are matching limitations, the test emits a warning. If you want to 
turn these warnings into errors, you'd have to uncomment an assertion 
in `tf_test_util.ConvertAndCompare`. 
 
IMPORTANT: If you need to customize the testing of a particular primitive 
conversion, you must create a class method in jax2tf_limitations.jax2tf_limitations, 
with the same name as the harness.group_name (typically the same as the 
primitive name). That class method should return the list of Jax2TfLimitation 
objects for the harness. 
See `jax2tf_limitations.limitations_for_harness`. If a group name does 
not need limitations, then it must be listed in the 
`jax2tf_limitations.harness_groups_no_limitations`. 
 
&quot;&quot;&quot;</span>

<span class="s3">import </span><span class="s1">datetime</span>
<span class="s3">import </span><span class="s1">os</span>
<span class="s3">from </span><span class="s1">typing </span><span class="s3">import </span><span class="s1">Any</span><span class="s3">, </span><span class="s1">Dict</span><span class="s3">, </span><span class="s1">Tuple</span>
<span class="s3">import </span><span class="s1">unittest</span>

<span class="s3">from </span><span class="s1">absl </span><span class="s3">import </span><span class="s1">logging</span>
<span class="s3">from </span><span class="s1">absl.testing </span><span class="s3">import </span><span class="s1">absltest</span>
<span class="s3">from </span><span class="s1">absl.testing </span><span class="s3">import </span><span class="s1">parameterized</span>

<span class="s3">import </span><span class="s1">jax</span>
<span class="s3">from </span><span class="s1">jax </span><span class="s3">import </span><span class="s1">dtypes</span>
<span class="s3">from </span><span class="s1">jax </span><span class="s3">import </span><span class="s1">numpy </span><span class="s3">as </span><span class="s1">jnp</span>
<span class="s3">from </span><span class="s1">jax._src </span><span class="s3">import </span><span class="s1">test_util </span><span class="s3">as </span><span class="s1">jtu</span>
<span class="s3">from </span><span class="s1">jax.config </span><span class="s3">import </span><span class="s1">config</span>
<span class="s3">from </span><span class="s1">jax.experimental </span><span class="s3">import </span><span class="s1">jax2tf</span>
<span class="s3">from </span><span class="s1">jax.interpreters </span><span class="s3">import </span><span class="s1">mlir</span>
<span class="s3">from </span><span class="s1">jax._src.interpreters </span><span class="s3">import </span><span class="s1">xla</span>

<span class="s3">import </span><span class="s1">numpy </span><span class="s3">as </span><span class="s1">np</span>
<span class="s3">import </span><span class="s1">tensorflow </span><span class="s3">as </span><span class="s1">tf  </span><span class="s0"># type: ignore[import]</span>

<span class="s1">config.parse_flags_with_absl()</span>

<span class="s0"># Import after parsing flags</span>
<span class="s3">from </span><span class="s1">jax.experimental.jax2tf.tests </span><span class="s3">import </span><span class="s1">tf_test_util</span>
<span class="s3">from </span><span class="s1">jax.experimental.jax2tf.tests.jax2tf_limitations </span><span class="s3">import </span><span class="s1">Jax2TfLimitation</span>
<span class="s3">from </span><span class="s1">jax.experimental.jax2tf.tests </span><span class="s3">import </span><span class="s1">primitive_harness</span>

<span class="s1">DType = Any</span>

<span class="s1">REDUCE = (</span>
    <span class="s1">jnp.all</span><span class="s3">,</span>
    <span class="s1">jnp.any</span><span class="s3">,</span>
    <span class="s1">jnp.max</span><span class="s3">,</span>
    <span class="s1">jnp.min</span><span class="s3">,</span>
    <span class="s1">jnp.prod</span><span class="s3">,</span>
    <span class="s1">jnp.sum</span><span class="s3">,</span>
<span class="s1">)</span>


<span class="s3">class </span><span class="s1">JaxPrimitiveTest(tf_test_util.JaxToTfTestCase):</span>

  <span class="s0"># This test runs for all primitive harnesses. For each primitive &quot;xxx&quot; the</span>
  <span class="s0"># test will be called &quot;test_prim_xxx_...&quot; and the custom parameters for</span>
  <span class="s0"># the test are defined in the class method &quot;jax2tf_limitations.Jax2TfLimitation.xxx&quot;.</span>
  <span class="s0"># See more details in the comment at top of file and in Jax2TfLimitation class.</span>
  <span class="s0"># If you want to run this test for only one harness, add parameter</span>
  <span class="s0"># `one_containing=&quot;foo&quot;` to parameterized below.</span>
  <span class="s1">@primitive_harness.parameterized(</span>
      <span class="s1">primitive_harness.all_harnesses</span><span class="s3">,</span>
      <span class="s1">include_jax_unimpl=</span><span class="s3">False,</span>
      <span class="s0">#one_containing=&quot;&quot;,</span>
  <span class="s1">)</span>
  <span class="s1">@jtu.ignore_warning(</span>
      <span class="s1">category=UserWarning</span><span class="s3">, </span><span class="s1">message=</span><span class="s4">&quot;Using reduced precision for gradient.*&quot;</span><span class="s1">)</span>
  <span class="s3">def </span><span class="s1">test_prim(self</span><span class="s3">, </span><span class="s1">harness: primitive_harness.Harness):</span>
    <span class="s1">limitations = Jax2TfLimitation.limitations_for_harness(harness)</span>
    <span class="s1">device = jtu.device_under_test()</span>
    <span class="s1">limitations = tuple(filter(</span><span class="s3">lambda </span><span class="s1">l: l.filter(device=device</span><span class="s3">,</span>
                                                  <span class="s1">dtype=harness.dtype)</span><span class="s3">, </span><span class="s1">limitations))</span>
    <span class="s1">func_jax = harness.dyn_fun</span>
    <span class="s1">args = harness.dyn_args_maker(self.rng())</span>
    <span class="s1">enable_xla = harness.params.get(</span><span class="s4">&quot;enable_xla&quot;</span><span class="s3">, True</span><span class="s1">)</span>
    <span class="s3">if </span><span class="s1">config.jax2tf_default_native_serialization </span><span class="s3">and not </span><span class="s1">enable_xla:</span>
      <span class="s3">raise </span><span class="s1">unittest.SkipTest(</span><span class="s4">&quot;native_serialization not supported with enable_xla=False&quot;</span><span class="s1">)</span>

    <span class="s3">if </span><span class="s1">(</span><span class="s4">&quot;eigh&quot; </span><span class="s1">== harness.group_name </span><span class="s3">and</span>
        <span class="s1">np.complex64 == harness.dtype </span><span class="s3">and</span>
        <span class="s1">device == </span><span class="s4">&quot;tpu&quot;</span><span class="s1">):</span>
      <span class="s3">raise </span><span class="s1">unittest.SkipTest(</span><span class="s4">&quot;b/264716764: error on tf.cast from c64 to f32&quot;</span><span class="s1">)</span>

    <span class="s3">if </span><span class="s1">(config.jax2tf_default_native_serialization </span><span class="s3">and</span>
        <span class="s1">device == </span><span class="s4">&quot;gpu&quot; </span><span class="s3">and</span>
        <span class="s4">&quot;lu&quot; </span><span class="s3">in </span><span class="s1">harness.fullname):</span>
      <span class="s3">raise </span><span class="s1">unittest.SkipTest(</span><span class="s4">&quot;b/269388847: lu failures on GPU&quot;</span><span class="s1">)</span>

    <span class="s3">def </span><span class="s1">skipCustomCallTest(target: str):</span>
      <span class="s3">raise </span><span class="s1">unittest.SkipTest(</span>
          <span class="s4">f&quot;TODO(b/272239584): custom call target not guaranteed stable: </span><span class="s3">{</span><span class="s1">target</span><span class="s3">}</span><span class="s4">&quot;</span><span class="s1">)</span>
    <span class="s3">if </span><span class="s1">config.jax2tf_default_native_serialization:</span>
      <span class="s3">if </span><span class="s1">device == </span><span class="s4">&quot;cpu&quot;</span><span class="s1">:</span>
        <span class="s3">if </span><span class="s4">&quot;cholesky_shape&quot; </span><span class="s3">in </span><span class="s1">harness.fullname:</span>
          <span class="s1">skipCustomCallTest(</span><span class="s4">&quot;lapack_spotrf, lapack_dpotrf, lapack_zpotrf, lapack_cpotrf&quot;</span><span class="s1">)</span>
        <span class="s3">if </span><span class="s4">&quot;eig_shape&quot; </span><span class="s3">in </span><span class="s1">harness.fullname:</span>
          <span class="s1">skipCustomCallTest(</span><span class="s4">&quot;lapack_cgeev, lapack_sgeev, lapack_dgeev, lapack_zgeev&quot;</span><span class="s1">)</span>
        <span class="s3">if </span><span class="s4">&quot;lu_shape&quot; </span><span class="s3">in </span><span class="s1">harness.fullname:</span>
          <span class="s1">skipCustomCallTest(</span><span class="s4">&quot;lapack_zgetrf, lapack_sgetrf&quot;</span><span class="s1">)</span>
        <span class="s3">if </span><span class="s4">&quot;svd_shape&quot; </span><span class="s3">in </span><span class="s1">harness.fullname:</span>
          <span class="s1">skipCustomCallTest(</span><span class="s4">&quot;lapack_sgesdd, lapack_zgesdd, lapack_cgesdd&quot;</span><span class="s1">)</span>
        <span class="s3">if </span><span class="s4">&quot;triangular_solve_&quot; </span><span class="s3">in </span><span class="s1">harness.fullname:</span>
          <span class="s1">skipCustomCallTest(</span><span class="s4">&quot;blas_ctrsm, blas_dtrsm, blas_ztrsm, blas_strsm&quot;</span><span class="s1">)</span>
        <span class="s3">if </span><span class="s4">&quot;custom_linear_solve&quot; </span><span class="s3">in </span><span class="s1">harness.fullname:</span>
          <span class="s1">skipCustomCallTest(</span><span class="s4">&quot;lapack_sgetrf, lapack_dgetrf&quot;</span><span class="s1">)</span>

      <span class="s3">elif </span><span class="s1">device == </span><span class="s4">&quot;tpu&quot;</span><span class="s1">:</span>
        <span class="s3">if </span><span class="s4">&quot;approx_top_k_large=True&quot; </span><span class="s3">in </span><span class="s1">harness.fullname:</span>
          <span class="s1">skipCustomCallTest(</span><span class="s4">&quot;PartialReduce&quot;</span><span class="s1">)  </span><span class="s0"># ApproxTopK</span>

      <span class="s3">elif </span><span class="s1">device == </span><span class="s4">&quot;gpu&quot;</span><span class="s1">:</span>
        <span class="s3">if </span><span class="s4">&quot;custom_linear_solve_&quot; </span><span class="s3">in </span><span class="s1">harness.fullname:</span>
          <span class="s1">skipCustomCallTest(</span><span class="s4">&quot;cusolver_geqrf, cublas_geqrf_batched&quot;</span><span class="s1">)</span>
        <span class="s3">if </span><span class="s4">&quot;svd_shape&quot; </span><span class="s3">in </span><span class="s1">harness.fullname:</span>
          <span class="s1">skipCustomCallTest(</span><span class="s4">&quot;cusolver_gesvdj&quot;</span><span class="s1">)</span>
        <span class="s3">if </span><span class="s4">&quot;tridiagonal_solve_shape&quot; </span><span class="s3">in </span><span class="s1">harness.fullname:</span>
          <span class="s1">skipCustomCallTest(</span><span class="s4">&quot;cusparse_gtsv2_f32, cusparse_gtsv2_f64&quot;</span><span class="s1">)</span>

    <span class="s1">associative_scan_reductions = harness.params.get(</span><span class="s4">&quot;associative_scan_reductions&quot;</span><span class="s3">, False</span><span class="s1">)</span>
    <span class="s3">try</span><span class="s1">:</span>
      <span class="s3">with </span><span class="s1">jax.jax2tf_associative_scan_reductions(associative_scan_reductions):</span>
        <span class="s1">self.ConvertAndCompare(func_jax</span><span class="s3">, </span><span class="s1">*args</span><span class="s3">, </span><span class="s1">limitations=limitations</span><span class="s3">,</span>
                               <span class="s1">enable_xla=enable_xla)</span>
    <span class="s3">except </span><span class="s1">Exception </span><span class="s3">as </span><span class="s1">e:</span>
      <span class="s0"># TODO(b/264596006): custom calls are not registered properly with TF in OSS</span>
      <span class="s3">if </span><span class="s1">(config.jax2tf_default_native_serialization </span><span class="s3">and</span>
          <span class="s4">&quot;does not work with custom calls&quot; </span><span class="s3">in </span><span class="s1">str(e)):</span>
        <span class="s1">logging.warning(</span><span class="s4">&quot;Supressing error %s&quot;</span><span class="s3">, </span><span class="s1">e)</span>
        <span class="s3">raise </span><span class="s1">unittest.SkipTest(</span><span class="s4">&quot;b/264596006: custom calls in native serialization fail in TF&quot;</span><span class="s1">)</span>
      <span class="s3">else</span><span class="s1">:</span>
        <span class="s3">raise </span><span class="s1">e</span>

  <span class="s3">def </span><span class="s1">test_primitive_coverage(self):</span>
    <span class="s2">&quot;&quot;&quot;Fail if there are JAX primitives that are not implemented.&quot;&quot;&quot;</span>
    <span class="s0"># Harvest primitives from XLA translation tables</span>
    <span class="s1">all_primitives = (</span>
        <span class="s1">set(xla._translations)</span>
        <span class="s1">| set(xla._backend_specific_translations[</span><span class="s4">&quot;cpu&quot;</span><span class="s1">])</span>
        <span class="s1">| set(xla._backend_specific_translations[</span><span class="s4">&quot;gpu&quot;</span><span class="s1">])</span>
        <span class="s1">| set(xla._backend_specific_translations[</span><span class="s4">&quot;tpu&quot;</span><span class="s1">])</span>
        <span class="s1">| set(mlir._lowerings)</span>
        <span class="s1">| set(mlir._platform_specific_lowerings[</span><span class="s4">&quot;cpu&quot;</span><span class="s1">])</span>
        <span class="s1">| set(mlir._platform_specific_lowerings[</span><span class="s4">&quot;gpu&quot;</span><span class="s1">])</span>
        <span class="s1">| set(mlir._platform_specific_lowerings[</span><span class="s4">&quot;tpu&quot;</span><span class="s1">]))</span>

    <span class="s1">tf_impl = set(jax.experimental.jax2tf.jax2tf.tf_impl) | set(</span>
        <span class="s1">jax.experimental.jax2tf.jax2tf.tf_impl_with_avals)</span>
    <span class="s1">tf_not_yet_impl = set(jax.experimental.jax2tf.jax2tf.tf_not_yet_impl)</span>

    <span class="s1">all_primitives = tuple(sorted(all_primitives</span><span class="s3">, </span><span class="s1">key=str))</span>
    <span class="s3">for </span><span class="s1">p </span><span class="s3">in </span><span class="s1">all_primitives:</span>
      <span class="s3">if </span><span class="s1">p.name == </span><span class="s4">&quot;axis_index&quot;</span><span class="s1">:</span>
        <span class="s3">continue</span>
      <span class="s0"># TODO: remove once we delete sharded_jit.py</span>
      <span class="s3">if </span><span class="s1">p.name </span><span class="s3">in </span><span class="s1">[</span><span class="s4">&quot;sharded_call&quot;</span><span class="s3">, </span><span class="s4">&quot;sharding_constraint&quot;</span><span class="s1">]:</span>
        <span class="s3">continue</span>
      <span class="s0"># TODO: Remove once tensorflow is 2.10.0 everywhere.</span>
      <span class="s3">if </span><span class="s1">p.name == </span><span class="s4">&quot;optimization_barrier&quot;</span><span class="s1">:</span>
        <span class="s3">continue</span>
      <span class="s3">if </span><span class="s1">p.name == </span><span class="s4">&quot;debug_callback&quot;</span><span class="s1">:</span>
        <span class="s0"># TODO(sharadmv,necula): enable debug callbacks in TF</span>
        <span class="s3">continue</span>
      <span class="s3">if </span><span class="s1">p.name </span><span class="s3">in </span><span class="s1">tf_not_yet_impl:</span>
        <span class="s1">self.assertNotIn(</span>
            <span class="s1">p</span><span class="s3">, </span><span class="s1">tf_impl)  </span><span class="s0"># Should not be in both tf_impl and tf_not_yet_impl</span>
      <span class="s3">else</span><span class="s1">:</span>
        <span class="s1">self.assertIn(p</span><span class="s3">, </span><span class="s1">tf_impl)</span>

  <span class="s3">def </span><span class="s1">test_generate_limitations_doc(self):</span>
    <span class="s2">&quot;&quot;&quot;Generates primitives_with_limited_support.md. 
 
    See the doc for instructions. 
    &quot;&quot;&quot;</span>

    <span class="s1">harnesses = [</span>
        <span class="s1">h </span><span class="s3">for </span><span class="s1">h </span><span class="s3">in </span><span class="s1">primitive_harness.all_harnesses</span>
        <span class="s3">if </span><span class="s1">h.filter(h</span><span class="s3">, </span><span class="s1">include_jax_unimpl=</span><span class="s3">True</span><span class="s1">)</span>
    <span class="s1">]</span>
    <span class="s1">print(</span><span class="s4">f&quot;Found </span><span class="s3">{</span><span class="s1">len(harnesses)</span><span class="s3">} </span><span class="s4">test harnesses that work in JAX&quot;</span><span class="s1">)</span>

    <span class="s3">def </span><span class="s1">unique_hash(h: primitive_harness.Harness</span><span class="s3">, </span><span class="s1">l: Jax2TfLimitation):</span>
      <span class="s3">return </span><span class="s1">(h.group_name</span><span class="s3">, </span><span class="s1">l.description</span><span class="s3">, </span><span class="s1">l.devices</span><span class="s3">,</span>
              <span class="s1">tuple(np.dtype(d).name </span><span class="s3">for </span><span class="s1">d </span><span class="s3">in </span><span class="s1">l.dtypes)</span><span class="s3">, </span><span class="s1">l.modes)</span>

    <span class="s1">unique_limitations: Dict[Any</span><span class="s3">, </span><span class="s1">Tuple[primitive_harness.Harness</span><span class="s3">, </span><span class="s1">Jax2TfLimitation]] = {}</span>
    <span class="s3">for </span><span class="s1">h </span><span class="s3">in </span><span class="s1">harnesses:</span>
      <span class="s3">for </span><span class="s1">l </span><span class="s3">in </span><span class="s1">h.jax_unimplemented:</span>
        <span class="s3">if </span><span class="s1">l.enabled:</span>
          <span class="s0"># Fake a Jax2TFLimitation from the Limitation</span>
          <span class="s1">tfl = Jax2TfLimitation(description=</span><span class="s4">&quot;Not implemented in JAX: &quot; </span><span class="s1">+ l.description</span><span class="s3">,</span>
                                 <span class="s1">devices = l.devices</span><span class="s3">,</span>
                                 <span class="s1">dtypes = l.dtypes</span><span class="s3">,</span>
                                 <span class="s1">expect_tf_error = </span><span class="s3">False,</span>
                                 <span class="s1">skip_tf_run = </span><span class="s3">True</span><span class="s1">)</span>
          <span class="s1">unique_limitations[hash(unique_hash(h</span><span class="s3">, </span><span class="s1">tfl))] = (h</span><span class="s3">, </span><span class="s1">tfl)</span>
    <span class="s3">for </span><span class="s1">h </span><span class="s3">in </span><span class="s1">harnesses:</span>
      <span class="s3">for </span><span class="s1">l </span><span class="s3">in </span><span class="s1">Jax2TfLimitation.limitations_for_harness(h):</span>
        <span class="s1">unique_limitations[hash(unique_hash(h</span><span class="s3">, </span><span class="s1">l))] = (h</span><span class="s3">, </span><span class="s1">l)</span>

    <span class="s1">print(</span><span class="s4">f&quot;Found </span><span class="s3">{</span><span class="s1">len(unique_limitations)</span><span class="s3">} </span><span class="s4">unique limitations&quot;</span><span class="s1">)</span>
    <span class="s1">tf_error_table = [</span>
        <span class="s4">&quot;&quot;&quot; 
| Affected primitive | Description of limitation | Affected dtypes | Affected devices | Affected compilation modes | 
| --- | --- | --- | --- | --- |&quot;&quot;&quot;</span>
    <span class="s1">]</span>
    <span class="s1">tf_numerical_discrepancies_table = list(tf_error_table)  </span><span class="s0"># a copy</span>
    <span class="s3">for </span><span class="s1">h</span><span class="s3">, </span><span class="s1">l </span><span class="s3">in </span><span class="s1">sorted(</span>
        <span class="s1">unique_limitations.values()</span><span class="s3">, </span><span class="s1">key=</span><span class="s3">lambda </span><span class="s1">pair: unique_hash(*pair)):</span>
      <span class="s1">devices = </span><span class="s4">&quot;, &quot;</span><span class="s1">.join(sorted(l.devices))</span>
      <span class="s1">modes = </span><span class="s4">&quot;, &quot;</span><span class="s1">.join(sorted(l.modes))</span>
      <span class="s1">description = l.description</span>
      <span class="s3">if </span><span class="s1">l.skip_comparison:</span>
        <span class="s1">description = </span><span class="s4">&quot;Numeric comparison disabled: &quot; </span><span class="s1">+ description</span>
      <span class="s3">if </span><span class="s1">l.expect_tf_error:</span>
        <span class="s1">description = </span><span class="s4">&quot;TF error: &quot; </span><span class="s1">+ description</span>
      <span class="s3">if </span><span class="s1">l.skip_tf_run:</span>
        <span class="s1">description = </span><span class="s4">&quot;TF test skipped: &quot; </span><span class="s1">+ description</span>

      <span class="s3">if </span><span class="s1">l.skip_tf_run </span><span class="s3">or </span><span class="s1">l.expect_tf_error:</span>
        <span class="s1">to_table = tf_error_table</span>
      <span class="s3">elif </span><span class="s1">l.skip_comparison </span><span class="s3">or </span><span class="s1">l.custom_assert:</span>
        <span class="s1">to_table = tf_numerical_discrepancies_table</span>
      <span class="s3">else</span><span class="s1">:</span>
        <span class="s3">continue</span>

      <span class="s1">to_table.append(</span>
          <span class="s4">f&quot;| </span><span class="s3">{</span><span class="s1">h.group_name</span><span class="s3">} </span><span class="s4">| </span><span class="s3">{</span><span class="s1">description</span><span class="s3">} </span><span class="s4">| &quot;</span>
          <span class="s4">f&quot;</span><span class="s3">{</span><span class="s1">primitive_harness.dtypes_to_str(l.dtypes</span><span class="s3">, </span><span class="s1">empty_means_all=</span><span class="s3">True</span><span class="s1">)</span><span class="s3">} </span><span class="s4">| </span><span class="s3">{</span><span class="s1">devices</span><span class="s3">} </span><span class="s4">| </span><span class="s3">{</span><span class="s1">modes</span><span class="s3">} </span><span class="s4">|&quot;</span>
      <span class="s1">)</span>

    <span class="s3">if not </span><span class="s1">os.environ.get(</span><span class="s4">&quot;JAX_OUTPUT_LIMITATIONS_DOC&quot;</span><span class="s1">):</span>
      <span class="s3">raise </span><span class="s1">unittest.SkipTest(</span>
          <span class="s4">&quot;Set JAX_OUTPUT_LIMITATIONS_DOC=1 to enable the generation of the documentation&quot;</span>
      <span class="s1">)</span>
    <span class="s0"># The CPU has more supported types, and harnesses</span>
    <span class="s1">self.assertEqual(</span><span class="s4">&quot;cpu&quot;</span><span class="s3">, </span><span class="s1">jtu.device_under_test())</span>
    <span class="s1">self.assertTrue(</span>
        <span class="s1">config.x64_enabled</span><span class="s3">,</span>
        <span class="s4">&quot;Documentation generation must be run with JAX_ENABLE_X64=1&quot;</span><span class="s1">)</span>

    <span class="s3">with </span><span class="s1">open(</span>
        <span class="s1">os.path.join(</span>
            <span class="s1">os.path.dirname(__file__)</span><span class="s3">,</span>
            <span class="s4">&quot;../g3doc/primitives_with_limited_support.md.template&quot;</span><span class="s1">)) </span><span class="s3">as </span><span class="s1">f:</span>
      <span class="s1">template = f.read()</span>
    <span class="s1">output_file = os.path.join(</span>
        <span class="s1">os.path.dirname(__file__)</span><span class="s3">,</span>
        <span class="s4">&quot;../g3doc/primitives_with_limited_support.md&quot;</span><span class="s1">)</span>

    <span class="s3">with </span><span class="s1">open(output_file</span><span class="s3">, </span><span class="s4">&quot;w&quot;</span><span class="s1">) </span><span class="s3">as </span><span class="s1">f:</span>
      <span class="s1">f.write(template.replace(</span><span class="s4">&quot;{{generation_date}}&quot;</span><span class="s3">, </span><span class="s1">str(datetime.date.today())) \</span>
              <span class="s1">.replace(</span><span class="s4">&quot;{{tf_error_table}}&quot;</span><span class="s3">, </span><span class="s4">&quot;</span><span class="s3">\n</span><span class="s4">&quot;</span><span class="s1">.join(tf_error_table)) \</span>
              <span class="s1">.replace(</span><span class="s4">&quot;{{tf_numerical_discrepancies_table}}&quot;</span><span class="s3">, </span><span class="s4">&quot;</span><span class="s3">\n</span><span class="s4">&quot;</span><span class="s1">.join(tf_numerical_discrepancies_table)) \</span>
              <span class="s1">)</span>

  <span class="s0"># The rest of the test are checking special cases</span>

  <span class="s1">@parameterized.named_parameters(</span>
      <span class="s1">dict(testcase_name=</span><span class="s4">f&quot;_</span><span class="s3">{</span><span class="s1">f_jax.__name__</span><span class="s3">}</span><span class="s4">&quot;</span><span class="s3">,</span>
           <span class="s1">f_jax=f_jax)</span>
      <span class="s3">for </span><span class="s1">f_jax </span><span class="s3">in </span><span class="s1">[jnp.add</span><span class="s3">, </span><span class="s1">jnp.subtract</span><span class="s3">, </span><span class="s1">jnp.multiply</span><span class="s3">, </span><span class="s1">jnp.divide</span><span class="s3">,</span>
                    <span class="s1">jnp.less</span><span class="s3">, </span><span class="s1">jnp.less_equal</span><span class="s3">, </span><span class="s1">jnp.equal</span><span class="s3">, </span><span class="s1">jnp.greater</span><span class="s3">,</span>
                    <span class="s1">jnp.greater_equal</span><span class="s3">, </span><span class="s1">jnp.not_equal</span><span class="s3">, </span><span class="s1">jnp.maximum</span><span class="s3">,</span>
                    <span class="s1">jnp.minimum])</span>
  <span class="s3">def </span><span class="s1">test_type_promotion(self</span><span class="s3">, </span><span class="s1">f_jax=jnp.add):</span>
    <span class="s0"># We only test a few types here, as tensorflow does not support many</span>
    <span class="s0"># types like uint* or bool in binary ops.</span>
    <span class="s1">types = [dtypes.bfloat16</span><span class="s3">, </span><span class="s1">np.int32</span><span class="s3">, </span><span class="s1">np.int64</span><span class="s3">, </span><span class="s1">np.float32]</span>
    <span class="s3">for </span><span class="s1">x_dtype </span><span class="s3">in </span><span class="s1">types:</span>
      <span class="s3">for </span><span class="s1">y_dtype </span><span class="s3">in </span><span class="s1">types:</span>
        <span class="s1">x = np.array([</span><span class="s5">1</span><span class="s3">, </span><span class="s5">2</span><span class="s1">]</span><span class="s3">, </span><span class="s1">dtype=x_dtype)</span>
        <span class="s1">y = np.array([</span><span class="s5">3</span><span class="s3">, </span><span class="s5">4</span><span class="s1">]</span><span class="s3">, </span><span class="s1">dtype=y_dtype)</span>
        <span class="s1">self.ConvertAndCompare(f_jax</span><span class="s3">, </span><span class="s1">x</span><span class="s3">, </span><span class="s1">y)</span>

  <span class="s3">def </span><span class="s1">test_integer_div(self):</span>
    <span class="s1">x = jnp.array([-</span><span class="s5">4</span><span class="s3">, </span><span class="s1">-</span><span class="s5">3</span><span class="s3">, </span><span class="s1">-</span><span class="s5">1</span><span class="s3">, </span><span class="s5">0</span><span class="s3">, </span><span class="s5">1</span><span class="s3">, </span><span class="s5">3</span><span class="s3">, </span><span class="s5">6</span><span class="s1">])</span>
    <span class="s1">y = np.int32(</span><span class="s5">3</span><span class="s1">)</span>
    <span class="s1">self.ConvertAndCompare(jnp.floor_divide</span><span class="s3">, </span><span class="s1">x</span><span class="s3">, </span><span class="s1">y)</span>
    <span class="s1">expected = jnp.floor_divide(x</span><span class="s3">, </span><span class="s1">y)</span>
    <span class="s3">if not </span><span class="s1">config.jax2tf_default_native_serialization:</span>
      <span class="s0"># With native serialization TF1 seems to want to run the converted code</span>
      <span class="s0"># on the CPU even when the default backend is the TPU.</span>
      <span class="s0"># Try it with TF 1 as well (#5831)</span>
      <span class="s3">with </span><span class="s1">tf.compat.v1.Session() </span><span class="s3">as </span><span class="s1">sess:</span>
        <span class="s1">tf1_res = sess.run(jax2tf.convert(jnp.floor_divide)(x</span><span class="s3">, </span><span class="s1">y))</span>
        <span class="s1">self.assertAllClose(expected</span><span class="s3">, </span><span class="s1">tf1_res)</span>

  <span class="s3">def </span><span class="s1">test_boolean_gather(self):</span>
    <span class="s1">values = np.array([[</span><span class="s3">True, True</span><span class="s1">]</span><span class="s3">, </span><span class="s1">[</span><span class="s3">False, True</span><span class="s1">]</span><span class="s3">, </span><span class="s1">[</span><span class="s3">False, False</span><span class="s1">]]</span><span class="s3">,</span>
                      <span class="s1">dtype=np.bool_)</span>
    <span class="s1">indices = np.array([</span><span class="s5">0</span><span class="s3">, </span><span class="s5">1</span><span class="s1">]</span><span class="s3">, </span><span class="s1">dtype=np.int32)</span>
    <span class="s3">for </span><span class="s1">axis </span><span class="s3">in </span><span class="s1">[</span><span class="s5">0</span><span class="s3">, </span><span class="s5">1</span><span class="s1">]:</span>
      <span class="s1">f_jax = jax.jit(</span><span class="s3">lambda </span><span class="s1">v</span><span class="s3">, </span><span class="s1">i: jnp.take(v</span><span class="s3">, </span><span class="s1">i</span><span class="s3">, </span><span class="s1">axis=axis))  </span><span class="s0"># pylint: disable=cell-var-from-loop</span>
      <span class="s1">self.ConvertAndCompare(f_jax</span><span class="s3">, </span><span class="s1">values</span><span class="s3">, </span><span class="s1">indices)</span>

  <span class="s3">def </span><span class="s1">test_gather_rank_change(self):</span>
    <span class="s1">params = jnp.array([[</span><span class="s5">1.0</span><span class="s3">, </span><span class="s5">1.5</span><span class="s3">, </span><span class="s5">2.0</span><span class="s1">]</span><span class="s3">, </span><span class="s1">[</span><span class="s5">2.0</span><span class="s3">, </span><span class="s5">2.5</span><span class="s3">, </span><span class="s5">3.0</span><span class="s1">]</span><span class="s3">, </span><span class="s1">[</span><span class="s5">3.0</span><span class="s3">, </span><span class="s5">3.5</span><span class="s3">, </span><span class="s5">4.0</span><span class="s1">]])</span>
    <span class="s1">indices = jnp.array([[</span><span class="s5">1</span><span class="s3">, </span><span class="s5">1</span><span class="s3">, </span><span class="s5">2</span><span class="s1">]</span><span class="s3">, </span><span class="s1">[</span><span class="s5">0</span><span class="s3">, </span><span class="s5">1</span><span class="s3">, </span><span class="s5">0</span><span class="s1">]])</span>
    <span class="s1">f_jax = jax.jit(</span><span class="s3">lambda </span><span class="s1">i: params[i])</span>
    <span class="s1">self.ConvertAndCompare(f_jax</span><span class="s3">, </span><span class="s1">indices)</span>

  <span class="s1">@jtu.sample_product(f_jax=REDUCE)</span>
  <span class="s3">def </span><span class="s1">test_reduce_ops_with_numerical_input(self</span><span class="s3">, </span><span class="s1">f_jax):</span>
    <span class="s1">values = np.array([</span><span class="s5">1</span><span class="s3">, </span><span class="s5">2</span><span class="s3">, </span><span class="s5">3</span><span class="s1">]</span><span class="s3">, </span><span class="s1">dtype=np.float32)</span>
    <span class="s1">self.ConvertAndCompare(f_jax</span><span class="s3">, </span><span class="s1">values)</span>

  <span class="s1">@jtu.sample_product(op=[</span><span class="s4">&quot;add&quot;</span><span class="s3">, </span><span class="s4">&quot;max&quot;</span><span class="s3">, </span><span class="s4">&quot;min&quot;</span><span class="s3">, </span><span class="s4">&quot;multiply&quot;</span><span class="s3">, </span><span class="s4">&quot;set&quot;</span><span class="s1">])</span>
  <span class="s3">def </span><span class="s1">test_scatter_static(self</span><span class="s3">, </span><span class="s1">op):</span>
    <span class="s1">values = np.ones((</span><span class="s5">5</span><span class="s3">, </span><span class="s5">6</span><span class="s1">)</span><span class="s3">, </span><span class="s1">dtype=np.float32)</span>
    <span class="s1">update = np.float32(</span><span class="s5">6.</span><span class="s1">)</span>
    <span class="s1">f_jax = jax.jit(</span><span class="s3">lambda </span><span class="s1">v</span><span class="s3">, </span><span class="s1">u: getattr(v.at[::</span><span class="s5">2</span><span class="s3">, </span><span class="s5">3</span><span class="s1">:]</span><span class="s3">, </span><span class="s1">op)(u))</span>
    <span class="s1">self.ConvertAndCompare(f_jax</span><span class="s3">, </span><span class="s1">values</span><span class="s3">, </span><span class="s1">update)</span>

  <span class="s1">@jtu.sample_product(f_jax=REDUCE)</span>
  <span class="s3">def </span><span class="s1">test_reduce_ops_with_boolean_input(self</span><span class="s3">, </span><span class="s1">f_jax):</span>
    <span class="s1">values = np.array([</span><span class="s3">True, False, True</span><span class="s1">]</span><span class="s3">, </span><span class="s1">dtype=np.bool_)</span>
    <span class="s1">self.ConvertAndCompare(f_jax</span><span class="s3">, </span><span class="s1">values)</span>

<span class="s3">if </span><span class="s1">__name__ == </span><span class="s4">&quot;__main__&quot;</span><span class="s1">:</span>
  <span class="s1">absltest.main(testLoader=jtu.JaxTestLoader())</span>
</pre>
</body>
</html>