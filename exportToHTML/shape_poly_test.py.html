<html>
<head>
<title>shape_poly_test.py</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<style type="text/css">
.s0 { color: #808080;}
.s1 { color: #a9b7c6;}
.s2 { color: #629755; font-style: italic;}
.s3 { color: #cc7832;}
.s4 { color: #6a8759;}
.s5 { color: #6897bb;}
</style>
</head>
<body bgcolor="#2b2b2b">
<table CELLSPACING=0 CELLPADDING=5 COLS=1 WIDTH="100%" BGCOLOR="#606060" >
<tr><td><center>
<font face="Arial, Helvetica" color="#000000">
shape_poly_test.py</font>
</center></td></tr></table>
<pre><span class="s0"># Copyright 2020 The JAX Authors.</span>
<span class="s0">#</span>
<span class="s0"># Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);</span>
<span class="s0"># you may not use this file except in compliance with the License.</span>
<span class="s0"># You may obtain a copy of the License at</span>
<span class="s0">#</span>
<span class="s0">#     https://www.apache.org/licenses/LICENSE-2.0</span>
<span class="s0">#</span>
<span class="s0"># Unless required by applicable law or agreed to in writing, software</span>
<span class="s0"># distributed under the License is distributed on an &quot;AS IS&quot; BASIS,</span>
<span class="s0"># WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span>
<span class="s0"># See the License for the specific language governing permissions and</span>
<span class="s0"># limitations under the License.</span>
<span class="s2">&quot;&quot;&quot;Tests for the shape-polymorphic jax2tf conversion.&quot;&quot;&quot;</span>
<span class="s3">import </span><span class="s1">contextlib</span>
<span class="s3">import </span><span class="s1">unittest</span>

<span class="s3">from </span><span class="s1">absl.testing </span><span class="s3">import </span><span class="s1">absltest</span><span class="s3">, </span><span class="s1">parameterized</span>
<span class="s3">from </span><span class="s1">typing </span><span class="s3">import </span><span class="s1">Any</span><span class="s3">, </span><span class="s1">Callable</span><span class="s3">, </span><span class="s1">Dict</span><span class="s3">, </span><span class="s1">List</span><span class="s3">, </span><span class="s1">Optional</span><span class="s3">, </span><span class="s1">Sequence</span><span class="s3">, </span><span class="s1">Tuple</span><span class="s3">, </span><span class="s1">Union</span>

<span class="s3">import </span><span class="s1">collections</span>
<span class="s3">import </span><span class="s1">functools</span>
<span class="s3">from </span><span class="s1">functools </span><span class="s3">import </span><span class="s1">partial</span>
<span class="s3">import </span><span class="s1">operator </span><span class="s3">as </span><span class="s1">op</span>
<span class="s3">import </span><span class="s1">re</span>

<span class="s3">import </span><span class="s1">jax</span>
<span class="s3">from </span><span class="s1">jax </span><span class="s3">import </span><span class="s1">core</span>
<span class="s3">from </span><span class="s1">jax.experimental </span><span class="s3">import </span><span class="s1">jax2tf</span>
<span class="s3">from </span><span class="s1">jax.experimental.jax2tf </span><span class="s3">import </span><span class="s1">shape_poly</span>
<span class="s3">from </span><span class="s1">jax.experimental </span><span class="s3">import </span><span class="s1">pjit</span>
<span class="s3">from </span><span class="s1">jax </span><span class="s3">import </span><span class="s1">lax</span>
<span class="s3">import </span><span class="s1">jax.numpy </span><span class="s3">as </span><span class="s1">jnp</span>
<span class="s3">from </span><span class="s1">jax </span><span class="s3">import </span><span class="s1">random</span>
<span class="s3">from </span><span class="s1">jax._src </span><span class="s3">import </span><span class="s1">test_util </span><span class="s3">as </span><span class="s1">jtu</span>
<span class="s3">from </span><span class="s1">jax._src </span><span class="s3">import </span><span class="s1">util</span>
<span class="s3">from </span><span class="s1">jax._src.lax </span><span class="s3">import </span><span class="s1">lax </span><span class="s3">as </span><span class="s1">lax_internal</span>
<span class="s3">from </span><span class="s1">jax._src.lax </span><span class="s3">import </span><span class="s1">control_flow </span><span class="s3">as </span><span class="s1">lax_control_flow</span>
<span class="s3">from </span><span class="s1">jax._src.lib </span><span class="s3">import </span><span class="s1">xla_client</span>
<span class="s3">import </span><span class="s1">numpy </span><span class="s3">as </span><span class="s1">np</span>

<span class="s3">from </span><span class="s1">jax.experimental.jax2tf.tests </span><span class="s3">import </span><span class="s1">tf_test_util</span>

<span class="s3">import </span><span class="s1">tensorflow </span><span class="s3">as </span><span class="s1">tf  </span><span class="s0"># type: ignore[import]</span>

<span class="s3">from </span><span class="s1">jax.config </span><span class="s3">import </span><span class="s1">config</span>
<span class="s3">from </span><span class="s1">jax._src.config </span><span class="s3">import </span><span class="s1">numpy_dtype_promotion</span>

<span class="s1">config.parse_flags_with_absl()</span>

<span class="s0"># Import after parsing flags</span>
<span class="s3">from </span><span class="s1">jax.experimental.jax2tf.tests </span><span class="s3">import </span><span class="s1">primitive_harness</span>
<span class="s3">from </span><span class="s1">jax.experimental.jax2tf.tests.primitive_harness </span><span class="s3">import </span><span class="s1">Harness</span><span class="s3">, </span><span class="s1">CustomArg</span><span class="s3">, </span><span class="s1">RandArg</span><span class="s3">, </span><span class="s1">StaticArg</span>
<span class="s3">from </span><span class="s1">jax.experimental.jax2tf.tests.jax2tf_limitations </span><span class="s3">import </span><span class="s1">Jax2TfLimitation</span>

<span class="s1">PS = jax2tf.PolyShape</span>
<span class="s1">_f32 = np.float32</span>
<span class="s1">_i32 = np.int32</span>

<span class="s1">expect_error_associative_scan = (</span>
    <span class="s1">(</span><span class="s3">None, None</span><span class="s1">) </span><span class="s3">if </span><span class="s1">(</span><span class="s3">not </span><span class="s1">config.jax2tf_default_native_serialization </span><span class="s3">or</span>
                                         <span class="s1">jtu.device_under_test() == </span><span class="s4">&quot;tpu&quot;</span><span class="s1">) </span><span class="s3">else</span>
    <span class="s1">(NotImplementedError</span><span class="s3">,</span>
     <span class="s4">&quot;associative scan over axis of non-constant size&quot;</span><span class="s1">))</span>


<span class="s3">class </span><span class="s1">DimExprTest(tf_test_util.JaxToTfTestCase):</span>

  <span class="s3">def </span><span class="s1">test_parse_poly_spec(self):</span>
    <span class="s1">self.assertEqual((</span><span class="s5">2</span><span class="s3">, </span><span class="s5">3</span><span class="s1">)</span><span class="s3">, </span><span class="s1">shape_poly._parse_spec(</span><span class="s3">None, </span><span class="s1">(</span><span class="s5">2</span><span class="s3">, </span><span class="s5">3</span><span class="s1">)))</span>
    <span class="s1">self.assertEqual((</span><span class="s5">2</span><span class="s3">, </span><span class="s5">3</span><span class="s1">)</span><span class="s3">, </span><span class="s1">shape_poly._parse_spec(</span><span class="s4">&quot;2, 3&quot;</span><span class="s3">, </span><span class="s1">(</span><span class="s5">2</span><span class="s3">, </span><span class="s5">3</span><span class="s1">)))</span>
    <span class="s1">self.assertEqual((</span><span class="s5">2</span><span class="s3">, </span><span class="s5">3</span><span class="s1">)</span><span class="s3">, </span><span class="s1">shape_poly._parse_spec(</span><span class="s4">&quot;2, _&quot;</span><span class="s3">, </span><span class="s1">(</span><span class="s5">2</span><span class="s3">, </span><span class="s5">3</span><span class="s1">)))</span>
    <span class="s1">self.assertEqual((</span><span class="s5">2</span><span class="s3">, </span><span class="s5">3</span><span class="s1">)</span><span class="s3">, </span><span class="s1">shape_poly._parse_spec(</span><span class="s4">&quot;2, ...&quot;</span><span class="s3">, </span><span class="s1">(</span><span class="s5">2</span><span class="s3">, </span><span class="s5">3</span><span class="s1">)))</span>
    <span class="s1">self.assertEqual((</span><span class="s5">2</span><span class="s3">, </span><span class="s5">3</span><span class="s1">)</span><span class="s3">, </span><span class="s1">shape_poly._parse_spec(</span><span class="s4">&quot;...&quot;</span><span class="s3">, </span><span class="s1">(</span><span class="s5">2</span><span class="s3">, </span><span class="s5">3</span><span class="s1">)))</span>
    <span class="s1">self.assertEqual((</span><span class="s5">2</span><span class="s3">, </span><span class="s5">3</span><span class="s1">)</span><span class="s3">, </span><span class="s1">shape_poly._parse_spec(</span><span class="s4">&quot; ( 2 , 3 ) &quot;</span><span class="s3">, </span><span class="s1">(</span><span class="s5">2</span><span class="s3">, </span><span class="s5">3</span><span class="s1">)))</span>

    <span class="s1">a</span><span class="s3">, </span><span class="s1">b = shape_poly._parse_spec(</span><span class="s4">&quot;a, b&quot;</span><span class="s3">, </span><span class="s1">(</span><span class="s5">2</span><span class="s3">, </span><span class="s5">3</span><span class="s1">))</span>
    <span class="s1">self.assertEqual((a</span><span class="s3">, </span><span class="s5">3</span><span class="s1">)</span><span class="s3">, </span><span class="s1">shape_poly._parse_spec(</span><span class="s4">&quot;(a, ...) &quot;</span><span class="s3">, </span><span class="s1">(</span><span class="s3">None, </span><span class="s5">3</span><span class="s1">)))</span>
    <span class="s1">tshape = tf.TensorShape([</span><span class="s3">None, </span><span class="s5">3</span><span class="s1">])</span>
    <span class="s1">self.assertEqual((a</span><span class="s3">, </span><span class="s5">3</span><span class="s1">)</span><span class="s3">, </span><span class="s1">shape_poly._parse_spec(</span><span class="s4">&quot;(a, ...) &quot;</span><span class="s3">, </span><span class="s1">tshape))</span>

  <span class="s1">a</span><span class="s3">, </span><span class="s1">b = shape_poly._parse_spec(</span><span class="s4">&quot;a, b&quot;</span><span class="s3">, </span><span class="s1">(</span><span class="s5">2</span><span class="s3">, </span><span class="s5">3</span><span class="s1">))</span>
  <span class="s1">@parameterized.named_parameters(</span>
      <span class="s1">dict(testcase_name=</span><span class="s4">f&quot;_</span><span class="s3">{</span><span class="s1">dim_spec=</span><span class="s3">}</span><span class="s4">&quot;</span><span class="s3">,</span>
           <span class="s1">dim_spec=dim_spec</span><span class="s3">, </span><span class="s1">dim_poly=dim_poly)</span>
      <span class="s3">for </span><span class="s1">dim_spec</span><span class="s3">, </span><span class="s1">dim_poly </span><span class="s3">in </span><span class="s1">[</span>
          <span class="s1">(</span><span class="s4">&quot;2*a*b&quot;</span><span class="s3">, </span><span class="s5">2 </span><span class="s1">* a * b)</span><span class="s3">,</span>
          <span class="s1">(</span><span class="s4">&quot;-2 * a^2 * b + b^2&quot;</span><span class="s3">, </span><span class="s1">-</span><span class="s5">2 </span><span class="s1">* a * a * b + b * b)</span><span class="s3">,</span>
          <span class="s1">(</span><span class="s4">&quot;-2 * a^2 * b + -1 *b^2*a&quot;</span><span class="s3">, </span><span class="s1">-</span><span class="s5">2 </span><span class="s1">* a * a * b - a * b * b)</span><span class="s3">,</span>
          <span class="s1">(</span><span class="s4">&quot;3 * a * b * a + -2&quot;</span><span class="s3">, </span><span class="s5">3 </span><span class="s1">* a * b * a - </span><span class="s5">2</span><span class="s1">)</span><span class="s3">,</span>
          <span class="s1">(</span><span class="s4">&quot;a + 1&quot;</span><span class="s3">, </span><span class="s1">a + </span><span class="s5">1</span><span class="s1">)</span><span class="s3">,</span>
          <span class="s1">(</span><span class="s4">&quot;a + -1&quot;</span><span class="s3">, </span><span class="s1">a - </span><span class="s5">1</span><span class="s1">)</span><span class="s3">,</span>
  <span class="s1">])</span>
  <span class="s3">def </span><span class="s1">test_parse_poly_spec_poly(self</span><span class="s3">,</span>
                                <span class="s1">dim_spec=</span><span class="s4">&quot;3 * a * b * a + -2&quot;</span><span class="s3">,</span>
                                <span class="s1">dim_poly=</span><span class="s5">3 </span><span class="s1">* a * b * a - </span><span class="s5">2</span><span class="s1">):</span>
    <span class="s0"># For internal usage only (the polymorphic_shapes of VJP) we need to</span>
    <span class="s0"># parse polynomials.</span>
    <span class="s1">self.assertEqual((dim_poly</span><span class="s3">,</span><span class="s1">)</span><span class="s3">, </span><span class="s1">shape_poly._parse_spec(dim_spec</span><span class="s3">, </span><span class="s1">(</span><span class="s5">2</span><span class="s3">,</span><span class="s1">)))</span>
    <span class="s1">self.assertEqual((dim_poly</span><span class="s3">,</span><span class="s1">)</span><span class="s3">, </span><span class="s1">shape_poly._parse_spec(str(dim_poly)</span><span class="s3">, </span><span class="s1">(</span><span class="s5">2</span><span class="s3">,</span><span class="s1">)))</span>

  <span class="s1">@parameterized.named_parameters(</span>
      <span class="s1">dict(testcase_name=</span><span class="s4">f&quot;_</span><span class="s3">{</span><span class="s1">dim_spec=</span><span class="s3">}</span><span class="s4">&quot;</span><span class="s3">,</span>
           <span class="s1">dim_spec=dim_spec</span><span class="s3">, </span><span class="s1">dim_poly=dim_poly)</span>
      <span class="s3">for </span><span class="s1">dim_spec</span><span class="s3">, </span><span class="s1">dim_poly </span><span class="s3">in </span><span class="s1">[</span>
          <span class="s1">(</span><span class="s4">&quot;2*a*b&quot;</span><span class="s3">, </span><span class="s5">2 </span><span class="s1">* a * b)</span><span class="s3">,</span>
          <span class="s1">(</span><span class="s4">&quot;-2 * a^2 * b + b^2&quot;</span><span class="s3">, </span><span class="s1">-</span><span class="s5">2 </span><span class="s1">* a * a * b + b * b)</span><span class="s3">,</span>
          <span class="s1">(</span><span class="s4">&quot;-2 * a^2 * b + -1 *b^2*a&quot;</span><span class="s3">, </span><span class="s1">-</span><span class="s5">2 </span><span class="s1">* a * a * b - a * b * b)</span><span class="s3">,</span>
          <span class="s1">(</span><span class="s4">&quot;3 * a * b * a + -2&quot;</span><span class="s3">, </span><span class="s5">3 </span><span class="s1">* a * b * a - </span><span class="s5">2</span><span class="s1">)</span><span class="s3">,</span>
          <span class="s1">(</span><span class="s4">&quot;a + 1&quot;</span><span class="s3">, </span><span class="s1">a + </span><span class="s5">1</span><span class="s1">)</span><span class="s3">,</span>
          <span class="s1">(</span><span class="s4">&quot;a + -1&quot;</span><span class="s3">, </span><span class="s1">a - </span><span class="s5">1</span><span class="s1">)</span><span class="s3">,</span>
  <span class="s1">])</span>
  <span class="s3">def </span><span class="s1">test_parse_poly_spec_shapeenv(self</span><span class="s3">,</span>
                                <span class="s1">dim_spec=</span><span class="s4">&quot;3 * a * b * a + -2&quot;</span><span class="s3">,</span>
                                <span class="s1">dim_poly=</span><span class="s5">3 </span><span class="s1">* a * b * a - </span><span class="s5">2</span><span class="s1">):</span>
    <span class="s0"># For internal usage only (the polymorphic_shapes of VJP) we need to</span>
    <span class="s0"># parse polynomials.</span>
    <span class="s1">self.assertEqual((dim_poly</span><span class="s3">,</span><span class="s1">)</span><span class="s3">, </span><span class="s1">shape_poly._parse_spec(dim_spec</span><span class="s3">, </span><span class="s1">(</span><span class="s5">2</span><span class="s3">,</span><span class="s1">)))</span>
    <span class="s1">self.assertEqual((dim_poly</span><span class="s3">,</span><span class="s1">)</span><span class="s3">, </span><span class="s1">shape_poly._parse_spec(str(dim_poly)</span><span class="s3">, </span><span class="s1">(</span><span class="s5">2</span><span class="s3">,</span><span class="s1">)))</span>

  <span class="s3">def </span><span class="s1">test_dim_vars(self):</span>
    <span class="s1">a</span><span class="s3">, </span><span class="s1">b</span><span class="s3">, </span><span class="s1">a1 = shape_poly._parse_spec(</span><span class="s4">&quot;a, b, a&quot;</span><span class="s3">, </span><span class="s1">(</span><span class="s5">2</span><span class="s3">, </span><span class="s5">3</span><span class="s3">, </span><span class="s5">2</span><span class="s1">))</span>
    <span class="s1">self.assertEqual(</span><span class="s3">True, </span><span class="s1">a == a)</span>
    <span class="s1">self.assertEqual(</span><span class="s3">True, </span><span class="s1">a == a1)</span>
    <span class="s1">self.assertEqual(</span><span class="s3">False, </span><span class="s1">a != a)</span>

    <span class="s1">self.assertFalse(a == b)</span>
    <span class="s1">self.assertTrue(a != b)</span>

    <span class="s1">self.assertLen({a</span><span class="s3">, </span><span class="s1">a}</span><span class="s3">, </span><span class="s5">1</span><span class="s1">)</span>
    <span class="s1">self.assertLen({a</span><span class="s3">, </span><span class="s1">b}</span><span class="s3">, </span><span class="s5">2</span><span class="s1">)</span>
    <span class="s1">self.assertIn(a</span><span class="s3">, </span><span class="s1">{a</span><span class="s3">, </span><span class="s1">b})</span>
    <span class="s1">self.assertIn(b</span><span class="s3">, </span><span class="s1">{a</span><span class="s3">, </span><span class="s1">b})</span>
    <span class="s1">self.assertIn(a</span><span class="s3">, </span><span class="s1">[a</span><span class="s3">, </span><span class="s1">b])</span>
    <span class="s1">self.assertIn(b</span><span class="s3">, </span><span class="s1">[a</span><span class="s3">, </span><span class="s1">b])</span>

  <span class="s3">def </span><span class="s1">test_get_vars(self):</span>
    <span class="s1">a</span><span class="s3">, </span><span class="s1">b = shape_poly._parse_spec(</span><span class="s4">&quot;a, b&quot;</span><span class="s3">, </span><span class="s1">(</span><span class="s5">2</span><span class="s3">, </span><span class="s5">3</span><span class="s1">))</span>

    <span class="s1">self.assertEqual({</span><span class="s4">&quot;a&quot;</span><span class="s1">}</span><span class="s3">, </span><span class="s1">a.get_vars())</span>
    <span class="s1">self.assertEqual({</span><span class="s4">&quot;a&quot;</span><span class="s3">, </span><span class="s4">&quot;b&quot;</span><span class="s1">}</span><span class="s3">, </span><span class="s1">(a * b * a).get_vars())</span>

  <span class="s3">def </span><span class="s1">test_evaluate(self):</span>
    <span class="s1">a</span><span class="s3">, </span><span class="s1">b = shape_poly._parse_spec(</span><span class="s4">&quot;a, b&quot;</span><span class="s3">, </span><span class="s1">(</span><span class="s5">2</span><span class="s3">, </span><span class="s5">3</span><span class="s1">))</span>

    <span class="s1">self.assertEqual(</span><span class="s5">1</span><span class="s3">, </span><span class="s1">(a * a - b).evaluate(dict(a=</span><span class="s5">2</span><span class="s3">, </span><span class="s1">b=</span><span class="s5">3</span><span class="s1">)))</span>
    <span class="s1">self.assertEqual(</span><span class="s5">1</span><span class="s3">, </span><span class="s1">((a * a) // b).evaluate(dict(a=</span><span class="s5">2</span><span class="s3">, </span><span class="s1">b=</span><span class="s5">3</span><span class="s1">)))</span>
    <span class="s1">self.assertEqual(</span><span class="s5">4</span><span class="s3">, </span><span class="s1">((a * a) % b).evaluate(dict(a=</span><span class="s5">5</span><span class="s3">, </span><span class="s1">b=</span><span class="s5">7</span><span class="s1">)))</span>

  <span class="s3">def </span><span class="s1">test_dim_vars_symbolic_equal(self):</span>
    <span class="s1">a</span><span class="s3">, </span><span class="s1">b = shape_poly._parse_spec(</span><span class="s4">&quot;a, b&quot;</span><span class="s3">, </span><span class="s1">(</span><span class="s5">2</span><span class="s3">, </span><span class="s5">3</span><span class="s1">))</span>
    <span class="s1">self.assertTrue(core.symbolic_equal_dim(a</span><span class="s3">, </span><span class="s1">a))</span>
    <span class="s1">self.assertFalse(core.symbolic_equal_dim(a</span><span class="s3">, </span><span class="s5">1</span><span class="s1">))</span>
    <span class="s1">self.assertFalse(core.symbolic_equal_dim(a</span><span class="s3">, </span><span class="s1">b))</span>

    <span class="s1">self.assertTrue(core.symbolic_equal_one_of_dim(a</span><span class="s3">, </span><span class="s1">[</span><span class="s5">2</span><span class="s3">, </span><span class="s1">a]))</span>
    <span class="s1">self.assertFalse(core.symbolic_equal_one_of_dim(a</span><span class="s3">, </span><span class="s1">[</span><span class="s5">2</span><span class="s3">, </span><span class="s1">b]))</span>
    <span class="s1">self.assertFalse(core.symbolic_equal_one_of_dim(a</span><span class="s3">, </span><span class="s1">[]))</span>

    <span class="s1">self.assertTrue(core.symbolic_equal_one_of_dim(</span><span class="s5">2</span><span class="s3">, </span><span class="s1">[a</span><span class="s3">, </span><span class="s5">3</span><span class="s3">, </span><span class="s5">2</span><span class="s1">]))</span>
    <span class="s1">self.assertFalse(core.symbolic_equal_one_of_dim(</span><span class="s5">1</span><span class="s3">, </span><span class="s1">[</span><span class="s5">2</span><span class="s3">, </span><span class="s1">b]))</span>
    <span class="s1">self.assertFalse(core.symbolic_equal_one_of_dim(</span><span class="s5">3</span><span class="s3">, </span><span class="s1">[]))</span>

    <span class="s1">self.assertTrue(core.symbolic_equal_dim(</span><span class="s5">1</span><span class="s3">, </span><span class="s1">jnp.add(</span><span class="s5">0</span><span class="s3">, </span><span class="s5">1</span><span class="s1">)))  </span><span class="s0"># A DeviceArray</span>
    <span class="s3">with </span><span class="s1">self.assertRaisesRegex(TypeError</span><span class="s3">,</span>
                                <span class="s1">re.escape(</span><span class="s4">&quot;Shapes must be 1D sequences of concrete values of integer type, got (1, 'a').&quot;</span><span class="s1">)):</span>
      <span class="s1">self.assertTrue(core.symbolic_equal_dim(</span><span class="s5">1</span><span class="s3">, </span><span class="s4">&quot;a&quot;</span><span class="s1">))</span>

  <span class="s3">def </span><span class="s1">test_poly_bounds(self):</span>
    <span class="s1">a</span><span class="s3">, </span><span class="s1">b = shape_poly._parse_spec(</span><span class="s4">&quot;a, b&quot;</span><span class="s3">, </span><span class="s1">(</span><span class="s5">2</span><span class="s3">, </span><span class="s5">3</span><span class="s1">))</span>
    <span class="s1">bounded_le4 = </span><span class="s5">5 </span><span class="s1">- a</span>
    <span class="s1">bounded_ge2 = b + </span><span class="s5">1</span>
    <span class="s1">bounded_ge0_le4 = a % </span><span class="s5">5</span>
    <span class="s1">self.assertEqual(a.bounds()</span><span class="s3">, </span><span class="s1">(</span><span class="s5">1</span><span class="s3">, </span><span class="s1">np.PINF))</span>
    <span class="s1">self.assertEqual(bounded_le4.bounds()</span><span class="s3">, </span><span class="s1">(np.NINF</span><span class="s3">, </span><span class="s5">4</span><span class="s1">))</span>
    <span class="s1">self.assertEqual(bounded_ge2.bounds()</span><span class="s3">, </span><span class="s1">(</span><span class="s5">2</span><span class="s3">, </span><span class="s1">np.PINF))</span>
    <span class="s1">self.assertEqual(bounded_ge0_le4.bounds()</span><span class="s3">, </span><span class="s1">(</span><span class="s5">0</span><span class="s3">, </span><span class="s5">4</span><span class="s1">))</span>

    <span class="s0"># Additions</span>
    <span class="s1">self.assertEqual((bounded_ge0_le4 + bounded_le4).bounds()</span><span class="s3">, </span><span class="s1">(np.NINF</span><span class="s3">, </span><span class="s5">8</span><span class="s1">))</span>
    <span class="s1">self.assertEqual((bounded_ge0_le4 + bounded_ge2).bounds()</span><span class="s3">, </span><span class="s1">(</span><span class="s5">2</span><span class="s3">, </span><span class="s1">np.PINF))</span>
    <span class="s1">self.assertEqual((bounded_le4 + bounded_ge2).bounds()</span><span class="s3">, </span><span class="s1">(np.NINF</span><span class="s3">, </span><span class="s1">np.PINF))</span>

    <span class="s0"># Subtractions</span>
    <span class="s1">self.assertEqual((bounded_ge0_le4 - bounded_le4).bounds()</span><span class="s3">, </span><span class="s1">(-</span><span class="s5">4</span><span class="s3">, </span><span class="s1">np.PINF))</span>
    <span class="s1">self.assertEqual((- bounded_ge0_le4 + bounded_le4).bounds()</span><span class="s3">, </span><span class="s1">(np.NINF</span><span class="s3">, </span><span class="s5">4</span><span class="s1">))</span>
    <span class="s1">self.assertEqual((bounded_ge0_le4 - bounded_ge2).bounds()</span><span class="s3">, </span><span class="s1">(np.NINF</span><span class="s3">, </span><span class="s5">2</span><span class="s1">))</span>
    <span class="s1">self.assertEqual((- bounded_ge0_le4 + bounded_ge2).bounds()</span><span class="s3">, </span><span class="s1">(-</span><span class="s5">2</span><span class="s3">, </span><span class="s1">np.PINF))</span>
    <span class="s1">self.assertEqual((bounded_le4 - bounded_ge2).bounds()</span><span class="s3">, </span><span class="s1">(np.NINF</span><span class="s3">, </span><span class="s5">2</span><span class="s1">))</span>
    <span class="s1">self.assertEqual((- bounded_le4 + bounded_ge2).bounds()</span><span class="s3">, </span><span class="s1">(-</span><span class="s5">2</span><span class="s3">, </span><span class="s1">np.PINF))</span>

    <span class="s0"># Multiplications</span>
    <span class="s1">self.assertEqual((</span><span class="s5">2 </span><span class="s1">* a - </span><span class="s5">3</span><span class="s1">).bounds()</span><span class="s3">, </span><span class="s1">(-</span><span class="s5">1</span><span class="s3">, </span><span class="s1">np.PINF))</span>
    <span class="s1">self.assertEqual((-</span><span class="s5">2 </span><span class="s1">* a - </span><span class="s5">3</span><span class="s1">).bounds()</span><span class="s3">, </span><span class="s1">(np.NINF</span><span class="s3">, </span><span class="s1">-</span><span class="s5">5</span><span class="s1">))</span>
    <span class="s1">self.assertEqual((</span><span class="s5">3 </span><span class="s1">* a * b * b + </span><span class="s5">5 </span><span class="s1">* a - </span><span class="s5">7</span><span class="s1">).bounds()</span><span class="s3">, </span><span class="s1">(</span><span class="s5">1</span><span class="s3">, </span><span class="s1">np.PINF))</span>
    <span class="s1">self.assertEqual((</span><span class="s5">3 </span><span class="s1">* a * b * b - </span><span class="s5">5 </span><span class="s1">* a - </span><span class="s5">7</span><span class="s1">).bounds()</span><span class="s3">, </span><span class="s1">(np.NINF</span><span class="s3">, </span><span class="s1">np.PINF))</span>
    <span class="s1">self.assertEqual((a + b - a * b + a * b * a).bounds()</span><span class="s3">, </span><span class="s1">(np.NINF</span><span class="s3">, </span><span class="s1">np.PINF))</span>
    <span class="s1">self.assertEqual((a + </span><span class="s5">2 </span><span class="s1">* b - a).bounds()</span><span class="s3">, </span><span class="s1">(</span><span class="s5">2</span><span class="s3">, </span><span class="s1">np.PINF))</span>
    <span class="s1">self.assertEqual((a + </span><span class="s5">2 </span><span class="s1">* b - a).bounds()</span><span class="s3">, </span><span class="s1">(</span><span class="s5">2</span><span class="s3">, </span><span class="s1">np.PINF))</span>

    <span class="s0"># mod</span>
    <span class="s1">self.assertEqual(((b + </span><span class="s5">1</span><span class="s1">) % </span><span class="s5">2</span><span class="s1">).bounds()</span><span class="s3">, </span><span class="s1">(</span><span class="s5">0</span><span class="s3">, </span><span class="s5">1</span><span class="s1">))</span>
    <span class="s1">self.assertEqual(((b + </span><span class="s5">1</span><span class="s1">) % -</span><span class="s5">2</span><span class="s1">).bounds()</span><span class="s3">, </span><span class="s1">(-</span><span class="s5">1</span><span class="s3">, </span><span class="s5">0</span><span class="s1">))</span>
    <span class="s1">self.assertEqual(((b - </span><span class="s5">4</span><span class="s1">) % </span><span class="s5">2</span><span class="s1">).bounds()</span><span class="s3">, </span><span class="s1">(</span><span class="s5">0</span><span class="s3">, </span><span class="s5">1</span><span class="s1">))</span>
    <span class="s1">self.assertEqual(((b + </span><span class="s5">1</span><span class="s1">) % a).bounds()</span><span class="s3">, </span><span class="s1">(</span><span class="s5">0</span><span class="s3">, </span><span class="s1">np.PINF))</span>
    <span class="s1">self.assertEqual((</span><span class="s5">11 </span><span class="s1">% (a + </span><span class="s5">1</span><span class="s1">)).bounds()</span><span class="s3">, </span><span class="s1">(</span><span class="s5">0</span><span class="s3">, </span><span class="s1">np.PINF))</span>
    <span class="s1">self.assertEqual((-</span><span class="s5">11 </span><span class="s1">% (a + </span><span class="s5">1</span><span class="s1">)).bounds()</span><span class="s3">, </span><span class="s1">(</span><span class="s5">0</span><span class="s3">, </span><span class="s1">np.PINF))</span>
    <span class="s1">self.assertEqual((b % (a - </span><span class="s5">2</span><span class="s1">)).bounds()</span><span class="s3">, </span><span class="s1">(np.NINF</span><span class="s3">, </span><span class="s1">np.PINF))</span>

    <span class="s0"># floordiv</span>
    <span class="s1">self.assertEqual(((a + </span><span class="s5">4</span><span class="s1">) // </span><span class="s5">2</span><span class="s1">).bounds()</span><span class="s3">, </span><span class="s1">(</span><span class="s5">2</span><span class="s3">, </span><span class="s1">np.PINF))</span>
    <span class="s1">self.assertEqual(((a + </span><span class="s5">4</span><span class="s1">) // -</span><span class="s5">2</span><span class="s1">).bounds()</span><span class="s3">, </span><span class="s1">(np.NINF</span><span class="s3">, </span><span class="s1">-</span><span class="s5">3</span><span class="s1">))</span>
    <span class="s1">self.assertEqual(((a + </span><span class="s5">5</span><span class="s1">) // </span><span class="s5">2</span><span class="s1">).bounds()</span><span class="s3">, </span><span class="s1">(</span><span class="s5">3</span><span class="s3">, </span><span class="s1">np.PINF))</span>
    <span class="s1">self.assertEqual(((a + </span><span class="s5">5</span><span class="s1">) // -</span><span class="s5">2</span><span class="s1">).bounds()</span><span class="s3">, </span><span class="s1">(np.NINF</span><span class="s3">, </span><span class="s1">-</span><span class="s5">3</span><span class="s1">))</span>
    <span class="s1">self.assertEqual((</span><span class="s5">11 </span><span class="s1">// (a + </span><span class="s5">1</span><span class="s1">)).bounds()</span><span class="s3">, </span><span class="s1">(</span><span class="s5">0</span><span class="s3">, </span><span class="s5">5</span><span class="s1">))</span>
    <span class="s1">self.assertEqual((-</span><span class="s5">11 </span><span class="s1">// (a + </span><span class="s5">1</span><span class="s1">)).bounds()</span><span class="s3">, </span><span class="s1">(-</span><span class="s5">6</span><span class="s3">, </span><span class="s1">-</span><span class="s5">1</span><span class="s1">))</span>
    <span class="s1">self.assertEqual((-</span><span class="s5">11 </span><span class="s1">// (- a)).bounds()</span><span class="s3">, </span><span class="s1">(</span><span class="s5">0</span><span class="s3">, </span><span class="s5">11</span><span class="s1">))  </span><span class="s0"># finite negative dividend, infinite divisor</span>
    <span class="s1">self.assertEqual(((b + </span><span class="s5">1</span><span class="s1">) // (a + </span><span class="s5">1</span><span class="s1">)).bounds()</span><span class="s3">, </span><span class="s1">(</span><span class="s5">0</span><span class="s3">, </span><span class="s1">np.PINF))</span>
    <span class="s1">self.assertEqual((-b // (a + </span><span class="s5">1</span><span class="s1">)).bounds()</span><span class="s3">, </span><span class="s1">(np.NINF</span><span class="s3">, </span><span class="s1">-</span><span class="s5">1</span><span class="s1">))</span>




    <span class="s0"># Generate test cases for floordiv and mod: (a + N) // +-2, (N - a) // +-2</span>
    <span class="s0"># and then evaluate them for a = 1, 5, 10000</span>
    <span class="s1">div_mod_atoms = [</span>
        <span class="s1">operation(op1 + n</span><span class="s3">, </span><span class="s1">div)</span>
        <span class="s3">for </span><span class="s1">op1 </span><span class="s3">in </span><span class="s1">(a</span><span class="s3">, </span><span class="s1">a + </span><span class="s5">10</span><span class="s3">, </span><span class="s1">a + </span><span class="s5">11</span><span class="s3">, </span><span class="s1">-a</span><span class="s3">, </span><span class="s1">-a + </span><span class="s5">10</span><span class="s3">, </span><span class="s1">-a + </span><span class="s5">11</span><span class="s1">)</span>
        <span class="s3">for </span><span class="s1">n </span><span class="s3">in </span><span class="s1">(-</span><span class="s5">3</span><span class="s3">, </span><span class="s1">-</span><span class="s5">1</span><span class="s3">, </span><span class="s5">0</span><span class="s3">, </span><span class="s5">1</span><span class="s3">, </span><span class="s5">3</span><span class="s1">)</span>
        <span class="s3">for </span><span class="s1">div </span><span class="s3">in </span><span class="s1">(-</span><span class="s5">2</span><span class="s3">, </span><span class="s5">2</span><span class="s3">, </span><span class="s1">a + </span><span class="s5">4</span><span class="s3">, </span><span class="s1">-</span><span class="s5">4 </span><span class="s1">- a)  </span><span class="s0"># Either negative, or positive</span>
        <span class="s3">for </span><span class="s1">operation </span><span class="s3">in </span><span class="s1">(op.floordiv</span><span class="s3">, </span><span class="s1">op.mod)</span>
        <span class="s1">]</span>
    <span class="s3">for </span><span class="s1">atom </span><span class="s3">in </span><span class="s1">div_mod_atoms:</span>
      <span class="s1">lb</span><span class="s3">, </span><span class="s1">ub = atom.bounds()</span>
      <span class="s1">self.assertLessEqual(lb</span><span class="s3">, </span><span class="s1">ub)</span>
      <span class="s3">for </span><span class="s1">a_val </span><span class="s3">in </span><span class="s1">(</span><span class="s5">1</span><span class="s3">, </span><span class="s5">5</span><span class="s3">, </span><span class="s5">10000</span><span class="s1">):</span>
        <span class="s1">atom_val = atom.evaluate(dict(a=a_val))</span>
        <span class="s1">self.assertGreaterEqual(atom_val</span><span class="s3">, </span><span class="s1">lb)</span>
        <span class="s1">self.assertLessEqual(atom_val</span><span class="s3">, </span><span class="s1">ub)</span>

  <span class="s3">def </span><span class="s1">test_poly_equal(self):</span>
    <span class="s1">a</span><span class="s3">, </span><span class="s1">b = shape_poly._parse_spec(</span><span class="s4">&quot;a, b&quot;</span><span class="s3">, </span><span class="s1">(</span><span class="s5">2</span><span class="s3">, </span><span class="s5">3</span><span class="s1">))</span>
    <span class="s1">poly3 = a + </span><span class="s5">3 </span><span class="s1">- a</span>
    <span class="s1">self.assertTrue(poly3 == </span><span class="s5">3</span><span class="s1">)</span>
    <span class="s1">self.assertTrue(poly3 == np.array(</span><span class="s5">3</span><span class="s3">, </span><span class="s1">np.int64))</span>
    <span class="s1">self.assertTrue(poly3 == np.array(</span><span class="s5">3</span><span class="s3">, </span><span class="s1">np.int64)[()])</span>
    <span class="s1">self.assertFalse((poly3 + </span><span class="s5">1</span><span class="s1">) == </span><span class="s5">3</span><span class="s1">)</span>
    <span class="s1">self.assertFalse(poly3 == poly3 + </span><span class="s5">1</span><span class="s1">)</span>
    <span class="s1">self.assertTrue((</span><span class="s5">2 </span><span class="s1">* a * b * a + </span><span class="s5">3</span><span class="s1">).eq(</span><span class="s5">1 </span><span class="s1">+ b * a * a + a * a * b + </span><span class="s5">2</span><span class="s1">))</span>
    <span class="s1">self.assertFalse((</span><span class="s5">2 </span><span class="s1">* a * b * a + </span><span class="s5">3</span><span class="s1">).eq(a * b * a + </span><span class="s5">3</span><span class="s1">))</span>

    <span class="s1">self.assertFalse((a * b * a + </span><span class="s5">3</span><span class="s1">).eq(a * b * a + </span><span class="s5">4</span><span class="s1">))</span>
    <span class="s1">self.assertFalse((</span><span class="s5">2 </span><span class="s1">* a * b * a).eq(a * b * a))</span>
    <span class="s1">self.assertFalse((</span><span class="s5">2 </span><span class="s1">* a * b * a + </span><span class="s5">1</span><span class="s1">).eq(a * b * a))</span>
    <span class="s1">self.assertFalse((</span><span class="s5">3 </span><span class="s1">* a * b * a - </span><span class="s5">1</span><span class="s1">).eq(a * b * a))</span>

    <span class="s1">self.assertFalse((</span><span class="s5">3 </span><span class="s1">* a * b * a - </span><span class="s5">2</span><span class="s1">).eq(a * b * a))</span>

    <span class="s1">self.assertTrue(a % b == a % b)</span>
    <span class="s1">self.assertTrue(a % b - a % b == </span><span class="s5">0</span><span class="s1">)</span>
    <span class="s1">self.assertTrue(a // b == a // b)</span>
    <span class="s1">self.assertTrue(a // b - a // b == </span><span class="s5">0</span><span class="s1">)</span>

    <span class="s1">self.assertTrue(a % b == (</span><span class="s5">2 </span><span class="s1">* a // </span><span class="s5">2</span><span class="s1">) % (a + b - a))</span>
    <span class="s1">self.assertTrue(a // b == (</span><span class="s5">2 </span><span class="s1">* a // </span><span class="s5">2</span><span class="s1">) // (a + b - a))</span>

    <span class="s1">self.assertTrue(a</span><span class="s3">, </span><span class="s1">a + (a + b) // b - (b + a) // b)</span>

    <span class="s0"># Test the normalization (a // b) * b == a - a % b</span>
    <span class="s1">self.assertTrue((a // </span><span class="s5">2</span><span class="s1">) * </span><span class="s5">2 </span><span class="s1">== a - a % </span><span class="s5">2</span><span class="s1">)</span>
    <span class="s1">self.assertTrue((a // </span><span class="s5">2</span><span class="s1">) + (a // </span><span class="s5">2</span><span class="s1">) == a - a % </span><span class="s5">2</span><span class="s1">)</span>
    <span class="s1">self.assertTrue((a // </span><span class="s5">2</span><span class="s1">) * </span><span class="s5">6 </span><span class="s1">== </span><span class="s5">3 </span><span class="s1">* a - </span><span class="s5">3 </span><span class="s1">* (a % </span><span class="s5">2</span><span class="s1">))</span>
    <span class="s1">self.assertTrue((a // b) * b == a - a % b)</span>
    <span class="s1">self.assertTrue(</span><span class="s5">2 </span><span class="s1">* (a // b) * b * b == </span><span class="s5">2 </span><span class="s1">* b * a - </span><span class="s5">2 </span><span class="s1">* b * (a % b))</span>
    <span class="s1">self.assertTrue(a // (</span><span class="s5">2 </span><span class="s1">* b) * </span><span class="s5">2 </span><span class="s1">* b == a - a % (</span><span class="s5">2 </span><span class="s1">* b))</span>

  <span class="s3">def </span><span class="s1">test_poly_compare(self):</span>
    <span class="s1">a</span><span class="s3">, </span><span class="s1">b = shape_poly._parse_spec(</span><span class="s4">&quot;a, b&quot;</span><span class="s3">, </span><span class="s1">(</span><span class="s5">2</span><span class="s3">, </span><span class="s5">3</span><span class="s1">))</span>
    <span class="s1">poly = </span><span class="s5">4 </span><span class="s1">* a + b + </span><span class="s5">3</span>
    <span class="s1">self.assertTrue(poly.ge(</span><span class="s5">0</span><span class="s1">))</span>
    <span class="s1">self.assertTrue(poly.ge(</span><span class="s5">8</span><span class="s1">))</span>
    <span class="s1">self.assertTrue(poly.ge(poly))</span>
    <span class="s1">self.assertTrue(poly.ge(poly - </span><span class="s5">1</span><span class="s1">))</span>

    <span class="s3">with </span><span class="s1">self.assertRaisesRegex(core.InconclusiveDimensionOperation</span><span class="s3">, </span><span class="s4">&quot;inconclusive&quot;</span><span class="s1">):</span>
      <span class="s1">poly.ge(</span><span class="s5">9</span><span class="s1">)</span>

    <span class="s3">with </span><span class="s1">self.assertRaisesRegex(core.InconclusiveDimensionOperation</span><span class="s3">, </span><span class="s4">&quot;inconclusive&quot;</span><span class="s1">):</span>
      <span class="s1">(</span><span class="s5">4 </span><span class="s1">* a - b).ge(</span><span class="s5">0</span><span class="s1">)</span>

  <span class="s3">def </span><span class="s1">test_poly_compare_overload(self):</span>
    <span class="s1">a</span><span class="s3">, </span><span class="s1">b = shape_poly._parse_spec(</span><span class="s4">&quot;a, b&quot;</span><span class="s3">, </span><span class="s1">(</span><span class="s5">2</span><span class="s3">, </span><span class="s5">3</span><span class="s1">))</span>
    <span class="s1">poly = </span><span class="s5">4 </span><span class="s1">* a + b + </span><span class="s5">3</span>
    <span class="s1">self.assertTrue(poly &gt;= </span><span class="s5">0</span><span class="s1">)</span>
    <span class="s1">self.assertTrue(poly &gt;= </span><span class="s5">8</span><span class="s1">)</span>
    <span class="s1">self.assertTrue(poly &gt; </span><span class="s5">7</span><span class="s1">)</span>
    <span class="s1">self.assertTrue(poly &gt;= poly)</span>
    <span class="s1">self.assertTrue(poly &gt;= poly - </span><span class="s5">1</span><span class="s1">)</span>

    <span class="s3">with </span><span class="s1">self.assertRaisesRegex(core.InconclusiveDimensionOperation</span><span class="s3">, </span><span class="s4">&quot;inconclusive&quot;</span><span class="s1">):</span>
      <span class="s1">poly &gt;= </span><span class="s5">9</span>

    <span class="s3">with </span><span class="s1">self.assertRaisesRegex(core.InconclusiveDimensionOperation</span><span class="s3">, </span><span class="s4">&quot;inconclusive&quot;</span><span class="s1">):</span>
      <span class="s1">(</span><span class="s5">4 </span><span class="s1">* a - b) &gt;= </span><span class="s5">0</span>

  <span class="s3">def </span><span class="s1">test_core_greater_equal(self):</span>
    <span class="s1">a</span><span class="s3">, </span><span class="s1">b = shape_poly._parse_spec(</span><span class="s4">&quot;a, b&quot;</span><span class="s3">, </span><span class="s1">(</span><span class="s5">2</span><span class="s3">, </span><span class="s5">3</span><span class="s1">))</span>
    <span class="s1">self.assertTrue(core.greater_equal_dim(a</span><span class="s3">, </span><span class="s1">a))</span>
    <span class="s1">self.assertTrue(core.greater_equal_dim(a</span><span class="s3">, </span><span class="s5">0</span><span class="s1">))</span>
    <span class="s1">self.assertTrue(core.greater_equal_dim(a</span><span class="s3">, </span><span class="s5">1</span><span class="s1">))</span>

    <span class="s1">self.assertTrue(core.greater_equal_shape((a</span><span class="s3">, </span><span class="s5">2</span><span class="s1">)</span><span class="s3">, </span><span class="s1">(</span><span class="s5">1</span><span class="s3">, </span><span class="s5">1</span><span class="s1">)))</span>

    <span class="s3">with </span><span class="s1">self.assertRaisesRegex(core.InconclusiveDimensionOperation</span><span class="s3">,</span>
                                <span class="s4">&quot;Symbolic dimension comparison .* is inconclusive&quot;</span><span class="s1">):</span>
      <span class="s1">core.greater_equal_dim(a</span><span class="s3">, </span><span class="s5">2</span><span class="s1">)</span>

    <span class="s3">with </span><span class="s1">self.assertRaisesRegex(core.InconclusiveDimensionOperation</span><span class="s3">,</span>
                                <span class="s4">&quot;Symbolic dimension comparison .* is inconclusive&quot;</span><span class="s1">):</span>
      <span class="s1">core.greater_equal_dim(a</span><span class="s3">, </span><span class="s1">b)</span>

  <span class="s3">def </span><span class="s1">test_poly_int_results(self):</span>
    <span class="s0"># Whenever the result is an integer, it should be represented as an</span>
    <span class="s0"># Python integer, not a symbolic dimension.</span>
    <span class="s1">a</span><span class="s3">, </span><span class="s1">b = shape_poly._parse_spec(</span><span class="s4">&quot;a, b&quot;</span><span class="s3">, </span><span class="s1">(</span><span class="s5">2</span><span class="s3">, </span><span class="s5">3</span><span class="s1">))</span>
    <span class="s1">self.assertEqual(a + </span><span class="s5">2 </span><span class="s1">- a</span><span class="s3">, </span><span class="s5">2</span><span class="s1">)</span>
    <span class="s1">self.assertIsInstance(a + </span><span class="s5">2 </span><span class="s1">- a</span><span class="s3">, </span><span class="s1">int)</span>
    <span class="s1">self.assertEqual(a + (</span><span class="s5">2 </span><span class="s1">- a)</span><span class="s3">, </span><span class="s5">2</span><span class="s1">)</span>
    <span class="s1">self.assertIsInstance(a + (</span><span class="s5">2 </span><span class="s1">- a)</span><span class="s3">, </span><span class="s1">int)</span>
    <span class="s1">self.assertEqual(a * </span><span class="s5">2 </span><span class="s1">// a</span><span class="s3">, </span><span class="s5">2</span><span class="s1">)</span>
    <span class="s1">self.assertIsInstance(a * </span><span class="s5">2 </span><span class="s1">// a</span><span class="s3">, </span><span class="s1">int)</span>

  <span class="s1">@parameterized.named_parameters(</span>
      <span class="s1">dict(testcase_name=</span><span class="s4">f&quot;_D=</span><span class="s3">{</span><span class="s1">dividend</span><span class="s3">}</span><span class="s4">_d=</span><span class="s3">{</span><span class="s1">divisor</span><span class="s3">}</span><span class="s4">_q=</span><span class="s3">{</span><span class="s1">quotient</span><span class="s3">}</span><span class="s4">_r=</span><span class="s3">{</span><span class="s1">remainder</span><span class="s3">}</span><span class="s4">&quot;</span><span class="s3">,</span>
           <span class="s1">dividend=dividend</span><span class="s3">, </span><span class="s1">divisor=divisor</span><span class="s3">, </span><span class="s1">quotient=quotient</span><span class="s3">,</span>
           <span class="s1">remainder=remainder)</span>
      <span class="s3">for </span><span class="s1">dividend</span><span class="s3">, </span><span class="s1">divisor</span><span class="s3">, </span><span class="s1">quotient</span><span class="s3">, </span><span class="s1">remainder </span><span class="s3">in </span><span class="s1">[</span>
          <span class="s1">(a</span><span class="s3">, </span><span class="s5">1</span><span class="s3">, </span><span class="s1">a</span><span class="s3">, </span><span class="s5">0</span><span class="s1">)</span><span class="s3">,</span>
          <span class="s1">(</span><span class="s5">3 </span><span class="s1">* a</span><span class="s3">, </span><span class="s5">3</span><span class="s3">, </span><span class="s1">a</span><span class="s3">, </span><span class="s5">0</span><span class="s1">)</span><span class="s3">,</span>
          <span class="s1">(</span><span class="s5">3 </span><span class="s1">* a + </span><span class="s5">3</span><span class="s3">, </span><span class="s5">3</span><span class="s3">, </span><span class="s1">a + </span><span class="s5">1</span><span class="s3">, </span><span class="s5">0</span><span class="s1">)</span><span class="s3">,</span>
          <span class="s1">(</span><span class="s5">3 </span><span class="s1">* a + </span><span class="s5">2</span><span class="s3">, </span><span class="s5">3</span><span class="s3">, </span><span class="s1">a</span><span class="s3">, </span><span class="s5">2</span><span class="s1">)</span><span class="s3">,</span>
          <span class="s1">(</span><span class="s5">3 </span><span class="s1">* a + </span><span class="s5">5</span><span class="s3">, </span><span class="s5">3</span><span class="s3">, </span><span class="s1">a + </span><span class="s5">1</span><span class="s3">, </span><span class="s5">2</span><span class="s1">)</span><span class="s3">,</span>
          <span class="s1">(</span><span class="s5">3 </span><span class="s1">* a - </span><span class="s5">2</span><span class="s3">, </span><span class="s5">3</span><span class="s3">, </span><span class="s1">a - </span><span class="s5">1</span><span class="s3">, </span><span class="s5">1</span><span class="s1">)</span><span class="s3">,</span>
          <span class="s1">(</span><span class="s5">3 </span><span class="s1">* a * a * b + </span><span class="s5">2 </span><span class="s1">* b * b * a</span><span class="s3">, </span><span class="s1">a * b</span><span class="s3">, </span><span class="s5">3 </span><span class="s1">* a + </span><span class="s5">2 </span><span class="s1">* b</span><span class="s3">, </span><span class="s5">0</span><span class="s1">)</span><span class="s3">,</span>
          <span class="s1">(a * a - b * b</span><span class="s3">, </span><span class="s1">a + b</span><span class="s3">, </span><span class="s1">a - b</span><span class="s3">, </span><span class="s5">0</span><span class="s1">)</span><span class="s3">,</span>
          <span class="s1">(a</span><span class="s3">, </span><span class="s1">b</span><span class="s3">, </span><span class="s4">&quot;floordiv(a, b)&quot;</span><span class="s3">, </span><span class="s4">&quot;mod(a, b)&quot;</span><span class="s1">)</span><span class="s3">,</span>
          <span class="s1">(</span><span class="s5">3 </span><span class="s1">* a</span><span class="s3">, </span><span class="s5">2</span><span class="s3">, </span><span class="s4">&quot;floordiv(3*a, 2)&quot;</span><span class="s3">, </span><span class="s4">&quot;mod(3*a, 2)&quot;</span><span class="s1">)</span><span class="s3">,</span>
          <span class="s1">(</span><span class="s5">2 </span><span class="s1">* a * b + b * b</span><span class="s3">, </span><span class="s1">a + b</span><span class="s3">, </span><span class="s4">&quot;floordiv(2*a*b + b^2, a + b)&quot;</span><span class="s3">, </span><span class="s4">&quot;mod(2*a*b + b^2, a + b)&quot;</span><span class="s1">)</span><span class="s3">,</span>
          <span class="s1">(</span><span class="s5">3</span><span class="s3">, </span><span class="s1">a</span><span class="s3">, </span><span class="s4">&quot;floordiv(3, a)&quot;</span><span class="s3">, </span><span class="s4">&quot;mod(3, a)&quot;</span><span class="s1">)</span><span class="s3">,</span>
  <span class="s1">])</span>
  <span class="s3">def </span><span class="s1">test_poly_divmod(self</span><span class="s3">, </span><span class="s1">*</span><span class="s3">, </span><span class="s1">dividend</span><span class="s3">, </span><span class="s1">quotient</span><span class="s3">, </span><span class="s1">divisor</span><span class="s3">, </span><span class="s1">remainder):</span>
    <span class="s3">if </span><span class="s1">isinstance(quotient</span><span class="s3">, </span><span class="s1">str):</span>
      <span class="s1">d1</span><span class="s3">, </span><span class="s1">d2 = divmod(dividend</span><span class="s3">, </span><span class="s1">divisor)</span>
      <span class="s1">self.assertEqual((quotient</span><span class="s3">, </span><span class="s1">remainder)</span><span class="s3">, </span><span class="s1">(str(d1)</span><span class="s3">, </span><span class="s1">str(d2)))</span>
    <span class="s3">else</span><span class="s1">:</span>
      <span class="s1">self.assertEqual((quotient</span><span class="s3">, </span><span class="s1">remainder)</span><span class="s3">, </span><span class="s1">divmod(dividend</span><span class="s3">, </span><span class="s1">divisor))</span>

  <span class="s3">def </span><span class="s1">test_dilate_shape(self):</span>
    <span class="s2">&quot;&quot;&quot;0 if d == 0 else 1 + dilation * (d - 1))&quot;&quot;&quot;</span>
    <span class="s1">a</span><span class="s3">, </span><span class="s1">= shape_poly._parse_spec(</span><span class="s4">&quot;a,&quot;</span><span class="s3">, </span><span class="s1">(</span><span class="s5">2</span><span class="s3">,</span><span class="s1">))</span>

    <span class="s1">self.assertEqual((</span><span class="s5">4</span><span class="s3">, </span><span class="s5">7</span><span class="s1">)</span><span class="s3">, </span><span class="s1">core.dilate_shape((</span><span class="s5">2</span><span class="s3">, </span><span class="s5">3</span><span class="s1">)</span><span class="s3">, </span><span class="s1">(</span><span class="s5">3</span><span class="s3">, </span><span class="s5">3</span><span class="s1">)))</span>
    <span class="s1">self.assertEqual((</span><span class="s5">0</span><span class="s3">, </span><span class="s5">7</span><span class="s1">)</span><span class="s3">, </span><span class="s1">core.dilate_shape((</span><span class="s5">0</span><span class="s3">, </span><span class="s5">3</span><span class="s1">)</span><span class="s3">, </span><span class="s1">(</span><span class="s5">3</span><span class="s3">, </span><span class="s5">3</span><span class="s1">)))</span>
    <span class="s1">self.assertEqual((a</span><span class="s3">, </span><span class="s5">7</span><span class="s1">)</span><span class="s3">, </span><span class="s1">core.dilate_shape((a</span><span class="s3">, </span><span class="s5">3</span><span class="s1">)</span><span class="s3">, </span><span class="s1">(</span><span class="s5">1</span><span class="s3">, </span><span class="s5">3</span><span class="s1">)))</span>
    <span class="s1">self.assertEqual((</span><span class="s5">2 </span><span class="s1">* a - </span><span class="s5">1</span><span class="s3">, </span><span class="s5">7</span><span class="s1">)</span><span class="s3">, </span><span class="s1">core.dilate_shape((a</span><span class="s3">, </span><span class="s5">3</span><span class="s1">)</span><span class="s3">, </span><span class="s1">(</span><span class="s5">2</span><span class="s3">, </span><span class="s5">3</span><span class="s1">)))</span>

  <span class="s3">def </span><span class="s1">test_stride_shape(self):</span>
    <span class="s2">&quot;&quot;&quot;(s - window_size) // window_stride + 1&quot;&quot;&quot;</span>
    <span class="s1">a</span><span class="s3">, </span><span class="s1">stride = shape_poly._parse_spec(</span><span class="s4">&quot;a, s&quot;</span><span class="s3">, </span><span class="s1">(</span><span class="s5">2</span><span class="s3">, </span><span class="s5">3</span><span class="s1">))</span>

    <span class="s1">self.assertEqual((</span><span class="s5">8</span><span class="s3">, </span><span class="s5">9</span><span class="s1">)</span><span class="s3">, </span><span class="s1">core.stride_shape((</span><span class="s5">10</span><span class="s3">, </span><span class="s5">20</span><span class="s1">)</span><span class="s3">, </span><span class="s1">window_size=(</span><span class="s5">3</span><span class="s3">, </span><span class="s5">3</span><span class="s1">)</span><span class="s3">, </span><span class="s1">window_stride=(</span><span class="s5">1</span><span class="s3">, </span><span class="s5">2</span><span class="s1">)))</span>
    <span class="s1">self.assertEqual((a</span><span class="s3">, </span><span class="s5">9</span><span class="s1">)</span><span class="s3">, </span><span class="s1">core.stride_shape((a</span><span class="s3">, </span><span class="s5">20</span><span class="s1">)</span><span class="s3">, </span><span class="s1">(</span><span class="s5">1</span><span class="s3">, </span><span class="s5">3</span><span class="s1">)</span><span class="s3">, </span><span class="s1">(</span><span class="s5">1</span><span class="s3">, </span><span class="s5">2</span><span class="s1">)))</span>

    <span class="s1">self.assertEqual((a - </span><span class="s5">1</span><span class="s3">, </span><span class="s5">9</span><span class="s1">)</span><span class="s3">, </span><span class="s1">core.stride_shape((a</span><span class="s3">, </span><span class="s5">20</span><span class="s1">)</span><span class="s3">, </span><span class="s1">(</span><span class="s5">2</span><span class="s3">, </span><span class="s5">3</span><span class="s1">)</span><span class="s3">, </span><span class="s1">(</span><span class="s5">1</span><span class="s3">, </span><span class="s5">2</span><span class="s1">)))</span>
    <span class="s1">self.assertEqual((a + </span><span class="s5">1</span><span class="s3">, </span><span class="s5">9</span><span class="s1">)</span><span class="s3">, </span><span class="s1">core.stride_shape((a * stride + </span><span class="s5">2</span><span class="s3">, </span><span class="s5">20</span><span class="s1">)</span><span class="s3">, </span><span class="s1">(</span><span class="s5">2</span><span class="s3">, </span><span class="s5">3</span><span class="s1">)</span><span class="s3">, </span><span class="s1">(stride</span><span class="s3">, </span><span class="s5">2</span><span class="s1">)))</span>

    <span class="s1">(stride0</span><span class="s3">, </span><span class="s1">stride1) = core.stride_shape((a</span><span class="s3">, </span><span class="s5">20</span><span class="s1">)</span><span class="s3">, </span><span class="s1">(</span><span class="s5">1</span><span class="s3">, </span><span class="s5">3</span><span class="s1">)</span><span class="s3">, </span><span class="s1">(</span><span class="s5">2</span><span class="s3">, </span><span class="s5">2</span><span class="s1">))</span>
    <span class="s1">self.assertEqual(</span><span class="s4">&quot;floordiv(a + -1, 2) + 1&quot;</span><span class="s3">, </span><span class="s1">str(stride0))</span>
    <span class="s1">self.assertEqual(</span><span class="s5">9</span><span class="s3">, </span><span class="s1">stride1)</span>


<span class="s3">class </span><span class="s1">PolyHarness(Harness):</span>
  <span class="s2">&quot;&quot;&quot;Tests a function with shape polymorphism. 
 
  Converts `fun` with shape polymorphism, creates a `tf.ConcreteFunction` 
  given `input_signature` and checks the inferred output shapes to match 
  `expected_output_shapes`, then checks that the JAX and the TF functions 
  produce the same results. 
  &quot;&quot;&quot;</span>
  <span class="s3">def </span><span class="s1">__init__(self</span><span class="s3">,</span>
               <span class="s1">group_name: str</span><span class="s3">, </span><span class="s1">name: str</span><span class="s3">,</span>
               <span class="s1">fun: Callable</span><span class="s3">,</span>
               <span class="s1">*</span><span class="s3">,</span>
               <span class="s1">arg_descriptors: Sequence[primitive_harness.ArgDescriptor] = ()</span><span class="s3">,</span>
               <span class="s1">polymorphic_shapes: Optional[Sequence[Any]] = </span><span class="s3">None,</span>
               <span class="s1">input_signature: Optional[Sequence[tf.TensorSpec]] = </span><span class="s3">None,</span>
               <span class="s1">poly_axes: Optional[Sequence[Optional[Union[int</span><span class="s3">, </span><span class="s1">Sequence[int]]]]] = </span><span class="s3">None,</span>
               <span class="s1">expected_output_signature: Optional[tf.TensorSpec] = </span><span class="s3">None,</span>
               <span class="s1">enable_xla: bool = </span><span class="s3">True,</span>
               <span class="s1">expect_error: Tuple[Optional[Any]</span><span class="s3">, </span><span class="s1">Optional[str]] = (</span><span class="s3">None, None</span><span class="s1">)</span><span class="s3">,</span>
               <span class="s1">skip_jax_run: bool = </span><span class="s3">False,</span>
               <span class="s1">check_result: bool = </span><span class="s3">True,</span>
               <span class="s1">tol: Optional[float] = </span><span class="s3">None</span><span class="s1">):</span>
    <span class="s2">&quot;&quot;&quot;Args: 
 
      group_name, name: The name for the harness. See `Harness.__init__`. 
      fun: the function to be converted, possbily after partial application to 
        static arguments from `arg_descriptors`. See `Harness.__init__`. 
      arg_descriptors: The argument descriptors. See `Harness.__init__`. May 
        be missing, in which case `skip_jax_run` should be `True` and 
        `poly_axes` cannot be used. 
      polymorphic_shapes: For `jax2tf.convert`. If missing, generated from 
        `poly_axes`. 
      input_signature: For `tf.function.get_concrete_function`. If missing, 
        generated from `poly_axes`. 
      poly_axes: If present, used to generate `polymorphic_shapes` and 
        `input_signature`. Must correspond to the non-static arguments, and for 
        each one it must specify which axes are polymorphic: None, or an int 
        (for the index of the polymorphic axis), or a tuple of ints 
        (for multiple polymorphic axes). For each argument, we use its 
        `poly_axes` entry to generate the polymorphic_shapes specification, 
        creating dimension variables `b0`, `b1, ..., for each of its polymorphic 
        axes. This means that separate arguments will share the same dimension 
        variable names, in the order in which the axes are listed in 
        `poly_axes`. We also generate the input_signature from `poly_axes`. 
      expected_output_signature: the expected inferred output shape. 
      enable_xla: For `jax2tf.convert`. 
      expect_error: a pair of an Exception type and a regular expression to 
        match the expected exception string. 
      skip_jax_run: If True, then neither the JAX nor the TF functions are 
        executed. 
      check_result: specifies if we want to check that the result of the shape 
        polymorphic conversion produces the same result and the JAX function. 
      tol: the tolerance to use for checking results. 
    &quot;&quot;&quot;</span>
    <span class="s1">super().__init__(group_name</span><span class="s3">, </span><span class="s1">name</span><span class="s3">, </span><span class="s1">fun</span><span class="s3">, </span><span class="s1">arg_descriptors</span><span class="s3">,</span>
                     <span class="s1">dtype=np.float32)</span>
    <span class="s1">self.poly_axes = poly_axes</span>
    <span class="s1">self.polymorphic_shapes = polymorphic_shapes</span>
    <span class="s1">self.input_signature = input_signature</span>
    <span class="s1">self.expected_output_signature = expected_output_signature</span>
    <span class="s1">self.skip_jax_run = skip_jax_run</span>
    <span class="s1">self.expect_error = expect_error</span>
    <span class="s1">self.enable_xla = enable_xla</span>
    <span class="s1">self.tol = tol</span>
    <span class="s1">self.check_result = check_result</span>

  <span class="s0"># Replicate the harness for both enable and disable xla</span>
  <span class="s3">def </span><span class="s1">both_enable_and_disable_xla(self) -&gt; Tuple[</span><span class="s4">&quot;PolyHarness&quot;</span><span class="s3">, </span><span class="s4">&quot;PolyHarness&quot;</span><span class="s1">]:</span>
    <span class="s3">assert </span><span class="s1">self.enable_xla</span>
    <span class="s1">other = PolyHarness(self.group_name</span><span class="s3">,</span>
                        <span class="s4">f&quot;</span><span class="s3">{</span><span class="s1">self.name</span><span class="s3">}</span><span class="s4">_enable_xla=False&quot;</span><span class="s3">,</span>
                        <span class="s1">self.fun</span><span class="s3">,</span>
                        <span class="s1">arg_descriptors=self.arg_descriptors</span><span class="s3">,</span>
                        <span class="s1">poly_axes=self.poly_axes</span><span class="s3">,</span>
                        <span class="s1">polymorphic_shapes=self.polymorphic_shapes</span><span class="s3">,</span>
                        <span class="s1">input_signature=self.input_signature</span><span class="s3">,</span>
                        <span class="s1">expected_output_signature=self.expected_output_signature</span><span class="s3">,</span>
                        <span class="s1">expect_error=self.expect_error</span><span class="s3">,</span>
                        <span class="s1">tol=self.tol</span><span class="s3">,</span>
                        <span class="s1">enable_xla=</span><span class="s3">False</span><span class="s1">)</span>
    <span class="s1">self.name = </span><span class="s4">f&quot;</span><span class="s3">{</span><span class="s1">self.name</span><span class="s3">}</span><span class="s4">_enable_xla=True&quot;</span>
    <span class="s3">return </span><span class="s1">(self</span><span class="s3">, </span><span class="s1">other)</span>

  <span class="s3">def </span><span class="s1">run_test(self</span><span class="s3">, </span><span class="s1">tst: tf_test_util.JaxToTfTestCase):</span>
    <span class="s0"># Make polymorphic_shapes and input_signature from poly_axes.</span>
    <span class="s3">if </span><span class="s1">self.poly_axes </span><span class="s3">is None</span><span class="s1">:</span>
      <span class="s1">polymorphic_shapes = self.polymorphic_shapes</span>
      <span class="s1">input_signature = self.input_signature</span>
      <span class="s3">assert </span><span class="s1">input_signature </span><span class="s3">is not None</span>
      <span class="s3">if not </span><span class="s1">self.skip_jax_run:</span>
        <span class="s1">args = self.dyn_args_maker(tst.rng())</span>

    <span class="s3">else</span><span class="s1">:</span>
      <span class="s3">assert </span><span class="s1">isinstance(self.poly_axes</span><span class="s3">, </span><span class="s1">Sequence)</span>
      <span class="s0"># Make poly_axes: Sequence[Sequence[int]], one top-level element for each argument</span>
      <span class="s1">poly_axes = tuple(map(</span><span class="s3">lambda </span><span class="s1">pa: pa </span><span class="s3">if </span><span class="s1">isinstance(pa</span><span class="s3">, </span><span class="s1">Sequence) </span><span class="s3">or </span><span class="s1">pa </span><span class="s3">is None else </span><span class="s1">(pa</span><span class="s3">,</span><span class="s1">)</span><span class="s3">,</span>
                            <span class="s1">self.poly_axes))</span>
      <span class="s1">args = self.dyn_args_maker(tst.rng())</span>

      <span class="s3">assert </span><span class="s1">self.polymorphic_shapes </span><span class="s3">is None</span>
      <span class="s3">assert </span><span class="s1">self.input_signature </span><span class="s3">is None</span>
      <span class="s3">assert </span><span class="s1">args </span><span class="s3">is not None and </span><span class="s1">len(args) == len(poly_axes)</span>
      <span class="s0"># Make the polymorphic_shapes and input_signature</span>
      <span class="s1">polymorphic_shapes = []</span>
      <span class="s1">input_signature = []</span>
      <span class="s3">for </span><span class="s1">arg</span><span class="s3">, </span><span class="s1">poly_axis </span><span class="s3">in </span><span class="s1">zip(args</span><span class="s3">, </span><span class="s1">poly_axes):</span>
        <span class="s3">if </span><span class="s1">poly_axis </span><span class="s3">is None</span><span class="s1">:</span>
          <span class="s1">polymorphic_shapes.append(</span><span class="s3">None</span><span class="s1">)</span>
          <span class="s1">input_signature.append(tf.TensorSpec(np.shape(arg)</span><span class="s3">, </span><span class="s1">arg.dtype))</span>
        <span class="s3">else</span><span class="s1">:</span>
          <span class="s3">def </span><span class="s1">make_arg_polymorphic_shapes(poly_axis: Sequence[int]) -&gt; Tuple[str</span><span class="s3">, </span><span class="s1">tf.TensorSpec]:</span>
            <span class="s1">idx = -</span><span class="s5">1</span>
            <span class="s1">dims = []</span>
            <span class="s1">tensorspec_dims: List[Optional[int]] = []</span>
            <span class="s3">for </span><span class="s1">i</span><span class="s3">, </span><span class="s1">d </span><span class="s3">in </span><span class="s1">enumerate(arg.shape):</span>
              <span class="s3">if </span><span class="s1">i </span><span class="s3">in </span><span class="s1">poly_axis:</span>
                <span class="s1">idx += </span><span class="s5">1</span>
                <span class="s1">dims.append(</span><span class="s4">f&quot;b</span><span class="s3">{</span><span class="s1">idx</span><span class="s3">}</span><span class="s4">&quot;</span><span class="s1">)</span>
                <span class="s1">tensorspec_dims.append(</span><span class="s3">None</span><span class="s1">)</span>
              <span class="s3">else</span><span class="s1">:</span>
                <span class="s1">dims.append(str(d))</span>
                <span class="s1">tensorspec_dims.append(d)</span>
            <span class="s3">return </span><span class="s4">&quot;, &quot;</span><span class="s1">.join(dims)</span><span class="s3">, </span><span class="s1">tf.TensorSpec(tensorspec_dims</span><span class="s3">, </span><span class="s1">arg.dtype)</span>

          <span class="s1">arg_polymorphic_shapes</span><span class="s3">, </span><span class="s1">arg_tensorspec = make_arg_polymorphic_shapes(poly_axis)</span>
          <span class="s1">polymorphic_shapes.append(arg_polymorphic_shapes)</span>
          <span class="s1">input_signature.append(arg_tensorspec)</span>

    <span class="s1">expect_error_type</span><span class="s3">, </span><span class="s1">expect_error_regex = self.expect_error</span>
    <span class="s3">if </span><span class="s1">self.skip_jax_run </span><span class="s3">and </span><span class="s1">self.arg_descriptors == ():</span>
      <span class="s1">f_jax = self.fun</span>
    <span class="s3">else</span><span class="s1">:</span>
      <span class="s1">f_jax = self.dyn_fun</span>

    <span class="s3">with </span><span class="s1">contextlib.ExitStack() </span><span class="s3">as </span><span class="s1">stack:</span>
      <span class="s3">if </span><span class="s1">expect_error_type </span><span class="s3">is not None</span><span class="s1">:</span>
        <span class="s1">stack.enter_context(tst.assertRaisesRegex(expect_error_type</span><span class="s3">, </span><span class="s1">expect_error_regex))</span>

      <span class="s1">f_tf = jax2tf.convert(f_jax</span><span class="s3">, </span><span class="s1">polymorphic_shapes=polymorphic_shapes</span><span class="s3">,</span>
                            <span class="s1">enable_xla=self.enable_xla)</span>
      <span class="s0"># Run in tf.Eager mode first, because it is friendlier to debuggers</span>
      <span class="s1">res_tf = f_tf(*args) </span><span class="s3">if not </span><span class="s1">self.skip_jax_run </span><span class="s3">else None</span>
      <span class="s1">f_tf_func = tf.function(</span>
          <span class="s1">f_tf</span><span class="s3">, </span><span class="s1">autograph=</span><span class="s3">False, </span><span class="s1">input_signature=input_signature)</span>
      <span class="s0"># Create tf.ConcreteFunction and check inferred output signature</span>
      <span class="s1">concrete_f_tf = f_tf_func.get_concrete_function(*input_signature)</span>

    <span class="s3">if </span><span class="s1">expect_error_type </span><span class="s3">is not None</span><span class="s1">:</span>
      <span class="s3">return</span>

    <span class="s3">if </span><span class="s1">self.expected_output_signature:</span>
      <span class="s0"># Strangely, output_shapes can be a single shape for a function with a</span>
      <span class="s0"># single result, or a list/tuple of shapes.</span>
      <span class="s1">expected_output_signature = self.expected_output_signature</span>
      <span class="s1">concrete_output_tf_shape = concrete_f_tf.output_shapes</span>
      <span class="s3">if not </span><span class="s1">isinstance(concrete_output_tf_shape</span><span class="s3">, </span><span class="s1">(tuple</span><span class="s3">, </span><span class="s1">list)):  </span><span class="s0"># Single result</span>
        <span class="s3">assert not </span><span class="s1">isinstance(self.expected_output_signature</span><span class="s3">, </span><span class="s1">(tuple</span><span class="s3">, </span><span class="s1">list))</span>
        <span class="s1">expected_output_signature = [self.expected_output_signature]</span>
        <span class="s1">concrete_output_tf_shape = [concrete_output_tf_shape]</span>
      <span class="s3">for </span><span class="s1">expected</span><span class="s3">, </span><span class="s1">found </span><span class="s3">in </span><span class="s1">util.safe_zip(expected_output_signature</span><span class="s3">,</span>
                                           <span class="s1">concrete_output_tf_shape):</span>
        <span class="s1">tst.assertEqual(tuple(expected.shape)</span><span class="s3">, </span><span class="s1">tuple(found))</span>

    <span class="s0"># Run the JAX and the TF functions and compare the results</span>
    <span class="s3">if not </span><span class="s1">self.skip_jax_run:</span>
      <span class="s1">res_jax = f_jax(*args)</span>
      <span class="s3">if </span><span class="s1">self.check_result:</span>
        <span class="s1">tst.assertAllClose(res_jax</span><span class="s3">, </span><span class="s1">res_tf</span><span class="s3">, </span><span class="s1">atol=self.tol</span><span class="s3">, </span><span class="s1">rtol=self.tol)</span>


<span class="s3">def </span><span class="s1">check_shape_poly(tst</span><span class="s3">, </span><span class="s1">f_jax: Callable</span><span class="s3">, </span><span class="s1">*</span><span class="s3">,</span>
                     <span class="s1">arg_descriptors: Sequence[primitive_harness.ArgDescriptor] = ()</span><span class="s3">,</span>
                     <span class="s1">skip_jax_run: bool = </span><span class="s3">False,</span>
                     <span class="s1">poly_axes = </span><span class="s3">None,</span>
                     <span class="s1">polymorphic_shapes: Optional[Sequence[Any]] = </span><span class="s3">None,</span>
                     <span class="s1">input_signature: Optional[Sequence[tf.TensorSpec]] = </span><span class="s3">None,</span>
                     <span class="s1">expected_output_signature: Optional[tf.TensorSpec] = </span><span class="s3">None,</span>
                     <span class="s1">expect_error=(</span><span class="s3">None, None</span><span class="s1">)):</span>
  <span class="s0"># Makes and tests a harness. See PolyHarness documentation.</span>
  <span class="s1">h = PolyHarness(</span><span class="s4">&quot;&quot;</span><span class="s3">, </span><span class="s4">&quot;&quot;</span><span class="s3">, </span><span class="s1">f_jax</span><span class="s3">,</span>
                  <span class="s1">arg_descriptors=arg_descriptors</span><span class="s3">,</span>
                  <span class="s1">skip_jax_run=skip_jax_run</span><span class="s3">, </span><span class="s1">poly_axes=poly_axes</span><span class="s3">,</span>
                  <span class="s1">polymorphic_shapes=polymorphic_shapes</span><span class="s3">,</span>
                  <span class="s1">input_signature=input_signature</span><span class="s3">,</span>
                  <span class="s1">expected_output_signature=expected_output_signature</span><span class="s3">,</span>
                  <span class="s1">expect_error=expect_error)</span>
  <span class="s1">h.run_test(tst)</span>


<span class="s3">class </span><span class="s1">ShapePolyTest(tf_test_util.JaxToTfTestCase):</span>

  <span class="s3">def </span><span class="s1">test_simple_unary(self):</span>
    <span class="s2">&quot;&quot;&quot;Test shape polymorphism for a simple case, unary function.&quot;&quot;&quot;</span>

    <span class="s3">def </span><span class="s1">f_jax(x):</span>
      <span class="s3">return </span><span class="s1">x + jnp.sin(x)</span>

    <span class="s1">check_shape_poly(self</span><span class="s3">,</span>
                     <span class="s1">f_jax</span><span class="s3">,</span>
                     <span class="s1">arg_descriptors=[RandArg((</span><span class="s5">2</span><span class="s3">, </span><span class="s5">3</span><span class="s1">)</span><span class="s3">, </span><span class="s1">_f32)]</span><span class="s3">,</span>
                     <span class="s1">input_signature=[tf.TensorSpec([</span><span class="s5">2</span><span class="s3">, </span><span class="s5">3</span><span class="s1">])]</span><span class="s3">,</span>
                     <span class="s1">polymorphic_shapes=</span><span class="s3">None,</span>
                     <span class="s1">expected_output_signature=tf.TensorSpec([</span><span class="s5">2</span><span class="s3">, </span><span class="s5">3</span><span class="s1">]))</span>

    <span class="s1">check_shape_poly(self</span><span class="s3">,</span>
                     <span class="s1">f_jax</span><span class="s3">,</span>
                     <span class="s1">arg_descriptors=[RandArg((</span><span class="s5">2</span><span class="s3">, </span><span class="s5">3</span><span class="s1">)</span><span class="s3">, </span><span class="s1">_f32)]</span><span class="s3">,</span>
                     <span class="s1">input_signature=[tf.TensorSpec([</span><span class="s5">2</span><span class="s3">, None</span><span class="s1">])]</span><span class="s3">,</span>
                     <span class="s1">polymorphic_shapes=[</span><span class="s4">&quot;_, h&quot;</span><span class="s1">]</span><span class="s3">,</span>
                     <span class="s1">expected_output_signature=tf.TensorSpec([</span><span class="s5">2</span><span class="s3">, None</span><span class="s1">]))</span>

    <span class="s1">check_shape_poly(self</span><span class="s3">,</span>
                     <span class="s1">f_jax</span><span class="s3">,</span>
                     <span class="s1">arg_descriptors=[RandArg((</span><span class="s5">3</span><span class="s3">, </span><span class="s5">3</span><span class="s1">)</span><span class="s3">, </span><span class="s1">_f32)]</span><span class="s3">,</span>
                     <span class="s1">input_signature=[tf.TensorSpec([</span><span class="s3">None, None</span><span class="s1">])]</span><span class="s3">,</span>
                     <span class="s1">polymorphic_shapes=[</span><span class="s4">&quot;h, h&quot;</span><span class="s1">]</span><span class="s3">,</span>
                     <span class="s1">expected_output_signature=tf.TensorSpec([</span><span class="s3">None, None</span><span class="s1">]))</span>

    <span class="s1">check_shape_poly(self</span><span class="s3">,</span>
                     <span class="s1">f_jax</span><span class="s3">,</span>
                     <span class="s1">arg_descriptors=[RandArg((</span><span class="s5">3</span><span class="s3">, </span><span class="s5">3</span><span class="s1">)</span><span class="s3">, </span><span class="s1">_f32)]</span><span class="s3">,</span>
                     <span class="s1">input_signature=[tf.TensorSpec([</span><span class="s3">None, None</span><span class="s1">])]</span><span class="s3">,</span>
                     <span class="s1">polymorphic_shapes=</span><span class="s4">&quot;h, h&quot;</span><span class="s3">,</span>
                     <span class="s1">expected_output_signature=tf.TensorSpec([</span><span class="s3">None, None</span><span class="s1">]))</span>

  <span class="s3">def </span><span class="s1">test_simple_binary(self):</span>
    <span class="s2">&quot;&quot;&quot;Test shape polymorphism for a simple case, binary function.&quot;&quot;&quot;</span>

    <span class="s3">def </span><span class="s1">f_jax(x</span><span class="s3">, </span><span class="s1">y):</span>
      <span class="s3">return </span><span class="s1">x + jnp.sin(y)</span>

    <span class="s1">check_shape_poly(self</span><span class="s3">,</span>
                     <span class="s1">f_jax</span><span class="s3">,</span>
                     <span class="s1">arg_descriptors=[RandArg((</span><span class="s5">2</span><span class="s3">, </span><span class="s5">3</span><span class="s1">)</span><span class="s3">, </span><span class="s1">_f32)</span><span class="s3">, </span><span class="s1">RandArg((</span><span class="s5">2</span><span class="s3">, </span><span class="s5">3</span><span class="s1">)</span><span class="s3">, </span><span class="s1">_f32)]</span><span class="s3">,</span>
                     <span class="s1">input_signature=[tf.TensorSpec([</span><span class="s5">2</span><span class="s3">, </span><span class="s5">3</span><span class="s1">])</span><span class="s3">, </span><span class="s1">tf.TensorSpec([</span><span class="s5">2</span><span class="s3">, </span><span class="s5">3</span><span class="s1">])]</span><span class="s3">,</span>
                     <span class="s1">polymorphic_shapes=</span><span class="s3">None,</span>
                     <span class="s1">expected_output_signature=tf.TensorSpec([</span><span class="s5">2</span><span class="s3">, </span><span class="s5">3</span><span class="s1">]))</span>

    <span class="s1">check_shape_poly(self</span><span class="s3">,</span>
                     <span class="s1">f_jax</span><span class="s3">,</span>
                     <span class="s1">arg_descriptors=[RandArg((</span><span class="s5">2</span><span class="s3">, </span><span class="s5">3</span><span class="s1">)</span><span class="s3">, </span><span class="s1">_f32)</span><span class="s3">, </span><span class="s1">RandArg((</span><span class="s5">2</span><span class="s3">, </span><span class="s5">3</span><span class="s1">)</span><span class="s3">, </span><span class="s1">_f32)]</span><span class="s3">,</span>
                     <span class="s1">input_signature=[tf.TensorSpec([</span><span class="s5">2</span><span class="s3">, None</span><span class="s1">])</span><span class="s3">, </span><span class="s1">tf.TensorSpec([</span><span class="s5">2</span><span class="s3">, </span><span class="s5">3</span><span class="s1">])]</span><span class="s3">,</span>
                     <span class="s1">polymorphic_shapes=</span><span class="s4">&quot;_, h&quot;</span><span class="s3">,</span>
                     <span class="s1">expected_output_signature=(</span>
                         <span class="s0"># for native serialization we cannot refine the inferred shape of the</span>
                         <span class="s0"># output if the input is more specific than polymorphic_shapes.</span>
                         <span class="s1">tf.TensorSpec([</span><span class="s5">2</span><span class="s3">, </span><span class="s5">3</span><span class="s1">]) </span><span class="s3">if not </span><span class="s1">config.jax2tf_default_native_serialization</span>
                         <span class="s3">else </span><span class="s1">tf.TensorSpec([</span><span class="s5">2</span><span class="s3">, None</span><span class="s1">])))</span>

    <span class="s1">check_shape_poly(self</span><span class="s3">,</span>
                     <span class="s1">f_jax</span><span class="s3">,</span>
                     <span class="s1">arg_descriptors=[RandArg((</span><span class="s5">3</span><span class="s3">, </span><span class="s5">3</span><span class="s1">)</span><span class="s3">, </span><span class="s1">_f32)</span><span class="s3">, </span><span class="s1">RandArg((</span><span class="s5">3</span><span class="s3">, </span><span class="s5">3</span><span class="s1">)</span><span class="s3">, </span><span class="s1">_f32)]</span><span class="s3">,</span>
                     <span class="s1">input_signature=[tf.TensorSpec([</span><span class="s3">None, None</span><span class="s1">])</span><span class="s3">, </span><span class="s1">tf.TensorSpec([</span><span class="s3">None, None</span><span class="s1">])]</span><span class="s3">,</span>
                     <span class="s1">polymorphic_shapes=PS(</span><span class="s4">&quot;h&quot;</span><span class="s3">, </span><span class="s4">&quot;h&quot;</span><span class="s1">)</span><span class="s3">,</span>
                     <span class="s1">expected_output_signature=tf.TensorSpec([</span><span class="s3">None, None</span><span class="s1">]))</span>

  <span class="s3">def </span><span class="s1">test_arange(self):</span>
    <span class="s3">def </span><span class="s1">f_jax(x):</span>
      <span class="s3">return </span><span class="s1">x + jnp.arange(x.shape[</span><span class="s5">0</span><span class="s1">]</span><span class="s3">, </span><span class="s1">dtype=np.float32)</span>
    <span class="s1">x = np.ones((</span><span class="s5">3</span><span class="s3">,</span><span class="s1">)</span><span class="s3">, </span><span class="s1">dtype=np.float32)</span>
    <span class="s1">self.assertAllClose(jax2tf.convert(f_jax</span><span class="s3">, </span><span class="s1">polymorphic_shapes=</span><span class="s4">&quot;b&quot;</span><span class="s1">)(x)</span><span class="s3">,</span>
                        <span class="s1">f_jax(x))</span>

  <span class="s3">def </span><span class="s1">test_argmax(self):</span>
    <span class="s3">def </span><span class="s1">f_jax(x):  </span><span class="s0"># x: f32[b, 4, 5]</span>
      <span class="s3">return </span><span class="s1">lax.argmax(x</span><span class="s3">, </span><span class="s1">axis=</span><span class="s5">1</span><span class="s3">, </span><span class="s1">index_dtype=np.int32)</span>
    <span class="s1">x = np.arange(</span><span class="s5">3 </span><span class="s1">* </span><span class="s5">4 </span><span class="s1">* </span><span class="s5">5</span><span class="s3">, </span><span class="s1">dtype=np.float32).reshape((</span><span class="s5">3</span><span class="s3">, </span><span class="s5">4</span><span class="s3">, </span><span class="s5">5</span><span class="s1">))</span>
    <span class="s1">self.assertAllClose(jax2tf.convert(f_jax</span><span class="s3">, </span><span class="s1">polymorphic_shapes=</span><span class="s4">&quot;(b, _, _)&quot;</span><span class="s1">)(x)</span><span class="s3">,</span>
                        <span class="s1">f_jax(x))</span>

  <span class="s1">@parameterized.named_parameters([</span>
      <span class="s1">dict(testcase_name=</span><span class="s4">f&quot;_expr=</span><span class="s3">{</span><span class="s1">name</span><span class="s3">}</span><span class="s4">&quot;</span><span class="s3">, </span><span class="s1">expr=expr)</span>
      <span class="s3">for </span><span class="s1">name</span><span class="s3">, </span><span class="s1">expr </span><span class="s3">in </span><span class="s1">[</span>
          <span class="s1">(</span><span class="s4">&quot;d + 2&quot;</span><span class="s3">, lambda </span><span class="s1">d: d + </span><span class="s5">2</span><span class="s1">)</span><span class="s3">,</span>
          <span class="s1">(</span><span class="s4">&quot;2 - d&quot;</span><span class="s3">, lambda </span><span class="s1">d: </span><span class="s5">2 </span><span class="s1">- d)</span><span class="s3">,</span>
          <span class="s1">(</span><span class="s4">&quot;d * 2&quot;</span><span class="s3">, lambda </span><span class="s1">d: d * </span><span class="s5">2</span><span class="s1">)</span><span class="s3">,</span>
          <span class="s1">(</span><span class="s4">&quot;d * d&quot;</span><span class="s3">, lambda </span><span class="s1">d: d * d)</span><span class="s3">,</span>
          <span class="s1">(</span><span class="s4">&quot;(- d) * d&quot;</span><span class="s3">, lambda </span><span class="s1">d: (- d) * d)</span><span class="s3">,</span>
          <span class="s1">(</span><span class="s4">&quot;d * d - d&quot;</span><span class="s3">, lambda </span><span class="s1">d: d * d - d)</span><span class="s3">,</span>
          <span class="s0"># Division</span>
          <span class="s1">(</span><span class="s4">&quot;d // 2&quot;</span><span class="s3">, lambda </span><span class="s1">d: d // </span><span class="s5">2</span><span class="s1">)</span><span class="s3">,</span>
          <span class="s1">(</span><span class="s4">&quot;(d + 1) // 2&quot;</span><span class="s3">, lambda </span><span class="s1">d: (d + </span><span class="s5">1</span><span class="s1">) // </span><span class="s5">2</span><span class="s1">)</span><span class="s3">,</span>
          <span class="s1">(</span><span class="s4">&quot;d // -2&quot;</span><span class="s3">, lambda </span><span class="s1">d: d // -</span><span class="s5">2</span><span class="s1">)</span><span class="s3">,</span>
          <span class="s1">(</span><span class="s4">&quot;(d + 1) // -2&quot;</span><span class="s3">, lambda </span><span class="s1">d: (d + </span><span class="s5">1</span><span class="s1">) // -</span><span class="s5">2</span><span class="s1">)</span><span class="s3">,</span>
          <span class="s1">(</span><span class="s4">&quot;(-d) // 2&quot;</span><span class="s3">, lambda </span><span class="s1">d: (-d) // </span><span class="s5">2</span><span class="s1">)</span><span class="s3">,</span>
          <span class="s1">(</span><span class="s4">&quot;(-d - 1) // 2&quot;</span><span class="s3">, lambda </span><span class="s1">d: (-d - </span><span class="s5">1</span><span class="s1">) // </span><span class="s5">2</span><span class="s1">)</span><span class="s3">,</span>
          <span class="s1">(</span><span class="s4">&quot;(-d) // -2&quot;</span><span class="s3">, lambda </span><span class="s1">d: (-d) // -</span><span class="s5">2</span><span class="s1">)</span><span class="s3">,</span>
          <span class="s1">(</span><span class="s4">&quot;(-d - 1) // -2&quot;</span><span class="s3">, lambda </span><span class="s1">d: (-d - </span><span class="s5">1</span><span class="s1">) // -</span><span class="s5">2</span><span class="s1">)</span><span class="s3">,</span>
          <span class="s0"># Remainder</span>
          <span class="s1">(</span><span class="s4">&quot;d % 2&quot;</span><span class="s3">, lambda </span><span class="s1">d: d % </span><span class="s5">2</span><span class="s1">)</span><span class="s3">,</span>
          <span class="s1">(</span><span class="s4">&quot;(d + 1) % 2&quot;</span><span class="s3">, lambda </span><span class="s1">d: (d + </span><span class="s5">1</span><span class="s1">) % </span><span class="s5">2</span><span class="s1">)</span><span class="s3">,</span>
          <span class="s1">(</span><span class="s4">&quot;d % -2&quot;</span><span class="s3">, lambda </span><span class="s1">d: d % -</span><span class="s5">2</span><span class="s1">)</span><span class="s3">,</span>
          <span class="s1">(</span><span class="s4">&quot;(d + 1) % -2&quot;</span><span class="s3">, lambda </span><span class="s1">d: (d + </span><span class="s5">1</span><span class="s1">) % -</span><span class="s5">2</span><span class="s1">)</span><span class="s3">,</span>
          <span class="s1">(</span><span class="s4">&quot;(-d) % 2&quot;</span><span class="s3">, lambda </span><span class="s1">d: (-d) % </span><span class="s5">2</span><span class="s1">)</span><span class="s3">,</span>
          <span class="s1">(</span><span class="s4">&quot;(-d - 1) % 2&quot;</span><span class="s3">, lambda </span><span class="s1">d: (-d - </span><span class="s5">1</span><span class="s1">) % </span><span class="s5">2</span><span class="s1">)</span><span class="s3">,</span>
          <span class="s1">(</span><span class="s4">&quot;(-d) % -2&quot;</span><span class="s3">, lambda </span><span class="s1">d: (-d) % -</span><span class="s5">2</span><span class="s1">)</span><span class="s3">,</span>
          <span class="s1">(</span><span class="s4">&quot;(-d - 1) % -2&quot;</span><span class="s3">, lambda </span><span class="s1">d: (-d - </span><span class="s5">1</span><span class="s1">) % -</span><span class="s5">2</span><span class="s1">)</span><span class="s3">,</span>
      <span class="s1">]</span>
  <span class="s1">])</span>
  <span class="s3">def </span><span class="s1">test_non_trivial_dim_expr(self</span><span class="s3">, </span><span class="s1">expr=</span><span class="s3">lambda </span><span class="s1">d: d % -</span><span class="s5">2</span><span class="s1">):</span>
    <span class="s0"># Check the lowering for shape expressions</span>
    <span class="s1">check_shape_poly(</span>
       <span class="s1">self</span><span class="s3">,</span>
       <span class="s3">lambda </span><span class="s1">x: x[</span><span class="s5">0</span><span class="s1">] * </span><span class="s5">0 </span><span class="s1">+ expr(x.shape[</span><span class="s5">0</span><span class="s1">])</span><span class="s3">,</span>
       <span class="s1">arg_descriptors=[RandArg((</span><span class="s5">3</span><span class="s3">,</span><span class="s1">)</span><span class="s3">, </span><span class="s1">np.int64)]</span><span class="s3">,</span>
       <span class="s1">poly_axes=[</span><span class="s5">0</span><span class="s1">])</span>

  <span class="s3">def </span><span class="s1">test_static_shape_result(self):</span>
    <span class="s2">&quot;&quot;&quot;The result has static shape.&quot;&quot;&quot;</span>

    <span class="s3">def </span><span class="s1">f_jax(x):</span>
      <span class="s3">return </span><span class="s1">jnp.sum(x + jnp.sin(x)</span><span class="s3">, </span><span class="s1">axis=</span><span class="s5">0</span><span class="s1">)</span>

    <span class="s1">check_shape_poly(self</span><span class="s3">,</span>
                     <span class="s1">f_jax</span><span class="s3">,</span>
                     <span class="s1">arg_descriptors=[RandArg((</span><span class="s5">2</span><span class="s3">, </span><span class="s5">3</span><span class="s1">)</span><span class="s3">, </span><span class="s1">_f32)]</span><span class="s3">,</span>
                     <span class="s1">input_signature=[tf.TensorSpec([</span><span class="s5">2</span><span class="s3">, </span><span class="s5">3</span><span class="s1">])]</span><span class="s3">,</span>
                     <span class="s1">polymorphic_shapes=</span><span class="s3">None,</span>
                     <span class="s1">expected_output_signature=tf.TensorSpec([</span><span class="s5">3</span><span class="s1">]))</span>

    <span class="s1">check_shape_poly(self</span><span class="s3">,</span>
                     <span class="s1">f_jax</span><span class="s3">,</span>
                     <span class="s1">arg_descriptors=[RandArg((</span><span class="s5">2</span><span class="s3">, </span><span class="s5">3</span><span class="s1">)</span><span class="s3">, </span><span class="s1">_f32)]</span><span class="s3">,</span>
                     <span class="s1">input_signature=[tf.TensorSpec([</span><span class="s3">None, </span><span class="s5">3</span><span class="s1">])]</span><span class="s3">,</span>
                     <span class="s1">polymorphic_shapes=</span><span class="s4">&quot;b, _&quot;</span><span class="s3">,</span>
                     <span class="s1">expected_output_signature=tf.TensorSpec([</span><span class="s5">3</span><span class="s1">]))</span>

  <span class="s3">def </span><span class="s1">test_forgot_polymorphic_shapes_error(self):</span>
    <span class="s1">msg_re = </span><span class="s4">&quot;polymorphic shape None in axis .* must contain a dimension variable for unknown dimension in argument shape .*. Perhaps you forgot to add the polymorphic_shapes&quot;</span>
    <span class="s3">with </span><span class="s1">self.assertRaisesRegex(ValueError</span><span class="s3">, </span><span class="s1">msg_re):</span>
      <span class="s1">check_shape_poly(self</span><span class="s3">,</span>
                       <span class="s1">jnp.sin</span><span class="s3">,</span>
                       <span class="s1">arg_descriptors=[RandArg((</span><span class="s5">1</span><span class="s3">, </span><span class="s5">3</span><span class="s3">,</span><span class="s1">)</span><span class="s3">, </span><span class="s1">_f32)]</span><span class="s3">,</span>
                       <span class="s1">input_signature=[tf.TensorSpec([</span><span class="s5">1</span><span class="s3">, None</span><span class="s1">])]</span><span class="s3">,</span>
                       <span class="s1">polymorphic_shapes=</span><span class="s3">None</span><span class="s1">)</span>

  <span class="s3">def </span><span class="s1">test_kwargs(self):</span>
    <span class="s2">&quot;&quot;&quot;Test shape polymorphism for a function with kwargs.&quot;&quot;&quot;</span>

    <span class="s1">x = np.ones(</span><span class="s5">3</span><span class="s3">, </span><span class="s1">dtype=np.float32)</span>
    <span class="s1">y = np.ones(</span><span class="s5">1</span><span class="s3">, </span><span class="s1">dtype=np.float32)</span>
    <span class="s3">def </span><span class="s1">f_jax(x</span><span class="s3">, </span><span class="s1">*</span><span class="s3">, </span><span class="s1">y):</span>
      <span class="s3">return </span><span class="s1">x + jnp.sin(y)</span>

    <span class="s1">f_tf: Callable[...</span><span class="s3">, </span><span class="s1">Any] = jax2tf.convert(f_jax</span><span class="s3">, </span><span class="s1">polymorphic_shapes=[</span><span class="s4">&quot;b, ...&quot;</span><span class="s1">])</span>
    <span class="s1">self.assertAllClose(f_jax(x</span><span class="s3">, </span><span class="s1">y=y)</span><span class="s3">, </span><span class="s1">f_tf(x</span><span class="s3">, </span><span class="s1">y=y))</span>

  <span class="s3">def </span><span class="s1">test_arg_avals(self):</span>
    <span class="s2">&quot;&quot;&quot;Test conversion of actual arguments to abstract values.&quot;&quot;&quot;</span>

    <span class="s3">def </span><span class="s1">check_avals(*</span><span class="s3">, </span><span class="s1">arg_shapes: Sequence[Sequence[Optional[int]]]</span><span class="s3">,</span>
                    <span class="s1">polymorphic_shapes: Sequence[Optional[Union[str</span><span class="s3">, </span><span class="s1">PS]]]</span><span class="s3">,</span>
                    <span class="s1">expected_avals: Optional[Sequence[core.ShapedArray]] = </span><span class="s3">None,</span>
                    <span class="s1">expected_shapeenv: Optional[Dict[str</span><span class="s3">, </span><span class="s1">int]] = </span><span class="s3">None,</span>
                    <span class="s1">eager_mode: bool = </span><span class="s3">False</span><span class="s1">):</span>
      <span class="s0"># Use eager mode only for when all arg_shapes are known, in order to</span>
      <span class="s0"># check expected_shapeenv.</span>
      <span class="s1">arg_dtypes = (_f32</span><span class="s3">,</span><span class="s1">) * len(arg_shapes)</span>
      <span class="s3">def </span><span class="s1">f_tf(*args_tf):</span>
        <span class="s1">avals = tuple(map(shape_poly.arg_aval</span><span class="s3">, </span><span class="s1">arg_shapes</span><span class="s3">, </span><span class="s1">arg_dtypes</span><span class="s3">, </span><span class="s1">polymorphic_shapes))</span>
        <span class="s1">dim_vars</span><span class="s3">, </span><span class="s1">get_dim_values_jax = shape_poly.prepare_dim_var_env(avals)</span>
        <span class="s1">dim_values</span><span class="s3">, </span><span class="s1">_ = jax2tf.jax2tf._interpret_fun_jax(get_dim_values_jax</span><span class="s3">,</span>
                                                         <span class="s1">args_tf</span><span class="s3">, </span><span class="s1">avals</span><span class="s3">, </span><span class="s4">&quot;&quot;</span><span class="s1">)</span>
        <span class="s3">if </span><span class="s1">expected_avals </span><span class="s3">is not None</span><span class="s1">:</span>
          <span class="s1">self.assertEqual(expected_avals</span><span class="s3">, </span><span class="s1">avals)</span>
        <span class="s3">return </span><span class="s1">dict(zip(dim_vars</span><span class="s3">, </span><span class="s1">dim_values))</span>
      <span class="s3">if </span><span class="s1">eager_mode:</span>
        <span class="s0"># If we want to check the shape_env then all arg_shapes must be known</span>
        <span class="s3">assert </span><span class="s1">all(all(d </span><span class="s3">is not None for </span><span class="s1">d </span><span class="s3">in </span><span class="s1">a_s)</span>
                   <span class="s3">for </span><span class="s1">a_s </span><span class="s3">in </span><span class="s1">arg_shapes)</span>
        <span class="s1">shape_env = f_tf(*[tf.ones(a_s</span><span class="s3">, </span><span class="s1">dtype=_f32) </span><span class="s3">for </span><span class="s1">a_s </span><span class="s3">in </span><span class="s1">arg_shapes])</span>
        <span class="s3">if </span><span class="s1">expected_shapeenv </span><span class="s3">is not None</span><span class="s1">:</span>
          <span class="s3">for </span><span class="s1">v</span><span class="s3">, </span><span class="s1">val </span><span class="s3">in </span><span class="s1">expected_shapeenv.items():</span>
            <span class="s1">self.assertEqual(val</span><span class="s3">, </span><span class="s1">shape_env.get(v))</span>
      <span class="s3">else</span><span class="s1">:</span>
        <span class="s1">f_tf = tf.function(autograph=</span><span class="s3">False</span><span class="s1">)(f_tf)</span>
        <span class="s1">f_tf.get_concrete_function(*[tf.TensorSpec(a_s</span><span class="s3">, </span><span class="s1">_f32)</span>
                                   <span class="s3">for </span><span class="s1">a_s </span><span class="s3">in </span><span class="s1">arg_shapes])</span>
        <span class="s3">assert not </span><span class="s1">expected_shapeenv</span><span class="s3">, </span><span class="s4">&quot;Should use eager_mode=True&quot;</span>

    <span class="s3">def </span><span class="s1">shaped_array(shape_spec: str</span><span class="s3">, </span><span class="s1">actual_shape: core.Shape):</span>
      <span class="s3">return </span><span class="s1">core.ShapedArray(</span>
          <span class="s1">shape_poly._parse_spec(shape_spec</span><span class="s3">, </span><span class="s1">actual_shape)</span><span class="s3">, </span><span class="s1">np.float32)</span>

    <span class="s0"># Known shapes for the arguments</span>
    <span class="s1">check_avals(</span>
        <span class="s1">arg_shapes=[(</span><span class="s5">2</span><span class="s3">, </span><span class="s5">3</span><span class="s1">)]</span><span class="s3">,</span>
        <span class="s1">polymorphic_shapes=[</span><span class="s3">None</span><span class="s1">]</span><span class="s3">,</span>
        <span class="s1">expected_avals=(shaped_array(</span><span class="s4">&quot;2, 3&quot;</span><span class="s3">, </span><span class="s1">[</span><span class="s5">2</span><span class="s3">, </span><span class="s5">3</span><span class="s1">])</span><span class="s3">,</span><span class="s1">))</span>

    <span class="s1">check_avals(</span>
        <span class="s1">arg_shapes=[(</span><span class="s5">2</span><span class="s3">, </span><span class="s5">3</span><span class="s1">)]</span><span class="s3">,</span>
        <span class="s1">polymorphic_shapes=[</span><span class="s4">&quot;(2, 3)&quot;</span><span class="s1">]</span><span class="s3">,</span>
        <span class="s1">expected_avals=(shaped_array(</span><span class="s4">&quot;2, 3&quot;</span><span class="s3">, </span><span class="s1">[</span><span class="s5">2</span><span class="s3">, </span><span class="s5">3</span><span class="s1">])</span><span class="s3">,</span><span class="s1">))</span>

    <span class="s1">check_avals(</span>
        <span class="s1">arg_shapes=[(</span><span class="s5">2</span><span class="s3">, </span><span class="s5">3</span><span class="s1">)]</span><span class="s3">,</span>
        <span class="s1">polymorphic_shapes=[</span><span class="s4">&quot;(_, 3)&quot;</span><span class="s1">]</span><span class="s3">,</span>
        <span class="s1">expected_avals=(shaped_array(</span><span class="s4">&quot;2, 3&quot;</span><span class="s3">, </span><span class="s1">[</span><span class="s5">2</span><span class="s3">, </span><span class="s5">3</span><span class="s1">])</span><span class="s3">,</span><span class="s1">))</span>

    <span class="s1">check_avals(</span>
        <span class="s1">arg_shapes=[(</span><span class="s5">2</span><span class="s3">, </span><span class="s5">3</span><span class="s1">)]</span><span class="s3">,</span>
        <span class="s1">polymorphic_shapes=[PS(</span><span class="s4">&quot;_&quot;</span><span class="s3">, </span><span class="s5">3</span><span class="s1">)]</span><span class="s3">,</span>
        <span class="s1">expected_avals=(shaped_array(</span><span class="s4">&quot;2, 3&quot;</span><span class="s3">, </span><span class="s1">[</span><span class="s5">2</span><span class="s3">, </span><span class="s5">3</span><span class="s1">])</span><span class="s3">,</span><span class="s1">))</span>

    <span class="s1">check_avals(</span>
        <span class="s1">arg_shapes=[(</span><span class="s5">2</span><span class="s3">, </span><span class="s5">3</span><span class="s1">)]</span><span class="s3">,</span>
        <span class="s1">polymorphic_shapes=[</span><span class="s4">&quot;...&quot;</span><span class="s1">]</span><span class="s3">,</span>
        <span class="s1">expected_avals=(shaped_array(</span><span class="s4">&quot;2, 3&quot;</span><span class="s3">, </span><span class="s1">[</span><span class="s5">2</span><span class="s3">, </span><span class="s5">3</span><span class="s1">])</span><span class="s3">,</span><span class="s1">))</span>

    <span class="s1">check_avals(</span>
        <span class="s1">arg_shapes=[(</span><span class="s5">2</span><span class="s3">, </span><span class="s5">3</span><span class="s1">)]</span><span class="s3">,</span>
        <span class="s1">polymorphic_shapes=[PS(...)]</span><span class="s3">,</span>
        <span class="s1">expected_avals=(shaped_array(</span><span class="s4">&quot;2, 3&quot;</span><span class="s3">, </span><span class="s1">[</span><span class="s5">2</span><span class="s3">, </span><span class="s5">3</span><span class="s1">])</span><span class="s3">,</span><span class="s1">))</span>

    <span class="s0"># Partially known shapes for the arguments</span>
    <span class="s1">check_avals(</span>
        <span class="s1">arg_shapes=[(</span><span class="s3">None, </span><span class="s5">3</span><span class="s1">)]</span><span class="s3">,</span>
        <span class="s1">polymorphic_shapes=[PS(</span><span class="s4">&quot;b&quot;</span><span class="s3">, </span><span class="s1">...)]</span><span class="s3">,</span>
        <span class="s1">expected_avals=(shaped_array(</span><span class="s4">&quot;(b, 3)&quot;</span><span class="s3">, </span><span class="s1">(</span><span class="s5">2</span><span class="s3">, </span><span class="s5">3</span><span class="s1">))</span><span class="s3">,</span><span class="s1">))</span>

    <span class="s1">check_avals(</span>
        <span class="s1">arg_shapes=[(</span><span class="s3">None, None</span><span class="s1">)]</span><span class="s3">,</span>
        <span class="s1">polymorphic_shapes=[</span><span class="s4">&quot;h, h&quot;</span><span class="s1">]</span><span class="s3">,</span>
        <span class="s1">expected_avals=(shaped_array(</span><span class="s4">&quot;(h, h)&quot;</span><span class="s3">, </span><span class="s1">(</span><span class="s5">2</span><span class="s3">, </span><span class="s5">2</span><span class="s1">))</span><span class="s3">,</span><span class="s1">))</span>

    <span class="s1">check_avals(</span>
        <span class="s1">arg_shapes=[(</span><span class="s5">2</span><span class="s3">, None</span><span class="s1">)]</span><span class="s3">,</span>
        <span class="s1">polymorphic_shapes=[</span><span class="s4">&quot;h, h&quot;</span><span class="s1">]</span><span class="s3">,</span>
        <span class="s1">expected_avals=(shaped_array(</span><span class="s4">&quot;(h, h)&quot;</span><span class="s3">, </span><span class="s1">(</span><span class="s5">2</span><span class="s3">, </span><span class="s5">2</span><span class="s1">))</span><span class="s3">,</span><span class="s1">))</span>

    <span class="s1">check_avals(</span>
        <span class="s1">arg_shapes=[(</span><span class="s3">None, </span><span class="s5">3</span><span class="s3">, </span><span class="s5">4</span><span class="s1">)]</span><span class="s3">,</span>
        <span class="s1">polymorphic_shapes=[</span><span class="s4">&quot;(c, b, a)&quot;</span><span class="s1">]</span><span class="s3">,</span>
        <span class="s1">expected_avals=(shaped_array(</span><span class="s4">&quot;(c, b, a)&quot;</span><span class="s3">, </span><span class="s1">(</span><span class="s5">2</span><span class="s3">, </span><span class="s5">3</span><span class="s3">, </span><span class="s5">4</span><span class="s1">))</span><span class="s3">,</span><span class="s1">)</span><span class="s3">,</span>
    <span class="s1">)</span>

    <span class="s0"># Check cases when the specifications are polynomials</span>
    <span class="s1">check_avals(</span>
        <span class="s1">arg_shapes=[(</span><span class="s5">2</span><span class="s3">, </span><span class="s5">3</span><span class="s1">)]</span><span class="s3">,</span>
        <span class="s1">polymorphic_shapes=[PS(</span><span class="s4">&quot;a + 1&quot;</span><span class="s3">, </span><span class="s4">&quot;b + 2&quot;</span><span class="s1">)]</span><span class="s3">,</span>
        <span class="s1">eager_mode=</span><span class="s3">True,</span>
        <span class="s1">expected_shapeenv=dict(a=</span><span class="s5">1</span><span class="s3">, </span><span class="s1">b=</span><span class="s5">1</span><span class="s1">))</span>

    <span class="s1">check_avals(</span>
        <span class="s1">arg_shapes=[(</span><span class="s5">7</span><span class="s3">, </span><span class="s5">5</span><span class="s1">)]</span><span class="s3">,</span>
        <span class="s1">polymorphic_shapes=[PS(</span><span class="s4">&quot;2 * a + b&quot;</span><span class="s3">, </span><span class="s4">&quot;b + 2&quot;</span><span class="s1">)]</span><span class="s3">,</span>
        <span class="s1">eager_mode=</span><span class="s3">True,</span>
        <span class="s1">expected_shapeenv=dict(a=</span><span class="s5">2</span><span class="s3">, </span><span class="s1">b=</span><span class="s5">3</span><span class="s1">))</span>

    <span class="s1">check_avals(</span>
        <span class="s1">arg_shapes=[(</span><span class="s5">7</span><span class="s3">, </span><span class="s5">11</span><span class="s3">, </span><span class="s5">4</span><span class="s1">)]</span><span class="s3">,</span>
        <span class="s1">polymorphic_shapes=[PS(</span><span class="s4">&quot;2 * a + b&quot;</span><span class="s3">, </span><span class="s4">&quot;b * b + 2&quot;</span><span class="s3">, </span><span class="s4">&quot;b + 1&quot;</span><span class="s1">)]</span><span class="s3">,</span>
        <span class="s1">eager_mode=</span><span class="s3">True,</span>
        <span class="s1">expected_shapeenv=dict(a=</span><span class="s5">2</span><span class="s3">, </span><span class="s1">b=</span><span class="s5">3</span><span class="s1">))</span>

    <span class="s1">check_avals(</span>
        <span class="s1">arg_shapes=[(</span><span class="s5">7</span><span class="s3">, </span><span class="s5">11</span><span class="s3">, </span><span class="s5">19</span><span class="s3">, </span><span class="s5">7</span><span class="s1">)]</span><span class="s3">,</span>
        <span class="s1">polymorphic_shapes=[PS(</span><span class="s4">&quot;2 * a + b&quot;</span><span class="s3">, </span><span class="s4">&quot;b * b + 2&quot;</span><span class="s3">, </span><span class="s4">&quot;b + c * c&quot;</span><span class="s3">, </span><span class="s4">&quot;2 * c + -1&quot;</span><span class="s1">)]</span><span class="s3">,</span>
        <span class="s1">eager_mode=</span><span class="s3">True,</span>
        <span class="s1">expected_shapeenv=dict(a=</span><span class="s5">2</span><span class="s3">, </span><span class="s1">b=</span><span class="s5">3</span><span class="s3">, </span><span class="s1">c=</span><span class="s5">4</span><span class="s1">))</span>

    <span class="s3">with </span><span class="s1">self.assertRaisesRegex(ValueError</span><span class="s3">,</span>
                                <span class="s4">&quot;Cannot solve for values of dimension variables {'b'}&quot;</span><span class="s1">):</span>
      <span class="s1">check_avals(</span>
          <span class="s1">arg_shapes=[(</span><span class="s5">4</span><span class="s3">, </span><span class="s5">36</span><span class="s3">, </span><span class="s5">3</span><span class="s1">)]</span><span class="s3">,</span>
          <span class="s1">polymorphic_shapes=[PS(</span><span class="s4">&quot;b * b&quot;</span><span class="s3">, </span><span class="s4">&quot;b * d * d&quot;</span><span class="s3">, </span><span class="s4">&quot;d&quot;</span><span class="s1">)])</span>

    <span class="s3">with </span><span class="s1">self.assertRaisesRegex(ValueError</span><span class="s3">,</span>
                                <span class="s4">&quot;Dimension variable b must have integer value &gt;= 1&quot;</span><span class="s1">):</span>
      <span class="s1">check_avals(</span>
          <span class="s1">arg_shapes=[(</span><span class="s5">5</span><span class="s3">, </span><span class="s5">36</span><span class="s1">)]</span><span class="s3">,</span>
          <span class="s1">polymorphic_shapes=[PS(</span><span class="s4">&quot;3 * b&quot;</span><span class="s3">, </span><span class="s1">...)]</span><span class="s3">,</span>
          <span class="s1">eager_mode=</span><span class="s3">True</span><span class="s1">)</span>

    <span class="s3">with </span><span class="s1">self.assertRaisesRegex(ValueError</span><span class="s3">,</span>
                                <span class="s4">&quot;Dimension variable b must have integer value &gt;= 1&quot;</span><span class="s1">):</span>
      <span class="s1">check_avals(</span>
          <span class="s1">arg_shapes=[(</span><span class="s5">10</span><span class="s3">, </span><span class="s5">3</span><span class="s1">)]</span><span class="s3">,</span>
          <span class="s1">polymorphic_shapes=[PS(</span><span class="s4">&quot;3 * b + 10&quot;</span><span class="s3">, </span><span class="s1">...)]</span><span class="s3">,</span>
          <span class="s1">eager_mode=</span><span class="s3">True</span><span class="s1">)</span>

    <span class="s3">with </span><span class="s1">self.assertRaisesRegex(ValueError</span><span class="s3">,</span>
                                <span class="s4">&quot;Dimension variable b must have integer value &gt;= 1&quot;</span><span class="s1">):</span>
      <span class="s1">check_avals(</span>
          <span class="s1">arg_shapes=[(</span><span class="s5">7</span><span class="s3">, </span><span class="s5">3</span><span class="s1">)]</span><span class="s3">,</span>
          <span class="s1">polymorphic_shapes=[PS(</span><span class="s4">&quot;3 * b + 10&quot;</span><span class="s3">, </span><span class="s1">...)]</span><span class="s3">,</span>
          <span class="s1">eager_mode=</span><span class="s3">True</span><span class="s1">)</span>


    <span class="s3">for </span><span class="s1">invalid_syntax </span><span class="s3">in </span><span class="s1">[</span><span class="s4">&quot;)(&quot;</span><span class="s3">, </span><span class="s4">&quot;2a&quot;</span><span class="s3">, </span><span class="s4">&quot;a@&quot;</span><span class="s3">, </span><span class="s4">&quot;a - 2&quot;</span><span class="s3">, </span><span class="s4">&quot;'a'&quot;</span><span class="s3">, </span><span class="s4">&quot;('a', ...)&quot;</span><span class="s1">]:</span>
      <span class="s3">with </span><span class="s1">self.assertRaisesRegex(ValueError</span><span class="s3">,</span>
                                  <span class="s1">re.escape(</span><span class="s4">&quot;has invalid syntax&quot;</span><span class="s1">)):</span>
        <span class="s1">check_avals(</span>
            <span class="s1">arg_shapes=[(</span><span class="s5">2</span><span class="s3">,</span><span class="s1">)]</span><span class="s3">, </span><span class="s1">polymorphic_shapes=[invalid_syntax])</span>

    <span class="s3">for </span><span class="s1">invalid_syntax </span><span class="s3">in </span><span class="s1">[</span><span class="s5">5.0</span><span class="s3">, </span><span class="s1">[</span><span class="s4">&quot;a list&quot;</span><span class="s1">]</span><span class="s3">, </span><span class="s1">(</span><span class="s4">&quot;a tuple&quot;</span><span class="s3">,</span><span class="s1">)</span><span class="s3">, </span><span class="s1">re.compile(</span><span class="s4">&quot;.&quot;</span><span class="s1">)]:</span>
      <span class="s3">with </span><span class="s1">self.assertRaisesRegex(ValueError</span><span class="s3">,</span>
                                  <span class="s1">re.escape(</span><span class="s4">&quot;Invalid polymorphic shape element&quot;</span><span class="s1">)):</span>
        <span class="s1">check_avals(</span>
            <span class="s1">arg_shapes=[(</span><span class="s5">2</span><span class="s3">,</span><span class="s1">)]</span><span class="s3">, </span><span class="s1">polymorphic_shapes=[PS([invalid_syntax])])</span>

    <span class="s3">with </span><span class="s1">self.assertRaisesRegex(</span>
        <span class="s1">ValueError</span><span class="s3">,</span>
        <span class="s1">re.escape(</span><span class="s4">&quot;polymorphic shape '..., 3' can contain Ellipsis only at the end.&quot;</span><span class="s1">)):</span>
      <span class="s1">check_avals(</span>
          <span class="s1">arg_shapes=[(</span><span class="s5">2</span><span class="s3">, </span><span class="s5">3</span><span class="s1">)]</span><span class="s3">,</span>
          <span class="s1">polymorphic_shapes=[</span><span class="s4">&quot;..., 3&quot;</span><span class="s1">])</span>

    <span class="s3">with </span><span class="s1">self.assertRaisesRegex(</span>
        <span class="s1">ValueError</span><span class="s3">,</span>
        <span class="s1">re.escape(</span>
            <span class="s4">&quot;polymorphic shape '2, 3, 4, ...' of rank 3 must match the rank 2 of argument shape (2, 3).&quot;</span><span class="s1">)</span>
    <span class="s1">):</span>
      <span class="s1">check_avals(</span>
          <span class="s1">arg_shapes=[(</span><span class="s5">2</span><span class="s3">, </span><span class="s5">3</span><span class="s1">)]</span><span class="s3">,</span>
          <span class="s1">polymorphic_shapes=[</span><span class="s4">&quot;2, 3, 4, ...&quot;</span><span class="s1">])</span>

    <span class="s3">with </span><span class="s1">self.assertRaisesRegex(</span>
        <span class="s1">ValueError</span><span class="s3">,</span>
        <span class="s1">re.escape(</span>
            <span class="s4">&quot;polymorphic shape (Ellipsis, 3) can contain Ellipsis only at the end.&quot;</span><span class="s1">)):</span>
      <span class="s1">check_avals(</span>
          <span class="s1">arg_shapes=[(</span><span class="s5">2</span><span class="s3">, </span><span class="s5">3</span><span class="s1">)]</span><span class="s3">,</span>
          <span class="s1">polymorphic_shapes=[PS(...</span><span class="s3">, </span><span class="s5">3</span><span class="s1">)])</span>

    <span class="s3">with </span><span class="s1">self.assertRaisesRegex(</span>
        <span class="s1">ValueError</span><span class="s3">,</span>
        <span class="s1">re.escape(</span>
            <span class="s4">&quot;polymorphic shape None in axis 1 must contain a dimension variable for unknown dimension in argument shape (2, None)&quot;</span>
        <span class="s1">)):</span>
      <span class="s1">check_avals(</span>
          <span class="s1">arg_shapes=[(</span><span class="s5">2</span><span class="s3">, None</span><span class="s1">)]</span><span class="s3">,</span>
          <span class="s1">polymorphic_shapes=[</span><span class="s3">None</span><span class="s1">])</span>

    <span class="s3">with </span><span class="s1">self.assertRaisesRegex(</span>
        <span class="s1">ValueError</span><span class="s3">,</span>
        <span class="s1">re.escape(</span><span class="s4">&quot;polymorphic shape '()' of rank 0 must match the rank 2 of argument shape (2, 3)&quot;</span><span class="s1">)):</span>
      <span class="s1">check_avals(</span>
          <span class="s1">arg_shapes=[(</span><span class="s5">2</span><span class="s3">, </span><span class="s5">3</span><span class="s1">)]</span><span class="s3">, </span><span class="s1">polymorphic_shapes=[</span><span class="s4">&quot;()&quot;</span><span class="s1">])</span>

    <span class="s3">with </span><span class="s1">self.assertRaisesRegex(</span>
        <span class="s1">ValueError</span><span class="s3">,</span>
        <span class="s1">re.escape(</span>
            <span class="s4">&quot;polymorphic shape '(_, _)' in axis 1 must contain a dimension variable &quot;</span>
            <span class="s4">&quot;for unknown dimension in argument shape (2, None)&quot;</span>
        <span class="s1">)):</span>
      <span class="s1">check_avals(</span>
          <span class="s1">arg_shapes=[(</span><span class="s5">2</span><span class="s3">, None</span><span class="s1">)]</span><span class="s3">,</span>
          <span class="s1">polymorphic_shapes=[</span><span class="s4">&quot;(_, _)&quot;</span><span class="s1">])</span>

    <span class="s3">with </span><span class="s1">self.assertRaisesRegex(</span>
        <span class="s1">ValueError</span><span class="s3">,</span>
        <span class="s1">re.escape(</span>
            <span class="s4">&quot;polymorphic shape '(2, 13)' in axis 1 must match the known dimension size 3 &quot;</span>
            <span class="s4">&quot;for argument shape (2, 3)&quot;</span>
        <span class="s1">)):</span>
      <span class="s1">check_avals(</span>
          <span class="s1">arg_shapes=[(</span><span class="s5">2</span><span class="s3">, </span><span class="s5">3</span><span class="s1">)]</span><span class="s3">,</span>
          <span class="s1">polymorphic_shapes=[</span><span class="s4">&quot;(2, 13)&quot;</span><span class="s1">])</span>

    <span class="s3">with </span><span class="s1">self.assertRaisesRegex(</span>
        <span class="s1">ValueError</span><span class="s3">,</span>
        <span class="s1">re.escape(</span>
            <span class="s4">&quot;polymorphic shape '(2, 3)' in axis 1 must contain a dimension variable for &quot;</span>
            <span class="s4">&quot;unknown dimension in argument shape (2, None)&quot;</span>
        <span class="s1">)):</span>
      <span class="s1">check_avals(</span>
          <span class="s1">arg_shapes=[(</span><span class="s5">2</span><span class="s3">, None</span><span class="s1">)]</span><span class="s3">,</span>
          <span class="s1">polymorphic_shapes=[</span><span class="s4">&quot;(2, 3)&quot;</span><span class="s1">])</span>

    <span class="s3">with </span><span class="s1">self.assertRaisesRegex(</span>
        <span class="s1">ValueError</span><span class="s3">,</span>
        <span class="s4">&quot;Found inconsistency when solving.*&quot;</span><span class="s1">):</span>
      <span class="s1">check_avals(</span>
          <span class="s1">arg_shapes=[(</span><span class="s5">2</span><span class="s3">, </span><span class="s5">3</span><span class="s1">)]</span><span class="s3">,</span>
          <span class="s1">polymorphic_shapes=[</span><span class="s4">&quot;(a, a)&quot;</span><span class="s1">]</span><span class="s3">,</span>
          <span class="s1">eager_mode=</span><span class="s3">True</span><span class="s1">)</span>

    <span class="s0"># Same error across multiple arguments</span>
    <span class="s3">with </span><span class="s1">self.assertRaisesRegex(</span>
        <span class="s1">ValueError</span><span class="s3">,</span>
        <span class="s4">&quot;Found inconsistency when solving.*&quot;</span><span class="s1">):</span>
      <span class="s1">check_avals(</span>
          <span class="s1">arg_shapes=[(</span><span class="s5">2</span><span class="s3">, </span><span class="s5">3</span><span class="s1">)</span><span class="s3">, </span><span class="s1">(</span><span class="s5">5</span><span class="s3">,</span><span class="s1">)]</span><span class="s3">,</span>
          <span class="s1">polymorphic_shapes=[</span><span class="s4">&quot;a, ...&quot;</span><span class="s3">, </span><span class="s4">&quot;a&quot;</span><span class="s1">]</span><span class="s3">,</span>
          <span class="s1">eager_mode=</span><span class="s3">True</span><span class="s1">)</span>

  <span class="s3">def </span><span class="s1">test_pytree(self):</span>
    <span class="s2">&quot;&quot;&quot;Arguments and polymorphic_shapes are pytrees.&quot;&quot;&quot;</span>

    <span class="s0"># Arguments are of the form [([x00, x01], [x10]), dict(a=ya, b=yb)]</span>
    <span class="s3">def </span><span class="s1">add_all_jax(x_pair_of_list</span><span class="s3">, </span><span class="s1">y_dict):</span>
      <span class="s1">x_list_0</span><span class="s3">, </span><span class="s1">x_list_1 = x_pair_of_list</span>
      <span class="s3">return </span><span class="s1">functools.reduce(op.add</span><span class="s3">,</span>
                              <span class="s1">x_list_0 + x_list_1 + [y_dict[</span><span class="s4">&quot;a&quot;</span><span class="s1">]</span><span class="s3">, </span><span class="s1">y_dict[</span><span class="s4">&quot;b&quot;</span><span class="s1">]])</span>

    <span class="s1">check_shape_poly(self</span><span class="s3">,</span>
                     <span class="s1">add_all_jax</span><span class="s3">,</span>
                     <span class="s1">skip_jax_run=</span><span class="s3">True,</span>
                     <span class="s1">input_signature=[([tf.TensorSpec([</span><span class="s3">None</span><span class="s1">])</span><span class="s3">, </span><span class="s1">tf.TensorSpec([</span><span class="s3">None</span><span class="s1">])]</span><span class="s3">,</span>
                                       <span class="s1">[tf.TensorSpec([</span><span class="s3">None</span><span class="s1">])])</span><span class="s3">,</span>
                                      <span class="s1">dict(a=tf.TensorSpec([</span><span class="s3">None</span><span class="s1">])</span><span class="s3">,</span>
                                           <span class="s1">b=tf.TensorSpec([</span><span class="s3">None</span><span class="s1">]))]</span><span class="s3">,</span>
                     <span class="s1">polymorphic_shapes=[([</span><span class="s4">&quot;v&quot;</span><span class="s3">, </span><span class="s4">&quot;v&quot;</span><span class="s1">]</span><span class="s3">, </span><span class="s1">[</span><span class="s4">&quot;v&quot;</span><span class="s1">])</span><span class="s3">,</span>
                                         <span class="s1">dict(a=</span><span class="s4">&quot;v&quot;</span><span class="s3">, </span><span class="s1">b=</span><span class="s4">&quot;v&quot;</span><span class="s1">)]</span><span class="s3">,</span>
                     <span class="s1">expected_output_signature=tf.TensorSpec([</span><span class="s3">None</span><span class="s1">]))</span>

    <span class="s0"># Now partial polymorphic_shapes; the parts of the polymorphic_shapes that</span>
    <span class="s0"># are not specified must have full input_signatures.</span>
    <span class="s1">check_shape_poly(self</span><span class="s3">,</span>
                     <span class="s1">add_all_jax</span><span class="s3">,</span>
                     <span class="s1">skip_jax_run=</span><span class="s3">True,</span>
                     <span class="s1">input_signature=[([tf.TensorSpec([</span><span class="s5">4</span><span class="s1">])</span><span class="s3">, </span><span class="s1">tf.TensorSpec([</span><span class="s5">4</span><span class="s1">])]</span><span class="s3">, </span><span class="s1">[tf.TensorSpec([</span><span class="s5">4</span><span class="s1">])])</span><span class="s3">,</span>
                                      <span class="s1">dict(a=tf.TensorSpec([</span><span class="s5">4</span><span class="s1">])</span><span class="s3">, </span><span class="s1">b=tf.TensorSpec([</span><span class="s5">4</span><span class="s1">]))]</span><span class="s3">,</span>
                     <span class="s1">polymorphic_shapes=[([</span><span class="s4">&quot;(4,)&quot;</span><span class="s3">, </span><span class="s4">&quot;(_,)&quot;</span><span class="s1">]</span><span class="s3">, </span><span class="s1">[(</span><span class="s4">&quot;4,&quot;</span><span class="s1">)])</span><span class="s3">,</span>
                                         <span class="s1">dict(a=</span><span class="s4">&quot;(_,)&quot;</span><span class="s3">, </span><span class="s1">b=</span><span class="s4">&quot;(4,)&quot;</span><span class="s1">)]</span><span class="s3">,</span>
                     <span class="s1">expected_output_signature=tf.TensorSpec([</span><span class="s5">4</span><span class="s1">]))</span>

  <span class="s3">def </span><span class="s1">test_with_nested_jit(self):</span>
    <span class="s3">def </span><span class="s1">f_jax(x):  </span><span class="s0"># x: f32[w, h]</span>
      <span class="s0"># x + (np.sin(x) + np.broadcast_to(np.arange(x.shape[1]), x.shape))</span>
      <span class="s3">return </span><span class="s1">jnp.sin(x) + jnp.arange(x.shape[</span><span class="s5">1</span><span class="s1">]</span><span class="s3">, </span><span class="s1">dtype=x.dtype)</span>
    <span class="s1">check_shape_poly(self</span><span class="s3">,</span>
                     <span class="s3">lambda </span><span class="s1">x: x + jax.jit(f_jax)(x)</span><span class="s3">,</span>
                     <span class="s1">arg_descriptors=[RandArg((</span><span class="s5">3</span><span class="s3">, </span><span class="s5">4</span><span class="s1">)</span><span class="s3">, </span><span class="s1">_f32)]</span><span class="s3">,</span>
                     <span class="s1">poly_axes=[(</span><span class="s5">0</span><span class="s3">, </span><span class="s5">1</span><span class="s1">)])</span>

  <span class="s3">def </span><span class="s1">test_non_trivial_polynomials_spec(self):</span>
    <span class="s3">if </span><span class="s1">config.jax_dynamic_shapes:</span>
      <span class="s3">raise </span><span class="s1">unittest.SkipTest(</span><span class="s4">&quot;--jax_dynamic_shapes supports only trivial polynomials&quot;</span><span class="s1">)</span>
    <span class="s0"># We can handle non-trivial polynomials in the input shape,</span>
    <span class="s0"># as long as all variables also occur in trivial polynoamials</span>
    <span class="s1">check_shape_poly(self</span><span class="s3">,</span>
                     <span class="s3">lambda </span><span class="s1">x</span><span class="s3">, </span><span class="s1">y: x + y.reshape((-</span><span class="s5">1</span><span class="s3">,</span><span class="s1">))</span><span class="s3">,</span>
                     <span class="s1">arg_descriptors=[RandArg((</span><span class="s5">9</span><span class="s3">,</span><span class="s1">)</span><span class="s3">, </span><span class="s1">_f32)</span><span class="s3">, </span><span class="s1">RandArg((</span><span class="s5">3</span><span class="s3">, </span><span class="s5">3</span><span class="s1">)</span><span class="s3">, </span><span class="s1">_f32)]</span><span class="s3">,</span>
                     <span class="s1">input_signature=[tf.TensorSpec([</span><span class="s3">None</span><span class="s1">])</span><span class="s3">, </span><span class="s1">tf.TensorSpec([</span><span class="s3">None, None</span><span class="s1">])]</span><span class="s3">,</span>
                     <span class="s1">polymorphic_shapes=[</span><span class="s4">&quot;b * b&quot;</span><span class="s3">, </span><span class="s4">&quot;b, b&quot;</span><span class="s1">])</span>

  <span class="s3">def </span><span class="s1">test_unused_args(self):</span>
    <span class="s0"># Tests with functions that do not use their inputs.</span>

    <span class="s0"># First arg unused, not polymorphic</span>
    <span class="s1">check_shape_poly(self</span><span class="s3">,</span>
                     <span class="s3">lambda </span><span class="s1">x_unused</span><span class="s3">, </span><span class="s1">y: y * </span><span class="s5">2.0</span><span class="s3">,</span>
                     <span class="s1">arg_descriptors=[RandArg((</span><span class="s5">2</span><span class="s3">, </span><span class="s5">3</span><span class="s1">)</span><span class="s3">, </span><span class="s1">_f32)</span><span class="s3">, </span><span class="s1">RandArg((</span><span class="s5">3</span><span class="s3">,</span><span class="s1">)</span><span class="s3">, </span><span class="s1">_f32)]</span><span class="s3">,</span>
                     <span class="s1">input_signature=[tf.TensorSpec([])</span><span class="s3">, </span><span class="s1">tf.TensorSpec([</span><span class="s3">None</span><span class="s1">])]</span><span class="s3">,</span>
                     <span class="s1">polymorphic_shapes=[</span><span class="s3">None, </span><span class="s4">&quot;b&quot;</span><span class="s1">])</span>

    <span class="s0"># Some args unused, not polymorphic</span>
    <span class="s1">check_shape_poly(self</span><span class="s3">,</span>
                     <span class="s3">lambda </span><span class="s1">x_unused</span><span class="s3">, </span><span class="s1">y</span><span class="s3">, </span><span class="s1">z_unused</span><span class="s3">, </span><span class="s1">w: jnp.concatenate([y</span><span class="s3">, </span><span class="s1">w])</span><span class="s3">,</span>
                     <span class="s1">arg_descriptors=[RandArg((</span><span class="s5">3</span><span class="s3">,</span><span class="s1">)</span><span class="s3">, </span><span class="s1">_f32)</span><span class="s3">, </span><span class="s1">RandArg((</span><span class="s5">4</span><span class="s3">,</span><span class="s1">)</span><span class="s3">, </span><span class="s1">_f32)</span><span class="s3">,</span>
                           <span class="s1">RandArg((</span><span class="s5">5</span><span class="s3">,</span><span class="s1">)</span><span class="s3">, </span><span class="s1">_f32)</span><span class="s3">, </span><span class="s1">RandArg((</span><span class="s5">6</span><span class="s3">,</span><span class="s1">)</span><span class="s3">, </span><span class="s1">_f32)]</span><span class="s3">,</span>
                     <span class="s1">input_signature=[tf.TensorSpec([])</span><span class="s3">, </span><span class="s1">tf.TensorSpec([</span><span class="s3">None</span><span class="s1">])</span><span class="s3">,</span>
                         <span class="s1">tf.TensorSpec([])</span><span class="s3">, </span><span class="s1">tf.TensorSpec([</span><span class="s3">None</span><span class="s1">])]</span><span class="s3">,</span>
                     <span class="s1">polymorphic_shapes=[</span><span class="s3">None, </span><span class="s4">&quot;b1&quot;</span><span class="s3">, None, </span><span class="s4">&quot;b2&quot;</span><span class="s1">])</span>

    <span class="s0"># A polymorphic arg is not used, but the dimension var appears</span>
    <span class="s0"># in a used arg also</span>
    <span class="s1">check_shape_poly(self</span><span class="s3">,</span>
                     <span class="s3">lambda </span><span class="s1">x_unused</span><span class="s3">, </span><span class="s1">y: y * </span><span class="s5">2.0</span><span class="s3">,</span>
                     <span class="s1">arg_descriptors=[RandArg((</span><span class="s5">3</span><span class="s3">,</span><span class="s1">)</span><span class="s3">, </span><span class="s1">_f32)</span><span class="s3">, </span><span class="s1">RandArg((</span><span class="s5">3</span><span class="s3">,</span><span class="s1">)</span><span class="s3">, </span><span class="s1">_f32)]</span><span class="s3">,</span>
                     <span class="s1">input_signature=[tf.TensorSpec([</span><span class="s3">None</span><span class="s1">])</span><span class="s3">, </span><span class="s1">tf.TensorSpec([</span><span class="s3">None</span><span class="s1">])]</span><span class="s3">,</span>
                     <span class="s1">polymorphic_shapes=[</span><span class="s4">&quot;b&quot;</span><span class="s3">, </span><span class="s4">&quot;b&quot;</span><span class="s1">])</span>

    <span class="s0"># A polymorphic arg is not used, and the dimension var does not appear</span>
    <span class="s0"># elsewhere.</span>
    <span class="s1">check_shape_poly(self</span><span class="s3">,</span>
        <span class="s3">lambda </span><span class="s1">x_unused</span><span class="s3">, </span><span class="s1">y: y * </span><span class="s5">2.0</span><span class="s3">,</span>
        <span class="s1">arg_descriptors=[RandArg((</span><span class="s5">4</span><span class="s3">,</span><span class="s1">)</span><span class="s3">, </span><span class="s1">_f32)</span><span class="s3">, </span><span class="s1">RandArg((</span><span class="s5">3</span><span class="s3">,</span><span class="s1">)</span><span class="s3">, </span><span class="s1">_f32)]</span><span class="s3">,</span>
        <span class="s1">input_signature=[tf.TensorSpec([</span><span class="s3">None</span><span class="s1">])</span><span class="s3">, </span><span class="s1">tf.TensorSpec([</span><span class="s3">None</span><span class="s1">])]</span><span class="s3">,</span>
        <span class="s1">polymorphic_shapes=[</span><span class="s4">&quot;b1&quot;</span><span class="s3">, </span><span class="s4">&quot;b2&quot;</span><span class="s1">])</span>

    <span class="s0"># A polymorphic arg is not used, and the dimension var does appear</span>
    <span class="s0"># elsewhere but not as a trivial monomial.</span>
    <span class="s1">check_shape_poly(self</span><span class="s3">,</span>
        <span class="s3">lambda </span><span class="s1">x_unused</span><span class="s3">, </span><span class="s1">y: y * </span><span class="s5">2.0</span><span class="s3">,</span>
        <span class="s1">arg_descriptors=[RandArg((</span><span class="s5">3</span><span class="s3">,</span><span class="s1">)</span><span class="s3">, </span><span class="s1">_f32)</span><span class="s3">, </span><span class="s1">RandArg((</span><span class="s5">9</span><span class="s3">,</span><span class="s1">)</span><span class="s3">, </span><span class="s1">_f32)]</span><span class="s3">,</span>
        <span class="s1">input_signature=[tf.TensorSpec([</span><span class="s3">None</span><span class="s1">])</span><span class="s3">, </span><span class="s1">tf.TensorSpec([</span><span class="s3">None</span><span class="s1">])]</span><span class="s3">,</span>
        <span class="s1">polymorphic_shapes=[</span><span class="s4">&quot;b1&quot;</span><span class="s3">, </span><span class="s4">&quot;b1 * b1&quot;</span><span class="s1">])</span>

    <span class="s0"># It is not sufficient to just use the shape of an input; it is still unused</span>
    <span class="s1">check_shape_poly(self</span><span class="s3">,</span>
        <span class="s3">lambda </span><span class="s1">x_unused</span><span class="s3">, </span><span class="s1">y: y + x_unused.shape[</span><span class="s5">0</span><span class="s1">]</span><span class="s3">,</span>
        <span class="s1">arg_descriptors=[RandArg((</span><span class="s5">3</span><span class="s3">,</span><span class="s1">)</span><span class="s3">, </span><span class="s1">_f32)</span><span class="s3">, </span><span class="s1">RandArg((</span><span class="s5">9</span><span class="s3">,</span><span class="s1">)</span><span class="s3">, </span><span class="s1">_f32)]</span><span class="s3">,</span>
        <span class="s1">input_signature=[tf.TensorSpec([</span><span class="s3">None</span><span class="s1">])</span><span class="s3">, </span><span class="s1">tf.TensorSpec([</span><span class="s3">None</span><span class="s1">])]</span><span class="s3">,</span>
        <span class="s1">polymorphic_shapes=[</span><span class="s4">&quot;b1&quot;</span><span class="s3">, </span><span class="s4">&quot;b2&quot;</span><span class="s1">])</span>

  <span class="s3">def </span><span class="s1">test_with_custom_vjp(self):</span>
    <span class="s2">&quot;&quot;&quot;Shape-polymorphic custom VJP.&quot;&quot;&quot;</span>

    <span class="s1">@jax.custom_vjp</span>
    <span class="s3">def </span><span class="s1">f(x):</span>
      <span class="s0"># x: [b1, b2, d1, d2] (a batch of matrices)</span>
      <span class="s0"># res: [b1, b2, d1, d1]</span>
      <span class="s3">return </span><span class="s1">jnp.matmul(x</span><span class="s3">, </span><span class="s1">jnp.transpose(x</span><span class="s3">, </span><span class="s1">axes=(</span><span class="s5">0</span><span class="s3">, </span><span class="s5">1</span><span class="s3">, </span><span class="s5">3</span><span class="s3">, </span><span class="s5">2</span><span class="s1">)))</span>

    <span class="s0"># f_fwd: a -&gt; (b, residual)</span>
    <span class="s3">def </span><span class="s1">f_fwd(x):</span>
      <span class="s0"># x: [b1, b2, d1, d2]</span>
      <span class="s0"># b: [b1, b2, d1, d1]</span>
      <span class="s0"># res: [b1, b2, d1, d1]</span>
      <span class="s0"># residual: [b1, b2, d1, d2]</span>
      <span class="s3">return </span><span class="s1">f(x)</span><span class="s3">, </span><span class="s5">3. </span><span class="s1">* x</span>

    <span class="s0"># f_bwd: (residual, CT b) -&gt; [CT a]</span>
    <span class="s3">def </span><span class="s1">f_bwd(residual</span><span class="s3">, </span><span class="s1">ct_b):</span>
      <span class="s0"># residual: [b1, b2, d1, d2]</span>
      <span class="s0"># ct_b: [b1, b2, d1, d1]</span>
      <span class="s0"># ct_a: [b1, b2, d1, d2]</span>
      <span class="s3">return </span><span class="s1">jnp.matmul(ct_b</span><span class="s3">, </span><span class="s1">residual)</span><span class="s3">,</span>

    <span class="s1">f.defvjp(f_fwd</span><span class="s3">, </span><span class="s1">f_bwd)</span>
    <span class="s1">x = np.ones((</span><span class="s5">2</span><span class="s3">, </span><span class="s5">3</span><span class="s3">, </span><span class="s5">4</span><span class="s3">, </span><span class="s5">5</span><span class="s1">)</span><span class="s3">, </span><span class="s1">dtype=np.float32)</span>
    <span class="s1">res_jax = f(x)</span>
    <span class="s1">res_jax_grad = jax.grad(</span><span class="s3">lambda </span><span class="s1">x: jnp.sum(f(x)))(x)</span>

    <span class="s1">f_tf = jax2tf.convert(f</span><span class="s3">, </span><span class="s1">polymorphic_shapes=[</span><span class="s4">&quot;(batch1, batch2, d1, d2)&quot;</span><span class="s1">])</span>
    <span class="s1">self.assertAllClose(res_jax</span><span class="s3">, </span><span class="s1">f_tf(x))</span>

    <span class="s1">xv = tf.Variable(x</span><span class="s3">, </span><span class="s1">dtype=np.float32)</span>

    <span class="s3">def </span><span class="s1">tf_value_and_grad(xv):</span>
      <span class="s3">with </span><span class="s1">tf.GradientTape() </span><span class="s3">as </span><span class="s1">tape:</span>
        <span class="s1">tape.watch(xv)</span>
        <span class="s1">res_tf = f_tf(xv)</span>
        <span class="s1">res_tf_grad = tape.gradient(res_tf</span><span class="s3">, </span><span class="s1">xv)</span>
        <span class="s3">return </span><span class="s1">res_tf</span><span class="s3">, </span><span class="s1">res_tf_grad</span>

    <span class="s1">res_tf</span><span class="s3">, </span><span class="s1">res_tf_grad = tf_value_and_grad(xv)</span>
    <span class="s1">self.assertAllClose(res_jax</span><span class="s3">, </span><span class="s1">res_tf)</span>
    <span class="s1">self.assertAllClose(res_jax_grad</span><span class="s3">, </span><span class="s1">res_tf_grad)</span>

    <span class="s0"># Now use TF tracing for the gradient</span>
    <span class="s1">tf_grad = tf.function(</span>
       <span class="s1">tf_value_and_grad</span><span class="s3">, </span><span class="s1">autograph=</span><span class="s3">False</span><span class="s1">).get_concrete_function(</span>
           <span class="s1">tf.TensorSpec([</span><span class="s5">3</span><span class="s3">, </span><span class="s5">4</span><span class="s3">, </span><span class="s5">8</span><span class="s3">, </span><span class="s5">9</span><span class="s1">]))</span>

    <span class="s0"># for native serialization we cannot refine the inferred shape of the</span>
    <span class="s0"># output if the input is more specific than polymorphic_shapes.</span>
    <span class="s3">if </span><span class="s1">config.jax2tf_default_native_serialization:</span>
      <span class="s1">self.assertEqual((</span><span class="s3">None, None, None, None</span><span class="s1">)</span><span class="s3">, </span><span class="s1">tuple(tf_grad.output_shapes[</span><span class="s5">0</span><span class="s1">]))</span>
      <span class="s1">self.assertEqual((</span><span class="s3">None, None, None, None</span><span class="s1">)</span><span class="s3">, </span><span class="s1">tuple(tf_grad.output_shapes[</span><span class="s5">1</span><span class="s1">]))</span>
    <span class="s3">else</span><span class="s1">:</span>
      <span class="s1">self.assertEqual((</span><span class="s5">3</span><span class="s3">, </span><span class="s5">4</span><span class="s3">, </span><span class="s5">8</span><span class="s3">, </span><span class="s5">8</span><span class="s1">)</span><span class="s3">, </span><span class="s1">tuple(tf_grad.output_shapes[</span><span class="s5">0</span><span class="s1">]))</span>
      <span class="s1">self.assertEqual((</span><span class="s5">3</span><span class="s3">, </span><span class="s5">4</span><span class="s3">, </span><span class="s5">8</span><span class="s3">, </span><span class="s5">9</span><span class="s1">)</span><span class="s3">, </span><span class="s1">tuple(tf_grad.output_shapes[</span><span class="s5">1</span><span class="s1">]))</span>

  <span class="s3">def </span><span class="s1">test_gradients_pytree(self):</span>
    <span class="s2">&quot;&quot;&quot;Shape polymorphism with gradients and pytrees for inputs and outputs.&quot;&quot;&quot;</span>

    <span class="s3">def </span><span class="s1">f(x):</span>
      <span class="s0"># x: dict(x=[b, 3, 4])</span>
      <span class="s0"># res: dict(res=[b, 3, 4])</span>
      <span class="s3">return </span><span class="s1">dict(res=x[</span><span class="s4">&quot;x&quot;</span><span class="s1">] * </span><span class="s5">2.</span><span class="s1">)</span>

    <span class="s1">check_shape_poly(self</span><span class="s3">,</span>
                     <span class="s1">f</span><span class="s3">,</span>
                     <span class="s1">skip_jax_run=</span><span class="s3">True,</span>
                     <span class="s1">input_signature=[dict(x=tf.TensorSpec([</span><span class="s3">None, </span><span class="s5">3</span><span class="s3">, </span><span class="s5">4</span><span class="s1">]))]</span><span class="s3">,</span>
                     <span class="s1">polymorphic_shapes=[dict(x=(</span><span class="s4">&quot;b, 3, 4&quot;</span><span class="s1">))])</span>

    <span class="s1">f_tf = jax2tf.convert(f</span><span class="s3">, </span><span class="s1">polymorphic_shapes=[dict(x=(</span><span class="s4">&quot;b, 3, 4&quot;</span><span class="s1">))])</span>
    <span class="s1">x = dict(x=np.ones((</span><span class="s5">2</span><span class="s3">, </span><span class="s5">3</span><span class="s3">, </span><span class="s5">4</span><span class="s1">)</span><span class="s3">, </span><span class="s1">dtype=np.float32))</span>
    <span class="s1">xv = tf.Variable(x[</span><span class="s4">&quot;x&quot;</span><span class="s1">]</span><span class="s3">, </span><span class="s1">dtype=np.float32)</span>

    <span class="s3">def </span><span class="s1">tf_value_and_grad(xv):</span>
      <span class="s0"># xv: [b, 3, 4]</span>
      <span class="s0"># res_value: dict(res=[b, 3, 4])</span>
      <span class="s0"># res_grad: dict(grad=[b, 3, 4])</span>
      <span class="s3">with </span><span class="s1">tf.GradientTape() </span><span class="s3">as </span><span class="s1">tape:</span>
        <span class="s1">tape.watch(xv)</span>
        <span class="s1">res_tf = f_tf(dict(x=xv))</span>
        <span class="s1">res_tf_grad = tape.gradient(res_tf</span><span class="s3">, </span><span class="s1">xv)</span>
        <span class="s3">return </span><span class="s1">res_tf</span><span class="s3">, </span><span class="s1">dict(grad=res_tf_grad)</span>

    <span class="s1">res_tf</span><span class="s3">, </span><span class="s1">res_tf_grad = tf_value_and_grad(xv)</span>
    <span class="s0"># Now use TF tracing for the gradient</span>
    <span class="s1">tf_grad = tf.function(</span>
        <span class="s1">tf_value_and_grad</span><span class="s3">,</span>
        <span class="s1">autograph=</span><span class="s3">False</span><span class="s1">).get_concrete_function(tf.TensorSpec([</span><span class="s3">None, </span><span class="s5">3</span><span class="s3">, </span><span class="s5">4</span><span class="s1">]))</span>
    <span class="s0"># The shape of the value</span>
    <span class="s1">self.assertEqual((</span><span class="s3">None, </span><span class="s5">3</span><span class="s3">, </span><span class="s5">4</span><span class="s1">)</span><span class="s3">, </span><span class="s1">tuple(tf_grad.output_shapes[</span><span class="s5">0</span><span class="s1">][</span><span class="s4">&quot;res&quot;</span><span class="s1">]))</span>
    <span class="s0"># The shape of the gradient should match the input</span>
    <span class="s1">self.assertEqual((</span><span class="s3">None, </span><span class="s5">3</span><span class="s3">, </span><span class="s5">4</span><span class="s1">)</span><span class="s3">, </span><span class="s1">tuple(tf_grad.output_shapes[</span><span class="s5">1</span><span class="s1">][</span><span class="s4">&quot;grad&quot;</span><span class="s1">]))</span>

  <span class="s3">def </span><span class="s1">test_grad_not_var_output(self):</span>
    <span class="s0"># Output of the function has poly shapes, non-variable</span>
    <span class="s3">if </span><span class="s1">config.jax2tf_default_native_serialization:</span>
      <span class="s3">raise </span><span class="s1">unittest.SkipTest(</span><span class="s4">&quot;Not supported with native serialization&quot;</span><span class="s1">)</span>
    <span class="s3">def </span><span class="s1">f_jax(x):  </span><span class="s0"># :[b, 3]</span>
      <span class="s3">return </span><span class="s1">jnp.reshape(x</span><span class="s3">, </span><span class="s1">(-</span><span class="s5">1</span><span class="s3">,</span><span class="s1">))  </span><span class="s0"># : [3b]</span>
    <span class="s1">x = np.arange(</span><span class="s5">12</span><span class="s3">, </span><span class="s1">dtype=np.float32).reshape((</span><span class="s5">4</span><span class="s3">, </span><span class="s5">3</span><span class="s1">))</span>
    <span class="s1">xv = tf.Variable(x)</span>

    <span class="s1">f_tf = jax2tf.convert(f_jax</span><span class="s3">, </span><span class="s1">with_gradient=</span><span class="s3">True,</span>
                          <span class="s1">polymorphic_shapes=[</span><span class="s4">&quot;b, ...&quot;</span><span class="s1">])</span>

    <span class="s3">with </span><span class="s1">tf.GradientTape() </span><span class="s3">as </span><span class="s1">tape:</span>
      <span class="s1">res_tf = f_tf(xv)</span>
    <span class="s1">grad_tf = tape.gradient(res_tf</span><span class="s3">, </span><span class="s1">xv)</span>
    <span class="s1">self.assertAllClose(np.ones(x.shape</span><span class="s3">, </span><span class="s1">dtype=np.float32)</span><span class="s3">, </span><span class="s1">grad_tf.numpy())</span>


  <span class="s3">def </span><span class="s1">test_cond(self):</span>
    <span class="s0"># Test the primitive under conditional</span>
    <span class="s3">def </span><span class="s1">f(x</span><span class="s3">, </span><span class="s1">y):</span>
      <span class="s0"># x: f32[B, H], y : f32[H]</span>
      <span class="s3">return </span><span class="s1">lax.cond(</span>
          <span class="s1">jnp.sum(x) &gt; </span><span class="s5">0.</span><span class="s3">,</span>
          <span class="s3">lambda </span><span class="s1">_: x + y</span><span class="s3">,</span>
          <span class="s3">lambda </span><span class="s1">_: jnp.zeros_like(x)</span><span class="s3">,</span>
          <span class="s1">operand=</span><span class="s3">None</span><span class="s1">)</span>

    <span class="s1">x = np.ones((</span><span class="s5">2</span><span class="s3">, </span><span class="s5">3</span><span class="s1">))</span>
    <span class="s1">y = np.ones((</span><span class="s5">3</span><span class="s3">,</span><span class="s1">))</span>
    <span class="s1">res_jax = f(x</span><span class="s3">, </span><span class="s1">y)</span>
    <span class="s1">self.assertAllClose(</span>
        <span class="s1">res_jax</span><span class="s3">,</span>
        <span class="s1">jax2tf.convert(f</span><span class="s3">, </span><span class="s1">polymorphic_shapes=[</span><span class="s4">&quot;(b, h)&quot;</span><span class="s3">, </span><span class="s4">&quot;h&quot;</span><span class="s1">])(x</span><span class="s3">, </span><span class="s1">y))</span>

  <span class="s3">def </span><span class="s1">test_while(self):</span>
    <span class="s3">def </span><span class="s1">f(x):</span>
      <span class="s0"># x: f32[B], iter: i32</span>
      <span class="s3">return </span><span class="s1">lax.while_loop(</span><span class="s3">lambda </span><span class="s1">x_iter: x_iter[</span><span class="s5">1</span><span class="s1">] &lt; </span><span class="s5">5</span><span class="s3">,</span>
                            <span class="s3">lambda </span><span class="s1">x_iter: (x_iter[</span><span class="s5">0</span><span class="s1">] + jnp.arange(x_iter[</span><span class="s5">0</span><span class="s1">].shape[</span><span class="s5">0</span><span class="s1">]</span><span class="s3">, </span><span class="s1">dtype=np.float32)</span><span class="s3">, </span><span class="s1">x_iter[</span><span class="s5">1</span><span class="s1">] + </span><span class="s5">1</span><span class="s1">)</span><span class="s3">,</span>
                            <span class="s1">(x</span><span class="s3">, </span><span class="s5">0</span><span class="s1">))</span>

    <span class="s1">x = np.ones((</span><span class="s5">3</span><span class="s3">,</span><span class="s1">)</span><span class="s3">, </span><span class="s1">dtype=np.float32)</span>
    <span class="s1">res_tf = jax2tf.convert(f</span><span class="s3">, </span><span class="s1">polymorphic_shapes=[</span><span class="s4">&quot;(b,)&quot;</span><span class="s1">])(x)</span>
    <span class="s1">self.assertAllClose(f(x)</span><span class="s3">, </span><span class="s1">res_tf)</span>

  <span class="s1">@jtu.sample_product(with_function=[</span><span class="s3">False, True</span><span class="s1">])</span>
  <span class="s3">def </span><span class="s1">test_grad_int(self</span><span class="s3">, </span><span class="s1">with_function=</span><span class="s3">True</span><span class="s1">):</span>
    <span class="s0"># https://github.com/google/jax/issues/7093</span>
    <span class="s0"># Also issue #6975.</span>
    <span class="s1">x_shape = (</span><span class="s5">2</span><span class="s3">, </span><span class="s5">3</span><span class="s3">, </span><span class="s5">4</span><span class="s1">)</span>
    <span class="s1">xi = np.arange(np.prod(x_shape)</span><span class="s3">, </span><span class="s1">dtype=np.int16).reshape(x_shape)</span>
    <span class="s1">yf = xi.astype(np.float32)</span>
    <span class="s1">xi_yf = (xi</span><span class="s3">, </span><span class="s1">yf)</span>
    <span class="s1">zb = np.array([</span><span class="s3">True, False</span><span class="s1">]</span><span class="s3">, </span><span class="s1">dtype=np.bool_)</span>
    <span class="s3">def </span><span class="s1">f_jax(xi_yf</span><span class="s3">, </span><span class="s1">zb):  </span><span class="s0"># xi: s16[2, 3, 4], yf: f32[2, 3, 4], zb: bool[2]</span>
      <span class="s1">xi</span><span class="s3">, </span><span class="s1">yf = xi_yf</span>
      <span class="s0"># Return a tuple:</span>
      <span class="s0">#   (1) float constant, with 0 tangent;</span>
      <span class="s0">#   (2) a tuple with:</span>
      <span class="s0">#     (2.1) the integer input;</span>
      <span class="s0">#     (2.2) the boolean input;</span>
      <span class="s0">#     (2.3) a float depending on both inputs.</span>
      <span class="s0"># TODO: there is a problem if we add a None output</span>
      <span class="s3">return </span><span class="s1">(jnp.zeros(xi.shape</span><span class="s3">, </span><span class="s1">dtype=jnp.float32)</span><span class="s3">,</span>
              <span class="s1">(xi</span><span class="s3">, </span><span class="s1">zb</span><span class="s3">, </span><span class="s1">xi.astype(np.float32) * </span><span class="s5">2. </span><span class="s1">* yf))</span>

    <span class="s1">args = (xi_yf</span><span class="s3">, </span><span class="s1">zb)</span>

    <span class="s1">f_tf = jax2tf.convert(f_jax</span><span class="s3">, </span><span class="s1">polymorphic_shapes=[(</span><span class="s4">&quot;b1, b2, 4&quot;</span><span class="s3">, </span><span class="s4">&quot;b1, b2, 4&quot;</span><span class="s1">)</span><span class="s3">, </span><span class="s4">&quot;b1&quot;</span><span class="s1">])</span>
    <span class="s3">if </span><span class="s1">with_function:</span>
      <span class="s1">f_tf = tf.function(f_tf</span><span class="s3">, </span><span class="s1">autograph=</span><span class="s3">False</span><span class="s1">)</span>

    <span class="s1">res_tf</span><span class="s3">, </span><span class="s1">g_tf = tf_test_util.ComputeTfValueAndGrad(f_tf</span><span class="s3">, </span><span class="s1">args)</span>
    <span class="s1">self.assertAllClose(g_tf[</span><span class="s5">0</span><span class="s1">][</span><span class="s5">0</span><span class="s1">]</span><span class="s3">, </span><span class="s1">np.zeros_like(xi))</span>
    <span class="s1">self.assertAllClose(g_tf[</span><span class="s5">0</span><span class="s1">][</span><span class="s5">1</span><span class="s1">]</span><span class="s3">, </span><span class="s1">(xi * </span><span class="s5">2</span><span class="s1">).astype(yf.dtype))</span>
    <span class="s1">self.assertAllClose(g_tf[</span><span class="s5">1</span><span class="s1">]</span><span class="s3">, </span><span class="s1">np.zeros_like(zb))</span>

  <span class="s3">def </span><span class="s1">test_prng(self):</span>
    <span class="s0"># The PRNG implementation uses opaque types, test shape polymorphism</span>
    <span class="s3">try</span><span class="s1">:</span>
      <span class="s1">prev_custom_prng = config.jax_enable_custom_prng</span>
      <span class="s1">config.update(</span><span class="s4">&quot;jax_enable_custom_prng&quot;</span><span class="s3">, True</span><span class="s1">)</span>

      <span class="s3">def </span><span class="s1">f_jax(x):  </span><span class="s0"># x: f32[b1, b2]</span>
        <span class="s1">key = random.PRNGKey(</span><span class="s5">123</span><span class="s1">)  </span><span class="s0">#  key</span>
        <span class="s0"># Exercise key operations that have custom lowering rules</span>
        <span class="s1">broadcast_keys = lax.broadcast_in_dim(key</span><span class="s3">, </span><span class="s1">x.shape</span><span class="s3">, </span><span class="s1">())  </span><span class="s0"># key[b1, b2]</span>
        <span class="s1">gather_keys = lax.broadcast_in_dim(broadcast_keys[</span><span class="s5">0</span><span class="s1">]</span><span class="s3">, </span><span class="s1">(</span><span class="s5">1</span><span class="s3">, </span><span class="s1">x.shape[</span><span class="s5">1</span><span class="s1">])</span><span class="s3">, </span><span class="s1">(</span><span class="s5">1</span><span class="s3">,</span><span class="s1">))  </span><span class="s0"># : key[1, b2]</span>
        <span class="s1">slice_keys1 = lax.slice(broadcast_keys</span><span class="s3">, </span><span class="s1">(</span><span class="s5">0</span><span class="s3">, </span><span class="s5">0</span><span class="s1">)</span><span class="s3">, </span><span class="s1">(</span><span class="s5">1</span><span class="s3">, </span><span class="s1">x.shape[</span><span class="s5">1</span><span class="s1">])</span><span class="s3">, </span><span class="s1">(</span><span class="s5">1</span><span class="s3">, </span><span class="s5">1</span><span class="s1">))  </span><span class="s0"># key[1, b2]</span>
        <span class="s1">slice_keys2 = lax.dynamic_slice(broadcast_keys</span><span class="s3">, </span><span class="s1">(</span><span class="s5">0</span><span class="s3">, </span><span class="s5">0</span><span class="s1">)</span><span class="s3">, </span><span class="s1">slice_sizes=(</span><span class="s5">1</span><span class="s3">, </span><span class="s1">x.shape[</span><span class="s5">1</span><span class="s1">]))  </span><span class="s0"># key[1, b2]</span>
        <span class="s1">upd1 = lax.dynamic_update_slice(slice_keys2</span><span class="s3">, </span><span class="s1">slice_keys1</span><span class="s3">, </span><span class="s1">start_indices=(</span><span class="s5">0</span><span class="s3">, </span><span class="s5">0</span><span class="s1">))  </span><span class="s0"># key[1, b2]</span>
        <span class="s1">_ = lax.dynamic_update_slice(upd1</span><span class="s3">, </span><span class="s1">gather_keys</span><span class="s3">, </span><span class="s1">start_indices=(</span><span class="s5">0</span><span class="s3">, </span><span class="s5">0</span><span class="s1">))</span>
        <span class="s3">return </span><span class="s1">x</span>

      <span class="s1">check_shape_poly(self</span><span class="s3">, </span><span class="s1">f_jax</span><span class="s3">,</span>
                       <span class="s1">arg_descriptors=[RandArg((</span><span class="s5">3</span><span class="s3">, </span><span class="s5">4</span><span class="s1">)</span><span class="s3">, </span><span class="s1">_f32)]</span><span class="s3">,</span>
                       <span class="s1">input_signature=[tf.TensorSpec([</span><span class="s3">None, None</span><span class="s1">]</span><span class="s3">, </span><span class="s1">dtype=tf.float32)]</span><span class="s3">,</span>
                       <span class="s1">polymorphic_shapes=[</span><span class="s4">&quot;b1, b2&quot;</span><span class="s1">])</span>
    <span class="s3">finally</span><span class="s1">:</span>
      <span class="s1">config.update(</span><span class="s4">&quot;jax_enable_custom_prng&quot;</span><span class="s3">, </span><span class="s1">prev_custom_prng)</span>


  <span class="s3">def </span><span class="s1">test_saved_model(self):</span>
    <span class="s1">f_jax = jnp.sin</span>
    <span class="s1">f_tf = jax2tf.convert(f_jax</span><span class="s3">, </span><span class="s1">polymorphic_shapes=[</span><span class="s4">&quot;(b, ...)&quot;</span><span class="s1">])</span>
    <span class="s1">x = np.array([</span><span class="s5">0.7</span><span class="s3">, </span><span class="s5">0.8</span><span class="s1">]</span><span class="s3">, </span><span class="s1">dtype=np.float32)</span>
    <span class="s1">restored_f</span><span class="s3">, </span><span class="s1">_ = tf_test_util.SaveAndLoadFunction(</span>
        <span class="s1">f_tf</span><span class="s3">, </span><span class="s1">input_signature=[tf.TensorSpec([</span><span class="s3">None</span><span class="s1">]</span><span class="s3">, </span><span class="s1">x.dtype)])</span>
    <span class="s1">self.assertAllClose(f_jax(x)</span><span class="s3">, </span><span class="s1">restored_f(x))</span>
    <span class="s0"># Ensure that restored_f works at other batch size as well</span>
    <span class="s1">y = np.concatenate([x</span><span class="s3">, </span><span class="s1">x])</span>
    <span class="s1">self.assertAllClose(f_jax(y)</span><span class="s3">, </span><span class="s1">restored_f(y))</span>

  <span class="s3">def </span><span class="s1">test_saved_model_int_function(self):</span>
    <span class="s0"># TODO(https://github.com/google/jax/issues/14437)</span>
    <span class="s3">if </span><span class="s1">config.jax2tf_default_native_serialization:</span>
      <span class="s3">raise </span><span class="s1">unittest.SkipTest(</span><span class="s4">&quot;Gradient function does not use the dimension  variables&quot;</span><span class="s1">)</span>

    <span class="s3">def </span><span class="s1">f_jax(x):  </span><span class="s0"># x:s32[b, 3, 4]</span>
      <span class="s3">return </span><span class="s1">jnp.reshape(x</span><span class="s3">, </span><span class="s1">(-</span><span class="s5">1</span><span class="s3">,</span><span class="s1">))  </span><span class="s0"># : s32[b * 12]</span>
    <span class="s1">f_tf = jax2tf.convert(f_jax</span><span class="s3">, </span><span class="s1">polymorphic_shapes=[</span><span class="s4">&quot;(b, ...)&quot;</span><span class="s1">])</span>
    <span class="s1">f_tf = tf.function(f_tf</span><span class="s3">, </span><span class="s1">autograph=</span><span class="s3">False</span><span class="s1">)</span>
    <span class="s1">x_shape = (</span><span class="s5">2</span><span class="s3">, </span><span class="s5">3</span><span class="s3">, </span><span class="s5">4</span><span class="s1">)</span>
    <span class="s1">x = np.arange(np.prod(x_shape)</span><span class="s3">, </span><span class="s1">dtype=np.int32).reshape(x_shape)</span>

    <span class="s0"># When saving the model with gradients, we trace the gradient function</span>
    <span class="s0"># and we used to get an error when creating zeros_like_aval for a</span>
    <span class="s0"># polymorphic shape</span>
    <span class="s1">restored_f</span><span class="s3">, </span><span class="s1">_ = tf_test_util.SaveAndLoadFunction(</span>
        <span class="s1">f_tf</span><span class="s3">, </span><span class="s1">input_signature=[tf.TensorSpec((</span><span class="s3">None,</span><span class="s1">) + x.shape[</span><span class="s5">1</span><span class="s1">:]</span><span class="s3">, </span><span class="s1">x.dtype)])</span>
    <span class="s1">f_jax_rt = jax2tf.call_tf(restored_f)</span>
    <span class="s1">res_jax_rt = f_jax_rt(x)</span>
    <span class="s1">self.assertAllClose(f_jax(x)</span><span class="s3">, </span><span class="s1">res_jax_rt)</span>

  <span class="s3">def </span><span class="s1">test_saved_model_constant_gradient(self):</span>
    <span class="s0"># TODO(https://github.com/google/jax/issues/14437)</span>
    <span class="s3">if </span><span class="s1">config.jax2tf_default_native_serialization:</span>
      <span class="s3">raise </span><span class="s1">unittest.SkipTest(</span><span class="s4">&quot;Gradient function does not use the dimension  variables&quot;</span><span class="s1">)</span>

    <span class="s3">def </span><span class="s1">f_jax(x):  </span><span class="s0"># A function whose gradient is a constant</span>
      <span class="s3">return </span><span class="s1">x</span>

    <span class="s1">f_tf = jax2tf.convert(f_jax</span><span class="s3">, </span><span class="s1">polymorphic_shapes=[</span><span class="s4">&quot;(b, ...)&quot;</span><span class="s1">])</span>
    <span class="s1">x = np.array([</span><span class="s5">0.7</span><span class="s3">, </span><span class="s5">0.8</span><span class="s1">]</span><span class="s3">, </span><span class="s1">dtype=np.float32)</span>
    <span class="s1">restored_f</span><span class="s3">, </span><span class="s1">_ = tf_test_util.SaveAndLoadFunction(</span>
        <span class="s1">f_tf</span><span class="s3">, </span><span class="s1">input_signature=[tf.TensorSpec([</span><span class="s3">None</span><span class="s1">]</span><span class="s3">, </span><span class="s1">x.dtype)])</span>
    <span class="s1">self.assertAllClose(f_jax(x)</span><span class="s3">, </span><span class="s1">restored_f(x))</span>


  <span class="s3">def </span><span class="s1">test_readme_examples(self):</span>
    <span class="s2">&quot;&quot;&quot;Some of the examples from the README.&quot;&quot;&quot;</span>

    <span class="s1">jax2tf.convert(</span><span class="s3">lambda </span><span class="s1">x: jnp.reshape(x</span><span class="s3">, </span><span class="s1">(x.shape[</span><span class="s5">0</span><span class="s1">] * x.shape[</span><span class="s5">1</span><span class="s1">]</span><span class="s3">,</span><span class="s1">))</span><span class="s3">,</span>
                   <span class="s1">polymorphic_shapes=[</span><span class="s4">&quot;(b, 4)&quot;</span><span class="s1">])(np.ones((</span><span class="s5">3</span><span class="s3">, </span><span class="s5">4</span><span class="s1">)))</span>

    <span class="s1">jax2tf.convert(</span><span class="s3">lambda </span><span class="s1">x: jnp.reshape(x</span><span class="s3">, </span><span class="s1">(np.prod(x.shape)</span><span class="s3">,</span><span class="s1">))</span><span class="s3">,</span>
                   <span class="s1">polymorphic_shapes=[</span><span class="s4">&quot;(b, 4)&quot;</span><span class="s1">])(np.ones((</span><span class="s5">3</span><span class="s3">, </span><span class="s5">4</span><span class="s1">)))</span>

    <span class="s1">jax2tf.convert(</span><span class="s3">lambda </span><span class="s1">x: x + x.shape[</span><span class="s5">0</span><span class="s1">] + jnp.sin(x.shape[</span><span class="s5">0</span><span class="s1">])</span><span class="s3">,</span>
                   <span class="s1">polymorphic_shapes=[</span><span class="s4">&quot;b&quot;</span><span class="s1">])(np.ones(</span><span class="s5">3</span><span class="s1">))</span>

    <span class="s1">jax2tf.convert(</span><span class="s3">lambda </span><span class="s1">x: jnp.sum(x</span><span class="s3">, </span><span class="s1">axis=</span><span class="s5">0</span><span class="s1">) / x.shape[</span><span class="s5">0</span><span class="s1">]</span><span class="s3">,</span>
                   <span class="s1">polymorphic_shapes=[</span><span class="s4">&quot;(v, _)&quot;</span><span class="s1">])(np.ones((</span><span class="s5">3</span><span class="s3">, </span><span class="s5">4</span><span class="s1">)))</span>

    <span class="s3">with </span><span class="s1">self.assertRaisesRegex(TypeError</span><span class="s3">,</span>
                                <span class="s4">&quot;prod requires ndarray or scalar arguments&quot;</span><span class="s1">):</span>
      <span class="s1">jax2tf.convert(</span><span class="s3">lambda </span><span class="s1">x: jnp.prod(x.shape) + x</span><span class="s3">,</span>
                     <span class="s1">polymorphic_shapes=[</span><span class="s4">&quot;(b, 4)&quot;</span><span class="s1">])(np.ones((</span><span class="s5">3</span><span class="s3">, </span><span class="s5">4</span><span class="s1">)))</span>

    <span class="s1">jax2tf.convert(</span><span class="s3">lambda </span><span class="s1">x: jnp.prod(jnp.array(x.shape)) + x</span><span class="s3">,</span>
                   <span class="s1">polymorphic_shapes=[</span><span class="s4">&quot;(b, 4)&quot;</span><span class="s1">])(np.ones((</span><span class="s5">3</span><span class="s3">, </span><span class="s5">4</span><span class="s1">)))</span>

    <span class="s1">four_ones = np.ones((</span><span class="s5">4</span><span class="s3">,</span><span class="s1">))</span>
    <span class="s3">with </span><span class="s1">self.assertRaisesRegex(</span>
        <span class="s1">TypeError</span><span class="s3">,</span>
        <span class="s1">re.escape(</span><span class="s4">&quot;add got incompatible shapes for broadcasting: (v,), (4,)&quot;</span><span class="s1">)):</span>
      <span class="s1">jax2tf.convert(</span><span class="s3">lambda </span><span class="s1">x</span><span class="s3">, </span><span class="s1">y: x + y</span><span class="s3">,</span>
                     <span class="s1">polymorphic_shapes=[</span><span class="s4">&quot;(v,)&quot;</span><span class="s3">, </span><span class="s4">&quot;(4,)&quot;</span><span class="s1">])(four_ones</span><span class="s3">, </span><span class="s1">four_ones)</span>

    <span class="s0"># We get the error even if we use correct actual arguments</span>
    <span class="s3">with </span><span class="s1">self.assertRaisesRegex(</span>
        <span class="s1">TypeError</span><span class="s3">,</span>
        <span class="s1">re.escape(</span><span class="s4">&quot;add got incompatible shapes for broadcasting: (v,), (4,)&quot;</span><span class="s1">)):</span>
      <span class="s1">jax2tf.convert(</span>
          <span class="s3">lambda </span><span class="s1">x</span><span class="s3">, </span><span class="s1">y: x + y</span><span class="s3">, </span><span class="s1">polymorphic_shapes=[</span><span class="s4">&quot;(v,)&quot;</span><span class="s3">, </span><span class="s4">&quot;(4,)&quot;</span><span class="s1">])(four_ones</span><span class="s3">,</span>
                                                                   <span class="s1">four_ones)</span>

    <span class="s3">with </span><span class="s1">self.assertRaisesRegex(TypeError</span><span class="s3">,</span>
                                <span class="s1">re.escape(</span><span class="s4">&quot;dot_general requires contracting dimensions to have the same shape, got (4,) and (v,)&quot;</span><span class="s1">)):</span>
      <span class="s1">jax2tf.convert(</span><span class="s3">lambda </span><span class="s1">x: jnp.matmul(x</span><span class="s3">, </span><span class="s1">x)</span><span class="s3">,</span>
                     <span class="s1">polymorphic_shapes=[</span><span class="s4">&quot;(v, 4)&quot;</span><span class="s1">])(np.ones((</span><span class="s5">4</span><span class="s3">, </span><span class="s5">4</span><span class="s1">)))</span>

    <span class="s3">with </span><span class="s1">self.assertRaisesRegex(core.InconclusiveDimensionOperation</span><span class="s3">,</span>
                                <span class="s1">re.compile(</span><span class="s4">&quot;Cannot divide evenly the sizes of shapes </span><span class="s3">\\</span><span class="s4">(b, 5, 7</span><span class="s3">\\</span><span class="s4">) and </span><span class="s3">\\</span><span class="s4">(2, -1</span><span class="s3">\\</span><span class="s4">)&quot;</span><span class="s3">,</span>
                                           <span class="s1">re.DOTALL)):</span>
      <span class="s1">jax2tf.convert(</span><span class="s3">lambda </span><span class="s1">x: jnp.reshape(x</span><span class="s3">, </span><span class="s1">(</span><span class="s5">2</span><span class="s3">, </span><span class="s1">-</span><span class="s5">1</span><span class="s1">))</span><span class="s3">,</span>
                     <span class="s1">polymorphic_shapes=[</span><span class="s4">&quot;(b, _, _)&quot;</span><span class="s1">])(np.ones((</span><span class="s5">4</span><span class="s3">, </span><span class="s5">5</span><span class="s3">, </span><span class="s5">7</span><span class="s1">)))</span>

    <span class="s1">jax2tf.convert(</span><span class="s3">lambda </span><span class="s1">x: jnp.reshape(x</span><span class="s3">, </span><span class="s1">(</span><span class="s5">2</span><span class="s3">, </span><span class="s1">-</span><span class="s5">1</span><span class="s1">))</span><span class="s3">,</span>
                   <span class="s1">polymorphic_shapes=[</span><span class="s4">&quot;(b, _, _)&quot;</span><span class="s1">])(np.ones((</span><span class="s5">4</span><span class="s3">, </span><span class="s5">5</span><span class="s3">, </span><span class="s5">6</span><span class="s1">)))</span>
    <span class="s1">jax2tf.convert(</span><span class="s3">lambda </span><span class="s1">x: jnp.reshape(x</span><span class="s3">, </span><span class="s1">(-</span><span class="s5">1</span><span class="s3">, </span><span class="s1">x.shape[</span><span class="s5">0</span><span class="s1">]))</span><span class="s3">,</span>
                   <span class="s1">polymorphic_shapes=[</span><span class="s4">&quot;(b1, b2, ...)&quot;</span><span class="s1">])(np.ones((</span><span class="s5">4</span><span class="s3">, </span><span class="s5">5</span><span class="s3">, </span><span class="s5">6</span><span class="s1">)))</span>

    <span class="s3">if not </span><span class="s1">config.jax2tf_default_native_serialization:</span>
      <span class="s0"># Does not support 2*b constraints</span>
      <span class="s1">jax2tf.convert(</span><span class="s3">lambda </span><span class="s1">x: jnp.reshape(x</span><span class="s3">, </span><span class="s1">(</span><span class="s5">2</span><span class="s3">, </span><span class="s1">-</span><span class="s5">1</span><span class="s1">))</span><span class="s3">,</span>
                     <span class="s1">polymorphic_shapes=[</span><span class="s4">&quot;(2*b, ...)&quot;</span><span class="s1">])(np.ones((</span><span class="s5">4</span><span class="s3">, </span><span class="s5">5</span><span class="s3">, </span><span class="s5">7</span><span class="s1">)))</span>

    <span class="s3">with </span><span class="s1">self.assertRaisesRegex(</span>
        <span class="s1">core.InconclusiveDimensionOperation</span><span class="s3">,</span>
        <span class="s1">re.escape(</span><span class="s4">&quot;Symbolic dimension comparison 'a + 1' &gt;= 'b' is inconclusive&quot;</span><span class="s1">)):</span>
      <span class="s1">jax2tf.convert(</span><span class="s3">lambda </span><span class="s1">x: </span><span class="s5">0 </span><span class="s3">if </span><span class="s1">x.shape[</span><span class="s5">0</span><span class="s1">] + </span><span class="s5">1 </span><span class="s1">&gt;= x.shape[</span><span class="s5">1</span><span class="s1">] </span><span class="s3">else </span><span class="s5">1</span><span class="s3">,</span>
                     <span class="s1">polymorphic_shapes=[</span><span class="s4">&quot;(a, b)&quot;</span><span class="s1">])(np.ones((</span><span class="s5">4</span><span class="s3">, </span><span class="s5">4</span><span class="s1">)))</span>

    <span class="s0"># Unsoundness: not checking that the dimension variable is 0</span>
    <span class="s3">def </span><span class="s1">f1_jax(x):  </span><span class="s0"># f32[b]</span>
      <span class="s0"># We have to use &quot;x&quot;</span>
      <span class="s3">return </span><span class="s1">jnp.concatenate([x</span><span class="s3">, </span><span class="s1">jnp.array([</span><span class="s5">0. </span><span class="s3">if </span><span class="s1">x.shape[</span><span class="s5">0</span><span class="s1">] == </span><span class="s5">0 </span><span class="s3">else </span><span class="s5">1.</span><span class="s1">]</span><span class="s3">,</span>
                                           <span class="s1">dtype=np.float32)])</span>

    <span class="s1">x0 = np.array([]</span><span class="s3">, </span><span class="s1">np.float32)</span>
    <span class="s0"># JAX with static shapes sees that the x.shape[0] == 0</span>
    <span class="s1">self.assertEqual(jnp.array([</span><span class="s5">0.</span><span class="s1">]</span><span class="s3">, </span><span class="s1">dtype=np.float32)</span><span class="s3">, </span><span class="s1">f1_jax(x0))</span>

    <span class="s0"># jax2tf catches the broken assumption b &gt;= 1 if the converted function is executed</span>
    <span class="s0"># eagerly.</span>
    <span class="s3">with </span><span class="s1">self.assertRaisesRegex(</span>
        <span class="s1">ValueError</span><span class="s3">,</span>
        <span class="s4">&quot;Dimension variable b must have integer value &gt;= 1. Found value 0 when solving .*&quot;</span><span class="s1">):</span>
      <span class="s1">jax2tf.convert(f1_jax</span><span class="s3">, </span><span class="s1">polymorphic_shapes=[</span><span class="s4">&quot;b&quot;</span><span class="s1">]</span><span class="s3">,</span>
                     <span class="s1">native_serialization=</span><span class="s3">False</span><span class="s1">)(x0)</span>

    <span class="s0"># TODO(https://github.com/google/jax/issues/14437)</span>
    <span class="s0"># In native serialization, or if we trace to a TF graph, we miss this</span>
    <span class="s1">res1_tf = jax2tf.convert(f1_jax</span><span class="s3">, </span><span class="s1">polymorphic_shapes=[</span><span class="s4">&quot;b&quot;</span><span class="s1">]</span><span class="s3">,</span>
                             <span class="s1">native_serialization=</span><span class="s3">True</span><span class="s1">)(x0)</span>
    <span class="s1">self.assertEqual(jnp.array([</span><span class="s5">1.</span><span class="s1">]</span><span class="s3">, </span><span class="s1">dtype=np.float32)</span><span class="s3">, </span><span class="s1">res1_tf)</span>

    <span class="s1">f1_tf = tf.function(</span>
        <span class="s1">jax2tf.convert(f1_jax</span><span class="s3">, </span><span class="s1">polymorphic_shapes=[</span><span class="s4">&quot;b&quot;</span><span class="s1">]</span><span class="s3">,</span>
                       <span class="s1">native_serialization=</span><span class="s3">False</span><span class="s1">)</span>
    <span class="s1">).get_concrete_function(tf.TensorSpec([</span><span class="s3">None</span><span class="s1">]</span><span class="s3">, </span><span class="s1">dtype=np.float32))</span>
    <span class="s1">self.assertEqual(jnp.array([</span><span class="s5">1.</span><span class="s1">]</span><span class="s3">, </span><span class="s1">dtype=np.float32)</span><span class="s3">, </span><span class="s1">f1_tf(x0))</span>

    <span class="s0"># Unsoundness: not checking that the actual dimensions denoted by the same</span>
    <span class="s0"># dimension variables have equal sizes.</span>
    <span class="s3">def </span><span class="s1">f2_jax(x):  </span><span class="s0"># f32[b, b]</span>
      <span class="s0"># We have to use &quot;x&quot;</span>
      <span class="s3">return </span><span class="s1">jnp.sum(x) + (</span><span class="s5">0. </span><span class="s3">if </span><span class="s1">x.shape[</span><span class="s5">0</span><span class="s1">] != x.shape[</span><span class="s5">1</span><span class="s1">] </span><span class="s3">else </span><span class="s5">1.</span><span class="s1">)</span>

    <span class="s1">x45 = np.ones((</span><span class="s5">4</span><span class="s3">, </span><span class="s5">5</span><span class="s1">)</span><span class="s3">, </span><span class="s1">dtype=np.float32)</span>
    <span class="s0"># JAX with static shapes sees that x.shape[0] != x.shape[1]</span>
    <span class="s1">self.assertEqual(jnp.sum(x45)</span><span class="s3">, </span><span class="s1">f2_jax(x45))</span>

    <span class="s0"># jax2tf catches the broken assumption b &gt;= 1 if the converted function is executed</span>
    <span class="s0"># eagerly.</span>
    <span class="s3">with </span><span class="s1">self.assertRaisesRegex(ValueError</span><span class="s3">,</span>
                                <span class="s4">&quot;Found inconsistency when solving b == .*&quot;</span><span class="s1">):</span>
      <span class="s1">jax2tf.convert(f2_jax</span><span class="s3">, </span><span class="s1">polymorphic_shapes=[</span><span class="s4">&quot;b, b&quot;</span><span class="s1">]</span><span class="s3">,</span>
                     <span class="s1">native_serialization=</span><span class="s3">False</span><span class="s1">)(x45)</span>

    <span class="s0"># TODO(https://github.com/google/jax/issues/14437)</span>
    <span class="s0"># In native serialization, or if we trace to a TF graph, we miss this</span>
    <span class="s1">res2_tf = jax2tf.convert(f2_jax</span><span class="s3">, </span><span class="s1">polymorphic_shapes=[</span><span class="s4">&quot;b, b&quot;</span><span class="s1">]</span><span class="s3">,</span>
                             <span class="s1">native_serialization=</span><span class="s3">True</span><span class="s1">)(x45)</span>
    <span class="s1">self.assertEqual(</span><span class="s5">1. </span><span class="s1">+ jnp.sum(x45)</span><span class="s3">, </span><span class="s1">res2_tf)</span>

    <span class="s1">f2_tf = tf.function(</span>
        <span class="s1">jax2tf.convert(f2_jax</span><span class="s3">, </span><span class="s1">polymorphic_shapes=[</span><span class="s4">&quot;b, b&quot;</span><span class="s1">]</span><span class="s3">,</span>
                       <span class="s1">native_serialization=</span><span class="s3">False</span><span class="s1">)</span>
    <span class="s1">).get_concrete_function(tf.TensorSpec([</span><span class="s3">None, None</span><span class="s1">]</span><span class="s3">, </span><span class="s1">dtype=np.float32))</span>
    <span class="s1">self.assertEqual(</span><span class="s5">1. </span><span class="s1">+ jnp.sum(x45)</span><span class="s3">, </span><span class="s1">f2_tf(x45))</span>

    <span class="s1">x = np.ones((</span><span class="s5">5</span><span class="s3">,</span><span class="s1">)</span><span class="s3">, </span><span class="s1">dtype=np.float32)</span>
    <span class="s3">with </span><span class="s1">self.assertRaisesRegex(</span>
        <span class="s1">ValueError</span><span class="s3">,</span>
        <span class="s4">&quot;Cannot solve for values of dimension variables&quot;</span><span class="s1">):</span>
      <span class="s1">jax2tf.convert(</span><span class="s3">lambda </span><span class="s1">x: jnp.sum(x)</span><span class="s3">, </span><span class="s1">polymorphic_shapes=[</span><span class="s4">&quot;a + b&quot;</span><span class="s1">]</span><span class="s3">,</span>
                     <span class="s1">native_serialization=</span><span class="s3">False</span><span class="s1">)(x)</span>
    <span class="s3">with </span><span class="s1">self.assertRaisesRegex(</span>
        <span class="s1">ValueError</span><span class="s3">,</span>
        <span class="s4">&quot;dimension variables cannot be computed from the static shapes of the array arguments&quot;</span><span class="s1">):</span>
      <span class="s1">jax2tf.convert(</span><span class="s3">lambda </span><span class="s1">x: jnp.sum(x)</span><span class="s3">, </span><span class="s1">polymorphic_shapes=[</span><span class="s4">&quot;a + b&quot;</span><span class="s1">]</span><span class="s3">,</span>
                     <span class="s1">native_serialization=</span><span class="s3">True</span><span class="s1">)(x)</span>

  <span class="s3">def </span><span class="s1">test_dynamic_shapes(self):</span>
    <span class="s0"># Test dim_as_value with dynamic shapes.</span>
    <span class="s3">def </span><span class="s1">f(x):</span>
      <span class="s3">return </span><span class="s1">jnp.sum(x</span><span class="s3">, </span><span class="s1">axis=</span><span class="s5">0</span><span class="s1">) * x.shape[</span><span class="s5">0</span><span class="s1">]</span>

    <span class="s1">x = np.arange(</span><span class="s5">3.</span><span class="s1">)</span>
    <span class="s1">self.assertAllClose(</span><span class="s5">9.</span><span class="s3">, </span><span class="s1">jax2tf.convert(f</span><span class="s3">, </span><span class="s1">polymorphic_shapes=[</span><span class="s4">&quot;(b,)&quot;</span><span class="s1">])(x))</span>
    <span class="s1">self.assertAllClose(</span>
        <span class="s5">9.</span><span class="s3">,</span>
        <span class="s1">jax2tf.convert(jax.jit(f)</span><span class="s3">, </span><span class="s1">polymorphic_shapes=[</span><span class="s4">&quot;(b,)&quot;</span><span class="s1">])(x))</span>
    <span class="s1">self.assertAllClose(</span>
        <span class="s5">9.</span><span class="s3">,</span>
        <span class="s1">tf.function(jax2tf.convert(f</span><span class="s3">, </span><span class="s1">polymorphic_shapes=[</span><span class="s4">&quot;(b,)&quot;</span><span class="s1">]))(x))</span>

    <span class="s1">res_primal</span><span class="s3">, </span><span class="s1">res_tangent = jax2tf.convert(</span>
        <span class="s3">lambda </span><span class="s1">x</span><span class="s3">, </span><span class="s1">xt: jax.jvp(f</span><span class="s3">, </span><span class="s1">(x</span><span class="s3">,</span><span class="s1">)</span><span class="s3">, </span><span class="s1">(xt</span><span class="s3">,</span><span class="s1">))</span><span class="s3">,</span>
        <span class="s1">polymorphic_shapes=[</span><span class="s4">&quot;b&quot;</span><span class="s3">, </span><span class="s4">&quot;b&quot;</span><span class="s1">])(x</span><span class="s3">, </span><span class="s1">np.array([</span><span class="s5">0.1</span><span class="s3">, </span><span class="s5">0.2</span><span class="s3">, </span><span class="s5">0.3</span><span class="s1">]))</span>
    <span class="s1">self.assertAllClose((</span><span class="s5">9.</span><span class="s3">, </span><span class="s5">1.8</span><span class="s1">)</span><span class="s3">, </span><span class="s1">(res_primal</span><span class="s3">, </span><span class="s1">res_tangent))</span>

    <span class="s0"># TODO(https://github.com/google/jax/issues/14437)</span>
    <span class="s3">if not </span><span class="s1">config.jax2tf_default_native_serialization:</span>
      <span class="s1">self.assertAllClose(</span>
          <span class="s1">np.array([</span><span class="s5">3.</span><span class="s3">, </span><span class="s5">3.</span><span class="s3">, </span><span class="s5">3.</span><span class="s1">])</span><span class="s3">,</span>
          <span class="s1">jax2tf.convert(jax.grad(f)</span><span class="s3">, </span><span class="s1">polymorphic_shapes=[</span><span class="s4">&quot;b&quot;</span><span class="s1">])(x))</span>

    <span class="s1">xv = np.arange(</span><span class="s5">24.</span><span class="s1">).reshape((</span><span class="s5">2</span><span class="s3">, </span><span class="s5">3</span><span class="s3">, </span><span class="s5">4</span><span class="s1">))</span>
    <span class="s1">res_vmap = jax.vmap(f</span><span class="s3">, </span><span class="s1">in_axes=</span><span class="s5">1</span><span class="s1">)(xv)</span>
    <span class="s0"># Implement by iteration</span>
    <span class="s1">res_iter = jnp.stack([f(xv[:</span><span class="s3">, </span><span class="s1">i</span><span class="s3">, </span><span class="s1">:]) </span><span class="s3">for </span><span class="s1">i </span><span class="s3">in </span><span class="s1">range(xv.shape[</span><span class="s5">1</span><span class="s1">])])</span>
    <span class="s1">self.assertAllClose(res_iter</span><span class="s3">, </span><span class="s1">res_vmap)</span>

    <span class="s1">res_vmap_tf = jax2tf.convert(jax.vmap(f</span><span class="s3">, </span><span class="s1">in_axes=</span><span class="s5">1</span><span class="s1">)</span><span class="s3">,</span>
                                 <span class="s1">polymorphic_shapes=[</span><span class="s4">&quot;b1, b2, ...&quot;</span><span class="s1">])(xv)</span>
    <span class="s1">self.assertAllClose(res_iter</span><span class="s3">, </span><span class="s1">res_vmap_tf.numpy())</span>

  <span class="s3">def </span><span class="s1">test_with_hash_collision_vmap(self):</span>
    <span class="s0"># Batching caches based on Jaxpr, and Jaxpr include _DimExpr. If we have</span>
    <span class="s0"># a collision for the hashing of a _DimExpr, then Python will call the</span>
    <span class="s0"># equality, which will raise InconclusiveDimensionOperation.</span>

    <span class="s3">def </span><span class="s1">f_jax(x):</span>
      <span class="s3">return </span><span class="s1">jnp.reshape(x</span><span class="s3">, </span><span class="s1">(</span><span class="s5">2</span><span class="s3">, </span><span class="s1">-</span><span class="s5">1</span><span class="s3">,</span><span class="s1">))</span>
    <span class="s3">try</span><span class="s1">:</span>
      <span class="s0"># Override the hashing to create collisions</span>
      <span class="s1">orig_hash = getattr(shape_poly._DimExpr</span><span class="s3">, </span><span class="s4">&quot;__hash__&quot;</span><span class="s1">)</span>
      <span class="s3">def </span><span class="s1">collision_hash(obj):</span>
        <span class="s3">return </span><span class="s1">hash(</span><span class="s5">5</span><span class="s1">)</span>

      <span class="s1">setattr(shape_poly._DimExpr</span><span class="s3">, </span><span class="s4">&quot;__hash__&quot;</span><span class="s3">, </span><span class="s1">collision_hash)</span>
      <span class="s1">xs = np.ones((</span><span class="s5">3</span><span class="s3">, </span><span class="s5">5</span><span class="s3">, </span><span class="s5">6</span><span class="s1">)</span><span class="s3">, </span><span class="s1">dtype=np.float32)</span>
      <span class="s1">f_toconvert = jax.vmap(pjit.pjit(f_jax))</span>
      <span class="s1">res_1 = jax2tf.convert(f_toconvert)(xs)</span>
      <span class="s1">res_2 = jax2tf.convert(f_toconvert</span><span class="s3">,</span>
                             <span class="s1">polymorphic_shapes = </span><span class="s4">&quot;b1, b2, ...&quot;</span><span class="s1">)(xs)</span>
      <span class="s1">self.assertAllClose(res_1</span><span class="s3">, </span><span class="s1">res_2)</span>
    <span class="s3">finally</span><span class="s1">:</span>
      <span class="s1">setattr(shape_poly._DimExpr</span><span class="s3">, </span><span class="s4">&quot;__hash__&quot;</span><span class="s3">, </span><span class="s1">orig_hash)</span>

  <span class="s1">@parameterized.named_parameters([</span>
      <span class="s1">dict(testcase_name=</span><span class="s4">f&quot;_</span><span class="s3">{</span><span class="s1">op_name</span><span class="s3">}</span><span class="s4">&quot;</span><span class="s3">,</span>
           <span class="s1">op=op)</span>
      <span class="s3">for </span><span class="s1">op</span><span class="s3">, </span><span class="s1">op_name </span><span class="s3">in </span><span class="s1">[</span>
          <span class="s1">(jnp.array</span><span class="s3">, </span><span class="s4">&quot;array&quot;</span><span class="s1">)</span><span class="s3">,</span>
          <span class="s1">(jnp.sin</span><span class="s3">, </span><span class="s4">&quot;sin&quot;</span><span class="s1">)</span><span class="s3">,</span>
          <span class="s1">(</span><span class="s3">lambda </span><span class="s1">x: x</span><span class="s3">, </span><span class="s4">&quot;id&quot;</span><span class="s1">)</span><span class="s3">,</span>
          <span class="s1">(core.dimension_as_value</span><span class="s3">, </span><span class="s4">&quot;dimension_as_value&quot;</span><span class="s1">)</span><span class="s3">,</span>
      <span class="s1">]</span>
  <span class="s1">])</span>
  <span class="s3">def </span><span class="s1">test_poly_unary_op(self</span><span class="s3">, </span><span class="s1">*</span><span class="s3">, </span><span class="s1">op=jnp.array):</span>
    <span class="s3">def </span><span class="s1">f_jax(x):  </span><span class="s0"># x: f32[b]</span>
      <span class="s1">poly = </span><span class="s5">2 </span><span class="s1">* x.shape[</span><span class="s5">0</span><span class="s1">]</span>
      <span class="s3">return </span><span class="s1">(op(poly)</span><span class="s3">, </span><span class="s1">x)  </span><span class="s0"># Make sure we are using x</span>

    <span class="s1">check_shape_poly(self</span><span class="s3">,</span>
                     <span class="s1">f_jax</span><span class="s3">,</span>
                     <span class="s1">arg_descriptors=[RandArg((</span><span class="s5">3</span><span class="s3">,</span><span class="s1">)</span><span class="s3">, </span><span class="s1">_f32)]</span><span class="s3">,</span>
                     <span class="s1">poly_axes=[</span><span class="s5">0</span><span class="s1">]</span><span class="s3">,</span>
                     <span class="s1">expected_output_signature=(tf.TensorSpec([])</span><span class="s3">, </span><span class="s1">tf.TensorSpec((</span><span class="s3">None,</span><span class="s1">)</span><span class="s3">, </span><span class="s1">_f32)))</span>

  <span class="s1">@parameterized.named_parameters([</span>
      <span class="s1">dict(testcase_name=</span><span class="s4">f&quot;_</span><span class="s3">{</span><span class="s1">op.__name__</span><span class="s3">}</span><span class="s4">_other=</span><span class="s3">{</span><span class="s1">other</span><span class="s3">}</span><span class="s4">:</span><span class="s3">{</span><span class="s1">type(other)</span><span class="s3">}{</span><span class="s4">'_other_jnp_array' </span><span class="s3">if </span><span class="s1">other_jnp_array </span><span class="s3">else </span><span class="s4">''</span><span class="s3">}{</span><span class="s4">'_swap' </span><span class="s3">if </span><span class="s1">swap </span><span class="s3">else </span><span class="s4">''</span><span class="s3">}</span><span class="s4">&quot;</span><span class="s3">,</span>
           <span class="s1">op=op</span><span class="s3">, </span><span class="s1">other=other</span><span class="s3">,</span>
           <span class="s1">other_jnp_array=other_jnp_array</span><span class="s3">, </span><span class="s1">swap=swap)</span>
      <span class="s3">for </span><span class="s1">op </span><span class="s3">in </span><span class="s1">[op.add</span><span class="s3">, </span><span class="s1">op.mul</span><span class="s3">, </span><span class="s1">op.sub</span><span class="s3">,</span>
                 <span class="s1">op.mod</span><span class="s3">, </span><span class="s1">op.floordiv</span><span class="s3">, </span><span class="s1">op.truediv]</span>
      <span class="s3">for </span><span class="s1">other </span><span class="s3">in </span><span class="s1">[</span>
          <span class="s5">2</span><span class="s3">, </span><span class="s1">np.int32(</span><span class="s5">2</span><span class="s1">)</span><span class="s3">, </span><span class="s5">2.</span><span class="s3">, </span><span class="s1">np.float32(</span><span class="s5">2</span><span class="s1">)</span><span class="s3">,</span>
          <span class="s1">np.array(</span><span class="s5">2</span><span class="s3">, </span><span class="s1">dtype=np.int32)</span><span class="s3">, </span><span class="s1">np.arange(</span><span class="s5">1</span><span class="s3">, </span><span class="s5">5</span><span class="s3">, </span><span class="s1">dtype=np.int32)</span><span class="s3">,</span>
          <span class="s1">np.array(</span><span class="s5">2.</span><span class="s3">, </span><span class="s1">dtype=np.float32)</span><span class="s3">, </span><span class="s1">np.arange(</span><span class="s5">1.</span><span class="s3">, </span><span class="s5">7.</span><span class="s3">, </span><span class="s1">dtype=np.float32)</span>
      <span class="s1">]</span>
      <span class="s3">for </span><span class="s1">other_jnp_array </span><span class="s3">in </span><span class="s1">(</span>
          <span class="s1">[</span><span class="s3">True, False</span><span class="s1">] </span><span class="s3">if </span><span class="s1">np.shape(other) == (</span><span class="s5">7</span><span class="s3">,</span><span class="s1">) </span><span class="s3">else </span><span class="s1">[</span><span class="s3">False</span><span class="s1">])  </span><span class="s0"># type: ignore</span>
      <span class="s3">for </span><span class="s1">swap </span><span class="s3">in </span><span class="s1">[</span><span class="s3">False, True</span><span class="s1">]  </span><span class="s0"># The poly is the left op by default</span>
  <span class="s1">])</span>
  <span class="s3">def </span><span class="s1">test_poly_binary_op(self</span><span class="s3">, </span><span class="s1">*</span><span class="s3">, </span><span class="s1">op=op.add</span><span class="s3">,</span>
                          <span class="s1">other=np.arange(</span><span class="s5">2</span><span class="s3">, </span><span class="s1">dtype=np.int32)</span><span class="s3">,</span>
                          <span class="s1">other_jnp_array=</span><span class="s3">False,</span>
                          <span class="s1">swap=</span><span class="s3">True</span><span class="s1">):</span>
    <span class="s0"># Test arithmetic operations with poly and a variety of other operand types</span>
    <span class="s3">def </span><span class="s1">f_jax(x):  </span><span class="s0"># x: f32[b]</span>
      <span class="s1">poly = </span><span class="s5">2 </span><span class="s1">* x.shape[</span><span class="s5">0</span><span class="s1">]  </span><span class="s0"># This will allow divisions with 2</span>
      <span class="s1">other_wrapped = jnp.array(other) </span><span class="s3">if </span><span class="s1">other_jnp_array </span><span class="s3">else </span><span class="s1">other</span>
      <span class="s1">ops = (poly</span><span class="s3">, </span><span class="s1">other_wrapped) </span><span class="s3">if not </span><span class="s1">swap </span><span class="s3">else </span><span class="s1">(other_wrapped</span><span class="s3">, </span><span class="s1">poly)</span>
      <span class="s1">res = op(*ops)</span>

      <span class="s0"># If the other op is an integer then the result is a symbolic dim</span>
      <span class="s3">try</span><span class="s1">:</span>
        <span class="s1">op.index(other)</span>
        <span class="s1">other_isint = </span><span class="s3">True</span>
      <span class="s3">except </span><span class="s1">Exception:</span>
        <span class="s1">other_isint = </span><span class="s3">False</span>

      <span class="s3">if </span><span class="s1">(hasattr(poly</span><span class="s3">, </span><span class="s4">&quot;dimension_as_value&quot;</span><span class="s1">) </span><span class="s3">and</span>
          <span class="s1">other_isint </span><span class="s3">and</span>
          <span class="s1">op.__name__ != </span><span class="s4">&quot;truediv&quot;</span><span class="s1">):</span>
        <span class="s0"># If we running under jax2tf and &quot;other&quot; is an integer the result</span>
        <span class="s0"># should be a symbolic dimension</span>
        <span class="s1">self.assertTrue(isinstance(res</span><span class="s3">, </span><span class="s1">int) </span><span class="s3">or </span><span class="s1">hasattr(res</span><span class="s3">, </span><span class="s4">&quot;dimension_as_value&quot;</span><span class="s1">))</span>

      <span class="s3">if </span><span class="s1">config.jax_enable_x64:</span>
        <span class="s0"># Outside jax2tf, x.shape[0] is a Python (64-bit) integer and for most</span>
        <span class="s0"># operations here JAX is not involved at all because the other operand</span>
        <span class="s0"># is a Python or NumPy constant. So the result will be 64-bits. But under</span>
        <span class="s0"># jax2tf, x.shape[0] is rewritten to jnp.array(x.shape[0]) which when</span>
        <span class="s0"># used with int32 or float32 values will produce 32-bit values.</span>
        <span class="s3">return </span><span class="s1">(lax.convert_element_type(res</span><span class="s3">, </span><span class="s1">np.float32)</span><span class="s3">, </span><span class="s1">x)</span>
      <span class="s3">return </span><span class="s1">(res</span><span class="s3">, </span><span class="s1">x)  </span><span class="s0"># Make sure we are using x</span>

    <span class="s1">check_shape_poly(self</span><span class="s3">,</span>
                     <span class="s1">f_jax</span><span class="s3">,</span>
                     <span class="s1">arg_descriptors=[RandArg((</span><span class="s5">3</span><span class="s3">,</span><span class="s1">)</span><span class="s3">, </span><span class="s1">np.int32)]</span><span class="s3">,</span>
                     <span class="s1">poly_axes=[</span><span class="s5">0</span><span class="s1">])</span>

  <span class="s3">def </span><span class="s1">test_mean0(self):</span>
    <span class="s3">def </span><span class="s1">f_jax(x):  </span><span class="s0"># x: f32[b, 4]</span>
      <span class="s3">return </span><span class="s1">jnp.sum(x</span><span class="s3">, </span><span class="s1">axis=</span><span class="s5">0</span><span class="s1">) / x.shape[</span><span class="s5">0</span><span class="s1">]</span>
    <span class="s1">check_shape_poly(self</span><span class="s3">,</span>
                     <span class="s1">f_jax</span><span class="s3">,</span>
                     <span class="s1">arg_descriptors=[RandArg((</span><span class="s5">3</span><span class="s3">, </span><span class="s5">4</span><span class="s1">)</span><span class="s3">, </span><span class="s1">_f32)]</span><span class="s3">,</span>
                     <span class="s1">poly_axes=[</span><span class="s5">0</span><span class="s1">]</span><span class="s3">,</span>
                     <span class="s1">expected_output_signature=tf.TensorSpec([</span><span class="s5">4</span><span class="s1">]))</span>


  <span class="s3">def </span><span class="s1">test_shape_as_array(self):</span>
    <span class="s3">def </span><span class="s1">f_jax(x):</span>
      <span class="s0"># The entire x.shape is passed to jnp.array</span>
      <span class="s3">return </span><span class="s1">x + jnp.sum(jnp.array(x.shape)).astype(np.int32)</span>

    <span class="s1">check_shape_poly(self</span><span class="s3">,</span>
                     <span class="s1">f_jax</span><span class="s3">,</span>
                     <span class="s1">arg_descriptors=[RandArg((</span><span class="s5">3</span><span class="s3">, </span><span class="s5">4</span><span class="s1">)</span><span class="s3">, </span><span class="s1">_f32)]</span><span class="s3">,</span>
                     <span class="s1">poly_axes=[</span><span class="s5">0</span><span class="s1">])</span>

  <span class="s3">def </span><span class="s1">test_dim_as_value_weak_type(self):</span>
    <span class="s3">def </span><span class="s1">f_jax(x):  </span><span class="s0"># x: f32[b]</span>
      <span class="s1">d0 = jnp.array(x.shape[</span><span class="s5">0</span><span class="s1">])  </span><span class="s0"># in JAX should have weak_type=True</span>
      <span class="s3">if </span><span class="s1">isinstance(d0</span><span class="s3">, </span><span class="s1">core.Tracer):</span>
        <span class="s1">self.assertTrue(d0.aval.weak_type)</span><span class="s3">, </span><span class="s1">d0</span>

      <span class="s0"># And an implicit conversion to array</span>
      <span class="s1">d1 = x.shape[</span><span class="s5">0</span><span class="s1">] + jnp.array(</span><span class="s5">4</span><span class="s1">)</span>
      <span class="s3">if </span><span class="s1">isinstance(d1</span><span class="s3">, </span><span class="s1">core.Tracer):</span>
        <span class="s1">self.assertTrue(d1.aval.weak_type)</span><span class="s3">, </span><span class="s1">d1</span>
      <span class="s3">return </span><span class="s1">d0 + np.array(</span><span class="s5">5.</span><span class="s3">, </span><span class="s1">dtype=np.float32) + d1 + x[</span><span class="s5">0</span><span class="s1">]</span>

    <span class="s3">with </span><span class="s1">numpy_dtype_promotion(</span><span class="s4">&quot;strict&quot;</span><span class="s1">):</span>
      <span class="s0"># strict type promotion is sensitive to weak_types</span>
      <span class="s1">check_shape_poly(self</span><span class="s3">,</span>
                       <span class="s1">f_jax</span><span class="s3">,</span>
                       <span class="s1">arg_descriptors=[RandArg((</span><span class="s5">3</span><span class="s3">,</span><span class="s1">)</span><span class="s3">, </span><span class="s1">_f32)]</span><span class="s3">,</span>
                       <span class="s1">poly_axes=[</span><span class="s5">0</span><span class="s1">])</span>

  <span class="s3">def </span><span class="s1">test_vmap_while(self):</span>
    <span class="s3">def </span><span class="s1">cond_func(x):  </span><span class="s0"># x: f32[3]</span>
      <span class="s3">return </span><span class="s1">jnp.sum(x) &gt;= </span><span class="s5">0.</span>
    <span class="s3">def </span><span class="s1">body_func(x):  </span><span class="s0"># x: f32[3]</span>
      <span class="s3">return </span><span class="s1">x - </span><span class="s5">1.</span>
    <span class="s3">def </span><span class="s1">f_jax(x):</span>
      <span class="s3">return </span><span class="s1">lax.while_loop(cond_func</span><span class="s3">, </span><span class="s1">body_func</span><span class="s3">, </span><span class="s1">x)</span>

    <span class="s1">check_shape_poly(self</span><span class="s3">,</span>
                     <span class="s1">jax.vmap(f_jax)</span><span class="s3">,</span>
                     <span class="s1">arg_descriptors=[RandArg((</span><span class="s5">3</span><span class="s3">,</span><span class="s1">)</span><span class="s3">, </span><span class="s1">_f32)]</span><span class="s3">,</span>
                     <span class="s1">input_signature=[tf.TensorSpec((</span><span class="s3">None, </span><span class="s5">3</span><span class="s1">)</span><span class="s3">, </span><span class="s1">dtype=tf.float32)]</span><span class="s3">,</span>
                     <span class="s1">polymorphic_shapes=[</span><span class="s4">&quot;b, ...&quot;</span><span class="s1">]</span><span class="s3">,</span>
                     <span class="s1">expected_output_signature=tf.TensorSpec((</span><span class="s3">None, </span><span class="s5">3</span><span class="s1">)</span><span class="s3">, </span><span class="s1">dtype=tf.float32)</span>
                     <span class="s1">)</span>

  <span class="s3">def </span><span class="s1">test_vmap_error(self):</span>
    <span class="s0"># vmap is careful to give nice error messages when mapped axes have</span>
    <span class="s0"># different sizes, but this can be foiled by InconsistentDimensionOperation</span>
    <span class="s1">x = y = np.ones((</span><span class="s5">3</span><span class="s3">, </span><span class="s5">5</span><span class="s1">)</span><span class="s3">, </span><span class="s1">dtype=np.float32)</span>
    <span class="s3">with </span><span class="s1">self.assertRaisesRegex(ValueError</span><span class="s3">,</span>
                                <span class="s4">&quot;vmap got inconsistent sizes for array axes to be mapped&quot;</span><span class="s1">):</span>
      <span class="s1">jax2tf.convert(jax.vmap(</span><span class="s3">lambda </span><span class="s1">x</span><span class="s3">, </span><span class="s1">y: x + y)</span><span class="s3">,</span>
                     <span class="s1">polymorphic_shapes=[</span><span class="s4">&quot;b, ...&quot;</span><span class="s3">, None</span><span class="s1">])(x</span><span class="s3">, </span><span class="s1">y)</span>

    <span class="s1">z = x</span>
    <span class="s3">with </span><span class="s1">self.assertRaisesRegex(ValueError</span><span class="s3">,</span>
                                <span class="s4">&quot;vmap got inconsistent sizes for array axes to be mapped&quot;</span><span class="s1">):</span>
      <span class="s1">jax2tf.convert(jax.vmap(</span><span class="s3">lambda </span><span class="s1">x</span><span class="s3">, </span><span class="s1">y</span><span class="s3">, </span><span class="s1">z: x + y + z)</span><span class="s3">,</span>
                     <span class="s1">polymorphic_shapes=[</span><span class="s4">&quot;b, ...&quot;</span><span class="s3">, </span><span class="s4">&quot;c, ...&quot;</span><span class="s3">, None</span><span class="s1">])(x</span><span class="s3">, </span><span class="s1">y</span><span class="s3">, </span><span class="s1">z)</span>

  <span class="s3">def </span><span class="s1">test_reshape_compiled(self):</span>
    <span class="s0"># We compile the result of conversion for two shapes, hence we need to</span>
    <span class="s0"># involve the TF compiler twice, but we trace only once with shape polymorphism</span>
    <span class="s1">traced = </span><span class="s3">False</span>

    <span class="s3">def </span><span class="s1">f_jax(x):</span>
      <span class="s3">nonlocal </span><span class="s1">traced</span>
      <span class="s1">traced = </span><span class="s3">True</span>
      <span class="s1">y = jnp.sin(x)</span>
      <span class="s3">return </span><span class="s1">y.reshape([x.shape[</span><span class="s5">0</span><span class="s1">]</span><span class="s3">, </span><span class="s1">-</span><span class="s5">1</span><span class="s1">])</span>

    <span class="s1">x = self.rng().rand(</span><span class="s5">4</span><span class="s3">, </span><span class="s5">2</span><span class="s3">, </span><span class="s5">3</span><span class="s1">)</span>
    <span class="s1">res_jax = f_jax(x)</span>

    <span class="s1">traced = </span><span class="s3">False</span>
    <span class="s0"># If we get_concrete_function we trace once</span>
    <span class="s1">f_tf = tf.function(</span>
        <span class="s1">jax2tf.convert(f_jax</span><span class="s3">, </span><span class="s1">polymorphic_shapes=[PS(</span><span class="s4">&quot;b&quot;</span><span class="s3">, </span><span class="s1">...)])</span><span class="s3">,</span>
        <span class="s1">autograph=</span><span class="s3">False,</span>
        <span class="s1">jit_compile=</span><span class="s3">True</span><span class="s1">).get_concrete_function(</span>
            <span class="s1">tf.TensorSpec([</span><span class="s3">None, </span><span class="s5">2</span><span class="s3">, </span><span class="s5">3</span><span class="s1">]</span><span class="s3">, </span><span class="s1">x.dtype))</span>
    <span class="s1">self.assertTrue(traced)</span>
    <span class="s1">traced = </span><span class="s3">False</span>
    <span class="s1">self.assertAllClose(res_jax</span><span class="s3">, </span><span class="s1">f_tf(x))</span>
    <span class="s1">self.assertFalse(traced)  </span><span class="s0"># We are not tracing again</span>

    <span class="s1">x = self.rng().rand(</span><span class="s5">6</span><span class="s3">, </span><span class="s5">2</span><span class="s3">, </span><span class="s5">3</span><span class="s1">)</span>
    <span class="s1">res_jax = f_jax(x)</span>
    <span class="s1">traced = </span><span class="s3">False</span>

    <span class="s1">self.assertAllClose(res_jax</span><span class="s3">, </span><span class="s1">f_tf(x))</span>
    <span class="s1">self.assertFalse(traced)  </span><span class="s0"># We are not tracing again</span>


<span class="s0"># List containing either harnesses, or lists of harnesses</span>
<span class="s1">_POLY_SHAPE_TEST_HARNESSES = [</span>
    <span class="s1">PolyHarness(</span><span class="s4">&quot;add&quot;</span><span class="s3">, </span><span class="s4">&quot;&quot;</span><span class="s3">,</span>
                <span class="s1">jnp.add</span><span class="s3">,</span>
                <span class="s1">arg_descriptors=[RandArg((</span><span class="s5">3</span><span class="s3">, </span><span class="s5">4</span><span class="s1">)</span><span class="s3">, </span><span class="s1">_f32)</span><span class="s3">, </span><span class="s1">RandArg((</span><span class="s5">2</span><span class="s3">, </span><span class="s5">3</span><span class="s3">, </span><span class="s5">4</span><span class="s1">)</span><span class="s3">, </span><span class="s1">_f32)]</span><span class="s3">,</span>
                <span class="s1">poly_axes=[</span><span class="s5">0</span><span class="s3">, </span><span class="s5">1</span><span class="s1">])</span><span class="s3">,</span>
    <span class="s1">PolyHarness(</span><span class="s4">&quot;add_transpose&quot;</span><span class="s3">, </span><span class="s4">&quot;&quot;</span><span class="s3">,</span>
                <span class="s1">jax.grad(</span><span class="s3">lambda </span><span class="s1">x: jnp.sum(jnp.sum(x</span><span class="s3">, </span><span class="s1">axis=</span><span class="s5">0</span><span class="s3">, </span><span class="s1">keepdims=</span><span class="s3">False</span><span class="s1">) + jnp.sin(x)))</span><span class="s3">,</span>
                <span class="s1">arg_descriptors=[RandArg((</span><span class="s5">3</span><span class="s3">, </span><span class="s5">4</span><span class="s1">)</span><span class="s3">, </span><span class="s1">_f32)]</span><span class="s3">,</span>
                <span class="s1">poly_axes=[</span><span class="s5">0</span><span class="s1">])</span><span class="s3">,</span>
    <span class="s1">PolyHarness(</span><span class="s4">&quot;arange&quot;</span><span class="s3">, </span><span class="s4">&quot;start&quot;</span><span class="s3">,</span>
                <span class="s3">lambda </span><span class="s1">op: jnp.arange(</span><span class="s5">2 </span><span class="s1">* op.shape[</span><span class="s5">0</span><span class="s1">]</span><span class="s3">, </span><span class="s1">dtype=_f32) + op[</span><span class="s5">0</span><span class="s1">]</span><span class="s3">,</span>
                <span class="s1">arg_descriptors=[RandArg((</span><span class="s5">3</span><span class="s3">,</span><span class="s1">)</span><span class="s3">, </span><span class="s1">_f32)]</span><span class="s3">,</span>
                <span class="s1">poly_axes=[</span><span class="s5">0</span><span class="s1">]).both_enable_and_disable_xla()</span><span class="s3">,</span>
    <span class="s1">PolyHarness(</span><span class="s4">&quot;arange&quot;</span><span class="s3">, </span><span class="s4">&quot;start_no_dtype&quot;</span><span class="s3">,</span>
                <span class="s3">lambda </span><span class="s1">op: jnp.arange(op.shape[</span><span class="s5">0</span><span class="s1">]) + op[</span><span class="s5">0</span><span class="s1">]</span><span class="s3">,</span>
                <span class="s1">arg_descriptors=[RandArg((</span><span class="s5">3</span><span class="s3">,</span><span class="s1">)</span><span class="s3">, </span><span class="s1">_f32)]</span><span class="s3">,</span>
                <span class="s1">poly_axes=[</span><span class="s5">0</span><span class="s1">])</span><span class="s3">,</span>
    <span class="s1">PolyHarness(</span><span class="s4">&quot;arange&quot;</span><span class="s3">, </span><span class="s4">&quot;error1&quot;</span><span class="s3">,</span>
                <span class="s3">lambda </span><span class="s1">op: jnp.arange(op.shape[</span><span class="s5">0</span><span class="s1">]</span><span class="s3">, </span><span class="s5">10</span><span class="s1">)</span><span class="s3">,</span>
                <span class="s1">arg_descriptors=[RandArg((</span><span class="s5">3</span><span class="s3">,</span><span class="s1">)</span><span class="s3">, </span><span class="s1">_f32)]</span><span class="s3">,</span>
                <span class="s1">poly_axes=[</span><span class="s5">0</span><span class="s1">]</span><span class="s3">,</span>
                <span class="s1">expect_error=(ValueError</span><span class="s3">, </span><span class="s4">&quot;jax.numpy.arange supports non-constant arguments only in single-argument form&quot;</span><span class="s1">))</span><span class="s3">,</span>
    <span class="s1">PolyHarness(</span><span class="s4">&quot;arange&quot;</span><span class="s3">, </span><span class="s4">&quot;error2&quot;</span><span class="s3">,</span>
                <span class="s3">lambda </span><span class="s1">op: jnp.arange(</span><span class="s5">1</span><span class="s3">, </span><span class="s1">op.shape[</span><span class="s5">0</span><span class="s1">])</span><span class="s3">,</span>
                <span class="s1">arg_descriptors=[RandArg((</span><span class="s5">3</span><span class="s3">,</span><span class="s1">)</span><span class="s3">, </span><span class="s1">_f32)]</span><span class="s3">,</span>
                <span class="s1">poly_axes=[</span><span class="s5">0</span><span class="s1">]</span><span class="s3">,</span>
                <span class="s1">expect_error=(ValueError</span><span class="s3">, </span><span class="s4">&quot;jax.numpy.arange supports non-constant arguments only in single-argument form&quot;</span><span class="s1">))</span><span class="s3">,</span>
    <span class="s1">PolyHarness(</span><span class="s4">&quot;arange&quot;</span><span class="s3">, </span><span class="s4">&quot;error3&quot;</span><span class="s3">,</span>
                <span class="s3">lambda </span><span class="s1">op: jnp.arange(</span><span class="s5">1</span><span class="s3">, </span><span class="s5">5</span><span class="s3">, </span><span class="s1">op.shape[</span><span class="s5">0</span><span class="s1">])</span><span class="s3">,</span>
                <span class="s1">arg_descriptors=[RandArg((</span><span class="s5">3</span><span class="s3">,</span><span class="s1">)</span><span class="s3">, </span><span class="s1">_f32)]</span><span class="s3">,</span>
                <span class="s1">poly_axes=[</span><span class="s5">0</span><span class="s1">]</span><span class="s3">,</span>
                <span class="s1">expect_error=(ValueError</span><span class="s3">, </span><span class="s4">&quot;jax.numpy.arange supports non-constant arguments only in single-argument form&quot;</span><span class="s1">))</span><span class="s3">,</span>
    <span class="s0"># Reduce the poly dimension</span>
    <span class="s1">PolyHarness(</span><span class="s4">&quot;argmax&quot;</span><span class="s3">, </span><span class="s4">&quot;0&quot;</span><span class="s3">,</span>
                <span class="s3">lambda </span><span class="s1">op: lax.argmax(op</span><span class="s3">, </span><span class="s1">axis=</span><span class="s5">0</span><span class="s3">, </span><span class="s1">index_dtype=np.int32)</span><span class="s3">,</span>
                <span class="s1">arg_descriptors=[RandArg((</span><span class="s5">3</span><span class="s3">, </span><span class="s5">4</span><span class="s3">, </span><span class="s5">5</span><span class="s1">)</span><span class="s3">, </span><span class="s1">_f32)]</span><span class="s3">,</span>
                <span class="s1">poly_axes=[</span><span class="s5">0</span><span class="s1">]).both_enable_and_disable_xla()</span><span class="s3">,</span>
    <span class="s0"># Reduce the non-poly dimension</span>
    <span class="s1">PolyHarness(</span><span class="s4">&quot;argmax&quot;</span><span class="s3">, </span><span class="s4">&quot;1&quot;</span><span class="s3">,</span>
                <span class="s3">lambda </span><span class="s1">op: lax.argmax(op</span><span class="s3">, </span><span class="s1">axis=</span><span class="s5">1</span><span class="s3">, </span><span class="s1">index_dtype=np.int32)</span><span class="s3">,</span>
                <span class="s1">arg_descriptors=[RandArg((</span><span class="s5">3</span><span class="s3">, </span><span class="s5">4</span><span class="s3">, </span><span class="s5">5</span><span class="s1">)</span><span class="s3">, </span><span class="s1">_f32)]</span><span class="s3">,</span>
                <span class="s1">poly_axes=[</span><span class="s5">0</span><span class="s1">]).both_enable_and_disable_xla()</span><span class="s3">,</span>
    <span class="s1">PolyHarness(</span><span class="s4">&quot;jnp.argsort&quot;</span><span class="s3">, </span><span class="s4">&quot;&quot;</span><span class="s3">,</span>
                <span class="s3">lambda </span><span class="s1">op: jnp.argsort(op)</span><span class="s3">,</span>
                <span class="s1">arg_descriptors=[RandArg((</span><span class="s5">3</span><span class="s3">, </span><span class="s5">4</span><span class="s3">, </span><span class="s5">5</span><span class="s1">)</span><span class="s3">, </span><span class="s1">_f32)]</span><span class="s3">,</span>
                <span class="s1">poly_axes=[</span><span class="s5">0</span><span class="s1">])</span><span class="s3">,</span>
    <span class="s1">[</span>
        <span class="s1">PolyHarness(</span><span class="s4">&quot;average&quot;</span><span class="s3">,</span>
                    <span class="s4">f&quot;</span><span class="s3">{</span><span class="s1">axis=</span><span class="s3">}</span><span class="s4">_weights=None&quot;</span><span class="s3">,</span>
                    <span class="s3">lambda </span><span class="s1">x</span><span class="s3">, </span><span class="s1">axis: jnp.average(x</span><span class="s3">, </span><span class="s1">axis=axis</span><span class="s3">, </span><span class="s1">returned=</span><span class="s3">False, </span><span class="s1">weights=</span><span class="s3">None</span><span class="s1">)</span><span class="s3">,</span>
                    <span class="s1">arg_descriptors=[RandArg((</span><span class="s5">7</span><span class="s3">, </span><span class="s5">8</span><span class="s3">, </span><span class="s5">4</span><span class="s1">)</span><span class="s3">, </span><span class="s1">_f32)</span><span class="s3">, </span><span class="s1">StaticArg(axis)]</span><span class="s3">,</span>
                    <span class="s1">poly_axes=[</span><span class="s5">0</span><span class="s1">])</span>
        <span class="s3">for </span><span class="s1">axis </span><span class="s3">in </span><span class="s1">[</span><span class="s3">None, </span><span class="s5">0</span><span class="s3">, </span><span class="s5">1</span><span class="s1">]</span>
    <span class="s1">]</span><span class="s3">,</span>
    <span class="s1">[</span>
        <span class="s1">PolyHarness(</span><span class="s4">&quot;average&quot;</span><span class="s3">,</span>
                    <span class="s4">f&quot;</span><span class="s3">{</span><span class="s1">axis=</span><span class="s3">}</span><span class="s4">_weights=Some&quot;</span><span class="s3">,</span>
                    <span class="s3">lambda </span><span class="s1">x</span><span class="s3">, </span><span class="s1">weights</span><span class="s3">, </span><span class="s1">axis: jnp.average(x</span><span class="s3">, </span><span class="s1">axis=axis</span><span class="s3">, </span><span class="s1">returned=</span><span class="s3">False, </span><span class="s1">weights=weights)</span><span class="s3">,</span>
                    <span class="s1">arg_descriptors=[RandArg((</span><span class="s5">7</span><span class="s3">, </span><span class="s5">8</span><span class="s3">, </span><span class="s5">4</span><span class="s1">)</span><span class="s3">, </span><span class="s1">_f32)</span><span class="s3">, </span><span class="s1">RandArg((</span><span class="s5">7</span><span class="s3">, </span><span class="s5">8</span><span class="s3">, </span><span class="s5">4</span><span class="s1">)</span><span class="s3">, </span><span class="s1">_f32)</span><span class="s3">, </span><span class="s1">StaticArg(axis)]</span><span class="s3">,</span>
                    <span class="s1">poly_axes=[</span><span class="s5">0</span><span class="s3">, </span><span class="s5">0</span><span class="s1">])</span>
        <span class="s3">for </span><span class="s1">axis </span><span class="s3">in </span><span class="s1">[</span><span class="s3">None, </span><span class="s5">0</span><span class="s3">, </span><span class="s5">1</span><span class="s1">]</span>
    <span class="s1">]</span><span class="s3">,</span>
    <span class="s1">PolyHarness(</span><span class="s4">&quot;jnp.bincount&quot;</span><span class="s3">, </span><span class="s4">&quot;length=constant&quot;</span><span class="s3">,</span>
                <span class="s3">lambda </span><span class="s1">x: jnp.bincount(x % </span><span class="s5">2</span><span class="s3">, </span><span class="s1">length=</span><span class="s5">4</span><span class="s1">)</span><span class="s3">,</span>
                <span class="s1">arg_descriptors=[RandArg((</span><span class="s5">12</span><span class="s3">,</span><span class="s1">)</span><span class="s3">, </span><span class="s1">np.int32)]</span><span class="s3">,</span>
                <span class="s1">poly_axes=[</span><span class="s5">0</span><span class="s1">])</span><span class="s3">,</span>
    <span class="s1">PolyHarness(</span><span class="s4">&quot;jnp.bincount&quot;</span><span class="s3">, </span><span class="s4">&quot;length=poly&quot;</span><span class="s3">,</span>
                <span class="s3">lambda </span><span class="s1">x: jnp.bincount(x % </span><span class="s5">4</span><span class="s3">, </span><span class="s1">length=x.shape[</span><span class="s5">0</span><span class="s1">])</span><span class="s3">,</span>
                <span class="s1">arg_descriptors=[RandArg((</span><span class="s5">12</span><span class="s3">,</span><span class="s1">)</span><span class="s3">, </span><span class="s1">np.int32)]</span><span class="s3">,</span>
                <span class="s1">poly_axes=[</span><span class="s5">0</span><span class="s1">])</span><span class="s3">,</span>
    <span class="s1">PolyHarness(</span><span class="s4">&quot;broadcast_to&quot;</span><span class="s3">, </span><span class="s4">&quot;&quot;</span><span class="s3">,</span>
                <span class="s3">lambda </span><span class="s1">x: jnp.broadcast_to(x</span><span class="s3">, </span><span class="s1">[x.shape[</span><span class="s5">0</span><span class="s1">]</span><span class="s3">, </span><span class="s1">x.shape[</span><span class="s5">0</span><span class="s1">]</span><span class="s3">, </span><span class="s5">4</span><span class="s1">])</span><span class="s3">,</span>
                <span class="s1">arg_descriptors=[RandArg((</span><span class="s5">3</span><span class="s3">, </span><span class="s5">4</span><span class="s1">)</span><span class="s3">, </span><span class="s1">_f32)]</span><span class="s3">,</span>
                <span class="s1">poly_axes=[</span><span class="s5">0</span><span class="s1">])</span><span class="s3">,</span>
    <span class="s1">PolyHarness(</span><span class="s4">&quot;broadcast_in_dim&quot;</span><span class="s3">, </span><span class="s4">&quot;0&quot;</span><span class="s3">,</span>
                <span class="s3">lambda </span><span class="s1">x: lax.broadcast_in_dim(x</span><span class="s3">, </span><span class="s1">[x.shape[</span><span class="s5">0</span><span class="s1">]</span><span class="s3">, </span><span class="s5">4</span><span class="s3">, </span><span class="s5">5</span><span class="s3">, </span><span class="s5">6</span><span class="s1">]</span><span class="s3">,</span>
                                               <span class="s1">broadcast_dimensions=(</span><span class="s5">0</span><span class="s3">, </span><span class="s5">2</span><span class="s3">, </span><span class="s5">3</span><span class="s1">))</span><span class="s3">,</span>
                <span class="s1">arg_descriptors=[RandArg((</span><span class="s5">3</span><span class="s3">, </span><span class="s5">1</span><span class="s3">, </span><span class="s5">6</span><span class="s1">)</span><span class="s3">, </span><span class="s1">_f32)]</span><span class="s3">,</span>
                <span class="s1">poly_axes=[</span><span class="s5">0</span><span class="s1">])</span><span class="s3">,</span>
    <span class="s1">PolyHarness(</span><span class="s4">&quot;broadcast_in_dim&quot;</span><span class="s3">, </span><span class="s4">&quot;poly&quot;</span><span class="s3">,</span>
                <span class="s3">lambda </span><span class="s1">x: lax.broadcast_in_dim(x</span><span class="s3">, </span><span class="s1">[x.shape[</span><span class="s5">0</span><span class="s1">]</span><span class="s3">, </span><span class="s1">x.shape[</span><span class="s5">0</span><span class="s1">] + x.shape[</span><span class="s5">0</span><span class="s1">]</span><span class="s3">, </span><span class="s5">4</span><span class="s1">]</span><span class="s3">,</span>
                                               <span class="s1">broadcast_dimensions=(</span><span class="s5">0</span><span class="s3">, </span><span class="s5">1</span><span class="s3">, </span><span class="s5">2</span><span class="s1">))</span><span class="s3">,</span>
                <span class="s1">arg_descriptors=[RandArg((</span><span class="s5">3</span><span class="s3">, </span><span class="s5">1</span><span class="s3">, </span><span class="s5">4</span><span class="s1">)</span><span class="s3">, </span><span class="s1">_f32)]</span><span class="s3">,</span>
                <span class="s1">poly_axes=[</span><span class="s5">0</span><span class="s1">])</span><span class="s3">,</span>
    <span class="s1">PolyHarness(</span><span class="s4">&quot;broadcast_in_dim&quot;</span><span class="s3">, </span><span class="s4">&quot;poly2&quot;</span><span class="s3">,</span>
                <span class="s3">lambda </span><span class="s1">x: lax.broadcast_in_dim(x</span><span class="s3">, </span><span class="s1">[x.shape[</span><span class="s5">0</span><span class="s1">]</span><span class="s3">, </span><span class="s5">5</span><span class="s3">, </span><span class="s5">6</span><span class="s3">, </span><span class="s1">x.shape[</span><span class="s5">2</span><span class="s1">]</span><span class="s3">, </span><span class="s5">4</span><span class="s1">]</span><span class="s3">,</span>
                                               <span class="s1">broadcast_dimensions=(</span><span class="s5">0</span><span class="s3">, </span><span class="s5">2</span><span class="s3">, </span><span class="s5">3</span><span class="s1">))</span><span class="s3">,</span>
                <span class="s1">arg_descriptors=[RandArg((</span><span class="s5">3</span><span class="s3">, </span><span class="s5">1</span><span class="s3">, </span><span class="s5">4</span><span class="s1">)</span><span class="s3">, </span><span class="s1">_f32)]</span><span class="s3">,</span>
                <span class="s1">poly_axes=[(</span><span class="s5">0</span><span class="s3">, </span><span class="s5">2</span><span class="s1">)])</span><span class="s3">,</span>
    <span class="s1">PolyHarness(</span><span class="s4">&quot;broadcast_in_dim&quot;</span><span class="s3">, </span><span class="s4">&quot;transpose&quot;</span><span class="s3">,</span>
                <span class="s1">jax.grad(</span><span class="s3">lambda </span><span class="s1">x: jnp.sum(</span>
                    <span class="s1">lax.broadcast_in_dim(jnp.sin(x)</span><span class="s3">, </span><span class="s1">[</span><span class="s5">2</span><span class="s3">, </span><span class="s1">x.shape[</span><span class="s5">0</span><span class="s1">]</span><span class="s3">, </span><span class="s5">5</span><span class="s3">, </span><span class="s1">x.shape[</span><span class="s5">2</span><span class="s1">]</span><span class="s3">, </span><span class="s5">4</span><span class="s1">]</span><span class="s3">,</span>
                                         <span class="s1">broadcast_dimensions=(</span><span class="s5">1</span><span class="s3">, </span><span class="s5">2</span><span class="s3">, </span><span class="s5">3</span><span class="s1">))))</span><span class="s3">,</span>
                <span class="s1">arg_descriptors=[RandArg((</span><span class="s5">3</span><span class="s3">, </span><span class="s5">1</span><span class="s3">, </span><span class="s5">4</span><span class="s1">)</span><span class="s3">, </span><span class="s1">_f32)]</span><span class="s3">,</span>
                <span class="s1">poly_axes=[(</span><span class="s5">0</span><span class="s3">, </span><span class="s5">2</span><span class="s1">)])</span><span class="s3">,</span>
    <span class="s1">PolyHarness(</span><span class="s4">&quot;clamp&quot;</span><span class="s3">, </span><span class="s4">&quot;&quot;</span><span class="s3">,</span>
                <span class="s1">lax.clamp</span><span class="s3">,</span>
                <span class="s1">arg_descriptors=[RandArg((</span><span class="s5">3</span><span class="s3">, </span><span class="s5">4</span><span class="s3">, </span><span class="s5">5</span><span class="s1">)</span><span class="s3">, </span><span class="s1">_f32)</span><span class="s3">, </span><span class="s1">RandArg((</span><span class="s5">3</span><span class="s3">, </span><span class="s5">4</span><span class="s3">, </span><span class="s5">5</span><span class="s1">)</span><span class="s3">, </span><span class="s1">_f32)</span><span class="s3">,</span>
                                 <span class="s1">RandArg((</span><span class="s5">3</span><span class="s3">, </span><span class="s5">4</span><span class="s3">, </span><span class="s5">5</span><span class="s1">)</span><span class="s3">, </span><span class="s1">_f32)]</span><span class="s3">,</span>
                <span class="s1">poly_axes=[</span><span class="s5">0</span><span class="s3">, </span><span class="s5">0</span><span class="s3">, </span><span class="s5">0</span><span class="s1">])</span><span class="s3">,</span>
    <span class="s1">PolyHarness(</span><span class="s4">&quot;collapse&quot;</span><span class="s3">, </span><span class="s4">&quot;&quot;</span><span class="s3">,</span>
                <span class="s3">lambda </span><span class="s1">x: lax.collapse(x</span><span class="s3">, </span><span class="s5">1</span><span class="s3">, </span><span class="s5">4</span><span class="s1">)</span><span class="s3">,</span>
                <span class="s1">arg_descriptors=[RandArg((</span><span class="s5">3</span><span class="s3">, </span><span class="s5">4</span><span class="s3">, </span><span class="s5">5</span><span class="s3">, </span><span class="s5">6</span><span class="s3">, </span><span class="s5">7</span><span class="s1">)</span><span class="s3">, </span><span class="s1">_f32)]</span><span class="s3">,</span>
                <span class="s1">poly_axes=[(</span><span class="s5">0</span><span class="s3">, </span><span class="s5">1</span><span class="s3">, </span><span class="s5">3</span><span class="s1">)])</span><span class="s3">,</span>
    <span class="s1">PolyHarness(</span><span class="s4">&quot;concatenate&quot;</span><span class="s3">, </span><span class="s4">&quot;&quot;</span><span class="s3">,</span>
                <span class="s3">lambda </span><span class="s1">x: jnp.concatenate([x</span><span class="s3">, </span><span class="s1">x]</span><span class="s3">, </span><span class="s1">axis=</span><span class="s5">0</span><span class="s1">)</span><span class="s3">,</span>
                <span class="s1">arg_descriptors=[RandArg((</span><span class="s5">3</span><span class="s3">, </span><span class="s5">4</span><span class="s3">, </span><span class="s5">5</span><span class="s1">)</span><span class="s3">, </span><span class="s1">_f32)]</span><span class="s3">,</span>
                <span class="s1">poly_axes=[(</span><span class="s5">0</span><span class="s3">, </span><span class="s5">1</span><span class="s1">)])</span><span class="s3">,</span>
    <span class="s1">PolyHarness(</span><span class="s4">&quot;concatenate&quot;</span><span class="s3">, </span><span class="s4">&quot;grad&quot;</span><span class="s3">,</span>
                <span class="s1">jax.grad(</span><span class="s3">lambda </span><span class="s1">x: jnp.sum(jnp.concatenate([x</span><span class="s3">, </span><span class="s1">jnp.sin(x)]</span><span class="s3">, </span><span class="s1">axis=</span><span class="s5">0</span><span class="s1">)))</span><span class="s3">,</span>
                <span class="s1">arg_descriptors=[RandArg((</span><span class="s5">3</span><span class="s3">, </span><span class="s5">4</span><span class="s3">, </span><span class="s5">5</span><span class="s1">)</span><span class="s3">, </span><span class="s1">_f32)]</span><span class="s3">,</span>
                <span class="s1">poly_axes=[(</span><span class="s5">0</span><span class="s3">, </span><span class="s5">1</span><span class="s1">)])</span><span class="s3">,</span>

    <span class="s1">PolyHarness(</span><span class="s4">&quot;conv_general_dilated&quot;</span><span class="s3">, </span><span class="s4">&quot;1d_stride=1&quot;</span><span class="s3">,</span>
                <span class="s3">lambda </span><span class="s1">lhs</span><span class="s3">, </span><span class="s1">rhs: lax.conv_general_dilated(</span>
                    <span class="s1">lhs</span><span class="s3">, </span><span class="s1">rhs</span><span class="s3">,</span>
                    <span class="s1">window_strides=(</span><span class="s5">1</span><span class="s3">,</span><span class="s1">)</span><span class="s3">,</span>
                    <span class="s1">padding=</span><span class="s4">&quot;SAME&quot;</span><span class="s3">,</span>
                    <span class="s1">rhs_dilation=</span><span class="s3">None,</span>
                    <span class="s1">dimension_numbers=lax.ConvDimensionNumbers(lhs_spec=(</span><span class="s5">0</span><span class="s3">, </span><span class="s5">2</span><span class="s3">, </span><span class="s5">1</span><span class="s1">)</span><span class="s3">,</span>
                                                               <span class="s1">rhs_spec=(</span><span class="s5">2</span><span class="s3">, </span><span class="s5">1</span><span class="s3">, </span><span class="s5">0</span><span class="s1">)</span><span class="s3">,</span>
                                                               <span class="s1">out_spec=(</span><span class="s5">0</span><span class="s3">, </span><span class="s5">2</span><span class="s3">, </span><span class="s5">1</span><span class="s1">)))</span><span class="s3">,</span>
                <span class="s1">arg_descriptors=[RandArg((</span><span class="s5">1</span><span class="s3">, </span><span class="s5">12</span><span class="s3">, </span><span class="s5">16</span><span class="s1">)</span><span class="s3">, </span><span class="s1">_f32)</span><span class="s3">, </span><span class="s1">RandArg((</span><span class="s5">4</span><span class="s3">, </span><span class="s5">16</span><span class="s3">, </span><span class="s5">16</span><span class="s1">)</span><span class="s3">, </span><span class="s1">_f32)]</span><span class="s3">,</span>
                <span class="s1">poly_axes=[</span><span class="s5">1</span><span class="s3">, None</span><span class="s1">]).both_enable_and_disable_xla()</span><span class="s3">,</span>
    <span class="s0"># The same example from above, but with stride=2.</span>
    <span class="s1">PolyHarness(</span><span class="s4">&quot;conv_general_dilated&quot;</span><span class="s3">, </span><span class="s4">&quot;1d_stride=2_even&quot;</span><span class="s3">,</span>
                <span class="s3">lambda </span><span class="s1">lhs</span><span class="s3">, </span><span class="s1">rhs: lax.conv_general_dilated(</span>
                    <span class="s1">lhs</span><span class="s3">, </span><span class="s1">rhs</span><span class="s3">,</span>
                    <span class="s1">window_strides=(</span><span class="s5">2</span><span class="s3">,</span><span class="s1">)</span><span class="s3">,</span>
                    <span class="s1">padding=</span><span class="s4">&quot;SAME&quot;</span><span class="s3">,</span>
                    <span class="s1">rhs_dilation=</span><span class="s3">None,</span>
                    <span class="s1">dimension_numbers=lax.ConvDimensionNumbers(lhs_spec=(</span><span class="s5">0</span><span class="s3">, </span><span class="s5">2</span><span class="s3">, </span><span class="s5">1</span><span class="s1">)</span><span class="s3">,</span>
                                                               <span class="s1">rhs_spec=(</span><span class="s5">2</span><span class="s3">, </span><span class="s5">1</span><span class="s3">, </span><span class="s5">0</span><span class="s1">)</span><span class="s3">,</span>
                                                               <span class="s1">out_spec=(</span><span class="s5">0</span><span class="s3">, </span><span class="s5">2</span><span class="s3">, </span><span class="s5">1</span><span class="s1">)))</span><span class="s3">,</span>
                <span class="s1">arg_descriptors=[RandArg((</span><span class="s5">1</span><span class="s3">, </span><span class="s5">12</span><span class="s3">, </span><span class="s5">16</span><span class="s1">)</span><span class="s3">, </span><span class="s1">_f32)</span><span class="s3">, </span><span class="s1">RandArg((</span><span class="s5">4</span><span class="s3">, </span><span class="s5">16</span><span class="s3">, </span><span class="s5">16</span><span class="s1">)</span><span class="s3">, </span><span class="s1">_f32)]</span><span class="s3">,</span>
                <span class="s1">poly_axes=[</span><span class="s5">1</span><span class="s3">, None</span><span class="s1">]</span><span class="s3">,</span>
              <span class="s1">).both_enable_and_disable_xla()</span><span class="s3">,</span>
    <span class="s0"># The same example from above, but with stride=2 and odd input size.</span>
    <span class="s1">PolyHarness(</span><span class="s4">&quot;conv_general_dilated&quot;</span><span class="s3">, </span><span class="s4">&quot;1d_stride=2_odd&quot;</span><span class="s3">,</span>
                <span class="s3">lambda </span><span class="s1">lhs</span><span class="s3">, </span><span class="s1">rhs: lax.conv_general_dilated(</span>
                    <span class="s1">lhs</span><span class="s3">, </span><span class="s1">rhs</span><span class="s3">,</span>
                    <span class="s1">window_strides=(</span><span class="s5">2</span><span class="s3">,</span><span class="s1">)</span><span class="s3">,</span>
                    <span class="s1">padding=</span><span class="s4">&quot;SAME&quot;</span><span class="s3">,</span>
                    <span class="s1">rhs_dilation=</span><span class="s3">None,</span>
                    <span class="s1">dimension_numbers=lax.ConvDimensionNumbers(lhs_spec=(</span><span class="s5">0</span><span class="s3">, </span><span class="s5">2</span><span class="s3">, </span><span class="s5">1</span><span class="s1">)</span><span class="s3">,</span>
                                                               <span class="s1">rhs_spec=(</span><span class="s5">2</span><span class="s3">, </span><span class="s5">1</span><span class="s3">, </span><span class="s5">0</span><span class="s1">)</span><span class="s3">,</span>
                                                               <span class="s1">out_spec=(</span><span class="s5">0</span><span class="s3">, </span><span class="s5">2</span><span class="s3">, </span><span class="s5">1</span><span class="s1">)))</span><span class="s3">,</span>
                <span class="s1">arg_descriptors=[RandArg((</span><span class="s5">1</span><span class="s3">, </span><span class="s5">13</span><span class="s3">, </span><span class="s5">16</span><span class="s1">)</span><span class="s3">, </span><span class="s1">_f32)</span><span class="s3">, </span><span class="s1">RandArg((</span><span class="s5">4</span><span class="s3">, </span><span class="s5">16</span><span class="s3">, </span><span class="s5">16</span><span class="s1">)</span><span class="s3">, </span><span class="s1">_f32)]</span><span class="s3">,</span>
                <span class="s1">poly_axes=[</span><span class="s5">1</span><span class="s3">, None</span><span class="s1">]</span><span class="s3">,</span>
                <span class="s1">).both_enable_and_disable_xla()</span><span class="s3">,</span>
    <span class="s0"># Issue #11402</span>
    <span class="s1">PolyHarness(</span><span class="s4">&quot;conv_general_dilated&quot;</span><span class="s3">, </span><span class="s4">&quot;1d_2&quot;</span><span class="s3">,</span>
                <span class="s3">lambda </span><span class="s1">lhs</span><span class="s3">, </span><span class="s1">rhs: lax.conv_transpose(lhs</span><span class="s3">, </span><span class="s1">rhs</span><span class="s3">,</span>
                                                    <span class="s1">strides=(</span><span class="s5">2</span><span class="s3">,</span><span class="s1">)</span><span class="s3">,</span>
                                                    <span class="s1">padding=</span><span class="s4">&quot;SAME&quot;</span><span class="s3">,</span>
                                                    <span class="s1">rhs_dilation=</span><span class="s3">None,</span>
                                                    <span class="s1">transpose_kernel=</span><span class="s3">False</span><span class="s1">)</span><span class="s3">,</span>
                <span class="s1">arg_descriptors=[RandArg((</span><span class="s5">5</span><span class="s3">, </span><span class="s5">12</span><span class="s3">, </span><span class="s5">16</span><span class="s1">)</span><span class="s3">, </span><span class="s1">_f32)</span><span class="s3">, </span><span class="s1">RandArg((</span><span class="s5">4</span><span class="s3">, </span><span class="s5">16</span><span class="s3">, </span><span class="s5">16</span><span class="s1">)</span><span class="s3">, </span><span class="s1">_f32)]</span><span class="s3">,</span>
                <span class="s1">poly_axes=[</span><span class="s5">0</span><span class="s3">, None</span><span class="s1">]</span><span class="s3">,</span>
                <span class="s1">tol=</span><span class="s5">1e-5</span><span class="s1">).both_enable_and_disable_xla()</span><span class="s3">,</span>
    <span class="s0"># Issue #11402</span>
    <span class="s1">PolyHarness(</span><span class="s4">&quot;conv_general_dilated&quot;</span><span class="s3">, </span><span class="s4">&quot;1d_3&quot;</span><span class="s3">,</span>
                <span class="s3">lambda </span><span class="s1">lhs</span><span class="s3">, </span><span class="s1">rhs: lax.conv_transpose(lhs</span><span class="s3">, </span><span class="s1">rhs</span><span class="s3">,</span>
                                                    <span class="s1">strides=(</span><span class="s5">2</span><span class="s3">,</span><span class="s1">)</span><span class="s3">,</span>
                                                    <span class="s1">padding=</span><span class="s4">&quot;SAME&quot;</span><span class="s3">,</span>
                                                    <span class="s1">rhs_dilation=</span><span class="s3">None,</span>
                                                    <span class="s1">transpose_kernel=</span><span class="s3">False</span><span class="s1">)</span><span class="s3">,</span>
                <span class="s1">arg_descriptors=[RandArg((</span><span class="s5">5</span><span class="s3">, </span><span class="s5">12</span><span class="s3">, </span><span class="s5">16</span><span class="s1">)</span><span class="s3">, </span><span class="s1">_f32)</span><span class="s3">, </span><span class="s1">RandArg((</span><span class="s5">4</span><span class="s3">, </span><span class="s5">16</span><span class="s3">, </span><span class="s5">16</span><span class="s1">)</span><span class="s3">, </span><span class="s1">_f32)]</span><span class="s3">,</span>
                <span class="s1">poly_axes=[</span><span class="s5">1</span><span class="s3">, None</span><span class="s1">]</span><span class="s3">,</span>
                <span class="s1">tol=</span><span class="s5">1e-5</span><span class="s1">).both_enable_and_disable_xla()</span><span class="s3">,</span>
    <span class="s1">PolyHarness(</span><span class="s4">&quot;conv_general_dilated&quot;</span><span class="s3">, </span><span class="s4">&quot;&quot;</span><span class="s3">,</span>
                <span class="s3">lambda </span><span class="s1">lhs</span><span class="s3">, </span><span class="s1">rhs: lax.conv_general_dilated(</span>
                    <span class="s1">lhs</span><span class="s3">, </span><span class="s1">rhs</span><span class="s3">,</span>
                    <span class="s1">window_strides=(</span><span class="s5">2</span><span class="s3">, </span><span class="s5">3</span><span class="s1">)</span><span class="s3">,</span>
                    <span class="s1">padding=((</span><span class="s5">0</span><span class="s3">, </span><span class="s5">0</span><span class="s1">)</span><span class="s3">, </span><span class="s1">(</span><span class="s5">0</span><span class="s3">, </span><span class="s5">0</span><span class="s1">))</span><span class="s3">,</span>
                    <span class="s1">lhs_dilation=(</span><span class="s5">1</span><span class="s3">, </span><span class="s5">1</span><span class="s1">)</span><span class="s3">,</span>
                    <span class="s1">rhs_dilation=(</span><span class="s5">1</span><span class="s3">, </span><span class="s5">2</span><span class="s1">)</span><span class="s3">,</span>
                    <span class="s1">dimension_numbers=(</span><span class="s4">&quot;NCHW&quot;</span><span class="s3">, </span><span class="s4">&quot;OIHW&quot;</span><span class="s3">, </span><span class="s4">&quot;NCHW&quot;</span><span class="s1">)</span><span class="s3">,</span>
                    <span class="s1">feature_group_count=</span><span class="s5">1</span><span class="s3">,</span>
                    <span class="s1">batch_group_count=</span><span class="s5">1</span><span class="s3">,</span>
                    <span class="s1">precision=</span><span class="s3">None</span><span class="s1">)</span><span class="s3">,</span>
                <span class="s1">arg_descriptors=[RandArg((</span><span class="s5">7</span><span class="s3">, </span><span class="s5">3</span><span class="s3">, </span><span class="s5">9</span><span class="s3">, </span><span class="s5">10</span><span class="s1">)</span><span class="s3">, </span><span class="s1">_f32)</span><span class="s3">, </span><span class="s1">RandArg((</span><span class="s5">3</span><span class="s3">, </span><span class="s5">3</span><span class="s3">, </span><span class="s5">4</span><span class="s3">, </span><span class="s5">5</span><span class="s1">)</span><span class="s3">, </span><span class="s1">_f32)]</span><span class="s3">,</span>
                <span class="s1">poly_axes=[</span><span class="s5">0</span><span class="s3">, None</span><span class="s1">]).both_enable_and_disable_xla()</span><span class="s3">,</span>
    <span class="s1">PolyHarness(</span><span class="s4">&quot;cummax&quot;</span><span class="s3">, </span><span class="s4">&quot;&quot;</span><span class="s3">,</span>
                <span class="s3">lambda </span><span class="s1">x: lax_control_flow.cummax(x</span><span class="s3">, </span><span class="s1">axis=</span><span class="s5">1</span><span class="s3">, </span><span class="s1">reverse=</span><span class="s3">False</span><span class="s1">)</span><span class="s3">,</span>
                <span class="s1">arg_descriptors=[RandArg((</span><span class="s5">3</span><span class="s3">, </span><span class="s5">4</span><span class="s3">, </span><span class="s5">5</span><span class="s1">)</span><span class="s3">, </span><span class="s1">_f32)]</span><span class="s3">,</span>
                <span class="s1">poly_axes=[</span><span class="s5">0</span><span class="s1">])</span><span class="s3">,</span>
    <span class="s1">PolyHarness(</span><span class="s4">&quot;jnp.cumsum&quot;</span><span class="s3">, </span><span class="s4">&quot;reduce_axis=poly&quot;</span><span class="s3">,</span>
                <span class="s3">lambda </span><span class="s1">x: jnp.cumsum(x</span><span class="s3">, </span><span class="s1">axis=</span><span class="s5">0</span><span class="s1">)</span><span class="s3">,</span>
                <span class="s1">arg_descriptors=[RandArg((</span><span class="s5">3</span><span class="s3">, </span><span class="s5">5</span><span class="s1">)</span><span class="s3">, </span><span class="s1">_f32)]</span><span class="s3">,</span>
                <span class="s1">poly_axes=[</span><span class="s5">0</span><span class="s1">]</span><span class="s3">,</span>
                <span class="s1">expect_error=(</span>
                  <span class="s1">(</span><span class="s3">None, None</span><span class="s1">) </span><span class="s3">if </span><span class="s1">(</span><span class="s3">not </span><span class="s1">config.jax2tf_default_native_serialization </span><span class="s3">or</span>
                                   <span class="s1">jtu.device_under_test() == </span><span class="s4">&quot;tpu&quot;</span><span class="s1">) </span><span class="s3">else</span>
                  <span class="s1">(NotImplementedError</span><span class="s3">,</span>
                   <span class="s4">&quot;associative scan over axis of non-constant size&quot;</span><span class="s1">)))</span><span class="s3">,</span>
    <span class="s1">PolyHarness(</span><span class="s4">&quot;jnp.cumsum&quot;</span><span class="s3">, </span><span class="s4">&quot;reduce_axis=static&quot;</span><span class="s3">,</span>
                <span class="s3">lambda </span><span class="s1">x: jnp.cumsum(x</span><span class="s3">, </span><span class="s1">axis=</span><span class="s5">1</span><span class="s1">)</span><span class="s3">,</span>
                <span class="s1">arg_descriptors=[RandArg((</span><span class="s5">3</span><span class="s3">, </span><span class="s5">5</span><span class="s1">)</span><span class="s3">, </span><span class="s1">_f32)]</span><span class="s3">,</span>
                <span class="s1">poly_axes=[</span><span class="s5">0</span><span class="s1">])</span><span class="s3">,</span>
    <span class="s1">PolyHarness(</span><span class="s4">&quot;delta&quot;</span><span class="s3">, </span><span class="s4">&quot;0&quot;</span><span class="s3">,</span>
                <span class="s3">lambda </span><span class="s1">x: lax_internal._delta(_f32</span><span class="s3">, </span><span class="s1">x.shape</span><span class="s3">, </span><span class="s1">axes=(</span><span class="s5">0</span><span class="s3">, </span><span class="s5">1</span><span class="s1">)) + x</span><span class="s3">,</span>
                <span class="s1">arg_descriptors=[RandArg((</span><span class="s5">3</span><span class="s3">, </span><span class="s5">1</span><span class="s1">)</span><span class="s3">, </span><span class="s1">_f32)]</span><span class="s3">,</span>
                <span class="s1">poly_axes=[</span><span class="s5">0</span><span class="s1">])</span><span class="s3">,</span>
    <span class="s1">PolyHarness(</span><span class="s4">&quot;dot_general&quot;</span><span class="s3">, </span><span class="s4">&quot;&quot;</span><span class="s3">,</span>
                <span class="s3">lambda </span><span class="s1">lhs</span><span class="s3">, </span><span class="s1">rhs: lax.dot_general(lhs</span><span class="s3">, </span><span class="s1">rhs</span><span class="s3">,</span>
                                                 <span class="s1">dimension_numbers=(((</span><span class="s5">2</span><span class="s3">,</span><span class="s1">)</span><span class="s3">, </span><span class="s1">(</span><span class="s5">1</span><span class="s3">,</span><span class="s1">))</span><span class="s3">, </span><span class="s1">((</span><span class="s5">0</span><span class="s3">,</span><span class="s1">)</span><span class="s3">, </span><span class="s1">(</span><span class="s5">0</span><span class="s3">,</span><span class="s1">))))</span><span class="s3">,</span>
                <span class="s1">arg_descriptors=[RandArg((</span><span class="s5">3</span><span class="s3">, </span><span class="s5">4</span><span class="s3">, </span><span class="s5">4</span><span class="s1">)</span><span class="s3">, </span><span class="s1">_f32)</span><span class="s3">, </span><span class="s1">RandArg((</span><span class="s5">3</span><span class="s3">, </span><span class="s5">4</span><span class="s1">)</span><span class="s3">, </span><span class="s1">_f32)]</span><span class="s3">,</span>
                <span class="s1">poly_axes=[</span><span class="s5">0</span><span class="s3">, </span><span class="s5">0</span><span class="s1">])</span><span class="s3">,</span>
    <span class="s1">PolyHarness(</span><span class="s4">&quot;dynamic_slice&quot;</span><span class="s3">, </span><span class="s4">&quot;idx=tuple_int&quot;</span><span class="s3">,</span>
                <span class="s0"># x:shape: (b, 4)</span>
                <span class="s3">lambda </span><span class="s1">x: lax.dynamic_slice(x</span><span class="s3">, </span><span class="s1">(</span><span class="s5">0</span><span class="s3">, </span><span class="s5">1</span><span class="s1">)</span><span class="s3">, </span><span class="s1">(x.shape[</span><span class="s5">0</span><span class="s1">]</span><span class="s3">, </span><span class="s5">2</span><span class="s1">))</span><span class="s3">,</span>
                <span class="s1">arg_descriptors=[RandArg((</span><span class="s5">3</span><span class="s3">, </span><span class="s5">4</span><span class="s1">)</span><span class="s3">, </span><span class="s1">_f32)]</span><span class="s3">,</span>
                <span class="s1">poly_axes=[</span><span class="s5">0</span><span class="s1">]).both_enable_and_disable_xla()</span><span class="s3">,</span>
    <span class="s1">PolyHarness(</span><span class="s4">&quot;dynamic_slice&quot;</span><span class="s3">, </span><span class="s4">&quot;idx=tuple_arg&quot;</span><span class="s3">,</span>
                <span class="s0"># x:shape: (b, 4)</span>
                <span class="s3">lambda </span><span class="s1">x</span><span class="s3">, </span><span class="s1">i0: lax.dynamic_slice(x</span><span class="s3">, </span><span class="s1">(i0</span><span class="s3">, </span><span class="s1">np.int32(</span><span class="s5">1</span><span class="s1">))</span><span class="s3">, </span><span class="s1">(x.shape[</span><span class="s5">0</span><span class="s1">]</span><span class="s3">, </span><span class="s5">2</span><span class="s1">))</span><span class="s3">,</span>
                <span class="s1">arg_descriptors=[RandArg((</span><span class="s5">3</span><span class="s3">, </span><span class="s5">4</span><span class="s1">)</span><span class="s3">, </span><span class="s1">_f32)</span><span class="s3">, </span><span class="s1">np.array(-</span><span class="s5">2</span><span class="s3">, </span><span class="s1">dtype=np.int32)]</span><span class="s3">,</span>
                <span class="s1">poly_axes=[</span><span class="s5">0</span><span class="s3">, None</span><span class="s1">]).both_enable_and_disable_xla()</span><span class="s3">,</span>
    <span class="s1">PolyHarness(</span><span class="s4">&quot;dynamic_slice&quot;</span><span class="s3">, </span><span class="s4">&quot;idx=array&quot;</span><span class="s3">,</span>
                <span class="s0"># x:shape: (b, 4)</span>
                <span class="s3">lambda </span><span class="s1">x</span><span class="s3">, </span><span class="s1">idx: lax.dynamic_slice(x</span><span class="s3">, </span><span class="s1">idx</span><span class="s3">, </span><span class="s1">(x.shape[</span><span class="s5">0</span><span class="s1">]</span><span class="s3">, </span><span class="s5">2</span><span class="s1">))</span><span class="s3">,</span>
                <span class="s1">arg_descriptors=[RandArg((</span><span class="s5">3</span><span class="s3">, </span><span class="s5">4</span><span class="s1">)</span><span class="s3">, </span><span class="s1">_f32)</span><span class="s3">, </span><span class="s1">np.array([-</span><span class="s5">2</span><span class="s3">, </span><span class="s1">-</span><span class="s5">1</span><span class="s1">]</span><span class="s3">, </span><span class="s1">dtype=np.int32)]</span><span class="s3">,</span>
                <span class="s1">poly_axes=[</span><span class="s5">0</span><span class="s3">, None</span><span class="s1">]).both_enable_and_disable_xla()</span><span class="s3">,</span>
    <span class="s1">PolyHarness(</span><span class="s4">&quot;dynamic_slice_in_dim&quot;</span><span class="s3">, </span><span class="s4">&quot;idx=0&quot;</span><span class="s3">,</span>
                <span class="s0"># x:shape: (b, 4)</span>
                <span class="s3">lambda </span><span class="s1">x: lax.dynamic_slice_in_dim(x</span><span class="s3">, </span><span class="s5">0</span><span class="s3">, </span><span class="s1">x.shape[</span><span class="s5">0</span><span class="s1">]</span><span class="s3">, </span><span class="s1">axis=</span><span class="s5">0</span><span class="s1">)</span><span class="s3">,</span>
                <span class="s1">arg_descriptors=[RandArg((</span><span class="s5">3</span><span class="s3">, </span><span class="s5">4</span><span class="s1">)</span><span class="s3">, </span><span class="s1">_f32)]</span><span class="s3">,</span>
                <span class="s1">poly_axes=[</span><span class="s5">0</span><span class="s1">]).both_enable_and_disable_xla()</span><span class="s3">,</span>
    <span class="s1">PolyHarness(</span><span class="s4">&quot;dynamic_update_slice&quot;</span><span class="s3">, </span><span class="s4">&quot;idx=tuple_int&quot;</span><span class="s3">,</span>
                <span class="s0"># x:shape: (b, 4)</span>
                <span class="s3">lambda </span><span class="s1">x: lax.dynamic_update_slice(x</span><span class="s3">, </span><span class="s1">x</span><span class="s3">, </span><span class="s1">(</span><span class="s5">0</span><span class="s3">, </span><span class="s5">0</span><span class="s1">))</span><span class="s3">,</span>
                <span class="s1">arg_descriptors=[RandArg((</span><span class="s5">3</span><span class="s3">, </span><span class="s5">4</span><span class="s1">)</span><span class="s3">, </span><span class="s1">_f32)]</span><span class="s3">,</span>
                <span class="s1">poly_axes=[</span><span class="s5">0</span><span class="s1">]).both_enable_and_disable_xla()</span><span class="s3">,</span>
    <span class="s1">PolyHarness(</span><span class="s4">&quot;dynamic_update_slice&quot;</span><span class="s3">, </span><span class="s4">&quot;idx=tuple_arg&quot;</span><span class="s3">,</span>
                <span class="s0"># x:shape: (b, 4)</span>
                <span class="s3">lambda </span><span class="s1">x</span><span class="s3">, </span><span class="s1">i0: lax.dynamic_update_slice(x</span><span class="s3">, </span><span class="s1">x</span><span class="s3">, </span><span class="s1">(i0</span><span class="s3">, </span><span class="s1">np.int32(</span><span class="s5">0</span><span class="s1">)))</span><span class="s3">,</span>
                <span class="s1">arg_descriptors=[RandArg((</span><span class="s5">3</span><span class="s3">, </span><span class="s5">4</span><span class="s1">)</span><span class="s3">, </span><span class="s1">_f32)</span><span class="s3">, </span><span class="s1">np.array(-</span><span class="s5">2</span><span class="s3">, </span><span class="s1">dtype=np.int32)]</span><span class="s3">,</span>
                <span class="s1">poly_axes=[</span><span class="s5">0</span><span class="s3">, None</span><span class="s1">]).both_enable_and_disable_xla()</span><span class="s3">,</span>
    <span class="s1">PolyHarness(</span><span class="s4">&quot;dynamic_update_slice&quot;</span><span class="s3">, </span><span class="s4">&quot;idx=array&quot;</span><span class="s3">,</span>
                <span class="s0"># x:shape: (b, 4)</span>
                <span class="s3">lambda </span><span class="s1">x</span><span class="s3">, </span><span class="s1">idx: lax.dynamic_update_slice(x</span><span class="s3">, </span><span class="s1">x</span><span class="s3">, </span><span class="s1">idx)</span><span class="s3">,</span>
                <span class="s1">arg_descriptors=[RandArg((</span><span class="s5">3</span><span class="s3">, </span><span class="s5">4</span><span class="s1">)</span><span class="s3">, </span><span class="s1">_f32)</span><span class="s3">, </span><span class="s1">np.array([-</span><span class="s5">2</span><span class="s3">, </span><span class="s1">-</span><span class="s5">1</span><span class="s1">]</span><span class="s3">, </span><span class="s1">dtype=np.int32)]</span><span class="s3">,</span>
                <span class="s1">poly_axes=[</span><span class="s5">0</span><span class="s3">, None</span><span class="s1">]).both_enable_and_disable_xla()</span><span class="s3">,</span>
    <span class="s1">PolyHarness(</span><span class="s4">&quot;einsum&quot;</span><span class="s3">, </span><span class="s4">&quot;0&quot;</span><span class="s3">,</span>
                <span class="s3">lambda </span><span class="s1">x: jnp.einsum(</span><span class="s4">&quot;...i-&gt;...&quot;</span><span class="s3">, </span><span class="s1">x)</span><span class="s3">,</span>
                <span class="s1">arg_descriptors=[RandArg((</span><span class="s5">3</span><span class="s3">, </span><span class="s5">4</span><span class="s1">)</span><span class="s3">, </span><span class="s1">_f32)]</span><span class="s3">,</span>
                <span class="s1">poly_axes=[</span><span class="s5">0</span><span class="s1">])</span><span class="s3">,</span>
    <span class="s1">PolyHarness(</span><span class="s4">&quot;einsum&quot;</span><span class="s3">, </span><span class="s4">&quot;0_alt&quot;</span><span class="s3">,</span>
                <span class="s3">lambda </span><span class="s1">x: jnp.einsum(x</span><span class="s3">, </span><span class="s1">(...</span><span class="s3">, </span><span class="s5">1</span><span class="s1">)</span><span class="s3">, </span><span class="s1">[...])</span><span class="s3">,</span>
                <span class="s1">arg_descriptors=[RandArg((</span><span class="s5">3</span><span class="s3">, </span><span class="s5">4</span><span class="s1">)</span><span class="s3">, </span><span class="s1">_f32)]</span><span class="s3">,</span>
                <span class="s1">poly_axes=[</span><span class="s5">0</span><span class="s1">])</span><span class="s3">,</span>
    <span class="s1">PolyHarness(</span><span class="s4">&quot;einsum&quot;</span><span class="s3">, </span><span class="s4">&quot;1&quot;</span><span class="s3">,</span>
                <span class="s3">lambda </span><span class="s1">x</span><span class="s3">, </span><span class="s1">y: jnp.einsum(</span><span class="s4">&quot;...ij,...jk-&gt;...ik&quot;</span><span class="s3">, </span><span class="s1">x</span><span class="s3">, </span><span class="s1">y)</span><span class="s3">,</span>
                <span class="s1">arg_descriptors=[RandArg((</span><span class="s5">3</span><span class="s3">, </span><span class="s5">4</span><span class="s3">, </span><span class="s5">5</span><span class="s1">)</span><span class="s3">, </span><span class="s1">_f32)</span><span class="s3">, </span><span class="s1">RandArg((</span><span class="s5">3</span><span class="s3">, </span><span class="s5">5</span><span class="s3">, </span><span class="s5">6</span><span class="s1">)</span><span class="s3">, </span><span class="s1">_f32)]</span><span class="s3">,</span>
                <span class="s1">poly_axes=[</span><span class="s5">0</span><span class="s3">, </span><span class="s5">0</span><span class="s1">])</span><span class="s3">,</span>
    <span class="s1">PolyHarness(</span><span class="s4">&quot;einsum&quot;</span><span class="s3">, </span><span class="s4">&quot;1_alt&quot;</span><span class="s3">,</span>
                <span class="s3">lambda </span><span class="s1">x</span><span class="s3">, </span><span class="s1">y: jnp.einsum(x</span><span class="s3">, </span><span class="s1">[...</span><span class="s3">, </span><span class="s5">0</span><span class="s3">, </span><span class="s5">1</span><span class="s1">]</span><span class="s3">, </span><span class="s1">y</span><span class="s3">, </span><span class="s1">(...</span><span class="s3">, </span><span class="s5">1</span><span class="s3">, </span><span class="s5">2</span><span class="s1">)</span><span class="s3">, </span><span class="s1">[...</span><span class="s3">, </span><span class="s5">0</span><span class="s3">, </span><span class="s5">2</span><span class="s1">])</span><span class="s3">,</span>
                <span class="s1">arg_descriptors=[RandArg((</span><span class="s5">3</span><span class="s3">, </span><span class="s5">4</span><span class="s3">, </span><span class="s5">5</span><span class="s1">)</span><span class="s3">, </span><span class="s1">_f32)</span><span class="s3">, </span><span class="s1">RandArg((</span><span class="s5">3</span><span class="s3">, </span><span class="s5">5</span><span class="s3">, </span><span class="s5">6</span><span class="s1">)</span><span class="s3">, </span><span class="s1">_f32)]</span><span class="s3">,</span>
                <span class="s1">poly_axes=[</span><span class="s5">0</span><span class="s3">, </span><span class="s5">0</span><span class="s1">])</span><span class="s3">,</span>
    <span class="s1">PolyHarness(</span><span class="s4">&quot;einsum&quot;</span><span class="s3">, </span><span class="s4">&quot;2&quot;</span><span class="s3">,</span>
                <span class="s3">lambda </span><span class="s1">x</span><span class="s3">, </span><span class="s1">y: jnp.einsum(</span><span class="s4">&quot;...ij,jk-&gt;...ik&quot;</span><span class="s3">, </span><span class="s1">x</span><span class="s3">, </span><span class="s1">y)</span><span class="s3">,</span>
                <span class="s1">arg_descriptors=[RandArg((</span><span class="s5">3</span><span class="s3">, </span><span class="s5">4</span><span class="s3">, </span><span class="s5">5</span><span class="s1">)</span><span class="s3">, </span><span class="s1">_f32)</span><span class="s3">, </span><span class="s1">RandArg((</span><span class="s5">5</span><span class="s3">, </span><span class="s5">6</span><span class="s1">)</span><span class="s3">, </span><span class="s1">_f32)]</span><span class="s3">,</span>
                <span class="s1">poly_axes=[</span><span class="s5">0</span><span class="s3">, None</span><span class="s1">])</span><span class="s3">,</span>
    <span class="s1">PolyHarness(</span><span class="s4">&quot;einsum&quot;</span><span class="s3">, </span><span class="s4">&quot;2_alt&quot;</span><span class="s3">,</span>
                <span class="s3">lambda </span><span class="s1">x</span><span class="s3">, </span><span class="s1">y: jnp.einsum(x</span><span class="s3">, </span><span class="s1">[...</span><span class="s3">, </span><span class="s5">0</span><span class="s3">, </span><span class="s5">1</span><span class="s1">]</span><span class="s3">, </span><span class="s1">y</span><span class="s3">, </span><span class="s1">[</span><span class="s5">1</span><span class="s3">, </span><span class="s5">2</span><span class="s1">]</span><span class="s3">, </span><span class="s1">[...</span><span class="s3">, </span><span class="s5">0</span><span class="s3">, </span><span class="s5">2</span><span class="s1">])</span><span class="s3">,</span>
                <span class="s1">arg_descriptors=[RandArg((</span><span class="s5">3</span><span class="s3">, </span><span class="s5">4</span><span class="s3">, </span><span class="s5">5</span><span class="s1">)</span><span class="s3">, </span><span class="s1">_f32)</span><span class="s3">, </span><span class="s1">RandArg((</span><span class="s5">5</span><span class="s3">, </span><span class="s5">6</span><span class="s1">)</span><span class="s3">, </span><span class="s1">_f32)]</span><span class="s3">,</span>
                <span class="s1">poly_axes=[</span><span class="s5">0</span><span class="s3">, None</span><span class="s1">])</span><span class="s3">,</span>
    <span class="s1">PolyHarness(</span><span class="s4">&quot;einsum&quot;</span><span class="s3">, </span><span class="s4">&quot;3&quot;</span><span class="s3">,</span>
                <span class="s0"># Reduced dimension is polymorphic</span>
                <span class="s3">lambda </span><span class="s1">x</span><span class="s3">, </span><span class="s1">y: jnp.einsum(</span><span class="s4">&quot;ij,jk-&gt;ik&quot;</span><span class="s3">, </span><span class="s1">x</span><span class="s3">, </span><span class="s1">y)</span><span class="s3">,</span>
                <span class="s1">arg_descriptors=[RandArg((</span><span class="s5">3</span><span class="s3">, </span><span class="s5">4</span><span class="s1">)</span><span class="s3">, </span><span class="s1">_f32)</span><span class="s3">, </span><span class="s1">RandArg((</span><span class="s5">4</span><span class="s3">, </span><span class="s5">5</span><span class="s1">)</span><span class="s3">, </span><span class="s1">_f32)]</span><span class="s3">,</span>
                <span class="s1">poly_axes=[</span><span class="s5">1</span><span class="s3">, </span><span class="s5">0</span><span class="s1">])</span><span class="s3">,</span>
    <span class="s1">PolyHarness(</span><span class="s4">&quot;einsum&quot;</span><span class="s3">, </span><span class="s4">&quot;3_alt&quot;</span><span class="s3">,</span>
                <span class="s0"># Reduced dimension is polymorphic</span>
                <span class="s3">lambda </span><span class="s1">x</span><span class="s3">, </span><span class="s1">y: jnp.einsum(x</span><span class="s3">, </span><span class="s1">[</span><span class="s5">0</span><span class="s3">, </span><span class="s5">1</span><span class="s1">]</span><span class="s3">, </span><span class="s1">y</span><span class="s3">, </span><span class="s1">[</span><span class="s5">1</span><span class="s3">, </span><span class="s5">2</span><span class="s1">]</span><span class="s3">, </span><span class="s1">[</span><span class="s5">0</span><span class="s3">, </span><span class="s5">2</span><span class="s1">])</span><span class="s3">,</span>
                <span class="s1">arg_descriptors=[RandArg((</span><span class="s5">3</span><span class="s3">, </span><span class="s5">4</span><span class="s1">)</span><span class="s3">, </span><span class="s1">_f32)</span><span class="s3">, </span><span class="s1">RandArg((</span><span class="s5">4</span><span class="s3">, </span><span class="s5">5</span><span class="s1">)</span><span class="s3">, </span><span class="s1">_f32)]</span><span class="s3">,</span>
                <span class="s1">poly_axes=[</span><span class="s5">1</span><span class="s3">, </span><span class="s5">0</span><span class="s1">])</span><span class="s3">,</span>
    <span class="s1">PolyHarness(</span><span class="s4">&quot;einsum&quot;</span><span class="s3">, </span><span class="s4">&quot;4&quot;</span><span class="s3">,</span>
                <span class="s0"># Reduced dimension is polymorphic, and is 2*b</span>
                <span class="s3">lambda </span><span class="s1">x</span><span class="s3">, </span><span class="s1">y: jnp.einsum(</span><span class="s4">&quot;ij,jk-&gt;ik&quot;</span><span class="s3">,</span>
                                        <span class="s1">jnp.concatenate([x</span><span class="s3">, </span><span class="s1">x]</span><span class="s3">, </span><span class="s1">axis=</span><span class="s5">1</span><span class="s1">)</span><span class="s3">,</span>
                                        <span class="s1">jnp.concatenate([y</span><span class="s3">, </span><span class="s1">y]</span><span class="s3">, </span><span class="s1">axis=</span><span class="s5">0</span><span class="s1">))</span><span class="s3">,</span>
                <span class="s1">arg_descriptors=[RandArg((</span><span class="s5">3</span><span class="s3">, </span><span class="s5">4</span><span class="s1">)</span><span class="s3">, </span><span class="s1">_f32)</span><span class="s3">, </span><span class="s1">RandArg((</span><span class="s5">4</span><span class="s3">, </span><span class="s5">5</span><span class="s1">)</span><span class="s3">, </span><span class="s1">_f32)]</span><span class="s3">,</span>
                <span class="s1">poly_axes=[</span><span class="s5">1</span><span class="s3">, </span><span class="s5">0</span><span class="s1">])</span><span class="s3">,</span>
    <span class="s1">PolyHarness(</span><span class="s4">&quot;einsum&quot;</span><span class="s3">, </span><span class="s4">&quot;4_alt&quot;</span><span class="s3">,</span>
                <span class="s0"># Reduced dimension is polymorphic, and is 2*b</span>
                <span class="s3">lambda </span><span class="s1">x</span><span class="s3">, </span><span class="s1">y: jnp.einsum(jnp.concatenate([x</span><span class="s3">, </span><span class="s1">x]</span><span class="s3">, </span><span class="s1">axis=</span><span class="s5">1</span><span class="s1">)</span><span class="s3">, </span><span class="s1">[</span><span class="s5">0</span><span class="s3">, </span><span class="s5">1</span><span class="s1">]</span><span class="s3">,</span>
                                        <span class="s1">jnp.concatenate([y</span><span class="s3">, </span><span class="s1">y]</span><span class="s3">, </span><span class="s1">axis=</span><span class="s5">0</span><span class="s1">)</span><span class="s3">, </span><span class="s1">[</span><span class="s5">1</span><span class="s3">, </span><span class="s5">2</span><span class="s1">]</span><span class="s3">,</span>
                                        <span class="s1">[</span><span class="s5">0</span><span class="s3">, </span><span class="s5">2</span><span class="s1">])</span><span class="s3">,</span>
                <span class="s1">arg_descriptors=[RandArg((</span><span class="s5">3</span><span class="s3">, </span><span class="s5">4</span><span class="s1">)</span><span class="s3">, </span><span class="s1">_f32)</span><span class="s3">, </span><span class="s1">RandArg((</span><span class="s5">4</span><span class="s3">, </span><span class="s5">5</span><span class="s1">)</span><span class="s3">, </span><span class="s1">_f32)]</span><span class="s3">,</span>
                <span class="s1">poly_axes=[</span><span class="s5">1</span><span class="s3">, </span><span class="s5">0</span><span class="s1">])</span><span class="s3">,</span>
    <span class="s1">PolyHarness(</span><span class="s4">&quot;einsum&quot;</span><span class="s3">, </span><span class="s4">&quot;multiple_contractions&quot;</span><span class="s3">,</span>
                <span class="s3">lambda </span><span class="s1">x</span><span class="s3">, </span><span class="s1">y</span><span class="s3">, </span><span class="s1">z: jnp.einsum(</span><span class="s4">&quot;ab,bc,cd-&gt;ad&quot;</span><span class="s3">, </span><span class="s1">x</span><span class="s3">, </span><span class="s1">y</span><span class="s3">, </span><span class="s1">z)</span><span class="s3">,</span>
                <span class="s1">arg_descriptors=[RandArg((</span><span class="s5">3</span><span class="s3">, </span><span class="s5">2</span><span class="s1">)</span><span class="s3">, </span><span class="s1">_f32)</span><span class="s3">, </span><span class="s1">RandArg((</span><span class="s5">2</span><span class="s3">, </span><span class="s5">3</span><span class="s1">)</span><span class="s3">, </span><span class="s1">_f32)</span><span class="s3">, </span><span class="s1">RandArg((</span><span class="s5">3</span><span class="s3">, </span><span class="s5">4</span><span class="s1">)</span><span class="s3">, </span><span class="s1">_f32)]</span><span class="s3">,</span>
                <span class="s1">poly_axes=[</span><span class="s5">0</span><span class="s3">, None, None</span><span class="s1">])</span><span class="s3">,</span>
    <span class="s1">PolyHarness(</span><span class="s4">&quot;einsum&quot;</span><span class="s3">, </span><span class="s4">&quot;incompatible_contractions_error&quot;</span><span class="s3">,</span>
                <span class="s3">lambda </span><span class="s1">x</span><span class="s3">, </span><span class="s1">y: jnp.einsum(</span><span class="s4">&quot;ab,cb-&gt;ac&quot;</span><span class="s3">, </span><span class="s1">x</span><span class="s3">, </span><span class="s1">y)</span><span class="s3">,</span>
                <span class="s1">arg_descriptors=[RandArg((</span><span class="s5">2</span><span class="s3">, </span><span class="s5">3</span><span class="s1">)</span><span class="s3">, </span><span class="s1">_f32)</span><span class="s3">, </span><span class="s1">RandArg((</span><span class="s5">2</span><span class="s3">, </span><span class="s5">3</span><span class="s1">)</span><span class="s3">, </span><span class="s1">_f32)]</span><span class="s3">,</span>
                <span class="s1">polymorphic_shapes=[</span><span class="s4">&quot;(2, b0)&quot;</span><span class="s3">, </span><span class="s4">&quot;(2, b1)&quot;</span><span class="s1">]</span><span class="s3">,</span>
                <span class="s1">input_signature=[tf.TensorSpec((</span><span class="s5">2</span><span class="s3">, None</span><span class="s1">))</span><span class="s3">, </span><span class="s1">tf.TensorSpec((</span><span class="s5">2</span><span class="s3">, None</span><span class="s1">))]</span><span class="s3">,</span>
                <span class="s1">expect_error=(AssertionError</span><span class="s3">,</span>
                              <span class="s4">&quot;Incompatible reduction dimensions&quot;</span><span class="s1">))</span><span class="s3">,</span>
    <span class="s1">PolyHarness(</span><span class="s4">&quot;eye&quot;</span><span class="s3">, </span><span class="s4">&quot;N=poly_M=None&quot;</span><span class="s3">,</span>
                <span class="s3">lambda </span><span class="s1">x: jnp.eye(x.shape[</span><span class="s5">0</span><span class="s1">]) + x</span><span class="s3">,</span>
                <span class="s1">arg_descriptors=[RandArg((</span><span class="s5">3</span><span class="s3">, </span><span class="s5">1</span><span class="s1">)</span><span class="s3">, </span><span class="s1">_f32)]</span><span class="s3">,</span>
                <span class="s1">poly_axes=[</span><span class="s5">0</span><span class="s1">])</span><span class="s3">,</span>
    <span class="s1">PolyHarness(</span><span class="s4">&quot;eye&quot;</span><span class="s3">, </span><span class="s4">&quot;N=poly_M=poly&quot;</span><span class="s3">,</span>
                <span class="s3">lambda </span><span class="s1">x: jnp.eye(x.shape[</span><span class="s5">0</span><span class="s1">]</span><span class="s3">, </span><span class="s1">M=x.shape[</span><span class="s5">0</span><span class="s1">] + </span><span class="s5">2</span><span class="s1">) + x</span><span class="s3">,</span>
                <span class="s1">arg_descriptors=[RandArg((</span><span class="s5">3</span><span class="s3">, </span><span class="s5">1</span><span class="s1">)</span><span class="s3">, </span><span class="s1">_f32)]</span><span class="s3">,</span>
                <span class="s1">poly_axes=[</span><span class="s5">0</span><span class="s1">])</span><span class="s3">,</span>
    <span class="s1">PolyHarness(</span><span class="s4">&quot;full&quot;</span><span class="s3">, </span><span class="s4">&quot;&quot;</span><span class="s3">,</span>
                <span class="s3">lambda </span><span class="s1">x: lax.full((x.shape[</span><span class="s5">0</span><span class="s1">]</span><span class="s3">, </span><span class="s5">2</span><span class="s1">)</span><span class="s3">, </span><span class="s5">3.</span><span class="s1">) + x</span><span class="s3">,</span>
                <span class="s1">arg_descriptors=[RandArg((</span><span class="s5">3</span><span class="s3">, </span><span class="s5">1</span><span class="s1">)</span><span class="s3">, </span><span class="s1">_f32)]</span><span class="s3">,</span>
                <span class="s1">poly_axes=[</span><span class="s5">0</span><span class="s1">])</span><span class="s3">,</span>
    <span class="s0"># operand is non-poly, index is poly</span>
    <span class="s1">PolyHarness(</span><span class="s4">&quot;getitem&quot;</span><span class="s3">, </span><span class="s4">&quot;op=static_idx=poly&quot;</span><span class="s3">,</span>
                <span class="s3">lambda </span><span class="s1">a</span><span class="s3">, </span><span class="s1">i: a[i]</span><span class="s3">,</span>
                <span class="s1">arg_descriptors=[RandArg((</span><span class="s5">3</span><span class="s3">, </span><span class="s5">4</span><span class="s1">)</span><span class="s3">, </span><span class="s1">_f32)</span><span class="s3">, </span><span class="s1">np.array([</span><span class="s5">2</span><span class="s3">, </span><span class="s5">2</span><span class="s1">]</span><span class="s3">, </span><span class="s1">np.int32)]</span><span class="s3">,</span>
                <span class="s1">poly_axes=[</span><span class="s3">None, </span><span class="s5">0</span><span class="s1">]).both_enable_and_disable_xla()</span><span class="s3">,</span>
    <span class="s0"># operand is poly, index is integer</span>
    <span class="s1">PolyHarness(</span><span class="s4">&quot;getitem&quot;</span><span class="s3">, </span><span class="s4">&quot;op=poly_idx=const&quot;</span><span class="s3">,</span>
                <span class="s3">lambda </span><span class="s1">a: a[</span><span class="s5">1</span><span class="s1">]</span><span class="s3">,</span>
                <span class="s1">arg_descriptors=[RandArg((</span><span class="s5">3</span><span class="s3">, </span><span class="s5">4</span><span class="s1">)</span><span class="s3">, </span><span class="s1">_f32)]</span><span class="s3">,</span>
                <span class="s1">poly_axes=[</span><span class="s5">0</span><span class="s1">]).both_enable_and_disable_xla()</span><span class="s3">,</span>
    <span class="s0"># operand is poly, index is dim poly</span>
    <span class="s1">PolyHarness(</span><span class="s4">&quot;getitem&quot;</span><span class="s3">, </span><span class="s4">&quot;op=poly_idx=dim&quot;</span><span class="s3">,</span>
                <span class="s3">lambda </span><span class="s1">a: a[jnp.array(a.shape[</span><span class="s5">0</span><span class="s1">] - </span><span class="s5">2</span><span class="s1">)]</span><span class="s3">,</span>
                <span class="s1">arg_descriptors=[RandArg((</span><span class="s5">3</span><span class="s3">, </span><span class="s5">4</span><span class="s1">)</span><span class="s3">, </span><span class="s1">_f32)]</span><span class="s3">,</span>
                <span class="s1">poly_axes=[</span><span class="s5">0</span><span class="s1">]).both_enable_and_disable_xla()</span><span class="s3">,</span>
    <span class="s0"># Both the operand and the index are poly</span>
    <span class="s1">PolyHarness(</span><span class="s4">&quot;getitem&quot;</span><span class="s3">, </span><span class="s4">&quot;op=poly_idx=poly&quot;</span><span class="s3">,</span>
                <span class="s3">lambda </span><span class="s1">a</span><span class="s3">, </span><span class="s1">i: a[i]</span><span class="s3">,</span>
                <span class="s1">arg_descriptors=[RandArg((</span><span class="s5">3</span><span class="s3">, </span><span class="s5">4</span><span class="s1">)</span><span class="s3">, </span><span class="s1">_f32)</span><span class="s3">, </span><span class="s1">np.array([</span><span class="s5">1</span><span class="s3">, </span><span class="s5">2</span><span class="s3">, </span><span class="s5">0</span><span class="s1">]</span><span class="s3">, </span><span class="s1">np.int32)]</span><span class="s3">,</span>
                <span class="s1">poly_axes=[</span><span class="s5">0</span><span class="s3">, </span><span class="s5">0</span><span class="s1">]).both_enable_and_disable_xla()</span><span class="s3">,</span>
    <span class="s0"># op is poly and index is an entire slice</span>
    <span class="s1">PolyHarness(</span><span class="s4">&quot;getitem&quot;</span><span class="s3">, </span><span class="s4">&quot;op=poly_idx=slice-all&quot;</span><span class="s3">,</span>
                <span class="s3">lambda </span><span class="s1">a: a[:]</span><span class="s3">,</span>
                <span class="s1">arg_descriptors=[RandArg((</span><span class="s5">3</span><span class="s3">, </span><span class="s5">4</span><span class="s1">)</span><span class="s3">, </span><span class="s1">_f32)]</span><span class="s3">,</span>
                <span class="s1">poly_axes=[</span><span class="s5">0</span><span class="s1">]).both_enable_and_disable_xla()</span><span class="s3">,</span>
    <span class="s0"># op is poly and index is a partial slice</span>
    <span class="s1">PolyHarness(</span><span class="s4">&quot;getitem&quot;</span><span class="s3">, </span><span class="s4">&quot;op=poly_idx=slice-ct-1&quot;</span><span class="s3">,</span>
                <span class="s3">lambda </span><span class="s1">a: a[:</span><span class="s5">2</span><span class="s1">]</span><span class="s3">,</span>
                <span class="s1">arg_descriptors=[RandArg((</span><span class="s5">3</span><span class="s3">, </span><span class="s5">4</span><span class="s1">)</span><span class="s3">, </span><span class="s1">_f32)]</span><span class="s3">,</span>
                <span class="s1">poly_axes=[</span><span class="s5">0</span><span class="s1">]</span><span class="s3">,</span>
                <span class="s1">expect_error=(IndexError</span><span class="s3">, </span><span class="s4">&quot;Cannot use NumPy slice indexing on an array dimension&quot;</span><span class="s1">)</span>
                <span class="s1">).both_enable_and_disable_xla()</span><span class="s3">,</span>
    <span class="s1">PolyHarness(</span><span class="s4">&quot;getitem&quot;</span><span class="s3">, </span><span class="s4">&quot;op=poly_idx=slice-ct-2&quot;</span><span class="s3">,</span>
                <span class="s3">lambda </span><span class="s1">a: a[:</span><span class="s3">, </span><span class="s1">:</span><span class="s5">2</span><span class="s1">]</span><span class="s3">,</span>
                <span class="s1">arg_descriptors=[RandArg((</span><span class="s5">3</span><span class="s3">, </span><span class="s5">4</span><span class="s1">)</span><span class="s3">, </span><span class="s1">_f32)]</span><span class="s3">,</span>
                <span class="s1">poly_axes=[</span><span class="s5">0</span><span class="s1">]).both_enable_and_disable_xla()</span><span class="s3">,</span>
    <span class="s1">PolyHarness(</span><span class="s4">&quot;getitem&quot;</span><span class="s3">, </span><span class="s4">&quot;op=poly_idx=slice-None-1&quot;</span><span class="s3">,</span>
                <span class="s3">lambda </span><span class="s1">a: a[:a.shape[</span><span class="s5">0</span><span class="s1">]]</span><span class="s3">,</span>
                <span class="s1">arg_descriptors=[RandArg((</span><span class="s5">3</span><span class="s3">, </span><span class="s5">4</span><span class="s1">)</span><span class="s3">, </span><span class="s1">_f32)]</span><span class="s3">,</span>
                <span class="s1">poly_axes=[</span><span class="s5">0</span><span class="s1">]).both_enable_and_disable_xla()</span><span class="s3">,</span>
    <span class="s1">PolyHarness(</span><span class="s4">&quot;getitem&quot;</span><span class="s3">, </span><span class="s4">&quot;op=poly_idx=slice-poly&quot;</span><span class="s3">,</span>
                <span class="s3">lambda </span><span class="s1">a: a[:a.shape[</span><span class="s5">0</span><span class="s1">] - </span><span class="s5">1</span><span class="s1">]</span><span class="s3">,</span>
                <span class="s1">arg_descriptors=[RandArg((</span><span class="s5">3</span><span class="s3">, </span><span class="s5">4</span><span class="s1">)</span><span class="s3">, </span><span class="s1">_f32)]</span><span class="s3">,</span>
                <span class="s1">poly_axes=[</span><span class="s5">0</span><span class="s1">]</span><span class="s3">,</span>
                <span class="s1">expect_error=(IndexError</span><span class="s3">, </span><span class="s4">&quot;Array slice indices must have static&quot;</span><span class="s1">)).both_enable_and_disable_xla()</span><span class="s3">,</span>
    <span class="s1">PolyHarness(</span><span class="s4">&quot;image_resize&quot;</span><span class="s3">, </span><span class="s4">&quot;linear_0&quot;</span><span class="s3">,</span>
                <span class="s3">lambda </span><span class="s1">x: jax.image.resize(x</span><span class="s3">, </span><span class="s1">(x.shape[</span><span class="s5">0</span><span class="s1">]</span><span class="s3">, </span><span class="s5">2 </span><span class="s1">* x.shape[</span><span class="s5">1</span><span class="s1">]</span><span class="s3">, </span><span class="s5">2 </span><span class="s1">* x.shape[</span><span class="s5">2</span><span class="s1">]</span><span class="s3">, </span><span class="s1">x.shape[</span><span class="s5">3</span><span class="s1">])</span><span class="s3">,</span>
                                           <span class="s1">method=</span><span class="s4">&quot;linear&quot;</span><span class="s1">)</span><span class="s3">,</span>
                <span class="s1">arg_descriptors=[RandArg((</span><span class="s5">3</span><span class="s3">, </span><span class="s5">16</span><span class="s3">, </span><span class="s5">32</span><span class="s3">, </span><span class="s5">3</span><span class="s1">)</span><span class="s3">, </span><span class="s1">_f32)]</span><span class="s3">,</span>
                <span class="s1">poly_axes=[(</span><span class="s5">1</span><span class="s3">, </span><span class="s5">2</span><span class="s1">)])</span><span class="s3">,</span>
    <span class="s1">PolyHarness(</span><span class="s4">&quot;image_resize&quot;</span><span class="s3">, </span><span class="s4">&quot;linear_to_fixed_dim&quot;</span><span class="s3">,</span>
                <span class="s3">lambda </span><span class="s1">x: jax.image.resize(x</span><span class="s3">, </span><span class="s1">(x.shape[</span><span class="s5">0</span><span class="s1">]</span><span class="s3">, </span><span class="s5">64</span><span class="s3">, </span><span class="s5">64</span><span class="s3">, </span><span class="s1">x.shape[</span><span class="s5">3</span><span class="s1">])</span><span class="s3">,</span>
                                           <span class="s1">method=</span><span class="s4">&quot;linear&quot;</span><span class="s1">)</span><span class="s3">,</span>
                <span class="s1">arg_descriptors=[RandArg((</span><span class="s5">3</span><span class="s3">, </span><span class="s5">16</span><span class="s3">, </span><span class="s5">32</span><span class="s3">, </span><span class="s5">3</span><span class="s1">)</span><span class="s3">, </span><span class="s1">_f32)]</span><span class="s3">,</span>
                <span class="s1">poly_axes=[(</span><span class="s5">1</span><span class="s3">, </span><span class="s5">2</span><span class="s1">)])</span><span class="s3">,</span>
    <span class="s1">PolyHarness(</span><span class="s4">&quot;image_resize&quot;</span><span class="s3">, </span><span class="s4">&quot;nearest_0&quot;</span><span class="s3">,</span>
                <span class="s3">lambda </span><span class="s1">x: jax.image.resize(x</span><span class="s3">, </span><span class="s1">(x.shape[</span><span class="s5">0</span><span class="s1">]</span><span class="s3">, </span><span class="s5">2 </span><span class="s1">* x.shape[</span><span class="s5">1</span><span class="s1">]</span><span class="s3">, </span><span class="s5">2 </span><span class="s1">* x.shape[</span><span class="s5">2</span><span class="s1">]</span><span class="s3">, </span><span class="s1">x.shape[</span><span class="s5">3</span><span class="s1">])</span><span class="s3">,</span>
                                           <span class="s1">method=</span><span class="s4">&quot;nearest&quot;</span><span class="s1">)</span><span class="s3">,</span>
                <span class="s1">arg_descriptors=[RandArg((</span><span class="s5">3</span><span class="s3">, </span><span class="s5">5</span><span class="s3">, </span><span class="s5">7</span><span class="s3">, </span><span class="s5">3</span><span class="s1">)</span><span class="s3">, </span><span class="s1">_f32)]</span><span class="s3">,</span>
                <span class="s1">poly_axes=[(</span><span class="s5">1</span><span class="s3">, </span><span class="s5">2</span><span class="s1">)])</span><span class="s3">,</span>
    <span class="s1">PolyHarness(</span><span class="s4">&quot;index_in_dim&quot;</span><span class="s3">, </span><span class="s4">&quot;0&quot;</span><span class="s3">,</span>
                <span class="s3">lambda </span><span class="s1">x: lax.index_in_dim(x</span><span class="s3">, </span><span class="s1">-</span><span class="s5">1</span><span class="s3">, </span><span class="s1">axis=</span><span class="s5">0</span><span class="s3">, </span><span class="s1">keepdims=</span><span class="s3">False</span><span class="s1">)</span><span class="s3">,</span>
                <span class="s1">arg_descriptors=[RandArg((</span><span class="s5">3</span><span class="s3">, </span><span class="s5">4</span><span class="s1">)</span><span class="s3">, </span><span class="s1">_f32)]</span><span class="s3">,</span>
                <span class="s1">poly_axes=[</span><span class="s5">0</span><span class="s1">])</span><span class="s3">,</span>
    <span class="s1">PolyHarness(</span><span class="s4">&quot;index_in_dim&quot;</span><span class="s3">, </span><span class="s4">&quot;idx=neg&quot;</span><span class="s3">,</span>
                <span class="s3">lambda </span><span class="s1">x: lax.index_in_dim(x</span><span class="s3">, </span><span class="s1">-</span><span class="s5">1</span><span class="s3">, </span><span class="s1">axis=</span><span class="s5">0</span><span class="s3">, </span><span class="s1">keepdims=</span><span class="s3">False</span><span class="s1">)</span><span class="s3">,</span>
                <span class="s1">arg_descriptors=[RandArg((</span><span class="s5">3</span><span class="s3">, </span><span class="s5">4</span><span class="s1">)</span><span class="s3">, </span><span class="s1">_f32)]</span><span class="s3">,</span>
                <span class="s1">poly_axes=[</span><span class="s5">0</span><span class="s1">])</span><span class="s3">,</span>
    <span class="s1">PolyHarness(</span><span class="s4">&quot;index_in_dim&quot;</span><span class="s3">, </span><span class="s4">&quot;idx=last&quot;</span><span class="s3">,</span>
                <span class="s3">lambda </span><span class="s1">x: lax.index_in_dim(x</span><span class="s3">, </span><span class="s1">x.shape[</span><span class="s5">0</span><span class="s1">] - </span><span class="s5">1</span><span class="s3">, </span><span class="s1">axis=</span><span class="s5">0</span><span class="s3">, </span><span class="s1">keepdims=</span><span class="s3">False</span><span class="s1">)</span><span class="s3">,</span>
                <span class="s1">arg_descriptors=[RandArg((</span><span class="s5">3</span><span class="s3">, </span><span class="s5">4</span><span class="s1">)</span><span class="s3">, </span><span class="s1">_f32)]</span><span class="s3">,</span>
                <span class="s1">poly_axes=[</span><span class="s5">0</span><span class="s1">])</span><span class="s3">,</span>
    <span class="s1">PolyHarness(</span><span class="s4">&quot;jnp.insert&quot;</span><span class="s3">, </span><span class="s4">&quot;insert=constant&quot;</span><span class="s3">,</span>
                <span class="s3">lambda </span><span class="s1">x: jnp.insert(x</span><span class="s3">, </span><span class="s1">jnp.arange(</span><span class="s5">3</span><span class="s3">, </span><span class="s1">dtype=_i32)</span><span class="s3">, </span><span class="s1">np.array([</span><span class="s5">3</span><span class="s3">, </span><span class="s5">4</span><span class="s3">, </span><span class="s5">5</span><span class="s1">]</span><span class="s3">, </span><span class="s1">dtype=_i32))</span><span class="s3">,</span>
                <span class="s1">arg_descriptors=[RandArg((</span><span class="s5">12</span><span class="s3">,</span><span class="s1">)</span><span class="s3">, </span><span class="s1">_i32)]</span><span class="s3">,</span>
                <span class="s1">poly_axes=[</span><span class="s5">0</span><span class="s1">]</span><span class="s3">,</span>
                <span class="s1">expect_error=expect_error_associative_scan)</span><span class="s3">,</span>
    <span class="s1">PolyHarness(</span><span class="s4">&quot;jnp.insert&quot;</span><span class="s3">, </span><span class="s4">&quot;insert=poly&quot;</span><span class="s3">,</span>
                <span class="s3">lambda </span><span class="s1">x: jnp.insert(x</span><span class="s3">, </span><span class="s1">jnp.arange(x.shape[</span><span class="s5">0</span><span class="s1">]</span><span class="s3">, </span><span class="s1">dtype=_i32)</span><span class="s3">, </span><span class="s1">x</span><span class="s3">, </span><span class="s1">axis=</span><span class="s5">0</span><span class="s1">)</span><span class="s3">,</span>
                <span class="s1">arg_descriptors=[RandArg((</span><span class="s5">12</span><span class="s3">, </span><span class="s5">3</span><span class="s1">)</span><span class="s3">, </span><span class="s1">_i32)]</span><span class="s3">,</span>
                <span class="s1">poly_axes=[(</span><span class="s5">0</span><span class="s3">, </span><span class="s5">1</span><span class="s1">)]</span><span class="s3">,</span>
                <span class="s1">expect_error=expect_error_associative_scan)</span><span class="s3">,</span>
    <span class="s1">PolyHarness(</span><span class="s4">&quot;iota&quot;</span><span class="s3">, </span><span class="s4">&quot;&quot;</span><span class="s3">,</span>
                <span class="s3">lambda </span><span class="s1">x: x + lax.iota(_f32</span><span class="s3">, </span><span class="s1">x.shape[</span><span class="s5">0</span><span class="s1">])</span><span class="s3">,</span>
                <span class="s1">arg_descriptors=[RandArg((</span><span class="s5">3</span><span class="s3">,</span><span class="s1">)</span><span class="s3">, </span><span class="s1">_f32)]</span><span class="s3">,</span>
                <span class="s1">poly_axes=[</span><span class="s5">0</span><span class="s1">])</span><span class="s3">,</span>
    <span class="s1">PolyHarness(</span><span class="s4">&quot;matmul&quot;</span><span class="s3">, </span><span class="s4">&quot;0&quot;</span><span class="s3">,</span>
                <span class="s1">jnp.matmul</span><span class="s3">,</span>
                <span class="s1">arg_descriptors=[RandArg((</span><span class="s5">7</span><span class="s3">, </span><span class="s5">8</span><span class="s3">, </span><span class="s5">4</span><span class="s1">)</span><span class="s3">, </span><span class="s1">_f32)</span><span class="s3">, </span><span class="s1">RandArg((</span><span class="s5">7</span><span class="s3">, </span><span class="s5">4</span><span class="s3">, </span><span class="s5">5</span><span class="s1">)</span><span class="s3">, </span><span class="s1">_f32)]</span><span class="s3">,</span>
                <span class="s1">poly_axes=[</span><span class="s5">0</span><span class="s3">, </span><span class="s5">0</span><span class="s1">]</span><span class="s3">,</span>
                <span class="s1">tol=</span><span class="s5">1e-5</span><span class="s1">)</span><span class="s3">,</span>
    <span class="s1">PolyHarness(</span><span class="s4">&quot;matmul&quot;</span><span class="s3">, </span><span class="s4">&quot;1&quot;</span><span class="s3">,</span>
                <span class="s1">jnp.matmul</span><span class="s3">,</span>
                <span class="s1">arg_descriptors=[RandArg((</span><span class="s5">7</span><span class="s3">, </span><span class="s5">8</span><span class="s3">, </span><span class="s5">4</span><span class="s1">)</span><span class="s3">, </span><span class="s1">_f32)</span><span class="s3">, </span><span class="s1">RandArg((</span><span class="s5">4</span><span class="s3">, </span><span class="s5">5</span><span class="s1">)</span><span class="s3">, </span><span class="s1">_f32)]</span><span class="s3">,</span>
                <span class="s1">poly_axes=[</span><span class="s5">0</span><span class="s3">, None</span><span class="s1">]</span><span class="s3">,</span>
                <span class="s1">tol=</span><span class="s5">1e-5</span><span class="s1">)</span><span class="s3">,</span>
    <span class="s1">[</span>
        <span class="s1">PolyHarness(</span><span class="s4">&quot;mean&quot;</span><span class="s3">,</span>
                    <span class="s4">f&quot;</span><span class="s3">{</span><span class="s1">axis=</span><span class="s3">}</span><span class="s4">_</span><span class="s3">{</span><span class="s1">keepdims=</span><span class="s3">}</span><span class="s4">_where=None&quot;</span><span class="s3">,</span>
                    <span class="s3">lambda </span><span class="s1">x</span><span class="s3">, </span><span class="s1">axis</span><span class="s3">, </span><span class="s1">keepdims: jnp.mean(x</span><span class="s3">, </span><span class="s1">axis=axis</span><span class="s3">, </span><span class="s1">keepdims=keepdims</span><span class="s3">, </span><span class="s1">where=</span><span class="s3">None</span><span class="s1">)</span><span class="s3">,</span>
                    <span class="s1">arg_descriptors=[RandArg((</span><span class="s5">7</span><span class="s3">, </span><span class="s5">8</span><span class="s3">, </span><span class="s5">4</span><span class="s1">)</span><span class="s3">, </span><span class="s1">_f32)</span><span class="s3">, </span><span class="s1">StaticArg(axis)</span><span class="s3">, </span><span class="s1">StaticArg(keepdims)]</span><span class="s3">,</span>
                    <span class="s1">poly_axes=[</span><span class="s5">0</span><span class="s1">])</span>
        <span class="s3">for </span><span class="s1">keepdims </span><span class="s3">in </span><span class="s1">[</span><span class="s3">False, True</span><span class="s1">]</span>
        <span class="s3">for </span><span class="s1">axis </span><span class="s3">in </span><span class="s1">[</span><span class="s3">None, </span><span class="s1">(</span><span class="s5">0</span><span class="s3">,</span><span class="s1">)</span><span class="s3">, </span><span class="s1">(</span><span class="s5">0</span><span class="s3">, </span><span class="s5">1</span><span class="s1">)</span><span class="s3">, </span><span class="s1">(</span><span class="s5">1</span><span class="s3">,</span><span class="s1">)]</span>
    <span class="s1">]</span><span class="s3">,</span>
    <span class="s1">[</span>
        <span class="s1">PolyHarness(</span><span class="s4">&quot;mean&quot;</span><span class="s3">,</span>
                    <span class="s4">f&quot;</span><span class="s3">{</span><span class="s1">axis=</span><span class="s3">}</span><span class="s4">_</span><span class="s3">{</span><span class="s1">keepdims=</span><span class="s3">}</span><span class="s4">_where=Some&quot;</span><span class="s3">,</span>
                    <span class="s3">lambda </span><span class="s1">x</span><span class="s3">, </span><span class="s1">where</span><span class="s3">, </span><span class="s1">axis</span><span class="s3">, </span><span class="s1">keepdims: jnp.mean(x</span><span class="s3">, </span><span class="s1">axis=axis</span><span class="s3">, </span><span class="s1">keepdims=keepdims</span><span class="s3">, </span><span class="s1">where=where)</span><span class="s3">,</span>
                    <span class="s1">arg_descriptors=[RandArg((</span><span class="s5">7</span><span class="s3">, </span><span class="s5">8</span><span class="s3">, </span><span class="s5">4</span><span class="s1">)</span><span class="s3">, </span><span class="s1">_f32)</span><span class="s3">, </span><span class="s1">RandArg((</span><span class="s5">7</span><span class="s3">, </span><span class="s5">8</span><span class="s3">, </span><span class="s5">4</span><span class="s1">)</span><span class="s3">, </span><span class="s1">np.bool_)</span><span class="s3">,</span>
                                     <span class="s1">StaticArg(axis)</span><span class="s3">, </span><span class="s1">StaticArg(keepdims)]</span><span class="s3">,</span>
                    <span class="s1">poly_axes=[</span><span class="s5">0</span><span class="s3">, </span><span class="s5">0</span><span class="s1">])</span>
        <span class="s3">for </span><span class="s1">keepdims </span><span class="s3">in </span><span class="s1">[</span><span class="s3">False, True</span><span class="s1">]</span>
        <span class="s3">for </span><span class="s1">axis </span><span class="s3">in </span><span class="s1">[</span><span class="s3">None, </span><span class="s1">(</span><span class="s5">0</span><span class="s3">,</span><span class="s1">)</span><span class="s3">, </span><span class="s1">(</span><span class="s5">0</span><span class="s3">, </span><span class="s5">1</span><span class="s1">)</span><span class="s3">, </span><span class="s1">(</span><span class="s5">1</span><span class="s3">,</span><span class="s1">)]</span>
    <span class="s1">]</span><span class="s3">,</span>
    <span class="s1">PolyHarness(</span><span class="s4">&quot;jnp.nonzero&quot;</span><span class="s3">, </span><span class="s4">&quot;size=constant&quot;</span><span class="s3">,</span>
                <span class="s3">lambda </span><span class="s1">x: jnp.nonzero(x % </span><span class="s5">3</span><span class="s3">, </span><span class="s1">size=</span><span class="s5">10</span><span class="s3">, </span><span class="s1">fill_value=</span><span class="s5">100</span><span class="s1">)</span><span class="s3">,</span>
                <span class="s1">arg_descriptors=[RandArg((</span><span class="s5">3</span><span class="s3">, </span><span class="s5">2</span><span class="s3">, </span><span class="s5">4</span><span class="s1">)</span><span class="s3">, </span><span class="s1">_i32)]</span><span class="s3">,</span>
                <span class="s1">poly_axes=[</span><span class="s5">0</span><span class="s1">]</span><span class="s3">,</span>
                <span class="s1">expect_error=expect_error_associative_scan)</span><span class="s3">,</span>
    <span class="s1">PolyHarness(</span><span class="s4">&quot;jnp.nonzero&quot;</span><span class="s3">, </span><span class="s4">&quot;size=poly&quot;</span><span class="s3">,</span>
                <span class="s3">lambda </span><span class="s1">x: jnp.nonzero(x % </span><span class="s5">3</span><span class="s3">, </span><span class="s1">size=x.shape[</span><span class="s5">0</span><span class="s1">] * </span><span class="s5">2</span><span class="s3">, </span><span class="s1">fill_value=</span><span class="s5">100</span><span class="s1">)</span><span class="s3">,</span>
                <span class="s1">arg_descriptors=[RandArg((</span><span class="s5">3</span><span class="s3">, </span><span class="s5">2</span><span class="s3">, </span><span class="s5">4</span><span class="s1">)</span><span class="s3">, </span><span class="s1">_i32)]</span><span class="s3">,</span>
                <span class="s1">poly_axes=[</span><span class="s5">0</span><span class="s1">]</span><span class="s3">,</span>
                <span class="s1">expect_error=expect_error_associative_scan)</span><span class="s3">,</span>
    <span class="s1">PolyHarness(</span><span class="s4">&quot;ones&quot;</span><span class="s3">, </span><span class="s4">&quot;&quot;</span><span class="s3">,</span>
                <span class="s3">lambda </span><span class="s1">x: jnp.ones(x.shape</span><span class="s3">, </span><span class="s1">dtype=_f32) + x</span><span class="s3">,</span>
                <span class="s1">arg_descriptors=[RandArg((</span><span class="s5">3</span><span class="s3">, </span><span class="s5">2</span><span class="s3">, </span><span class="s5">4</span><span class="s1">)</span><span class="s3">, </span><span class="s1">_f32)]</span><span class="s3">,</span>
                <span class="s1">poly_axes=[</span><span class="s5">0</span><span class="s1">])</span><span class="s3">,</span>
    <span class="s1">PolyHarness(</span><span class="s4">&quot;pad&quot;</span><span class="s3">, </span><span class="s4">&quot;&quot;</span><span class="s3">,</span>
                <span class="s1">lax.pad</span><span class="s3">,</span>
                <span class="s1">arg_descriptors=[RandArg((</span><span class="s5">3</span><span class="s3">, </span><span class="s5">2</span><span class="s3">, </span><span class="s5">5</span><span class="s1">)</span><span class="s3">, </span><span class="s1">_f32)</span><span class="s3">, </span><span class="s1">np.float32(</span><span class="s5">5.</span><span class="s1">)</span><span class="s3">,</span>
                                 <span class="s1">StaticArg(((</span><span class="s5">0</span><span class="s3">, </span><span class="s5">0</span><span class="s3">, </span><span class="s5">0</span><span class="s1">)</span><span class="s3">, </span><span class="s1">(</span><span class="s5">0</span><span class="s3">, </span><span class="s5">0</span><span class="s3">, </span><span class="s5">0</span><span class="s1">)</span><span class="s3">, </span><span class="s1">(</span><span class="s5">1</span><span class="s3">, </span><span class="s5">1</span><span class="s3">, </span><span class="s5">1</span><span class="s1">)))]</span><span class="s3">,</span>
                <span class="s1">poly_axes=[</span><span class="s5">0</span><span class="s3">, None</span><span class="s1">])</span><span class="s3">,</span>
    <span class="s1">PolyHarness(</span><span class="s4">&quot;pad&quot;</span><span class="s3">, </span><span class="s4">&quot;poly_padding_config&quot;</span><span class="s3">,</span>
                <span class="s3">lambda </span><span class="s1">x: lax.pad(x</span><span class="s3">, </span><span class="s1">_f32(</span><span class="s5">0.</span><span class="s1">)</span><span class="s3">,</span>
                                  <span class="s1">((x.shape[</span><span class="s5">0</span><span class="s1">]</span><span class="s3">, </span><span class="s1">x.shape[</span><span class="s5">1</span><span class="s1">]</span><span class="s3">, </span><span class="s1">x.shape[</span><span class="s5">0</span><span class="s1">])</span><span class="s3">,</span>
                                   <span class="s1">(</span><span class="s5">0</span><span class="s3">, </span><span class="s5">0</span><span class="s3">, </span><span class="s5">0</span><span class="s1">)))</span><span class="s3">,</span>
                <span class="s1">arg_descriptors=[RandArg((</span><span class="s5">3</span><span class="s3">, </span><span class="s5">2</span><span class="s1">)</span><span class="s3">, </span><span class="s1">_f32)]</span><span class="s3">,</span>
                <span class="s1">poly_axes=[</span><span class="s5">0</span><span class="s1">])</span><span class="s3">,</span>
    <span class="s1">PolyHarness(</span><span class="s4">&quot;jnp.pad&quot;</span><span class="s3">, </span><span class="s4">&quot;mode=constant&quot;</span><span class="s3">,</span>
                <span class="s3">lambda </span><span class="s1">x: jnp.pad(x</span><span class="s3">, </span><span class="s1">[[x.shape[</span><span class="s5">0</span><span class="s1">]</span><span class="s3">, </span><span class="s5">0</span><span class="s1">]</span><span class="s3">, </span><span class="s1">[x.shape[</span><span class="s5">1</span><span class="s1">]</span><span class="s3">, </span><span class="s5">1</span><span class="s1">]]</span><span class="s3">,</span>
                                  <span class="s1">mode=</span><span class="s4">&quot;constant&quot;</span><span class="s1">)</span><span class="s3">,</span>
                <span class="s1">arg_descriptors=[RandArg((</span><span class="s5">3</span><span class="s3">, </span><span class="s5">5</span><span class="s1">)</span><span class="s3">, </span><span class="s1">_f32)]</span><span class="s3">,</span>
                <span class="s1">poly_axes=[</span><span class="s5">0</span><span class="s1">])</span><span class="s3">,</span>
    <span class="s1">PolyHarness(</span><span class="s4">&quot;jnp.pad&quot;</span><span class="s3">, </span><span class="s4">&quot;mode=constant_bminus1&quot;</span><span class="s3">,</span>
                <span class="s0"># We slice first the unknown dimension to make it of size b - 1</span>
                <span class="s0"># which may be 0.</span>
                <span class="s3">lambda </span><span class="s1">x: jnp.pad(lax.dynamic_slice_in_dim(x</span><span class="s3">, </span><span class="s5">1</span><span class="s3">, </span><span class="s1">x.shape[</span><span class="s5">0</span><span class="s1">] - </span><span class="s5">1</span><span class="s3">,</span>
                                                           <span class="s1">axis=</span><span class="s5">0</span><span class="s1">)</span><span class="s3">,</span>
                                  <span class="s1">[[x.shape[</span><span class="s5">0</span><span class="s1">]</span><span class="s3">, </span><span class="s5">0</span><span class="s1">]</span><span class="s3">, </span><span class="s1">[x.shape[</span><span class="s5">1</span><span class="s1">]</span><span class="s3">, </span><span class="s5">1</span><span class="s1">]]</span><span class="s3">,</span>
                                  <span class="s1">mode=</span><span class="s4">&quot;constant&quot;</span><span class="s1">)</span><span class="s3">,</span>
                <span class="s1">arg_descriptors=[RandArg((</span><span class="s5">3</span><span class="s3">, </span><span class="s5">5</span><span class="s1">)</span><span class="s3">, </span><span class="s1">_f32)]</span><span class="s3">,</span>
                <span class="s1">poly_axes=[</span><span class="s5">0</span><span class="s1">])</span><span class="s3">,</span>
    <span class="s1">PolyHarness(</span><span class="s4">&quot;jnp.pad&quot;</span><span class="s3">, </span><span class="s4">&quot;mode=edge&quot;</span><span class="s3">,</span>
                <span class="s3">lambda </span><span class="s1">x: jnp.pad(x</span><span class="s3">, </span><span class="s1">[[x.shape[</span><span class="s5">0</span><span class="s1">]</span><span class="s3">, </span><span class="s5">0</span><span class="s1">]</span><span class="s3">, </span><span class="s1">[x.shape[</span><span class="s5">1</span><span class="s1">]</span><span class="s3">, </span><span class="s5">1</span><span class="s1">]]</span><span class="s3">,</span>
                                  <span class="s1">mode=</span><span class="s4">&quot;edge&quot;</span><span class="s1">)</span><span class="s3">,</span>
                <span class="s1">arg_descriptors=[RandArg((</span><span class="s5">3</span><span class="s3">, </span><span class="s5">5</span><span class="s1">)</span><span class="s3">, </span><span class="s1">_f32)]</span><span class="s3">,</span>
                <span class="s1">poly_axes=[</span><span class="s5">0</span><span class="s1">])</span><span class="s3">,</span>
    <span class="s1">PolyHarness(</span><span class="s4">&quot;percentile&quot;</span><span class="s3">, </span><span class="s4">&quot;axis=None&quot;</span><span class="s3">,</span>
                <span class="s3">lambda </span><span class="s1">x: jnp.percentile(x</span><span class="s3">, </span><span class="s5">50</span><span class="s3">, </span><span class="s1">axis=</span><span class="s3">None</span><span class="s1">)</span><span class="s3">,</span>
                <span class="s1">arg_descriptors=[RandArg((</span><span class="s5">3</span><span class="s3">, </span><span class="s5">5</span><span class="s1">)</span><span class="s3">, </span><span class="s1">_f32)]</span><span class="s3">,</span>
                <span class="s1">poly_axes=[</span><span class="s5">0</span><span class="s1">])</span><span class="s3">,</span>
    <span class="s1">PolyHarness(</span><span class="s4">&quot;nanquantile&quot;</span><span class="s3">, </span><span class="s4">&quot;axis=None&quot;</span><span class="s3">,</span>
                <span class="s3">lambda </span><span class="s1">x: jnp.nanquantile(x</span><span class="s3">, </span><span class="s5">.5</span><span class="s3">, </span><span class="s1">axis=</span><span class="s3">None</span><span class="s1">)</span><span class="s3">,</span>
                <span class="s1">arg_descriptors=[RandArg((</span><span class="s5">3</span><span class="s3">, </span><span class="s5">5</span><span class="s1">)</span><span class="s3">, </span><span class="s1">_f32)]</span><span class="s3">,</span>
                <span class="s1">poly_axes=[</span><span class="s5">0</span><span class="s1">])</span><span class="s3">,</span>
    <span class="s1">PolyHarness(</span><span class="s4">&quot;percentile&quot;</span><span class="s3">, </span><span class="s4">&quot;axis=0&quot;</span><span class="s3">,</span>
                <span class="s3">lambda </span><span class="s1">x: jnp.percentile(x</span><span class="s3">, </span><span class="s5">50</span><span class="s3">, </span><span class="s1">axis=</span><span class="s5">0</span><span class="s1">)</span><span class="s3">,</span>
                <span class="s1">arg_descriptors=[RandArg((</span><span class="s5">3</span><span class="s3">, </span><span class="s5">5</span><span class="s1">)</span><span class="s3">, </span><span class="s1">_f32)]</span><span class="s3">,</span>
                <span class="s1">poly_axes=[</span><span class="s5">0</span><span class="s1">])</span><span class="s3">,</span>
    <span class="s1">PolyHarness(</span><span class="s4">&quot;nanquantile&quot;</span><span class="s3">, </span><span class="s4">&quot;axis=0&quot;</span><span class="s3">,</span>
                <span class="s3">lambda </span><span class="s1">x: jnp.nanquantile(x</span><span class="s3">, </span><span class="s5">.5</span><span class="s3">, </span><span class="s1">axis=</span><span class="s5">0</span><span class="s1">)</span><span class="s3">,</span>
                <span class="s1">arg_descriptors=[RandArg((</span><span class="s5">3</span><span class="s3">, </span><span class="s5">5</span><span class="s1">)</span><span class="s3">, </span><span class="s1">_f32)]</span><span class="s3">,</span>
                <span class="s1">poly_axes=[</span><span class="s5">0</span><span class="s1">])</span><span class="s3">,</span>
    <span class="s1">PolyHarness(</span><span class="s4">&quot;random_gamma&quot;</span><span class="s3">, </span><span class="s4">&quot;&quot;</span><span class="s3">,</span>
                <span class="s3">lambda </span><span class="s1">key</span><span class="s3">, </span><span class="s1">a: jax.random.gamma(key</span><span class="s3">, </span><span class="s1">a)</span><span class="s3">,</span>
                <span class="s1">arg_descriptors=[RandArg((</span><span class="s5">3</span><span class="s3">, </span><span class="s5">2</span><span class="s1">)</span><span class="s3">, </span><span class="s1">np.uint32)</span><span class="s3">, </span><span class="s1">RandArg((</span><span class="s5">3</span><span class="s3">, </span><span class="s5">3</span><span class="s1">)</span><span class="s3">, </span><span class="s1">_f32)]</span><span class="s3">,</span>
                <span class="s1">poly_axes=[</span><span class="s5">0</span><span class="s3">, </span><span class="s5">0</span><span class="s1">])</span><span class="s3">,</span>
    <span class="s0"># The known dimensions product must be even.</span>
    <span class="s1">PolyHarness(</span><span class="s4">&quot;random_categorical&quot;</span><span class="s3">, </span><span class="s4">&quot;axis=0&quot;</span><span class="s3">,</span>
                <span class="s3">lambda </span><span class="s1">key</span><span class="s3">, </span><span class="s1">a: jax.random.categorical(key</span><span class="s3">, </span><span class="s1">a</span><span class="s3">, </span><span class="s1">axis=</span><span class="s5">0</span><span class="s1">)</span><span class="s3">,</span>
                <span class="s1">arg_descriptors=[RandArg((</span><span class="s5">2</span><span class="s3">,</span><span class="s1">)</span><span class="s3">, </span><span class="s1">np.uint32)</span><span class="s3">, </span><span class="s1">RandArg((</span><span class="s5">3</span><span class="s3">, </span><span class="s5">8</span><span class="s1">)</span><span class="s3">, </span><span class="s1">_f32)]</span><span class="s3">,</span>
                <span class="s1">poly_axes=[</span><span class="s3">None, </span><span class="s5">0</span><span class="s1">])</span><span class="s3">,</span>
    <span class="s1">PolyHarness(</span><span class="s4">&quot;random_categorical&quot;</span><span class="s3">, </span><span class="s4">&quot;axis=1&quot;</span><span class="s3">,</span>
                <span class="s3">lambda </span><span class="s1">key</span><span class="s3">, </span><span class="s1">a: jax.random.categorical(key</span><span class="s3">, </span><span class="s1">a</span><span class="s3">, </span><span class="s1">axis=</span><span class="s5">1</span><span class="s1">)</span><span class="s3">,</span>
                <span class="s1">arg_descriptors=[RandArg((</span><span class="s5">2</span><span class="s3">,</span><span class="s1">)</span><span class="s3">, </span><span class="s1">np.uint32)</span><span class="s3">, </span><span class="s1">RandArg((</span><span class="s5">3</span><span class="s3">, </span><span class="s5">5</span><span class="s3">, </span><span class="s5">8</span><span class="s1">)</span><span class="s3">, </span><span class="s1">_f32)]</span><span class="s3">,</span>
                <span class="s1">poly_axes=[</span><span class="s3">None, </span><span class="s1">(</span><span class="s5">0</span><span class="s3">, </span><span class="s5">1</span><span class="s1">)])</span><span class="s3">,</span>
    <span class="s1">PolyHarness(</span><span class="s4">&quot;random_categorical&quot;</span><span class="s3">, </span><span class="s4">&quot;axis=1_then_reshape&quot;</span><span class="s3">,</span>
                <span class="s3">lambda </span><span class="s1">key</span><span class="s3">, </span><span class="s1">a: jax.random.categorical(key</span><span class="s3">, </span><span class="s1">a</span><span class="s3">, </span><span class="s1">axis=</span><span class="s5">1</span><span class="s1">).reshape((-</span><span class="s5">1</span><span class="s1">))</span><span class="s3">,</span>
                <span class="s1">arg_descriptors=[RandArg((</span><span class="s5">2</span><span class="s3">,</span><span class="s1">)</span><span class="s3">, </span><span class="s1">np.uint32)</span><span class="s3">, </span><span class="s1">RandArg((</span><span class="s5">3</span><span class="s3">, </span><span class="s5">5</span><span class="s3">, </span><span class="s5">8</span><span class="s1">)</span><span class="s3">, </span><span class="s1">_f32)]</span><span class="s3">,</span>
                <span class="s1">poly_axes=[</span><span class="s3">None, </span><span class="s1">(</span><span class="s5">0</span><span class="s3">, </span><span class="s5">1</span><span class="s1">)])</span><span class="s3">,</span>
    <span class="s1">PolyHarness(</span><span class="s4">&quot;random_categorical&quot;</span><span class="s3">, </span><span class="s4">&quot;0_dim&quot;</span><span class="s3">,  </span><span class="s0"># One axis has 0 size</span>
                <span class="s3">lambda </span><span class="s1">key</span><span class="s3">, </span><span class="s1">a: jax.random.categorical(key</span><span class="s3">, </span><span class="s1">a</span><span class="s3">, </span><span class="s1">axis=</span><span class="s5">1</span><span class="s1">)</span><span class="s3">,</span>
                <span class="s1">arg_descriptors=[RandArg((</span><span class="s5">2</span><span class="s3">,</span><span class="s1">)</span><span class="s3">, </span><span class="s1">np.uint32)</span><span class="s3">, </span><span class="s1">RandArg((</span><span class="s5">3</span><span class="s3">, </span><span class="s5">5</span><span class="s3">, </span><span class="s5">0</span><span class="s1">)</span><span class="s3">, </span><span class="s1">_f32)]</span><span class="s3">,</span>
                <span class="s1">poly_axes=[</span><span class="s3">None, </span><span class="s1">(</span><span class="s5">0</span><span class="s3">, </span><span class="s5">1</span><span class="s1">)])</span><span class="s3">,</span>
    <span class="s0"># Works when the known dimensions are known to be even or odd.</span>
    <span class="s1">PolyHarness(</span><span class="s4">&quot;random_uniform&quot;</span><span class="s3">, </span><span class="s4">&quot;even_1&quot;</span><span class="s3">,</span>
                <span class="s3">lambda </span><span class="s1">key</span><span class="s3">, </span><span class="s1">a: jax.random.uniform(key</span><span class="s3">, </span><span class="s1">a.shape</span><span class="s3">, </span><span class="s1">dtype=_f32) + a</span><span class="s3">,</span>
                <span class="s1">arg_descriptors=[RandArg((</span><span class="s5">2</span><span class="s3">,</span><span class="s1">)</span><span class="s3">, </span><span class="s1">np.uint32)</span><span class="s3">, </span><span class="s1">RandArg((</span><span class="s5">3</span><span class="s3">, </span><span class="s5">4</span><span class="s1">)</span><span class="s3">, </span><span class="s1">_f32)]</span><span class="s3">,</span>
                <span class="s1">poly_axes=[</span><span class="s3">None, </span><span class="s5">0</span><span class="s1">])</span><span class="s3">,</span>
    <span class="s1">PolyHarness(</span><span class="s4">&quot;random_uniform&quot;</span><span class="s3">, </span><span class="s4">&quot;even_2&quot;</span><span class="s3">,</span>
                <span class="s3">lambda </span><span class="s1">key</span><span class="s3">, </span><span class="s1">a: jax.random.uniform(key</span><span class="s3">, </span><span class="s1">(</span><span class="s5">2 </span><span class="s1">* a.shape[</span><span class="s5">0</span><span class="s1">]</span><span class="s3">, </span><span class="s1">a.shape[</span><span class="s5">1</span><span class="s1">])</span><span class="s3">,</span>
                                                  <span class="s1">dtype=_f32) + jnp.concatenate([a</span><span class="s3">, </span><span class="s1">a]</span><span class="s3">, </span><span class="s1">axis=</span><span class="s5">0</span><span class="s1">)</span><span class="s3">,</span>
                <span class="s1">arg_descriptors=[RandArg((</span><span class="s5">2</span><span class="s3">,</span><span class="s1">)</span><span class="s3">, </span><span class="s1">np.uint32)</span><span class="s3">, </span><span class="s1">RandArg((</span><span class="s5">3</span><span class="s3">, </span><span class="s5">1</span><span class="s1">)</span><span class="s3">, </span><span class="s1">_f32)]</span><span class="s3">,</span>
                <span class="s1">poly_axes=[</span><span class="s3">None, </span><span class="s5">0</span><span class="s1">])</span><span class="s3">,</span>
    <span class="s1">PolyHarness(</span><span class="s4">&quot;random_uniform&quot;</span><span class="s3">, </span><span class="s4">&quot;error_not_even&quot;</span><span class="s3">,</span>
                <span class="s3">lambda </span><span class="s1">key</span><span class="s3">, </span><span class="s1">a: jax.random.uniform(key</span><span class="s3">, </span><span class="s1">a.shape</span><span class="s3">, </span><span class="s1">dtype=_f32)</span><span class="s3">,</span>
                <span class="s1">arg_descriptors=[RandArg((</span><span class="s5">2</span><span class="s3">,</span><span class="s1">)</span><span class="s3">, </span><span class="s1">np.uint32)</span><span class="s3">, </span><span class="s1">RandArg((</span><span class="s5">3</span><span class="s3">, </span><span class="s5">5</span><span class="s1">)</span><span class="s3">, </span><span class="s1">_f32)]</span><span class="s3">,</span>
                <span class="s1">poly_axes=[</span><span class="s3">None, </span><span class="s5">0</span><span class="s1">]</span><span class="s3">,</span>
                <span class="s1">expect_error=(core.InconclusiveDimensionOperation</span><span class="s3">,</span>
                              <span class="s4">&quot;the product of the known dimensions must be even&quot;</span><span class="s1">))</span><span class="s3">,</span>
    <span class="s1">PolyHarness(</span><span class="s4">&quot;reduce_window&quot;</span><span class="s3">, </span><span class="s4">&quot;min&quot;</span><span class="s3">,</span>
                <span class="s0"># x.shape = (b, 8)</span>
                <span class="s3">lambda </span><span class="s1">x: lax.reduce_window(x</span><span class="s3">, </span><span class="s1">np.array(</span><span class="s5">1.</span><span class="s3">, </span><span class="s1">_f32)</span><span class="s3">, </span><span class="s1">lax.min</span><span class="s3">,</span>
                                            <span class="s1">(</span><span class="s5">2</span><span class="s3">, </span><span class="s5">2</span><span class="s1">)</span><span class="s3">, </span><span class="s1">(</span><span class="s5">1</span><span class="s3">, </span><span class="s5">1</span><span class="s1">)</span><span class="s3">, </span><span class="s4">&quot;VALID&quot;</span><span class="s1">)</span><span class="s3">,</span>
                <span class="s1">arg_descriptors=[RandArg((</span><span class="s5">3</span><span class="s3">, </span><span class="s5">8</span><span class="s1">)</span><span class="s3">, </span><span class="s1">_f32)]</span><span class="s3">,</span>
                <span class="s1">poly_axes=[</span><span class="s5">0</span><span class="s1">]).both_enable_and_disable_xla()</span><span class="s3">,</span>
    <span class="s1">PolyHarness(</span><span class="s4">&quot;reduce_window&quot;</span><span class="s3">, </span><span class="s4">&quot;add_0&quot;</span><span class="s3">,</span>
                <span class="s0"># x.shape = (b, 8)</span>
                <span class="s3">lambda </span><span class="s1">x: lax.reduce_window(x</span><span class="s3">, </span><span class="s5">0</span><span class="s3">, </span><span class="s1">lax.add</span><span class="s3">, </span><span class="s1">(</span><span class="s5">2</span><span class="s3">, </span><span class="s5">2</span><span class="s1">)</span><span class="s3">, </span><span class="s1">(</span><span class="s5">1</span><span class="s3">, </span><span class="s5">1</span><span class="s1">)</span><span class="s3">,</span>
                                            <span class="s4">&quot;VALID&quot;</span><span class="s1">)</span><span class="s3">,</span>
                <span class="s1">arg_descriptors=[RandArg((</span><span class="s5">3</span><span class="s3">, </span><span class="s5">8</span><span class="s1">)</span><span class="s3">, </span><span class="s1">_f32)]</span><span class="s3">,</span>
                <span class="s1">poly_axes=[</span><span class="s5">0</span><span class="s1">]).both_enable_and_disable_xla()</span><span class="s3">,</span>
    <span class="s0"># https://github.com/google/jax/issues/11804</span>
    <span class="s0"># Use the reshape trick to simulate a polymorphic dimension of 16*b.</span>
    <span class="s0"># (See test &quot;conv_general_dilated.1d_1&quot; above for more details.)</span>
    <span class="s1">PolyHarness(</span><span class="s4">&quot;reduce_window&quot;</span><span class="s3">, </span><span class="s4">&quot;add_1&quot;</span><span class="s3">,</span>
                <span class="s0"># x.shape = (1, 16*b, 1)</span>
                <span class="s3">lambda </span><span class="s1">x: lax.reduce_window(</span>
                    <span class="s1">jnp.reshape(x</span><span class="s3">, </span><span class="s1">(</span><span class="s5">1</span><span class="s3">, </span><span class="s1">-</span><span class="s5">1</span><span class="s3">, </span><span class="s5">1</span><span class="s1">))</span><span class="s3">,</span>
                    <span class="s5">0.</span><span class="s3">, </span><span class="s1">lax.add</span><span class="s3">, </span><span class="s1">(</span><span class="s5">1</span><span class="s3">, </span><span class="s5">4</span><span class="s3">, </span><span class="s5">1</span><span class="s1">)</span><span class="s3">, </span><span class="s1">(</span><span class="s5">1</span><span class="s3">, </span><span class="s5">2</span><span class="s3">, </span><span class="s5">1</span><span class="s1">)</span><span class="s3">, </span><span class="s4">&quot;SAME&quot;</span><span class="s1">)</span><span class="s3">,</span>
                <span class="s1">arg_descriptors=[RandArg((</span><span class="s5">1</span><span class="s3">, </span><span class="s5">128</span><span class="s3">, </span><span class="s5">16</span><span class="s1">)</span><span class="s3">, </span><span class="s1">_f32)]</span><span class="s3">,</span>
                <span class="s1">poly_axes=[</span><span class="s5">1</span><span class="s1">]).both_enable_and_disable_xla()</span><span class="s3">,</span>
    <span class="s0"># TODO(necula): not yet supported, but also unlikely to come up.</span>
    <span class="s0"># PolyHarness(&quot;random_uniform&quot;, &quot;odd&quot;,</span>
    <span class="s0">#               lambda key, a: jax.random.uniform(key, (2 * a.shape[0] + 1, a.shape[1]),</span>
    <span class="s0">#                                                 dtype=_f32),</span>
    <span class="s0">#               [RandArg((2,), np.uint32), RandArg((3, 5), _f32)],</span>
    <span class="s0">#               poly_axes=[None, 0]),</span>
    <span class="s1">[</span>
        <span class="s1">PolyHarness(</span><span class="s4">&quot;reduce&quot;</span><span class="s3">, </span><span class="s1">reduce_op.__name__</span><span class="s3">,</span>
                    <span class="s3">lambda </span><span class="s1">x: reduce_op(x</span><span class="s3">, </span><span class="s1">axis=-</span><span class="s5">1</span><span class="s3">, </span><span class="s1">keepdims=</span><span class="s3">True</span><span class="s1">)</span><span class="s3">,  </span><span class="s0"># type: ignore</span>
                    <span class="s1">arg_descriptors=[RandArg((</span><span class="s5">3</span><span class="s3">, </span><span class="s5">5</span><span class="s1">)</span><span class="s3">, </span><span class="s1">_f32)]</span><span class="s3">,</span>
                    <span class="s1">poly_axes=[</span><span class="s5">0</span><span class="s1">])</span>
        <span class="s3">for </span><span class="s1">reduce_op </span><span class="s3">in </span><span class="s1">[jnp.all</span><span class="s3">, </span><span class="s1">jnp.any</span><span class="s3">, </span><span class="s1">jnp.max</span><span class="s3">, </span><span class="s1">jnp.min</span><span class="s3">, </span><span class="s1">jnp.prod</span><span class="s3">, </span><span class="s1">jnp.sum]</span>
    <span class="s1">]</span><span class="s3">,</span>
    <span class="s0"># Repeat f32[b, 2] * 3</span>
    <span class="s1">PolyHarness(</span><span class="s4">&quot;repeat&quot;</span><span class="s3">, </span><span class="s4">&quot;repeats=int_axis=0&quot;</span><span class="s3">,</span>
                <span class="s3">lambda </span><span class="s1">x: jnp.repeat(x</span><span class="s3">, </span><span class="s1">repeats=</span><span class="s5">3</span><span class="s3">, </span><span class="s1">axis=</span><span class="s5">0</span><span class="s1">)</span><span class="s3">,</span>
                <span class="s1">arg_descriptors=[RandArg((</span><span class="s5">3</span><span class="s3">, </span><span class="s5">2</span><span class="s1">)</span><span class="s3">, </span><span class="s1">_f32)]</span><span class="s3">,</span>
                <span class="s1">poly_axes=[</span><span class="s5">0</span><span class="s1">])</span><span class="s3">,</span>
    <span class="s0"># Repeat f32[b, 2] * b</span>
    <span class="s1">PolyHarness(</span><span class="s4">&quot;repeat&quot;</span><span class="s3">, </span><span class="s4">&quot;repeats=poly_axis=0&quot;</span><span class="s3">,</span>
                <span class="s3">lambda </span><span class="s1">x: jnp.repeat(x</span><span class="s3">, </span><span class="s1">repeats=x.shape[</span><span class="s5">0</span><span class="s1">]</span><span class="s3">, </span><span class="s1">axis=</span><span class="s5">0</span><span class="s1">)</span><span class="s3">,</span>
                <span class="s1">arg_descriptors=[RandArg((</span><span class="s5">3</span><span class="s3">, </span><span class="s5">2</span><span class="s1">)</span><span class="s3">, </span><span class="s1">_f32)]</span><span class="s3">,</span>
                <span class="s1">poly_axes=[</span><span class="s5">0</span><span class="s1">])</span><span class="s3">,</span>
    <span class="s0"># Repeat f32[b, 2] * b</span>
    <span class="s1">PolyHarness(</span><span class="s4">&quot;repeat&quot;</span><span class="s3">, </span><span class="s4">&quot;repeats=poly_axis=None&quot;</span><span class="s3">,</span>
                <span class="s3">lambda </span><span class="s1">x: jnp.repeat(x</span><span class="s3">, </span><span class="s1">repeats=x.shape[</span><span class="s5">0</span><span class="s1">]</span><span class="s3">, </span><span class="s1">axis=</span><span class="s3">None</span><span class="s1">)</span><span class="s3">,</span>
                <span class="s1">arg_descriptors=[RandArg((</span><span class="s5">3</span><span class="s3">, </span><span class="s5">2</span><span class="s1">)</span><span class="s3">, </span><span class="s1">_f32)]</span><span class="s3">,</span>
                <span class="s1">poly_axes=[</span><span class="s5">0</span><span class="s1">])</span><span class="s3">,</span>
    <span class="s0"># Repeat f32 * b</span>
    <span class="s1">PolyHarness(</span><span class="s4">&quot;repeat&quot;</span><span class="s3">, </span><span class="s4">&quot;repeats=poly_axis=None_scalar&quot;</span><span class="s3">,</span>
                <span class="s3">lambda </span><span class="s1">x</span><span class="s3">, </span><span class="s1">y: jnp.repeat(x</span><span class="s3">, </span><span class="s1">repeats=y.shape[</span><span class="s5">0</span><span class="s1">]</span><span class="s3">, </span><span class="s1">axis=</span><span class="s3">None</span><span class="s1">) + y</span><span class="s3">,</span>
                <span class="s1">arg_descriptors=[RandArg(()</span><span class="s3">, </span><span class="s1">_f32)</span><span class="s3">, </span><span class="s1">RandArg((</span><span class="s5">3</span><span class="s3">, </span><span class="s5">1</span><span class="s1">)</span><span class="s3">, </span><span class="s1">_f32)]</span><span class="s3">,</span>
                <span class="s1">poly_axes=[</span><span class="s3">None, </span><span class="s5">0</span><span class="s1">])</span><span class="s3">,</span>
    <span class="s1">PolyHarness(</span><span class="s4">&quot;repeat&quot;</span><span class="s3">, </span><span class="s4">&quot;repeats=poly_axis=None_total_repeat_length1&quot;</span><span class="s3">,</span>
                <span class="s3">lambda </span><span class="s1">x: jnp.repeat(x</span><span class="s3">, </span><span class="s1">repeats=x.shape[</span><span class="s5">0</span><span class="s1">]</span><span class="s3">, </span><span class="s1">axis=</span><span class="s3">None, </span><span class="s1">total_repeat_length=</span><span class="s5">8</span><span class="s1">)</span><span class="s3">,</span>
                <span class="s1">arg_descriptors=[RandArg((</span><span class="s5">3</span><span class="s3">, </span><span class="s5">2</span><span class="s1">)</span><span class="s3">, </span><span class="s1">_f32)]</span><span class="s3">,</span>
                <span class="s1">poly_axes=[</span><span class="s5">0</span><span class="s1">]</span><span class="s3">,</span>
                <span class="s1">expect_error=(ValueError</span><span class="s3">, </span><span class="s4">&quot;jnp.repeat with a non-constant `repeats` is supported only .*&quot;</span><span class="s1">))</span><span class="s3">,</span>
    <span class="s1">PolyHarness(</span><span class="s4">&quot;reshape&quot;</span><span class="s3">, </span><span class="s4">&quot;0&quot;</span><span class="s3">,</span>
                <span class="s3">lambda </span><span class="s1">x: x.reshape([x.shape[</span><span class="s5">0</span><span class="s1">]</span><span class="s3">, </span><span class="s1">-</span><span class="s5">1</span><span class="s1">])</span><span class="s3">,</span>
                <span class="s1">arg_descriptors=[RandArg((</span><span class="s5">3</span><span class="s3">, </span><span class="s5">2</span><span class="s3">, </span><span class="s5">3</span><span class="s1">)</span><span class="s3">, </span><span class="s1">_f32)]</span><span class="s3">,</span>
                <span class="s1">poly_axes=[</span><span class="s5">0</span><span class="s1">])</span><span class="s3">,</span>
    <span class="s1">PolyHarness(</span><span class="s4">&quot;reshape&quot;</span><span class="s3">, </span><span class="s4">&quot;1&quot;</span><span class="s3">,</span>
                <span class="s3">lambda </span><span class="s1">x: x.reshape([x.shape[</span><span class="s5">0</span><span class="s1">]</span><span class="s3">, </span><span class="s1">-</span><span class="s5">1</span><span class="s1">])</span><span class="s3">,</span>
                <span class="s1">arg_descriptors=[RandArg((</span><span class="s5">3</span><span class="s3">, </span><span class="s5">2</span><span class="s3">, </span><span class="s5">3</span><span class="s1">)</span><span class="s3">, </span><span class="s1">_f32)]</span><span class="s3">,</span>
                <span class="s1">poly_axes=[(</span><span class="s5">0</span><span class="s3">, </span><span class="s5">1</span><span class="s1">)])</span><span class="s3">,</span>
    <span class="s1">PolyHarness(</span><span class="s4">&quot;reshape&quot;</span><span class="s3">, </span><span class="s4">&quot;2&quot;</span><span class="s3">,</span>
                <span class="s3">lambda </span><span class="s1">x: x.reshape([x.shape[</span><span class="s5">0</span><span class="s1">]</span><span class="s3">, </span><span class="s1">-</span><span class="s5">1</span><span class="s3">, </span><span class="s1">x.shape[</span><span class="s5">3</span><span class="s1">]</span><span class="s3">, </span><span class="s1">x.shape[</span><span class="s5">2</span><span class="s1">]])</span><span class="s3">,</span>
                <span class="s1">arg_descriptors=[RandArg((</span><span class="s5">3</span><span class="s3">, </span><span class="s5">4</span><span class="s3">, </span><span class="s5">5</span><span class="s3">, </span><span class="s5">6</span><span class="s3">, </span><span class="s5">7</span><span class="s1">)</span><span class="s3">, </span><span class="s1">_f32)]</span><span class="s3">,</span>
                <span class="s1">poly_axes=[(</span><span class="s5">0</span><span class="s3">, </span><span class="s5">2</span><span class="s3">, </span><span class="s5">3</span><span class="s1">)])</span><span class="s3">,</span>
    <span class="s1">PolyHarness(</span><span class="s4">&quot;reshape&quot;</span><span class="s3">, </span><span class="s4">&quot;3&quot;</span><span class="s3">,</span>
                <span class="s3">lambda </span><span class="s1">x: jnp.reshape(x</span><span class="s3">, </span><span class="s1">[</span><span class="s5">2</span><span class="s3">, </span><span class="s1">-</span><span class="s5">1</span><span class="s1">])</span><span class="s3">,</span>
                <span class="s1">arg_descriptors=[RandArg((</span><span class="s5">3</span><span class="s3">, </span><span class="s5">4</span><span class="s3">, </span><span class="s5">5</span><span class="s3">, </span><span class="s5">6</span><span class="s3">, </span><span class="s5">7</span><span class="s1">)</span><span class="s3">, </span><span class="s1">_f32)]</span><span class="s3">,</span>
                <span class="s1">poly_axes=[(</span><span class="s5">0</span><span class="s3">, </span><span class="s5">2</span><span class="s1">)])</span><span class="s3">,</span>
    <span class="s1">PolyHarness(</span><span class="s4">&quot;reshape&quot;</span><span class="s3">, </span><span class="s4">&quot;_issue_9975&quot;</span><span class="s3">,</span>
                <span class="s0"># The newshape is a scalar</span>
                <span class="s3">lambda </span><span class="s1">x: jnp.reshape(x</span><span class="s3">, </span><span class="s1">x.shape[</span><span class="s5">0</span><span class="s1">] * x.shape[</span><span class="s5">1</span><span class="s1">])</span><span class="s3">,</span>
                <span class="s1">arg_descriptors=[RandArg((</span><span class="s5">3</span><span class="s3">, </span><span class="s5">4</span><span class="s1">)</span><span class="s3">, </span><span class="s1">_f32)]</span><span class="s3">,</span>
                <span class="s1">poly_axes=[</span><span class="s5">0</span><span class="s1">])</span><span class="s3">,</span>
    <span class="s1">PolyHarness(</span><span class="s4">&quot;reshape&quot;</span><span class="s3">, </span><span class="s4">&quot;error&quot;</span><span class="s3">,</span>
                <span class="s3">lambda </span><span class="s1">x: x.reshape([x.shape[</span><span class="s5">0</span><span class="s1">]</span><span class="s3">, </span><span class="s1">-</span><span class="s5">1</span><span class="s3">, </span><span class="s5">3</span><span class="s1">])</span><span class="s3">,</span>
                <span class="s1">arg_descriptors=[RandArg((</span><span class="s5">3</span><span class="s3">, </span><span class="s5">2</span><span class="s3">, </span><span class="s5">4</span><span class="s1">)</span><span class="s3">, </span><span class="s1">_f32)]</span><span class="s3">,</span>
                <span class="s1">poly_axes=[</span><span class="s5">0</span><span class="s1">]</span><span class="s3">,</span>
                <span class="s1">skip_jax_run=</span><span class="s3">True,</span>
                <span class="s1">expect_error=(core.InconclusiveDimensionOperation</span><span class="s3">,</span>
                              <span class="s1">re.escape(</span>
                                <span class="s4">&quot;Cannot divide evenly the sizes of shapes (b0, 2, 4) and (b0, -1, 3)&quot;</span><span class="s1">)))</span><span class="s3">,</span>
    <span class="s1">PolyHarness(</span><span class="s4">&quot;roll&quot;</span><span class="s3">, </span><span class="s4">&quot;axis=0&quot;</span><span class="s3">,</span>
                <span class="s3">lambda </span><span class="s1">x: jnp.roll(x</span><span class="s3">, </span><span class="s5">2</span><span class="s3">, </span><span class="s1">axis=</span><span class="s5">0</span><span class="s1">)</span><span class="s3">,</span>
                <span class="s1">arg_descriptors=[RandArg((</span><span class="s5">3</span><span class="s3">, </span><span class="s5">4</span><span class="s1">)</span><span class="s3">, </span><span class="s1">_f32)]</span><span class="s3">,</span>
                <span class="s1">poly_axes=[</span><span class="s5">0</span><span class="s1">])</span><span class="s3">,</span>
    <span class="s1">PolyHarness(</span><span class="s4">&quot;roll&quot;</span><span class="s3">, </span><span class="s4">&quot;axis=None&quot;</span><span class="s3">,</span>
                <span class="s3">lambda </span><span class="s1">x: jnp.roll(x</span><span class="s3">, </span><span class="s5">2</span><span class="s1">)</span><span class="s3">,</span>
                <span class="s1">arg_descriptors=[RandArg((</span><span class="s5">3</span><span class="s3">, </span><span class="s5">4</span><span class="s1">)</span><span class="s3">, </span><span class="s1">_f32)]</span><span class="s3">,</span>
                <span class="s1">poly_axes=[</span><span class="s5">0</span><span class="s1">])</span><span class="s3">,</span>
    <span class="s1">PolyHarness(</span><span class="s4">&quot;scatter_add&quot;</span><span class="s3">, </span><span class="s4">&quot;&quot;</span><span class="s3">,</span>
                <span class="s1">partial(lax.scatter_add</span><span class="s3">, </span><span class="s1">indices_are_sorted=</span><span class="s3">False, </span><span class="s1">unique_indices=</span><span class="s3">True</span><span class="s1">)</span><span class="s3">,</span>
                <span class="s1">arg_descriptors=[RandArg((</span><span class="s5">7</span><span class="s3">, </span><span class="s5">4</span><span class="s1">)</span><span class="s3">, </span><span class="s1">_f32)</span><span class="s3">,</span>
                                 <span class="s1">np.array([[</span><span class="s5">1</span><span class="s1">]</span><span class="s3">, </span><span class="s1">[</span><span class="s5">2</span><span class="s1">]]</span><span class="s3">, </span><span class="s1">np.int32)</span><span class="s3">,  </span><span class="s0"># indices: [2, 1]</span>
                                 <span class="s1">RandArg((</span><span class="s5">7</span><span class="s3">, </span><span class="s5">2</span><span class="s1">)</span><span class="s3">, </span><span class="s1">_f32)</span><span class="s3">,  </span><span class="s0"># updates: [7, 2]</span>
                                 <span class="s1">StaticArg(lax.ScatterDimensionNumbers((</span><span class="s5">0</span><span class="s3">,</span><span class="s1">)</span><span class="s3">, </span><span class="s1">(</span><span class="s5">1</span><span class="s3">,</span><span class="s1">)</span><span class="s3">, </span><span class="s1">(</span><span class="s5">1</span><span class="s3">,</span><span class="s1">)))]</span><span class="s3">,</span>
                <span class="s1">poly_axes=[</span><span class="s5">0</span><span class="s3">, None, </span><span class="s5">0</span><span class="s1">])</span><span class="s3">,</span>
    <span class="s1">PolyHarness(</span><span class="s4">&quot;scatter_add&quot;</span><span class="s3">, </span><span class="s4">&quot;clip0&quot;</span><span class="s3">,</span>
                <span class="s1">partial(lax.scatter_add</span><span class="s3">, </span><span class="s1">indices_are_sorted=</span><span class="s3">False, </span><span class="s1">unique_indices=</span><span class="s3">True, </span><span class="s1">mode=lax.GatherScatterMode.CLIP)</span><span class="s3">,</span>
                <span class="s1">arg_descriptors=[RandArg((</span><span class="s5">7</span><span class="s3">, </span><span class="s5">4</span><span class="s1">)</span><span class="s3">, </span><span class="s1">_f32)</span><span class="s3">,  </span><span class="s0"># [b, 4]</span>
                                 <span class="s1">np.array([[</span><span class="s5">1</span><span class="s1">]</span><span class="s3">, </span><span class="s1">[</span><span class="s5">2</span><span class="s1">]]</span><span class="s3">, </span><span class="s1">np.int32)</span><span class="s3">,  </span><span class="s0"># indices: [2, 1]</span>
                                 <span class="s1">RandArg((</span><span class="s5">7</span><span class="s3">, </span><span class="s5">2</span><span class="s1">)</span><span class="s3">, </span><span class="s1">_f32)</span><span class="s3">,  </span><span class="s0"># updates: [b, 2]</span>
                                 <span class="s1">StaticArg(lax.ScatterDimensionNumbers((</span><span class="s5">0</span><span class="s3">,</span><span class="s1">)</span><span class="s3">, </span><span class="s1">(</span><span class="s5">1</span><span class="s3">,</span><span class="s1">)</span><span class="s3">, </span><span class="s1">(</span><span class="s5">1</span><span class="s3">,</span><span class="s1">)))]</span><span class="s3">,</span>
                <span class="s1">poly_axes=[</span><span class="s5">0</span><span class="s3">, None, </span><span class="s5">0</span><span class="s1">])</span><span class="s3">,</span>
    <span class="s1">PolyHarness(</span><span class="s4">&quot;scatter_add&quot;</span><span class="s3">, </span><span class="s4">&quot;clip1&quot;</span><span class="s3">,</span>
                <span class="s1">partial(lax.scatter_add</span><span class="s3">, </span><span class="s1">indices_are_sorted=</span><span class="s3">False, </span><span class="s1">unique_indices=</span><span class="s3">True, </span><span class="s1">mode=lax.GatherScatterMode.CLIP)</span><span class="s3">,</span>
                <span class="s1">arg_descriptors=[RandArg((</span><span class="s5">7</span><span class="s3">, </span><span class="s5">4</span><span class="s1">)</span><span class="s3">, </span><span class="s1">_f32)</span><span class="s3">,  </span><span class="s0"># [b, 4]</span>
                                 <span class="s1">np.array([[</span><span class="s5">1</span><span class="s3">, </span><span class="s5">2</span><span class="s1">]</span><span class="s3">, </span><span class="s1">[-</span><span class="s5">2</span><span class="s3">, </span><span class="s5">0</span><span class="s1">]</span><span class="s3">, </span><span class="s1">[</span><span class="s5">6</span><span class="s3">, </span><span class="s5">4</span><span class="s1">]</span><span class="s3">, </span><span class="s1">[</span><span class="s5">7</span><span class="s3">, </span><span class="s1">-</span><span class="s5">1</span><span class="s1">]</span><span class="s3">, </span><span class="s1">[</span><span class="s5">1</span><span class="s3">, </span><span class="s5">0</span><span class="s1">]</span><span class="s3">, </span><span class="s1">[</span><span class="s5">3</span><span class="s3">, </span><span class="s5">0</span><span class="s1">]</span><span class="s3">, </span><span class="s1">[</span><span class="s5">0</span><span class="s3">, </span><span class="s5">5</span><span class="s1">]]</span><span class="s3">, </span><span class="s1">np.int32)</span><span class="s3">,  </span><span class="s0"># indices: [b, 2]</span>
                                 <span class="s1">RandArg((</span><span class="s5">7</span><span class="s3">, </span><span class="s5">1</span><span class="s1">)</span><span class="s3">, </span><span class="s1">_f32)</span><span class="s3">,  </span><span class="s0"># updates: [b, 1]</span>
                                 <span class="s1">StaticArg(lax.ScatterDimensionNumbers((</span><span class="s5">1</span><span class="s3">,</span><span class="s1">)</span><span class="s3">, </span><span class="s1">(</span><span class="s5">0</span><span class="s3">,</span><span class="s1">)</span><span class="s3">, </span><span class="s1">(</span><span class="s5">0</span><span class="s3">, </span><span class="s5">1</span><span class="s3">,</span><span class="s1">)))]</span><span class="s3">,</span>
                <span class="s1">poly_axes=[</span><span class="s5">0</span><span class="s3">, </span><span class="s5">0</span><span class="s3">, </span><span class="s5">0</span><span class="s1">])</span><span class="s3">,</span>
    <span class="s1">PolyHarness(</span><span class="s4">&quot;select&quot;</span><span class="s3">, </span><span class="s4">&quot;0&quot;</span><span class="s3">,</span>
                <span class="s0"># x.shape = (b, 3)</span>
                <span class="s3">lambda </span><span class="s1">x: lax.select(x &gt; </span><span class="s5">5.</span><span class="s3">, </span><span class="s1">x</span><span class="s3">, </span><span class="s1">x)</span><span class="s3">,</span>
                <span class="s1">arg_descriptors=[RandArg((</span><span class="s5">7</span><span class="s3">, </span><span class="s5">3</span><span class="s1">)</span><span class="s3">, </span><span class="s1">_f32)]</span><span class="s3">,</span>
                <span class="s1">poly_axes=[</span><span class="s5">0</span><span class="s1">])</span><span class="s3">,</span>
    <span class="s1">PolyHarness(</span><span class="s4">&quot;select&quot;</span><span class="s3">, </span><span class="s4">&quot;1&quot;</span><span class="s3">,</span>
                <span class="s0"># x.shape = (b, 3); y.shape = (3,)</span>
                <span class="s1">jax.vmap(</span><span class="s3">lambda </span><span class="s1">x</span><span class="s3">, </span><span class="s1">y: lax.select(x &gt; </span><span class="s5">5.</span><span class="s3">, </span><span class="s1">x</span><span class="s3">, </span><span class="s1">y)</span><span class="s3">, </span><span class="s1">in_axes=[</span><span class="s5">0</span><span class="s3">, None</span><span class="s1">])</span><span class="s3">,</span>
                <span class="s1">arg_descriptors=[RandArg((</span><span class="s5">7</span><span class="s3">, </span><span class="s5">3</span><span class="s1">)</span><span class="s3">, </span><span class="s1">_f32)</span><span class="s3">, </span><span class="s1">RandArg((</span><span class="s5">3</span><span class="s3">,</span><span class="s1">)</span><span class="s3">, </span><span class="s1">_f32)]</span><span class="s3">,</span>
                <span class="s1">poly_axes=[</span><span class="s5">0</span><span class="s3">, None</span><span class="s1">])</span><span class="s3">,</span>
    <span class="s1">PolyHarness(</span><span class="s4">&quot;slice&quot;</span><span class="s3">, </span><span class="s4">&quot;entire_axis&quot;</span><span class="s3">,</span>
                <span class="s3">lambda </span><span class="s1">x: lax.slice(x</span><span class="s3">, </span><span class="s1">start_indices=(</span><span class="s5">0</span><span class="s3">, </span><span class="s5">1</span><span class="s1">)</span><span class="s3">, </span><span class="s1">limit_indices=(x.shape[</span><span class="s5">0</span><span class="s1">]</span><span class="s3">, </span><span class="s5">3</span><span class="s1">))</span><span class="s3">,</span>
                <span class="s1">arg_descriptors=[RandArg((</span><span class="s5">7</span><span class="s3">, </span><span class="s5">3</span><span class="s1">)</span><span class="s3">, </span><span class="s1">_f32)]</span><span class="s3">,</span>
                <span class="s1">poly_axes=[</span><span class="s5">0</span><span class="s1">])</span><span class="s3">,</span>
    <span class="s1">PolyHarness(</span><span class="s4">&quot;slice_in_dim&quot;</span><span class="s3">, </span><span class="s4">&quot;entire_axis&quot;</span><span class="s3">,</span>
                <span class="s3">lambda </span><span class="s1">x: lax.slice_in_dim(x</span><span class="s3">, </span><span class="s5">0</span><span class="s3">, </span><span class="s1">x.shape[</span><span class="s5">0</span><span class="s1">]</span><span class="s3">, </span><span class="s1">stride=</span><span class="s5">1</span><span class="s3">, </span><span class="s1">axis=</span><span class="s5">0</span><span class="s1">)</span><span class="s3">,</span>
                <span class="s1">arg_descriptors=[RandArg((</span><span class="s5">3</span><span class="s3">, </span><span class="s5">4</span><span class="s1">)</span><span class="s3">, </span><span class="s1">_f32)]</span><span class="s3">,</span>
                <span class="s1">poly_axes=[</span><span class="s5">0</span><span class="s1">])</span><span class="s3">,</span>
    <span class="s1">PolyHarness(</span><span class="s4">&quot;slice_in_dim&quot;</span><span class="s3">, </span><span class="s4">&quot;start=neg&quot;</span><span class="s3">,</span>
                <span class="s3">lambda </span><span class="s1">x: lax.slice_in_dim(x</span><span class="s3">, </span><span class="s1">-</span><span class="s5">1</span><span class="s3">, </span><span class="s1">x.shape[</span><span class="s5">0</span><span class="s1">]</span><span class="s3">, </span><span class="s1">stride=</span><span class="s5">1</span><span class="s3">, </span><span class="s1">axis=</span><span class="s5">0</span><span class="s1">)</span><span class="s3">,</span>
                <span class="s1">arg_descriptors=[RandArg((</span><span class="s5">3</span><span class="s3">, </span><span class="s5">4</span><span class="s1">)</span><span class="s3">, </span><span class="s1">_f32)]</span><span class="s3">,</span>
                <span class="s1">poly_axes=[</span><span class="s5">0</span><span class="s1">])</span><span class="s3">,</span>
    <span class="s1">PolyHarness(</span><span class="s4">&quot;slice_in_dim&quot;</span><span class="s3">, </span><span class="s4">&quot;limit=neg&quot;</span><span class="s3">,</span>
                <span class="s3">lambda </span><span class="s1">x: lax.slice_in_dim(x</span><span class="s3">, </span><span class="s5">0</span><span class="s3">, </span><span class="s1">-</span><span class="s5">1</span><span class="s3">, </span><span class="s1">stride=</span><span class="s5">1</span><span class="s3">, </span><span class="s1">axis=</span><span class="s5">0</span><span class="s1">)</span><span class="s3">,</span>
                <span class="s1">arg_descriptors=[RandArg((</span><span class="s5">3</span><span class="s3">, </span><span class="s5">4</span><span class="s1">)</span><span class="s3">, </span><span class="s1">_f32)]</span><span class="s3">,</span>
                <span class="s1">poly_axes=[</span><span class="s5">0</span><span class="s1">])</span><span class="s3">,</span>
    <span class="s1">PolyHarness(</span><span class="s4">&quot;slice_in_dim&quot;</span><span class="s3">, </span><span class="s4">&quot;stride=2_even&quot;</span><span class="s3">,</span>
                <span class="s3">lambda </span><span class="s1">x: lax.slice_in_dim(x</span><span class="s3">, </span><span class="s5">0</span><span class="s3">, </span><span class="s1">x.shape[</span><span class="s5">0</span><span class="s1">]</span><span class="s3">, </span><span class="s1">stride=</span><span class="s5">2</span><span class="s3">, </span><span class="s1">axis=</span><span class="s5">0</span><span class="s1">)</span><span class="s3">,</span>
                <span class="s1">arg_descriptors=[RandArg((</span><span class="s5">12</span><span class="s3">, </span><span class="s5">4</span><span class="s1">)</span><span class="s3">, </span><span class="s1">_f32)]</span><span class="s3">,</span>
                <span class="s1">poly_axes=[</span><span class="s5">0</span><span class="s1">])</span><span class="s3">,</span>
    <span class="s1">PolyHarness(</span><span class="s4">&quot;slice_in_dim&quot;</span><span class="s3">, </span><span class="s4">&quot;stride=2_odd&quot;</span><span class="s3">,</span>
                <span class="s3">lambda </span><span class="s1">x: lax.slice_in_dim(x</span><span class="s3">, </span><span class="s5">0</span><span class="s3">, </span><span class="s1">x.shape[</span><span class="s5">0</span><span class="s1">]</span><span class="s3">, </span><span class="s1">stride=</span><span class="s5">2</span><span class="s3">, </span><span class="s1">axis=</span><span class="s5">0</span><span class="s1">)</span><span class="s3">,</span>
                <span class="s1">arg_descriptors=[RandArg((</span><span class="s5">13</span><span class="s3">, </span><span class="s5">4</span><span class="s1">)</span><span class="s3">, </span><span class="s1">_f32)]</span><span class="s3">,</span>
                <span class="s1">poly_axes=[</span><span class="s5">0</span><span class="s1">])</span><span class="s3">,</span>
    <span class="s0"># Not yet, the slice_in_dim does int(stride)</span>
    <span class="s0"># PolyHarness(&quot;slice_in_dim&quot;, &quot;stride=sym&quot;,</span>
    <span class="s0">#             lambda x: lax.slice_in_dim(x, 0, x.shape[0], stride=x.shape[0] // 4, axis=0),</span>
    <span class="s0">#             arg_descriptors=[RandArg((13, 4), _f32)],</span>
    <span class="s0">#             poly_axes=[0]),</span>
    <span class="s1">PolyHarness(</span><span class="s4">&quot;squeeze&quot;</span><span class="s3">, </span><span class="s4">&quot;axis=empty&quot;</span><span class="s3">,</span>
                <span class="s1">jnp.squeeze</span><span class="s3">,</span>
                <span class="s1">arg_descriptors=[RandArg((</span><span class="s5">5</span><span class="s3">,</span><span class="s1">)</span><span class="s3">, </span><span class="s1">_f32)</span><span class="s3">, </span><span class="s1">StaticArg(())]</span><span class="s3">,</span>
                <span class="s1">poly_axes=[</span><span class="s5">0</span><span class="s1">])</span><span class="s3">,</span>
    <span class="s1">PolyHarness(</span><span class="s4">&quot;squeeze&quot;</span><span class="s3">, </span><span class="s4">&quot;axis=None&quot;</span><span class="s3">,</span>
                <span class="s1">jnp.squeeze</span><span class="s3">,</span>
                <span class="s1">arg_descriptors=[RandArg((</span><span class="s5">5</span><span class="s3">,</span><span class="s1">)</span><span class="s3">, </span><span class="s1">_f32)</span><span class="s3">, </span><span class="s1">StaticArg(</span><span class="s3">None</span><span class="s1">)]</span><span class="s3">,</span>
                <span class="s1">poly_axes=[</span><span class="s5">0</span><span class="s1">]</span><span class="s3">,</span>
                <span class="s1">expect_error=(ValueError</span><span class="s3">, </span><span class="s4">&quot;jnp.squeeze with axis=None is not supported with shape polymorphism&quot;</span><span class="s1">))</span><span class="s3">,</span>
    <span class="s1">PolyHarness(</span><span class="s4">&quot;squeeze&quot;</span><span class="s3">, </span><span class="s4">&quot;axis=1&quot;</span><span class="s3">,</span>
                <span class="s1">jnp.squeeze</span><span class="s3">,</span>
                <span class="s1">arg_descriptors=[RandArg((</span><span class="s5">4</span><span class="s3">, </span><span class="s5">1</span><span class="s1">)</span><span class="s3">, </span><span class="s1">_f32)</span><span class="s3">, </span><span class="s1">StaticArg((</span><span class="s5">1</span><span class="s3">,</span><span class="s1">))]</span><span class="s3">,</span>
                <span class="s1">poly_axes=[</span><span class="s5">0</span><span class="s1">])</span><span class="s3">,</span>
    <span class="s1">PolyHarness(</span><span class="s4">&quot;squeeze&quot;</span><span class="s3">, </span><span class="s4">&quot;axis=1_2&quot;</span><span class="s3">,</span>
                <span class="s1">jnp.squeeze</span><span class="s3">,</span>
                <span class="s1">arg_descriptors=[RandArg((</span><span class="s5">4</span><span class="s3">, </span><span class="s5">1</span><span class="s3">, </span><span class="s5">1</span><span class="s1">)</span><span class="s3">, </span><span class="s1">_f32)</span><span class="s3">, </span><span class="s1">StaticArg((</span><span class="s5">1</span><span class="s3">, </span><span class="s5">2</span><span class="s1">))]</span><span class="s3">,</span>
                <span class="s1">poly_axes=[</span><span class="s5">0</span><span class="s1">])</span><span class="s3">,</span>
    <span class="s1">PolyHarness(</span><span class="s4">&quot;squeeze&quot;</span><span class="s3">, </span><span class="s4">&quot;error&quot;</span><span class="s3">,</span>
                <span class="s1">jnp.squeeze</span><span class="s3">,</span>
                <span class="s1">arg_descriptors=[RandArg((</span><span class="s5">3</span><span class="s3">, </span><span class="s5">33</span><span class="s1">)</span><span class="s3">, </span><span class="s1">_f32)</span><span class="s3">, </span><span class="s1">StaticArg(-</span><span class="s5">1</span><span class="s1">)]</span><span class="s3">,</span>
                <span class="s1">poly_axes=[(</span><span class="s5">0</span><span class="s3">, </span><span class="s5">1</span><span class="s1">)]</span><span class="s3">,</span>
                <span class="s1">skip_jax_run=</span><span class="s3">True,</span>
                <span class="s1">expect_error=(ValueError</span><span class="s3">,</span>
                              <span class="s1">re.escape(</span>
                                <span class="s4">&quot;cannot select an axis to squeeze out which has size not equal to one, got shape=(b0, b1) and dimensions=(1,)&quot;</span><span class="s1">))</span>
                <span class="s1">)</span><span class="s3">,</span>
    <span class="s1">PolyHarness(</span><span class="s4">&quot;take&quot;</span><span class="s3">, </span><span class="s4">&quot;&quot;</span><span class="s3">,</span>
                <span class="s3">lambda </span><span class="s1">a</span><span class="s3">, </span><span class="s1">i: jnp.take(a</span><span class="s3">, </span><span class="s1">i</span><span class="s3">, </span><span class="s1">axis=</span><span class="s5">1</span><span class="s1">)</span><span class="s3">,</span>
                <span class="s1">arg_descriptors=[RandArg((</span><span class="s5">3</span><span class="s3">, </span><span class="s5">4</span><span class="s3">, </span><span class="s5">5</span><span class="s1">)</span><span class="s3">, </span><span class="s1">_f32)</span><span class="s3">, </span><span class="s1">np.array([</span><span class="s5">1</span><span class="s3">, </span><span class="s5">2</span><span class="s1">]</span><span class="s3">, </span><span class="s1">np.int32)]</span><span class="s3">,</span>
                <span class="s1">poly_axes=[</span><span class="s5">0</span><span class="s3">, None</span><span class="s1">]).both_enable_and_disable_xla()</span><span class="s3">,</span>
    <span class="s1">PolyHarness(</span><span class="s4">&quot;take_along_axis&quot;</span><span class="s3">, </span><span class="s4">&quot;0&quot;</span><span class="s3">,</span>
                <span class="s3">lambda </span><span class="s1">x</span><span class="s3">, </span><span class="s1">y: jnp.take_along_axis(x</span><span class="s3">, </span><span class="s1">y</span><span class="s3">, </span><span class="s1">axis=</span><span class="s5">0</span><span class="s1">)</span><span class="s3">,</span>
                <span class="s1">arg_descriptors=[RandArg((</span><span class="s5">5</span><span class="s3">, </span><span class="s5">2</span><span class="s1">)</span><span class="s3">, </span><span class="s1">_f32)</span><span class="s3">, </span><span class="s1">RandArg((</span><span class="s5">5</span><span class="s3">, </span><span class="s5">1</span><span class="s1">)</span><span class="s3">, </span><span class="s1">np.int32)]</span><span class="s3">,</span>
                <span class="s1">poly_axes=[</span><span class="s5">0</span><span class="s3">, </span><span class="s5">0</span><span class="s1">])</span><span class="s3">,</span>
    <span class="s1">PolyHarness(</span><span class="s4">&quot;take_along_axis&quot;</span><span class="s3">, </span><span class="s4">&quot;1&quot;</span><span class="s3">,</span>
                <span class="s3">lambda </span><span class="s1">x</span><span class="s3">, </span><span class="s1">y: jnp.take_along_axis(x</span><span class="s3">, </span><span class="s1">y</span><span class="s3">, </span><span class="s1">axis=</span><span class="s5">1</span><span class="s1">)</span><span class="s3">,</span>
                <span class="s1">arg_descriptors=[RandArg((</span><span class="s5">5</span><span class="s3">, </span><span class="s5">2</span><span class="s1">)</span><span class="s3">, </span><span class="s1">_f32)</span><span class="s3">, </span><span class="s1">RandArg((</span><span class="s5">5</span><span class="s3">, </span><span class="s5">1</span><span class="s1">)</span><span class="s3">, </span><span class="s1">np.int32)]</span><span class="s3">,</span>
                <span class="s1">poly_axes=[</span><span class="s5">0</span><span class="s3">, </span><span class="s5">0</span><span class="s1">])</span><span class="s3">,</span>
    <span class="s1">PolyHarness(</span><span class="s4">&quot;tile&quot;</span><span class="s3">, </span><span class="s4">&quot;0&quot;</span><span class="s3">,</span>
                <span class="s3">lambda </span><span class="s1">x: jnp.tile(x</span><span class="s3">, </span><span class="s1">(</span><span class="s5">1</span><span class="s3">, </span><span class="s5">2</span><span class="s1">))</span><span class="s3">,</span>
                <span class="s1">arg_descriptors=[RandArg((</span><span class="s5">4</span><span class="s3">, </span><span class="s5">3</span><span class="s1">)</span><span class="s3">, </span><span class="s1">_f32)]</span><span class="s3">,</span>
                <span class="s1">poly_axes=[</span><span class="s5">0</span><span class="s1">])</span><span class="s3">,</span>
    <span class="s1">PolyHarness(</span><span class="s4">&quot;tile&quot;</span><span class="s3">, </span><span class="s4">&quot;1&quot;</span><span class="s3">,</span>
                <span class="s0"># The repetitions are polys</span>
                <span class="s3">lambda </span><span class="s1">x: jnp.tile(x</span><span class="s3">, </span><span class="s1">(</span><span class="s5">1</span><span class="s3">, </span><span class="s1">x.shape[</span><span class="s5">0</span><span class="s1">]))</span><span class="s3">,</span>
                <span class="s1">arg_descriptors=[RandArg((</span><span class="s5">4</span><span class="s3">, </span><span class="s5">2</span><span class="s1">)</span><span class="s3">, </span><span class="s1">_f32)]</span><span class="s3">,</span>
                <span class="s1">poly_axes=[</span><span class="s5">0</span><span class="s1">])</span><span class="s3">,</span>
    <span class="s1">PolyHarness(</span><span class="s4">&quot;tri&quot;</span><span class="s3">, </span><span class="s4">&quot;N=poly_M=None&quot;</span><span class="s3">,</span>
                <span class="s3">lambda </span><span class="s1">x: jnp.tri(x.shape[</span><span class="s5">0</span><span class="s1">]) + x</span><span class="s3">,</span>
                <span class="s1">arg_descriptors=[RandArg((</span><span class="s5">3</span><span class="s3">, </span><span class="s5">1</span><span class="s1">)</span><span class="s3">, </span><span class="s1">_f32)]</span><span class="s3">,</span>
                <span class="s1">poly_axes=[</span><span class="s5">0</span><span class="s1">])</span><span class="s3">,</span>
    <span class="s1">PolyHarness(</span><span class="s4">&quot;tri&quot;</span><span class="s3">, </span><span class="s4">&quot;N=poly_M=poly&quot;</span><span class="s3">,</span>
                <span class="s3">lambda </span><span class="s1">x: jnp.tri(x.shape[</span><span class="s5">0</span><span class="s1">]</span><span class="s3">, </span><span class="s1">M=x.shape[</span><span class="s5">0</span><span class="s1">] + </span><span class="s5">2</span><span class="s1">) + x</span><span class="s3">,</span>
                <span class="s1">arg_descriptors=[RandArg((</span><span class="s5">3</span><span class="s3">, </span><span class="s5">1</span><span class="s1">)</span><span class="s3">, </span><span class="s1">_f32)]</span><span class="s3">,</span>
                <span class="s1">poly_axes=[</span><span class="s5">0</span><span class="s1">])</span><span class="s3">,</span>
    <span class="s1">[</span>
        <span class="s1">PolyHarness(</span><span class="s4">&quot;var&quot;</span><span class="s3">,</span>
                    <span class="s4">f&quot;</span><span class="s3">{</span><span class="s1">axis=</span><span class="s3">}</span><span class="s4">_</span><span class="s3">{</span><span class="s1">keepdims=</span><span class="s3">}</span><span class="s4">_where=None&quot;</span><span class="s3">,</span>
                    <span class="s3">lambda </span><span class="s1">x</span><span class="s3">, </span><span class="s1">axis</span><span class="s3">, </span><span class="s1">keepdims: jnp.var(x</span><span class="s3">, </span><span class="s1">axis=axis</span><span class="s3">, </span><span class="s1">keepdims=keepdims</span><span class="s3">, </span><span class="s1">where=</span><span class="s3">None</span><span class="s1">)</span><span class="s3">,</span>
                    <span class="s1">arg_descriptors=[RandArg((</span><span class="s5">7</span><span class="s3">, </span><span class="s5">8</span><span class="s3">, </span><span class="s5">4</span><span class="s1">)</span><span class="s3">, </span><span class="s1">_f32)</span><span class="s3">, </span><span class="s1">StaticArg(axis)</span><span class="s3">, </span><span class="s1">StaticArg(keepdims)]</span><span class="s3">,</span>
                    <span class="s1">poly_axes=[</span><span class="s5">0</span><span class="s1">])</span>
        <span class="s3">for </span><span class="s1">keepdims </span><span class="s3">in </span><span class="s1">[</span><span class="s3">False, True</span><span class="s1">]</span>
        <span class="s3">for </span><span class="s1">axis </span><span class="s3">in </span><span class="s1">[</span><span class="s3">None, </span><span class="s1">(</span><span class="s5">0</span><span class="s3">,</span><span class="s1">)</span><span class="s3">, </span><span class="s1">(</span><span class="s5">0</span><span class="s3">, </span><span class="s5">1</span><span class="s1">)</span><span class="s3">, </span><span class="s1">(</span><span class="s5">1</span><span class="s3">,</span><span class="s1">)]</span>
    <span class="s1">]</span><span class="s3">,</span>
    <span class="s1">[</span>
        <span class="s1">PolyHarness(</span><span class="s4">&quot;var&quot;</span><span class="s3">,</span>
                    <span class="s4">f&quot;</span><span class="s3">{</span><span class="s1">axis=</span><span class="s3">}</span><span class="s4">_</span><span class="s3">{</span><span class="s1">keepdims=</span><span class="s3">}</span><span class="s4">_where=Some&quot;</span><span class="s3">,</span>
                    <span class="s3">lambda </span><span class="s1">x</span><span class="s3">, </span><span class="s1">where</span><span class="s3">, </span><span class="s1">axis</span><span class="s3">, </span><span class="s1">keepdims: jnp.var(x</span><span class="s3">, </span><span class="s1">axis=axis</span><span class="s3">, </span><span class="s1">keepdims=keepdims</span><span class="s3">, </span><span class="s1">where=where)</span><span class="s3">,</span>
                    <span class="s1">arg_descriptors=[RandArg((</span><span class="s5">7</span><span class="s3">, </span><span class="s5">8</span><span class="s3">, </span><span class="s5">4</span><span class="s1">)</span><span class="s3">, </span><span class="s1">_f32)</span><span class="s3">, </span><span class="s1">RandArg((</span><span class="s5">7</span><span class="s3">, </span><span class="s5">8</span><span class="s3">, </span><span class="s5">4</span><span class="s1">)</span><span class="s3">, </span><span class="s1">np.bool_)</span><span class="s3">, </span><span class="s1">StaticArg(axis)</span><span class="s3">, </span><span class="s1">StaticArg(keepdims)]</span><span class="s3">,</span>
                    <span class="s1">poly_axes=[</span><span class="s5">0</span><span class="s3">, </span><span class="s5">0</span><span class="s1">])</span>
        <span class="s3">for </span><span class="s1">keepdims </span><span class="s3">in </span><span class="s1">[</span><span class="s3">False, True</span><span class="s1">]</span>
        <span class="s3">for </span><span class="s1">axis </span><span class="s3">in </span><span class="s1">[</span><span class="s3">None, </span><span class="s1">(</span><span class="s5">0</span><span class="s3">,</span><span class="s1">)</span><span class="s3">, </span><span class="s1">(</span><span class="s5">0</span><span class="s3">, </span><span class="s5">1</span><span class="s1">)</span><span class="s3">, </span><span class="s1">(</span><span class="s5">1</span><span class="s3">,</span><span class="s1">)]</span>
    <span class="s1">]</span><span class="s3">,</span>
    <span class="s1">PolyHarness(</span><span class="s4">&quot;where&quot;</span><span class="s3">, </span><span class="s4">&quot;&quot;</span><span class="s3">,</span>
                <span class="s1">jnp.where</span><span class="s3">,</span>
                <span class="s1">arg_descriptors=[RandArg((</span><span class="s5">2</span><span class="s3">,</span><span class="s1">)</span><span class="s3">, </span><span class="s1">np.bool_)</span><span class="s3">, </span><span class="s1">RandArg(()</span><span class="s3">, </span><span class="s1">_f32)</span><span class="s3">, </span><span class="s1">RandArg((</span><span class="s5">2</span><span class="s3">,</span><span class="s1">)</span><span class="s3">, </span><span class="s1">_f32)]</span><span class="s3">,</span>
                <span class="s1">poly_axes=[</span><span class="s5">0</span><span class="s3">, None, </span><span class="s5">0</span><span class="s1">])</span><span class="s3">,</span>
<span class="s1">]</span>

<span class="s3">def </span><span class="s1">_get_jax2tf_limitations(</span>
    <span class="s1">device</span><span class="s3">, </span><span class="s1">h: primitive_harness.Harness) -&gt; Sequence[Jax2TfLimitation]:</span>
  <span class="s0"># And the jax2tf limitations</span>
  <span class="s3">def </span><span class="s1">applicable_jax2tf_limitation(l: Jax2TfLimitation) -&gt; bool:</span>
    <span class="s0"># The CheckShapePolymorphism uses tf.function, so we care about &quot;graph&quot;</span>
    <span class="s3">return </span><span class="s1">l.filter(device=device</span><span class="s3">, </span><span class="s1">dtype=h.dtype</span><span class="s3">, </span><span class="s1">mode=</span><span class="s4">&quot;graph&quot;</span><span class="s1">)</span>

  <span class="s1">limitations = Jax2TfLimitation.limitations_for_harness(h)</span>
  <span class="s3">return </span><span class="s1">tuple(filter(applicable_jax2tf_limitation</span><span class="s3">, </span><span class="s1">limitations))</span>

<span class="s0">### We add to the test harnesses some that are obtained from the</span>
<span class="s0">### primitive harnesses by applying vmap to the function and then asserting</span>
<span class="s0">### that we can convert shape polymorphically the result.</span>

<span class="s3">def </span><span class="s1">_make_vmap_primitive_harnesses() -&gt; Sequence[PolyHarness]:</span>
  <span class="s2">&quot;&quot;&quot;For each harness group, pick a single dtype. 
 
  See PolyHarness for documentation. 
 
  Ignore harnesses that fail in graph mode in jax2tf. 
  &quot;&quot;&quot;</span>
  <span class="s1">all_h = primitive_harness.all_harnesses</span>
  <span class="s1">res = []</span>

  <span class="s0"># Index by group</span>
  <span class="s1">harness_groups: Dict[</span>
    <span class="s1">str</span><span class="s3">, </span><span class="s1">Sequence[primitive_harness.Harness]] = collections.defaultdict(list)</span>
  <span class="s1">device = jtu.device_under_test()</span>

  <span class="s3">for </span><span class="s1">h </span><span class="s3">in </span><span class="s1">all_h:</span>
    <span class="s0"># Drop the JAX limitations</span>
    <span class="s3">if not </span><span class="s1">h.filter(device_under_test=device</span><span class="s3">, </span><span class="s1">include_jax_unimpl=</span><span class="s3">False</span><span class="s1">):</span>
      <span class="s3">continue</span>
    <span class="s0"># And the jax2tf limitations that are known to result in TF error.</span>
    <span class="s3">if </span><span class="s1">any(l.expect_tf_error </span><span class="s3">for </span><span class="s1">l </span><span class="s3">in </span><span class="s1">_get_jax2tf_limitations(device</span><span class="s3">, </span><span class="s1">h)):</span>
      <span class="s3">continue</span>
    <span class="s0"># TODO(marcvanzee): We currently exclude tests with enable_xla=False because</span>
    <span class="s0"># this doesn't work with vmap due to a call to lax.gather. We should include</span>
    <span class="s0"># them once vmap works with enable_xla=False.</span>
    <span class="s3">if not </span><span class="s1">h.params.get(</span><span class="s4">&quot;enable_xla&quot;</span><span class="s3">, True</span><span class="s1">):</span>
      <span class="s3">continue</span>
    <span class="s1">harness_groups[h.group_name].append(h)</span>

  <span class="s1">selected_harnesses = []</span>
  <span class="s3">for </span><span class="s1">group_name</span><span class="s3">, </span><span class="s1">hlist </span><span class="s3">in </span><span class="s1">harness_groups.items():</span>
    <span class="s0"># Pick the dtype with the most harnesses in this group. Some harness</span>
    <span class="s0"># groups only test different use cases at a few dtypes.</span>
    <span class="s1">c = collections.Counter([h.dtype </span><span class="s3">for </span><span class="s1">h </span><span class="s3">in </span><span class="s1">hlist])</span>
    <span class="s1">(dtype</span><span class="s3">, </span><span class="s1">_)</span><span class="s3">, </span><span class="s1">= c.most_common(</span><span class="s5">1</span><span class="s1">)</span>
    <span class="s1">selected_harnesses.extend([h </span><span class="s3">for </span><span class="s1">h </span><span class="s3">in </span><span class="s1">hlist </span><span class="s3">if </span><span class="s1">h.dtype == dtype])</span>

  <span class="s0"># We do not yet support shape polymorphism for vmap for some primitives</span>
  <span class="s1">_NOT_SUPPORTED_YET = frozenset([</span>
      <span class="s0"># In linalg._lu_python we do reshape(-1, ...)</span>
      <span class="s4">&quot;lu&quot;</span><span class="s3">,</span>
      <span class="s4">&quot;custom_linear_solve&quot;</span><span class="s3">,</span>

      <span class="s0"># We do *= shapes in the batching rule for conv_general_dilated</span>
      <span class="s4">&quot;conv_general_dilated&quot;</span><span class="s3">,</span>

      <span class="s4">&quot;tridiagonal_solve&quot;</span><span class="s3">,  </span><span class="s0"># batching not implemented in JAX</span>
      <span class="s4">&quot;iota&quot;</span><span class="s3">,  </span><span class="s0"># vmap does not make sense for 0-argument functions</span>
      <span class="s4">&quot;rng_bit_generator&quot;</span><span class="s3">,  </span><span class="s0"># vmap not implemented</span>
  <span class="s1">])</span>

  <span class="s1">batch_size = </span><span class="s5">3</span>
  <span class="s3">for </span><span class="s1">h </span><span class="s3">in </span><span class="s1">selected_harnesses:</span>
    <span class="s3">if </span><span class="s1">h.group_name </span><span class="s3">in </span><span class="s1">_NOT_SUPPORTED_YET:</span>
      <span class="s3">continue</span>

    <span class="s3">def </span><span class="s1">make_batched_arg_descriptor(</span>
        <span class="s1">ad: primitive_harness.ArgDescriptor) -&gt; Optional[primitive_harness.ArgDescriptor]:</span>
      <span class="s3">if </span><span class="s1">isinstance(ad</span><span class="s3">, </span><span class="s1">RandArg):</span>
        <span class="s3">return </span><span class="s1">RandArg((batch_size</span><span class="s3">,</span><span class="s1">) + ad.shape</span><span class="s3">, </span><span class="s1">ad.dtype)</span>
      <span class="s3">elif </span><span class="s1">isinstance(ad</span><span class="s3">, </span><span class="s1">CustomArg):</span>
        <span class="s3">def </span><span class="s1">wrap_custom(rng):</span>
          <span class="s1">arg = ad.make(rng)</span>
          <span class="s3">return </span><span class="s1">np.stack([arg] * batch_size)</span>

        <span class="s3">return </span><span class="s1">CustomArg(wrap_custom)</span>
      <span class="s3">else</span><span class="s1">:</span>
        <span class="s3">assert </span><span class="s1">isinstance(ad</span><span class="s3">, </span><span class="s1">np.ndarray)</span><span class="s3">, </span><span class="s1">ad</span>
        <span class="s3">return </span><span class="s1">np.stack([ad] * batch_size)</span>

    <span class="s1">new_args = [make_batched_arg_descriptor(ad)</span>
                <span class="s3">for </span><span class="s1">ad </span><span class="s3">in </span><span class="s1">h.arg_descriptors</span>
                <span class="s3">if not </span><span class="s1">isinstance(ad</span><span class="s3">, </span><span class="s1">StaticArg)]</span>

    <span class="s0"># This test does not make sense for nullary functions</span>
    <span class="s3">if not </span><span class="s1">new_args:</span>
      <span class="s3">continue</span>

    <span class="s0"># We do not check the result of harnesses that require custom assertions.</span>
    <span class="s1">check_result = all(</span><span class="s3">not </span><span class="s1">l.custom_assert </span><span class="s3">and not </span><span class="s1">l.skip_comparison </span><span class="s3">and </span><span class="s1">l.tol </span><span class="s3">is None</span>
                       <span class="s3">for </span><span class="s1">l </span><span class="s3">in </span><span class="s1">_get_jax2tf_limitations(device</span><span class="s3">, </span><span class="s1">h))</span>
    <span class="s3">if </span><span class="s1">h.group_name == </span><span class="s4">&quot;cumsum&quot;</span><span class="s1">:</span>
      <span class="s0"># TODO(necula): why do we need to adjust the cumsum tolerance?</span>
      <span class="s1">tol = </span><span class="s5">1e-5</span>
    <span class="s3">else</span><span class="s1">:</span>
      <span class="s1">tol = </span><span class="s3">None</span>
    <span class="s1">vmap_harness = PolyHarness(</span><span class="s4">&quot;vmap_&quot; </span><span class="s1">+ h.group_name</span><span class="s3">, </span><span class="s1">h.name</span><span class="s3">,</span>
                               <span class="s1">jax.vmap(h.dyn_fun</span><span class="s3">, </span><span class="s1">in_axes=</span><span class="s5">0</span><span class="s3">, </span><span class="s1">out_axes=</span><span class="s5">0</span><span class="s1">)</span><span class="s3">,</span>
                               <span class="s1">arg_descriptors=new_args</span><span class="s3">,</span>
                               <span class="s1">poly_axes=[</span><span class="s5">0</span><span class="s1">] * len(new_args)</span><span class="s3">,</span>
                               <span class="s1">check_result=check_result</span><span class="s3">,</span>
                               <span class="s1">tol=tol)</span>
    <span class="s1">vmap_harness.original_harness = h</span>
    <span class="s1">res.append(vmap_harness)</span>
  <span class="s3">return </span><span class="s1">res</span>

<span class="s1">_POLY_SHAPE_TEST_HARNESSES.append(_make_vmap_primitive_harnesses())</span>

<span class="s3">def </span><span class="s1">_flatten_harnesses(harnesses):</span>
  <span class="s1">res = []</span>
  <span class="s3">for </span><span class="s1">h </span><span class="s3">in </span><span class="s1">harnesses:</span>
    <span class="s3">if </span><span class="s1">isinstance(h</span><span class="s3">, </span><span class="s1">Sequence):</span>
      <span class="s1">res.extend(h)</span>
    <span class="s3">else</span><span class="s1">:</span>
      <span class="s1">res.append(h)</span>
  <span class="s3">return </span><span class="s1">res</span>


<span class="s3">class </span><span class="s1">ShapePolyPrimitivesTest(tf_test_util.JaxToTfTestCase):</span>
  <span class="s2">&quot;&quot;&quot;Tests for primitives that take shape values as parameters.&quot;&quot;&quot;</span>

  <span class="s0"># This test runs for all _POLY_SHAPE_PRIMITIVE_HARNESSES.</span>

  <span class="s0"># For each primitive &quot;xxx&quot; the test will be called &quot;test_harness_xxx_...&quot;.</span>
  <span class="s0"># If you want to run this test for only one harness that includes &quot;foo&quot;</span>
  <span class="s0"># in the name (after test_harness), add parameter `one_containing=&quot;foo&quot;`</span>
  <span class="s0"># to parameterized below.</span>
  <span class="s1">@primitive_harness.parameterized(</span>
      <span class="s1">_flatten_harnesses(_POLY_SHAPE_TEST_HARNESSES)</span><span class="s3">,</span>
      <span class="s0">#one_containing=&quot;&quot;,</span>
  <span class="s1">)</span>
  <span class="s3">def </span><span class="s1">test_harness(self</span><span class="s3">, </span><span class="s1">harness: PolyHarness):</span>
    <span class="s0"># Exclude some harnesses that are known to fail for native serialization</span>
    <span class="s3">if </span><span class="s1">config.jax2tf_default_native_serialization:</span>
      <span class="s3">if not </span><span class="s1">harness.enable_xla:</span>
        <span class="s3">raise </span><span class="s1">unittest.SkipTest(</span><span class="s4">&quot;disabled for native_serialization and enable_xla=False&quot;</span><span class="s1">)</span>

      <span class="s0"># Set of harness.group_name:platform that are implemented with custom call</span>
      <span class="s1">custom_call_harnesses = {</span>
          <span class="s4">&quot;vmap_cholesky:cpu&quot;</span><span class="s3">, </span><span class="s4">&quot;vmap_cholesky:gpu&quot;</span><span class="s3">, </span><span class="s4">&quot;vmap_eig:cpu&quot;</span><span class="s3">,</span>
          <span class="s4">&quot;vmap_eigh:cpu&quot;</span><span class="s3">, </span><span class="s4">&quot;vmap_eigh:gpu&quot;</span><span class="s3">, </span><span class="s4">&quot;vmap_fft:cpu&quot;</span><span class="s3">,</span>
          <span class="s4">&quot;householder_product:cpu&quot;</span><span class="s3">, </span><span class="s4">&quot;householder_product:gpu&quot;</span><span class="s3">,</span>
          <span class="s4">&quot;vmap_geqrf:cpu&quot;</span><span class="s3">, </span><span class="s4">&quot;vmap_geqrf:gpu&quot;</span><span class="s3">,</span>
          <span class="s4">&quot;vmap_lu:cpu&quot;</span><span class="s3">, </span><span class="s4">&quot;vmap_lu:gpu&quot;</span><span class="s3">, </span><span class="s4">&quot;vmap_qr:cpu&quot;</span><span class="s3">, </span><span class="s4">&quot;vmap_qr:gpu&quot;</span><span class="s3">,</span>
          <span class="s4">&quot;vmap_svd:cpu&quot;</span><span class="s3">, </span><span class="s4">&quot;vmap_svd:gpu&quot;</span><span class="s3">,</span>
          <span class="s4">&quot;random_gamma:gpu&quot;</span><span class="s3">, </span><span class="s4">&quot;vmap_random_gamma:gpu&quot;</span><span class="s3">,</span>
          <span class="s4">&quot;random_categorical:gpu&quot;</span><span class="s3">, </span><span class="s4">&quot;vmap_random_categorical:gpu&quot;</span><span class="s3">,</span>
          <span class="s4">&quot;random_randint:gpu&quot;</span><span class="s3">, </span><span class="s4">&quot;vmap_random_randint:gpu&quot;</span><span class="s3">,</span>
          <span class="s4">&quot;random_uniform:gpu&quot;</span><span class="s3">, </span><span class="s4">&quot;vmap_random_uniform:gpu&quot;</span><span class="s3">,</span>
          <span class="s4">&quot;vmap_random_split:gpu&quot;</span><span class="s1">}</span>
      <span class="s3">if </span><span class="s4">f&quot;</span><span class="s3">{</span><span class="s1">harness.group_name</span><span class="s3">}</span><span class="s4">:</span><span class="s3">{</span><span class="s1">jtu.device_under_test()</span><span class="s3">}</span><span class="s4">&quot; </span><span class="s3">in </span><span class="s1">custom_call_harnesses:</span>
        <span class="s3">raise </span><span class="s1">unittest.SkipTest(</span><span class="s4">&quot;native serialization with shape polymorphism not implemented for custom calls; b/261671778&quot;</span><span class="s1">)</span>

      <span class="s0"># Set of harness.group_name or harness.group_name:platform that are implemented with HLO fallback lowering rules</span>
      <span class="s1">fallback_lowering_harnesses = {</span>
          <span class="s4">&quot;vmap_approx_top_k&quot;</span><span class="s3">, </span><span class="s4">&quot;vmap_bessel_i0e&quot;</span><span class="s3">, </span><span class="s4">&quot;vmap_eigh:tpu&quot;</span><span class="s3">,</span>
          <span class="s4">&quot;vmap_erf_inv&quot;</span><span class="s3">, </span><span class="s4">&quot;vmap_igamma&quot;</span><span class="s3">, </span><span class="s4">&quot;vmap_igammac&quot;</span><span class="s3">, </span><span class="s4">&quot;vmap_lu&quot;</span><span class="s3">,</span>
          <span class="s4">&quot;vmap_regularized_incomplete_beta&quot;</span><span class="s3">, </span><span class="s4">&quot;vmap_qr:tpu&quot;</span><span class="s3">,</span>
          <span class="s4">&quot;vmap_random_gamma:cpu&quot;</span><span class="s3">, </span><span class="s4">&quot;random_gamma:tpu&quot;</span><span class="s3">,</span>
          <span class="s4">&quot;vmap_random_gamma:tpu&quot;</span><span class="s3">, </span><span class="s4">&quot;vmap_svd:tpu&quot;</span><span class="s1">}</span>
      <span class="s3">if </span><span class="s1">(harness.group_name </span><span class="s3">in </span><span class="s1">fallback_lowering_harnesses </span><span class="s3">or</span>
          <span class="s4">f&quot;</span><span class="s3">{</span><span class="s1">harness.group_name</span><span class="s3">}</span><span class="s4">:</span><span class="s3">{</span><span class="s1">jtu.device_under_test()</span><span class="s3">}</span><span class="s4">&quot; </span><span class="s3">in </span><span class="s1">fallback_lowering_harnesses):</span>
        <span class="s3">raise </span><span class="s1">unittest.SkipTest(</span>
            <span class="s4">&quot;native serialization with shape polymorphism not implemented for JAX primitives still using HLO fallback lowering; b/261682623&quot;</span><span class="s1">)</span>

      <span class="s0"># Set of harness.group_name that are unsupported in serialization</span>
      <span class="s1">require_stablehlo_feature_support = {</span>
          <span class="s0"># Tan (b/274462307) and TopK (openxla/stablehlo#1255) require support.</span>
          <span class="s4">&quot;vmap_tan&quot;</span><span class="s3">, </span><span class="s4">&quot;vmap_top_k&quot;</span><span class="s3">,</span>
          <span class="s0"># Crash due to openxla/stablehlo#1328</span>
          <span class="s4">&quot;vmap_random_randint&quot;</span><span class="s3">, </span><span class="s4">&quot;vmap_random_uniform&quot;</span>
      <span class="s1">}</span>
      <span class="s3">if </span><span class="s1">harness.group_name </span><span class="s3">in </span><span class="s1">require_stablehlo_feature_support:</span>
        <span class="s3">raise </span><span class="s1">unittest.SkipTest(</span>
            <span class="s4">&quot;native lowering with shape polymorphism requires additional StableHLO feature support&quot;</span><span class="s1">)</span>
      <span class="s3">if </span><span class="s1">(jtu.device_under_test() == </span><span class="s4">&quot;tpu&quot; </span><span class="s3">and</span>
          <span class="s1">harness.fullname </span><span class="s3">in </span><span class="s1">[</span>
              <span class="s4">&quot;jnp.cumsum_reduce_axis=poly&quot;</span><span class="s3">,</span>
              <span class="s4">&quot;jnp.insert_insert=constant&quot;</span><span class="s3">, </span><span class="s4">&quot;jnp.insert_insert=poly&quot;</span><span class="s3">,</span>
              <span class="s4">&quot;jnp.nonzero_size=constant&quot;</span><span class="s3">, </span><span class="s4">&quot;jnp.nonzero_size=poly&quot;</span><span class="s1">]):</span>
        <span class="s0"># https://github.com/openxla/stablehlo/issues/1258</span>
        <span class="s3">raise </span><span class="s1">unittest.SkipTest(</span>
            <span class="s4">&quot;native serialization with shape polymorphism not implemented for window_reductions on TPU&quot;</span><span class="s1">)</span>
      <span class="s3">if </span><span class="s1">(jtu.device_under_test() == </span><span class="s4">&quot;gpu&quot; </span><span class="s3">and</span>
          <span class="s1">harness.fullname </span><span class="s3">in </span><span class="s1">[</span>
              <span class="s4">&quot;jnp.cumsum_reduce_axis=poly&quot;</span><span class="s3">,</span>
              <span class="s4">&quot;jnp.insert_insert=constant&quot;</span><span class="s3">, </span><span class="s4">&quot;jnp.insert_insert=poly&quot;</span><span class="s3">,</span>
              <span class="s4">&quot;jnp.nonzero_size=constant&quot;</span><span class="s3">, </span><span class="s4">&quot;jnp.nonzero_size=poly&quot;</span><span class="s1">]):</span>
        <span class="s3">raise </span><span class="s1">unittest.SkipTest(</span>
            <span class="s4">&quot;TODO(b/271645610): investigate inconclusive dimension operation for cumsum on gpu&quot;</span><span class="s1">)</span>

    <span class="s1">harness.run_test(self)</span>


<span class="s3">if </span><span class="s1">__name__ == </span><span class="s4">&quot;__main__&quot;</span><span class="s1">:</span>
  <span class="s1">absltest.main(testLoader=jtu.JaxTestLoader())</span>
</pre>
</body>
</html>