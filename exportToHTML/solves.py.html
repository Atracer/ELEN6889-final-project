<html>
<head>
<title>solves.py</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<style type="text/css">
.s0 { color: #808080;}
.s1 { color: #a9b7c6;}
.s2 { color: #629755; font-style: italic;}
.s3 { color: #cc7832;}
.s4 { color: #6a8759;}
.s5 { color: #6897bb;}
</style>
</head>
<body bgcolor="#2b2b2b">
<table CELLSPACING=0 CELLPADDING=5 COLS=1 WIDTH="100%" BGCOLOR="#606060" >
<tr><td><center>
<font face="Arial, Helvetica" color="#000000">
solves.py</font>
</center></td></tr></table>
<pre><span class="s0"># Copyright 2022 The JAX Authors.</span>
<span class="s0">#</span>
<span class="s0"># Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);</span>
<span class="s0"># you may not use this file except in compliance with the License.</span>
<span class="s0"># You may obtain a copy of the License at</span>
<span class="s0">#</span>
<span class="s0">#     https://www.apache.org/licenses/LICENSE-2.0</span>
<span class="s0">#</span>
<span class="s0"># Unless required by applicable law or agreed to in writing, software</span>
<span class="s0"># distributed under the License is distributed on an &quot;AS IS&quot; BASIS,</span>
<span class="s0"># WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span>
<span class="s0"># See the License for the specific language governing permissions and</span>
<span class="s0"># limitations under the License.</span>
<span class="s2">&quot;&quot;&quot;Module for the custom linear solve and utilities.&quot;&quot;&quot;</span>
<span class="s3">import </span><span class="s1">collections</span>
<span class="s3">from </span><span class="s1">functools </span><span class="s3">import </span><span class="s1">partial</span>
<span class="s3">import </span><span class="s1">operator</span>

<span class="s3">import </span><span class="s1">jax</span>
<span class="s3">from </span><span class="s1">jax </span><span class="s3">import </span><span class="s1">lax</span>
<span class="s3">from </span><span class="s1">jax.interpreters </span><span class="s3">import </span><span class="s1">ad</span>
<span class="s3">from </span><span class="s1">jax.interpreters </span><span class="s3">import </span><span class="s1">batching</span>
<span class="s3">from </span><span class="s1">jax.interpreters </span><span class="s3">import </span><span class="s1">mlir</span>
<span class="s3">from </span><span class="s1">jax.interpreters </span><span class="s3">import </span><span class="s1">xla</span>
<span class="s3">from </span><span class="s1">jax.tree_util </span><span class="s3">import </span><span class="s1">(tree_flatten</span><span class="s3">, </span><span class="s1">treedef_children</span><span class="s3">, </span><span class="s1">tree_leaves</span><span class="s3">,</span>
                           <span class="s1">tree_unflatten</span><span class="s3">, </span><span class="s1">treedef_tuple)</span>
<span class="s3">from </span><span class="s1">jax._src </span><span class="s3">import </span><span class="s1">ad_util</span>
<span class="s3">from </span><span class="s1">jax._src </span><span class="s3">import </span><span class="s1">core</span>
<span class="s3">from </span><span class="s1">jax._src </span><span class="s3">import </span><span class="s1">linear_util </span><span class="s3">as </span><span class="s1">lu</span>
<span class="s3">from </span><span class="s1">jax._src.core </span><span class="s3">import </span><span class="s1">raise_to_shaped</span>
<span class="s3">from </span><span class="s1">jax._src.traceback_util </span><span class="s3">import </span><span class="s1">api_boundary</span>
<span class="s3">from </span><span class="s1">jax._src.util </span><span class="s3">import </span><span class="s1">split_list</span><span class="s3">, </span><span class="s1">safe_map</span>
<span class="s3">import </span><span class="s1">numpy </span><span class="s3">as </span><span class="s1">np</span>

<span class="s3">from </span><span class="s1">jax._src.lax.control_flow.common </span><span class="s3">import </span><span class="s1">(</span>
    <span class="s1">_abstractify</span><span class="s3">,</span>
    <span class="s1">_check_tree</span><span class="s3">,</span>
    <span class="s1">_initial_style_jaxpr</span><span class="s3">,</span>
    <span class="s1">)</span>

<span class="s1">_map = safe_map</span>

<span class="s1">_RootTuple = collections.namedtuple(</span><span class="s4">'_RootTuple'</span><span class="s3">, </span><span class="s4">'f, solve, l_and_s'</span><span class="s1">)</span>


<span class="s3">def </span><span class="s1">_split_root_args(args</span><span class="s3">, </span><span class="s1">const_lengths):</span>
  <span class="s1">params_list = split_list(args</span><span class="s3">, </span><span class="s1">list(const_lengths))</span>
  <span class="s3">return </span><span class="s1">_RootTuple(*params_list[:-</span><span class="s5">1</span><span class="s1">])</span><span class="s3">, </span><span class="s1">params_list[-</span><span class="s5">1</span><span class="s1">]</span>


<span class="s1">@api_boundary</span>
<span class="s3">def </span><span class="s1">custom_root(f</span><span class="s3">, </span><span class="s1">initial_guess</span><span class="s3">, </span><span class="s1">solve</span><span class="s3">, </span><span class="s1">tangent_solve</span><span class="s3">, </span><span class="s1">has_aux=</span><span class="s3">False</span><span class="s1">):</span>
  <span class="s2">&quot;&quot;&quot;Differentiably solve for a roots of a function. 
 
  This is a low-level routine, mostly intended for internal use in JAX. 
  Gradients of custom_root() are defined with respect to closed-over variables 
  from the provided function ``f`` via the implicit function theorem: 
  https://en.wikipedia.org/wiki/Implicit_function_theorem 
 
  Args: 
    f: function for which to find a root. Should accept a single argument, 
      return a tree of arrays with the same structure as its input. 
    initial_guess: initial guess for a zero of f. 
    solve: function to solve for the roots of f. Should take two positional 
      arguments, f and initial_guess, and return a solution with the same 
      structure as initial_guess such that func(solution) = 0. In other words, 
      the following is assumed to be true (but not checked):: 
 
        solution = solve(f, initial_guess) 
        error = f(solution) 
        assert all(error == 0) 
 
    tangent_solve: function to solve the tangent system. Should take two 
      positional arguments, a linear function ``g`` (the function ``f`` 
      linearized at its root) and a tree of array(s) ``y`` with the same 
      structure as initial_guess, and return a solution ``x`` such that 
      ``g(x)=y``: 
 
      - For scalar ``y``, use ``lambda g, y: y / g(1.0)``. 
      - For vector ``y``, you could use a linear solve with the Jacobian, if 
        dimensionality of ``y`` is not too large: 
        ``lambda g, y: np.linalg.solve(jacobian(g)(y), y)``. 
    has_aux: bool indicating whether the ``solve`` function returns 
      auxiliary data like solver diagnostics as a second argument. 
 
  Returns: 
    The result of calling solve(f, initial_guess) with gradients defined via 
    implicit differentiation assuming ``f(solve(f, initial_guess)) == 0``. 
  &quot;&quot;&quot;</span>
  <span class="s1">guess_flat</span><span class="s3">, </span><span class="s1">in_args_tree = tree_flatten((initial_guess</span><span class="s3">,</span><span class="s1">))</span>
  <span class="s1">guess_avals = tuple(_map(_abstractify</span><span class="s3">, </span><span class="s1">guess_flat))</span>
  <span class="s1">f_jaxpr</span><span class="s3">, </span><span class="s1">f_consts</span><span class="s3">, </span><span class="s1">out_tree = _initial_style_jaxpr(</span>
      <span class="s1">f</span><span class="s3">, </span><span class="s1">in_args_tree</span><span class="s3">, </span><span class="s1">guess_avals)</span>

  <span class="s1">in_tree</span><span class="s3">, </span><span class="s1">= treedef_children(in_args_tree)</span>
  <span class="s1">_check_tree(</span><span class="s4">&quot;f&quot;</span><span class="s3">, </span><span class="s4">&quot;initial_guess&quot;</span><span class="s3">, </span><span class="s1">out_tree</span><span class="s3">, </span><span class="s1">in_tree</span><span class="s3">, False</span><span class="s1">)</span>

  <span class="s1">solve_jaxpr</span><span class="s3">, </span><span class="s1">solve_consts</span><span class="s3">, </span><span class="s1">solution_tree = _initial_style_jaxpr(</span>
      <span class="s1">partial(solve</span><span class="s3">, </span><span class="s1">f)</span><span class="s3">, </span><span class="s1">in_args_tree</span><span class="s3">, </span><span class="s1">guess_avals)</span>
  <span class="s1">_check_tree(</span><span class="s4">&quot;solve&quot;</span><span class="s3">, </span><span class="s4">&quot;initial_guess&quot;</span><span class="s3">, </span><span class="s1">solution_tree</span><span class="s3">, </span><span class="s1">in_tree</span><span class="s3">, </span><span class="s1">has_aux)</span>

  <span class="s3">def </span><span class="s1">linearize_and_solve(x</span><span class="s3">, </span><span class="s1">b):</span>
    <span class="s1">unchecked_zeros</span><span class="s3">, </span><span class="s1">f_jvp = jax.linearize(f</span><span class="s3">, </span><span class="s1">x)</span>
    <span class="s3">return </span><span class="s1">tangent_solve(f_jvp</span><span class="s3">, </span><span class="s1">b)</span>

  <span class="s1">l_and_s_jaxpr</span><span class="s3">, </span><span class="s1">l_and_s_consts</span><span class="s3">, </span><span class="s1">out_tree = _initial_style_jaxpr(</span>
      <span class="s1">linearize_and_solve</span><span class="s3">, </span><span class="s1">treedef_tuple((in_tree</span><span class="s3">,</span><span class="s1">) * </span><span class="s5">2</span><span class="s1">)</span><span class="s3">, </span><span class="s1">guess_avals * </span><span class="s5">2</span><span class="s1">)</span>
  <span class="s1">_check_tree(</span><span class="s4">&quot;tangent_solve&quot;</span><span class="s3">, </span><span class="s4">&quot;x&quot;</span><span class="s3">, </span><span class="s1">out_tree</span><span class="s3">, </span><span class="s1">in_tree</span><span class="s3">, False</span><span class="s1">)</span>

  <span class="s1">all_consts = [f_consts</span><span class="s3">, </span><span class="s1">solve_consts</span><span class="s3">, </span><span class="s1">l_and_s_consts]</span>
  <span class="s1">const_lengths = _RootTuple(*_map(len</span><span class="s3">, </span><span class="s1">all_consts))</span>
  <span class="s1">jaxprs = _RootTuple(f_jaxpr</span><span class="s3">, </span><span class="s1">solve_jaxpr</span><span class="s3">, </span><span class="s1">l_and_s_jaxpr)</span>

  <span class="s1">solution_flat = _custom_root(</span>
      <span class="s1">const_lengths</span><span class="s3">, </span><span class="s1">jaxprs</span><span class="s3">, </span><span class="s1">*(_flatten(all_consts) + guess_flat))</span>
  <span class="s3">return </span><span class="s1">tree_unflatten(solution_tree</span><span class="s3">, </span><span class="s1">solution_flat)</span>


<span class="s1">@partial(jax.custom_jvp</span><span class="s3">, </span><span class="s1">nondiff_argnums=(</span><span class="s5">0</span><span class="s3">, </span><span class="s5">1</span><span class="s1">))</span>
<span class="s3">def </span><span class="s1">_custom_root(const_lengths</span><span class="s3">, </span><span class="s1">jaxprs</span><span class="s3">, </span><span class="s1">*args):</span>
  <span class="s1">params</span><span class="s3">, </span><span class="s1">initial_guess = _split_root_args(args</span><span class="s3">, </span><span class="s1">const_lengths)</span>
  <span class="s1">solution = core.jaxpr_as_fun(jaxprs.solve)(*(params.solve + initial_guess))</span>
  <span class="s3">return </span><span class="s1">solution</span>


<span class="s1">@_custom_root.defjvp</span>
<span class="s3">def </span><span class="s1">_root_jvp(const_lengths</span><span class="s3">, </span><span class="s1">jaxprs</span><span class="s3">, </span><span class="s1">primals</span><span class="s3">, </span><span class="s1">tangents):</span>
  <span class="s1">params</span><span class="s3">, </span><span class="s1">_ = _split_root_args(primals</span><span class="s3">, </span><span class="s1">const_lengths)</span>
  <span class="s1">sol = _custom_root(const_lengths</span><span class="s3">, </span><span class="s1">jaxprs</span><span class="s3">, </span><span class="s1">*primals)</span>

  <span class="s1">f_out_vals = len(jaxprs.f.out_avals)</span>
  <span class="s1">solution</span><span class="s3">, </span><span class="s1">aux = split_list(sol</span><span class="s3">, </span><span class="s1">[f_out_vals])</span>

  <span class="s1">params_dot</span><span class="s3">, </span><span class="s1">_ = _split_root_args(tangents</span><span class="s3">, </span><span class="s1">const_lengths)</span>

  <span class="s0"># F(m, u) = 0      # system of equations in u, parameterized by m</span>
  <span class="s0">#                  # solution is u*(m) defined in a neighborhood</span>
  <span class="s0"># F(m, u*(m)) = 0  # satisfied in a neighborhood</span>
  <span class="s0">#</span>
  <span class="s0"># ∂_0 F(m, u*(m)) + ∂_1 F(m, u*(m)) ∂ u*(m) = 0       # implied by line above</span>
  <span class="s0"># ∂ u*(m) = - (∂_1 F(m, u*(m)))^{-1} ∂_0 F(m, u*(m))  # rearrange</span>
  <span class="s0">#</span>
  <span class="s0"># ∂ u*(m)[v] = - (∂_1 F(m, u*(m)))^{-1} [∂_0 F(m, u*(m))[v]]  # jvp</span>

  <span class="s1">f = core.jaxpr_as_fun(jaxprs.f)</span>
  <span class="s1">linearize_and_solve = partial(</span>
      <span class="s1">core.jaxpr_as_fun(jaxprs.l_and_s)</span><span class="s3">, </span><span class="s1">*params.l_and_s)</span>
  <span class="s1">f_at_solution = </span><span class="s3">lambda </span><span class="s1">*params: f(*params</span><span class="s3">, </span><span class="s1">*solution)</span>
  <span class="s1">_</span><span class="s3">, </span><span class="s1">rhs = ad.jvp(lu.wrap_init(f_at_solution)).call_wrapped(</span>
      <span class="s1">params.f</span><span class="s3">, </span><span class="s1">params_dot.f)</span>
  <span class="s1">solution_dot = _map(</span>
      <span class="s1">operator.neg</span><span class="s3">, </span><span class="s1">linearize_and_solve(*solution</span><span class="s3">, </span><span class="s1">*rhs))</span>
  <span class="s0"># append aux, create symbolic zero tangents for the aux values</span>
  <span class="s1">solution += aux</span>
  <span class="s1">solution_dot += _map(lax.zeros_like_array</span><span class="s3">, </span><span class="s1">aux)</span>

  <span class="s3">return </span><span class="s1">solution</span><span class="s3">, </span><span class="s1">solution_dot</span>


<span class="s3">class </span><span class="s1">_LinearSolveTuple(collections.namedtuple(</span>
    <span class="s4">'_LinearSolveTuple'</span><span class="s3">, </span><span class="s4">'matvec, vecmat, solve, transpose_solve'</span><span class="s1">)):</span>

  <span class="s3">def </span><span class="s1">transpose(self):</span>
    <span class="s3">return </span><span class="s1">type(self)(self.vecmat</span><span class="s3">, </span><span class="s1">self.matvec</span><span class="s3">, </span><span class="s1">self.transpose_solve</span><span class="s3">, </span><span class="s1">self.solve)</span>


<span class="s3">def </span><span class="s1">_split_linear_solve_args(args</span><span class="s3">, </span><span class="s1">const_lengths):</span>
  <span class="s1">params_list = split_list(args</span><span class="s3">, </span><span class="s1">list(const_lengths))</span>
  <span class="s3">return </span><span class="s1">_LinearSolveTuple(*params_list[:-</span><span class="s5">1</span><span class="s1">])</span><span class="s3">, </span><span class="s1">params_list[-</span><span class="s5">1</span><span class="s1">]</span>


<span class="s3">def </span><span class="s1">_transpose_one_output(linear_fun</span><span class="s3">, </span><span class="s1">primals):</span>
  <span class="s1">transpose_fun = jax.linear_transpose(linear_fun</span><span class="s3">, </span><span class="s1">primals)</span>
  <span class="s3">def </span><span class="s1">transposed_fun(x):</span>
    <span class="s1">(y</span><span class="s3">,</span><span class="s1">) = transpose_fun(x)</span>
    <span class="s3">return </span><span class="s1">y</span>
  <span class="s3">return </span><span class="s1">transposed_fun</span>


<span class="s3">def </span><span class="s1">_flatten(args):</span>
  <span class="s3">return </span><span class="s1">[x </span><span class="s3">for </span><span class="s1">arg </span><span class="s3">in </span><span class="s1">args </span><span class="s3">for </span><span class="s1">x </span><span class="s3">in </span><span class="s1">arg]</span>


<span class="s3">def </span><span class="s1">_check_shapes(func_name</span><span class="s3">, </span><span class="s1">expected_name</span><span class="s3">, </span><span class="s1">actual</span><span class="s3">, </span><span class="s1">expected):</span>
  <span class="s1">actual_shapes = _map(np.shape</span><span class="s3">, </span><span class="s1">tree_leaves(actual))</span>
  <span class="s1">expected_shapes = _map(np.shape</span><span class="s3">, </span><span class="s1">tree_leaves(expected))</span>
  <span class="s3">if </span><span class="s1">actual_shapes != expected_shapes:</span>
    <span class="s3">raise </span><span class="s1">ValueError(</span>
        <span class="s4">f&quot;</span><span class="s3">{</span><span class="s1">func_name</span><span class="s3">}</span><span class="s4">() output shapes must match </span><span class="s3">{</span><span class="s1">expected_name</span><span class="s3">}</span><span class="s4">, &quot;</span>
        <span class="s4">f&quot;got </span><span class="s3">{</span><span class="s1">actual_shapes</span><span class="s3">} </span><span class="s4">and </span><span class="s3">{</span><span class="s1">expected_shapes</span><span class="s3">}</span><span class="s4">&quot;</span><span class="s1">)</span>


<span class="s1">@api_boundary</span>
<span class="s3">def </span><span class="s1">custom_linear_solve(</span>
    <span class="s1">matvec</span><span class="s3">, </span><span class="s1">b</span><span class="s3">, </span><span class="s1">solve</span><span class="s3">, </span><span class="s1">transpose_solve=</span><span class="s3">None, </span><span class="s1">symmetric=</span><span class="s3">False, </span><span class="s1">has_aux=</span><span class="s3">False</span><span class="s1">):</span>
  <span class="s2">&quot;&quot;&quot;Perform a matrix-free linear solve with implicitly defined gradients. 
 
  This function allows for overriding or defining gradients for a linear 
  solve directly via implicit differentiation at the solution, rather than by 
  differentiating *through* the solve operation. This can sometimes be much faster 
  or more numerically stable, or differentiating through the solve operation 
  may not even be implemented (e.g., if ``solve`` uses ``lax.while_loop``). 
 
  Required invariant:: 
 
      x = solve(matvec, b)  # solve the linear equation 
      assert matvec(x) == b  # not checked 
 
  Args: 
    matvec: linear function to invert. Must be differentiable. 
    b: constant right handle side of the equation. May be any nested structure 
      of arrays. 
    solve: higher level function that solves for solution to the linear 
      equation, i.e., ``solve(matvec, x) == x`` for all ``x`` of the same form 
      as ``b``. This function need not be differentiable. 
    transpose_solve: higher level function for solving the transpose linear 
      equation, i.e., ``transpose_solve(vecmat, x) == x``, where ``vecmat`` is 
      the transpose of the linear map ``matvec`` (computed automatically with 
      autodiff). Required for backwards mode automatic differentiation, unless 
      ``symmetric=True``, in which case ``solve`` provides the default value. 
    symmetric: bool indicating if it is safe to assume the linear map 
      corresponds to a symmetric matrix, i.e., ``matvec == vecmat``. 
    has_aux: bool indicating whether the ``solve`` and ``transpose_solve`` functions 
      return auxiliary data like solver diagnostics as a second argument. 
 
  Returns: 
    Result of ``solve(matvec, b)``, with gradients defined assuming that the 
      solution ``x`` satisfies the linear equation ``matvec(x) == b``. 
  &quot;&quot;&quot;</span>
  <span class="s3">if </span><span class="s1">transpose_solve </span><span class="s3">is None and </span><span class="s1">symmetric:</span>
    <span class="s1">transpose_solve = solve</span>

  <span class="s1">b_flat</span><span class="s3">, </span><span class="s1">in_args_tree = tree_flatten((b</span><span class="s3">,</span><span class="s1">))</span>
  <span class="s1">b_avals = tuple(_map(_abstractify</span><span class="s3">, </span><span class="s1">b_flat))</span>

  <span class="s1">tree</span><span class="s3">, </span><span class="s1">= treedef_children(in_args_tree)</span>

  <span class="s3">def </span><span class="s1">_shape_checked(fun</span><span class="s3">, </span><span class="s1">name</span><span class="s3">, </span><span class="s1">has_aux):</span>
    <span class="s3">def </span><span class="s1">f(x):</span>
      <span class="s1">y = fun(x)</span>
      <span class="s1">_check_shapes(name</span><span class="s3">, </span><span class="s4">&quot;b&quot;</span><span class="s3">, </span><span class="s1">y</span><span class="s3">, </span><span class="s1">b_flat)</span>
      <span class="s3">return </span><span class="s1">y</span>

    <span class="s3">def </span><span class="s1">f_aux(x):</span>
      <span class="s1">y</span><span class="s3">, </span><span class="s1">aux = fun(x)</span>
      <span class="s1">_check_shapes(name</span><span class="s3">, </span><span class="s4">&quot;b&quot;</span><span class="s3">, </span><span class="s1">y</span><span class="s3">, </span><span class="s1">b_flat)</span>
      <span class="s3">return </span><span class="s1">y</span><span class="s3">, </span><span class="s1">aux</span>

    <span class="s3">return </span><span class="s1">f_aux </span><span class="s3">if </span><span class="s1">has_aux </span><span class="s3">else </span><span class="s1">f</span>

  <span class="s0"># no auxiliary data assumed for matvec</span>
  <span class="s1">matvec_jaxpr</span><span class="s3">, </span><span class="s1">matvec_consts</span><span class="s3">, </span><span class="s1">out_tree = _initial_style_jaxpr(</span>
      <span class="s1">_shape_checked(matvec</span><span class="s3">, </span><span class="s4">&quot;matvec&quot;</span><span class="s3">, False</span><span class="s1">)</span><span class="s3">, </span><span class="s1">in_args_tree</span><span class="s3">, </span><span class="s1">b_avals</span><span class="s3">,</span>
      <span class="s4">'custom_linear_solve'</span><span class="s1">)</span>
  <span class="s1">_check_tree(</span><span class="s4">&quot;matvec&quot;</span><span class="s3">, </span><span class="s4">&quot;b&quot;</span><span class="s3">, </span><span class="s1">out_tree</span><span class="s3">, </span><span class="s1">tree</span><span class="s3">, False</span><span class="s1">)</span>

  <span class="s1">solve_jaxpr</span><span class="s3">, </span><span class="s1">solve_consts</span><span class="s3">, </span><span class="s1">out_tree = _initial_style_jaxpr(</span>
      <span class="s1">_shape_checked(partial(solve</span><span class="s3">, </span><span class="s1">matvec)</span><span class="s3">, </span><span class="s4">&quot;solve&quot;</span><span class="s3">, </span><span class="s1">has_aux)</span><span class="s3">, </span><span class="s1">in_args_tree</span><span class="s3">, </span><span class="s1">b_avals</span><span class="s3">,</span>
      <span class="s4">'custom_linear_solve'</span><span class="s1">)</span>
  <span class="s1">_check_tree(</span><span class="s4">&quot;solve&quot;</span><span class="s3">, </span><span class="s4">&quot;b&quot;</span><span class="s3">, </span><span class="s1">out_tree</span><span class="s3">, </span><span class="s1">tree</span><span class="s3">, </span><span class="s1">has_aux)</span>

  <span class="s3">if </span><span class="s1">transpose_solve </span><span class="s3">is None</span><span class="s1">:</span>
    <span class="s1">vecmat_jaxpr = tr_solve_jaxpr = </span><span class="s3">None</span>
    <span class="s1">vecmat_consts = tr_solve_consts = []</span>
  <span class="s3">else</span><span class="s1">:</span>
    <span class="s3">if </span><span class="s1">symmetric:</span>
      <span class="s1">vecmat = matvec</span>
      <span class="s1">vecmat_jaxpr = matvec_jaxpr</span>
      <span class="s1">vecmat_consts = matvec_consts</span>
    <span class="s3">else</span><span class="s1">:</span>
      <span class="s1">vecmat = _transpose_one_output(matvec</span><span class="s3">, </span><span class="s1">b)</span>
      <span class="s1">vecmat_jaxpr</span><span class="s3">, </span><span class="s1">vecmat_consts</span><span class="s3">, </span><span class="s1">out_tree = _initial_style_jaxpr(</span>
          <span class="s1">vecmat</span><span class="s3">, </span><span class="s1">in_args_tree</span><span class="s3">, </span><span class="s1">b_avals</span><span class="s3">, </span><span class="s4">'custom_linear_solve'</span><span class="s1">)</span>
      <span class="s3">assert </span><span class="s1">out_tree == tree</span>

    <span class="s1">tr_solve_jaxpr</span><span class="s3">, </span><span class="s1">tr_solve_consts</span><span class="s3">, </span><span class="s1">out_tree = _initial_style_jaxpr(</span>
        <span class="s1">_shape_checked(partial(transpose_solve</span><span class="s3">, </span><span class="s1">vecmat)</span><span class="s3">, </span><span class="s4">&quot;transpose_solve&quot;</span><span class="s3">, </span><span class="s1">has_aux)</span><span class="s3">,</span>
        <span class="s1">in_args_tree</span><span class="s3">, </span><span class="s1">b_avals</span><span class="s3">, </span><span class="s4">'custom_linear_solve'</span><span class="s1">)</span>
    <span class="s1">_check_tree(</span><span class="s4">&quot;transpose_solve&quot;</span><span class="s3">, </span><span class="s4">&quot;b&quot;</span><span class="s3">, </span><span class="s1">out_tree</span><span class="s3">, </span><span class="s1">tree</span><span class="s3">, </span><span class="s1">has_aux)</span>

  <span class="s1">all_consts = [matvec_consts</span><span class="s3">, </span><span class="s1">vecmat_consts</span><span class="s3">, </span><span class="s1">solve_consts</span><span class="s3">, </span><span class="s1">tr_solve_consts]</span>
  <span class="s1">const_lengths = _LinearSolveTuple(*_map(len</span><span class="s3">, </span><span class="s1">all_consts))</span>
  <span class="s1">jaxprs = _LinearSolveTuple(</span>
      <span class="s1">matvec_jaxpr</span><span class="s3">, </span><span class="s1">vecmat_jaxpr</span><span class="s3">, </span><span class="s1">solve_jaxpr</span><span class="s3">, </span><span class="s1">tr_solve_jaxpr)</span>

  <span class="s1">out_flat = linear_solve_p.bind(</span>
      <span class="s1">*(_flatten(all_consts) + b_flat)</span><span class="s3">,</span>
      <span class="s1">const_lengths=const_lengths</span><span class="s3">, </span><span class="s1">jaxprs=jaxprs)</span>

  <span class="s3">return </span><span class="s1">tree_unflatten(out_tree</span><span class="s3">, </span><span class="s1">out_flat)</span>


<span class="s3">def </span><span class="s1">_linear_solve_abstract_eval(*args</span><span class="s3">, </span><span class="s1">const_lengths</span><span class="s3">, </span><span class="s1">jaxprs):</span>
  <span class="s1">args_to_raise = args[sum(const_lengths):]</span>

  <span class="s0"># raise aux_args to shaped arrays as well if present</span>
  <span class="s0"># number of aux args is the difference in out_avals</span>
  <span class="s0"># of solve and matvec (since they map to the same vector space)</span>

  <span class="s1">num_aux = len(jaxprs.solve.out_avals) - len(jaxprs.matvec.out_avals)</span>
  <span class="s3">if </span><span class="s1">num_aux &gt; </span><span class="s5">0</span><span class="s1">:</span>
    <span class="s1">args_to_raise += tuple(jaxprs.solve.out_avals[-num_aux:])</span>
  <span class="s3">return </span><span class="s1">_map(raise_to_shaped</span><span class="s3">, </span><span class="s1">args_to_raise)</span>


<span class="s3">def </span><span class="s1">_custom_linear_solve_impl(*args</span><span class="s3">, </span><span class="s1">const_lengths</span><span class="s3">, </span><span class="s1">jaxprs):</span>
  <span class="s1">params</span><span class="s3">, </span><span class="s1">b = _split_linear_solve_args(args</span><span class="s3">, </span><span class="s1">const_lengths)</span>
  <span class="s1">x = core.jaxpr_as_fun(jaxprs.solve)(*(params.solve + b))</span>
  <span class="s3">return </span><span class="s1">x</span>


<span class="s3">def </span><span class="s1">_tangent_linear_map(func</span><span class="s3">, </span><span class="s1">params</span><span class="s3">, </span><span class="s1">params_dot</span><span class="s3">, </span><span class="s1">*x):</span>
  <span class="s2">&quot;&quot;&quot;Compute the tangent of a linear map. 
 
  Assuming ``func(*params, *x)`` is linear in ``x`` and computes ``A @ x``, 
  this function computes ``∂A @ x``. 
  &quot;&quot;&quot;</span>
  <span class="s3">assert </span><span class="s1">any(type(p) </span><span class="s3">is not </span><span class="s1">ad_util.Zero </span><span class="s3">for </span><span class="s1">p </span><span class="s3">in </span><span class="s1">params_dot)</span>
  <span class="s1">zeros = _map(ad_util.Zero.from_value</span><span class="s3">, </span><span class="s1">x)</span>
  <span class="s1">_</span><span class="s3">, </span><span class="s1">out_tangent = ad.jvp(lu.wrap_init(func)).call_wrapped(</span>
      <span class="s1">params + list(x)</span><span class="s3">, </span><span class="s1">params_dot + zeros)</span>
  <span class="s3">return </span><span class="s1">out_tangent</span>


<span class="s3">def </span><span class="s1">_custom_linear_solve_jvp(primals</span><span class="s3">, </span><span class="s1">tangents</span><span class="s3">, </span><span class="s1">const_lengths</span><span class="s3">, </span><span class="s1">jaxprs):</span>
  <span class="s0"># A x - b = 0</span>
  <span class="s0"># ∂A x + A ∂x - ∂b = 0</span>
  <span class="s0"># ∂x = A^{-1} (∂b - ∂A x)</span>

  <span class="s1">kwargs = dict(const_lengths=const_lengths</span><span class="s3">, </span><span class="s1">jaxprs=jaxprs)</span>
  <span class="s1">x = linear_solve_p.bind(*primals</span><span class="s3">, </span><span class="s1">**kwargs)</span>

  <span class="s1">params</span><span class="s3">, </span><span class="s1">_ = _split_linear_solve_args(primals</span><span class="s3">, </span><span class="s1">const_lengths)</span>
  <span class="s1">params_dot</span><span class="s3">, </span><span class="s1">b_dot = _split_linear_solve_args(tangents</span><span class="s3">, </span><span class="s1">const_lengths)</span>

  <span class="s1">num_x_leaves = len(b_dot)</span>
  <span class="s0"># x is a flat tree with possible aux values appended</span>
  <span class="s0"># since x_tree == b_tree == b_dot_tree, we can cut off</span>
  <span class="s0"># aux values with len info provided by b_dot tree here</span>
  <span class="s1">x_leaves</span><span class="s3">, </span><span class="s1">_ = split_list(x</span><span class="s3">, </span><span class="s1">[num_x_leaves])</span>

  <span class="s3">if </span><span class="s1">all(type(p) </span><span class="s3">is </span><span class="s1">ad_util.Zero </span><span class="s3">for </span><span class="s1">p </span><span class="s3">in </span><span class="s1">params_dot.matvec):</span>
    <span class="s0"># no need to evaluate matvec_tangents</span>
    <span class="s1">rhs = b_dot</span>
  <span class="s3">else</span><span class="s1">:</span>
    <span class="s1">matvec_tangents = _tangent_linear_map(</span>
        <span class="s1">core.jaxpr_as_fun(jaxprs.matvec)</span><span class="s3">, </span><span class="s1">params.matvec</span><span class="s3">, </span><span class="s1">params_dot.matvec</span><span class="s3">, </span><span class="s1">*x_leaves)</span>
    <span class="s1">rhs = _map(ad.add_tangents</span><span class="s3">, </span><span class="s1">b_dot</span><span class="s3">, </span><span class="s1">_map(operator.neg</span><span class="s3">, </span><span class="s1">matvec_tangents))</span>

  <span class="s1">x_dot = linear_solve_p.bind(*(_flatten(params) + rhs)</span><span class="s3">, </span><span class="s1">**kwargs)</span>

  <span class="s0"># split into x tangents and aux tangents (these become zero)</span>
  <span class="s1">dx_leaves</span><span class="s3">, </span><span class="s1">daux_leaves = split_list(x_dot</span><span class="s3">, </span><span class="s1">[num_x_leaves])</span>

  <span class="s1">daux_leaves = _map(ad_util.Zero.from_value</span><span class="s3">, </span><span class="s1">daux_leaves)</span>

  <span class="s1">x_dot = dx_leaves + daux_leaves</span>

  <span class="s3">return </span><span class="s1">x</span><span class="s3">, </span><span class="s1">x_dot</span>


<span class="s3">def </span><span class="s1">_linear_solve_transpose_rule(cotangent</span><span class="s3">, </span><span class="s1">*primals</span><span class="s3">, </span><span class="s1">const_lengths</span><span class="s3">, </span><span class="s1">jaxprs):</span>
  <span class="s3">if </span><span class="s1">jaxprs.transpose_solve </span><span class="s3">is None</span><span class="s1">:</span>
    <span class="s3">raise </span><span class="s1">TypeError(</span><span class="s4">'transpose_solve required for backwards mode automatic '</span>
                    <span class="s4">'differentiation of custom_linear_solve'</span><span class="s1">)</span>

  <span class="s1">params</span><span class="s3">, </span><span class="s1">b = _split_linear_solve_args(primals</span><span class="s3">, </span><span class="s1">const_lengths)</span>
  <span class="s0"># split off symbolic zeros in the cotangent if present</span>
  <span class="s1">x_cotangent</span><span class="s3">, </span><span class="s1">_ = split_list(cotangent</span><span class="s3">, </span><span class="s1">[len(b)])</span>
  <span class="s3">assert </span><span class="s1">all(ad.is_undefined_primal(x) </span><span class="s3">for </span><span class="s1">x </span><span class="s3">in </span><span class="s1">b)</span>
  <span class="s1">cotangent_b_full = linear_solve_p.bind(</span>
      <span class="s1">*(_flatten(params.transpose()) + x_cotangent)</span><span class="s3">,</span>
      <span class="s1">const_lengths=const_lengths.transpose()</span><span class="s3">, </span><span class="s1">jaxprs=jaxprs.transpose())</span>
  <span class="s0"># drop aux values in cotangent computation</span>
  <span class="s1">cotangent_b</span><span class="s3">, </span><span class="s1">_ = split_list(cotangent_b_full</span><span class="s3">, </span><span class="s1">[len(b)])</span>
  <span class="s3">return </span><span class="s1">[</span><span class="s3">None</span><span class="s1">] * sum(const_lengths) + cotangent_b</span>


<span class="s3">def </span><span class="s1">_linear_solve_batching_rule(spmd_axis_name</span><span class="s3">, </span><span class="s1">axis_size</span><span class="s3">, </span><span class="s1">axis_name</span><span class="s3">, </span><span class="s1">main_type</span><span class="s3">,</span>
                                <span class="s1">args</span><span class="s3">, </span><span class="s1">dims</span><span class="s3">, </span><span class="s1">const_lengths</span><span class="s3">, </span><span class="s1">jaxprs):</span>
  <span class="s1">orig_bat = [d </span><span class="s3">is not </span><span class="s1">batching.not_mapped </span><span class="s3">for </span><span class="s1">d </span><span class="s3">in </span><span class="s1">dims]</span>

  <span class="s1">params</span><span class="s3">, </span><span class="s1">b = _split_linear_solve_args(args</span><span class="s3">, </span><span class="s1">const_lengths)</span>
  <span class="s1">params_dims</span><span class="s3">, </span><span class="s1">b_dims = _split_linear_solve_args(dims</span><span class="s3">, </span><span class="s1">const_lengths)</span>
  <span class="s1">params_bat</span><span class="s3">, </span><span class="s1">orig_b_bat = _split_linear_solve_args(orig_bat</span><span class="s3">, </span><span class="s1">const_lengths)</span>

  <span class="s1">(matvec</span><span class="s3">, </span><span class="s1">vecmat</span><span class="s3">, </span><span class="s1">solve</span><span class="s3">, </span><span class="s1">solve_t) = jaxprs</span>
  <span class="s1">(matvec_bat</span><span class="s3">, </span><span class="s1">vecmat_bat</span><span class="s3">, </span><span class="s1">solve_bat</span><span class="s3">, </span><span class="s1">solve_t_bat) = params_bat</span>

  <span class="s0"># number of operator out avals is assumed to be the same for matvec/vecmat</span>
  <span class="s1">num_operator_out_avals = len(matvec.out_avals)</span>
  <span class="s1">num_aux = len(solve.out_avals) - num_operator_out_avals</span>
  <span class="s0"># Fixpoint computation of which parts of x and b are batched; we need to</span>
  <span class="s0"># ensure this is consistent between all four jaxprs</span>
  <span class="s1">b_bat = orig_b_bat</span>
  <span class="s1">x_bat = [</span><span class="s3">False</span><span class="s1">] * len(solve.out_avals)</span>
  <span class="s3">for </span><span class="s1">i </span><span class="s3">in </span><span class="s1">range(</span><span class="s5">1 </span><span class="s1">+ len(orig_b_bat) + len(solve.out_avals)):</span>
    <span class="s0"># Apply vecmat and solve -&gt; new batched parts of x</span>
    <span class="s1">solve_jaxpr_batched</span><span class="s3">, </span><span class="s1">solve_x_bat = batching.batch_jaxpr(</span>
        <span class="s1">solve</span><span class="s3">, </span><span class="s1">axis_size</span><span class="s3">, </span><span class="s1">solve_bat + b_bat</span><span class="s3">, </span><span class="s1">instantiate=x_bat</span><span class="s3">,</span>
        <span class="s1">axis_name=axis_name</span><span class="s3">, </span><span class="s1">spmd_axis_name=spmd_axis_name</span><span class="s3">, </span><span class="s1">main_type=main_type)</span>
    <span class="s3">if </span><span class="s1">vecmat </span><span class="s3">is None</span><span class="s1">:</span>
      <span class="s1">vecmat_jaxpr_batched = </span><span class="s3">None</span>
      <span class="s1">x_bat_out = solve_x_bat</span>
    <span class="s3">else</span><span class="s1">:</span>
      <span class="s1">vecmat_jaxpr_batched</span><span class="s3">, </span><span class="s1">vecmat_x_bat = batching.batch_jaxpr(</span>
          <span class="s1">vecmat</span><span class="s3">, </span><span class="s1">axis_size</span><span class="s3">, </span><span class="s1">vecmat_bat + b_bat</span><span class="s3">, </span><span class="s1">instantiate=b_bat</span><span class="s3">,</span>
          <span class="s1">axis_name=axis_name</span><span class="s3">, </span><span class="s1">spmd_axis_name=spmd_axis_name</span><span class="s3">, </span><span class="s1">main_type=main_type)</span>
      <span class="s0"># batch all aux data by default</span>
      <span class="s1">x_bat_out = _map(operator.or_</span><span class="s3">, </span><span class="s1">vecmat_x_bat + [</span><span class="s3">True</span><span class="s1">] * num_aux</span><span class="s3">, </span><span class="s1">solve_x_bat)</span>
    <span class="s0"># keep a slice of only the linear operator part of solve's avals</span>
    <span class="s1">x_bat_noaux = x_bat_out[:num_operator_out_avals]</span>

    <span class="s0"># Apply matvec and solve_t -&gt; new batched parts of b</span>
    <span class="s1">matvec_jaxpr_batched</span><span class="s3">, </span><span class="s1">matvec_b_bat = batching.batch_jaxpr(</span>
        <span class="s1">matvec</span><span class="s3">, </span><span class="s1">axis_size</span><span class="s3">, </span><span class="s1">matvec_bat + x_bat_noaux</span><span class="s3">, </span><span class="s1">instantiate=b_bat</span><span class="s3">,</span>
        <span class="s1">axis_name=axis_name</span><span class="s3">, </span><span class="s1">spmd_axis_name=spmd_axis_name</span><span class="s3">, </span><span class="s1">main_type=main_type)</span>
    <span class="s3">if </span><span class="s1">solve_t </span><span class="s3">is None</span><span class="s1">:</span>
      <span class="s1">solve_t_jaxpr_batched = </span><span class="s3">None</span>
      <span class="s1">b_bat_out = _map(operator.or_</span><span class="s3">, </span><span class="s1">matvec_b_bat</span><span class="s3">, </span><span class="s1">orig_b_bat)</span>
    <span class="s3">else</span><span class="s1">:</span>
      <span class="s1">solve_t_jaxpr_batched</span><span class="s3">, </span><span class="s1">solve_t_b_aux_bat = batching.batch_jaxpr(</span>
          <span class="s1">solve_t</span><span class="s3">, </span><span class="s1">axis_size</span><span class="s3">, </span><span class="s1">solve_t_bat + x_bat_noaux</span><span class="s3">, </span><span class="s1">instantiate=x_bat_out</span><span class="s3">,</span>
          <span class="s1">axis_name=axis_name</span><span class="s3">, </span><span class="s1">spmd_axis_name=spmd_axis_name</span><span class="s3">, </span><span class="s1">main_type=main_type)</span>
      <span class="s3">assert </span><span class="s1">len(solve_t_b_aux_bat) == len(orig_b_bat) + num_aux</span>
      <span class="s1">solve_t_b_bat</span><span class="s3">, </span><span class="s1">_ = split_list(solve_t_b_aux_bat</span><span class="s3">, </span><span class="s1">[len(orig_b_bat)])</span>
      <span class="s1">b_bat_out = _map(</span><span class="s3">lambda </span><span class="s1">m</span><span class="s3">, </span><span class="s1">s</span><span class="s3">, </span><span class="s1">o: m </span><span class="s3">or </span><span class="s1">s </span><span class="s3">or </span><span class="s1">o</span><span class="s3">, </span><span class="s1">matvec_b_bat</span><span class="s3">, </span><span class="s1">solve_t_b_bat</span><span class="s3">,</span>
                      <span class="s1">orig_b_bat)</span>
    <span class="s3">if </span><span class="s1">x_bat_out == x_bat </span><span class="s3">and </span><span class="s1">b_bat_out == b_bat:</span>
      <span class="s3">break</span>
    <span class="s3">else</span><span class="s1">:</span>
      <span class="s1">x_bat = x_bat_out</span>
      <span class="s1">b_bat = b_bat_out</span>
  <span class="s3">else</span><span class="s1">:</span>
    <span class="s3">assert False, </span><span class="s4">&quot;Fixedpoint not reached&quot;</span>

  <span class="s1">batched_jaxprs = _LinearSolveTuple(matvec_jaxpr_batched</span><span class="s3">, </span><span class="s1">vecmat_jaxpr_batched</span><span class="s3">,</span>
                                     <span class="s1">solve_jaxpr_batched</span><span class="s3">, </span><span class="s1">solve_t_jaxpr_batched)</span>

  <span class="s0"># Move batched axes to the front</span>
  <span class="s1">new_params = [</span>
      <span class="s1">batching.moveaxis(x</span><span class="s3">, </span><span class="s1">d</span><span class="s3">, </span><span class="s5">0</span><span class="s1">)</span>
      <span class="s3">if </span><span class="s1">d </span><span class="s3">is not </span><span class="s1">batching.not_mapped </span><span class="s3">and </span><span class="s1">d != </span><span class="s5">0 </span><span class="s3">else </span><span class="s1">x</span>
      <span class="s3">for </span><span class="s1">x</span><span class="s3">, </span><span class="s1">d </span><span class="s3">in </span><span class="s1">zip(_flatten(params)</span><span class="s3">, </span><span class="s1">_flatten(params_dims))</span>
  <span class="s1">]</span>
  <span class="s0"># Broadcast out b if necessary</span>
  <span class="s1">new_b = [</span>
      <span class="s1">batching.broadcast(x</span><span class="s3">, </span><span class="s1">axis_size</span><span class="s3">, </span><span class="s5">0</span><span class="s1">) </span><span class="s3">if </span><span class="s1">now_bat </span><span class="s3">and not </span><span class="s1">was_bat </span><span class="s3">else</span>
      <span class="s1">batching.moveaxis(x</span><span class="s3">, </span><span class="s1">d</span><span class="s3">, </span><span class="s5">0</span><span class="s1">) </span><span class="s3">if </span><span class="s1">now_bat </span><span class="s3">and </span><span class="s1">d != </span><span class="s5">0 </span><span class="s3">else </span><span class="s1">x</span>
      <span class="s3">for </span><span class="s1">x</span><span class="s3">, </span><span class="s1">d</span><span class="s3">, </span><span class="s1">was_bat</span><span class="s3">, </span><span class="s1">now_bat </span><span class="s3">in </span><span class="s1">zip(b</span><span class="s3">, </span><span class="s1">b_dims</span><span class="s3">, </span><span class="s1">orig_b_bat</span><span class="s3">, </span><span class="s1">b_bat)</span>
  <span class="s1">]</span>

  <span class="s1">outs = linear_solve_p.bind(</span>
      <span class="s1">*(new_params + new_b)</span><span class="s3">,</span>
      <span class="s1">const_lengths=const_lengths</span><span class="s3">,</span>
      <span class="s1">jaxprs=batched_jaxprs)</span>
  <span class="s1">out_dims = [</span><span class="s5">0 </span><span class="s3">if </span><span class="s1">batched </span><span class="s3">else </span><span class="s1">batching.not_mapped </span><span class="s3">for </span><span class="s1">batched </span><span class="s3">in </span><span class="s1">solve_x_bat]</span>
  <span class="s3">return </span><span class="s1">outs</span><span class="s3">, </span><span class="s1">out_dims</span>


<span class="s1">linear_solve_p = core.AxisPrimitive(</span><span class="s4">'custom_linear_solve'</span><span class="s1">)</span>
<span class="s1">linear_solve_p.multiple_results = </span><span class="s3">True</span>
<span class="s1">linear_solve_p.def_impl(_custom_linear_solve_impl)</span>
<span class="s1">linear_solve_p.def_abstract_eval(_linear_solve_abstract_eval)</span>
<span class="s1">ad.primitive_jvps[linear_solve_p] = _custom_linear_solve_jvp</span>
<span class="s1">xla.register_initial_style_primitive(linear_solve_p)</span>
<span class="s1">mlir.register_lowering(</span>
    <span class="s1">linear_solve_p</span><span class="s3">, </span><span class="s1">mlir.lower_fun(_custom_linear_solve_impl</span><span class="s3">,</span>
                                   <span class="s1">multiple_results=</span><span class="s3">True</span><span class="s1">))</span>
<span class="s1">ad.primitive_transposes[linear_solve_p] = _linear_solve_transpose_rule</span>
<span class="s1">batching.axis_primitive_batchers[linear_solve_p] = partial(_linear_solve_batching_rule</span><span class="s3">, None</span><span class="s1">)</span>
<span class="s1">batching.spmd_axis_primitive_batchers[linear_solve_p] = _linear_solve_batching_rule</span>
</pre>
</body>
</html>