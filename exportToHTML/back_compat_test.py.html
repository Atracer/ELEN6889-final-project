<html>
<head>
<title>back_compat_test.py</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<style type="text/css">
.s0 { color: #808080;}
.s1 { color: #a9b7c6;}
.s2 { color: #629755; font-style: italic;}
.s3 { color: #cc7832;}
.s4 { color: #6897bb;}
.s5 { color: #6a8759;}
.s6 { color: #a5c261;}
</style>
</head>
<body bgcolor="#2b2b2b">
<table CELLSPACING=0 CELLPADDING=5 COLS=1 WIDTH="100%" BGCOLOR="#606060" >
<tr><td><center>
<font face="Arial, Helvetica" color="#000000">
back_compat_test.py</font>
</center></td></tr></table>
<pre><span class="s0"># Copyright 2023 The JAX Authors.</span>
<span class="s0">#</span>
<span class="s0"># Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);</span>
<span class="s0"># you may not use this file except in compliance with the License.</span>
<span class="s0"># You may obtain a copy of the License at</span>
<span class="s0">#</span>
<span class="s0">#     https://www.apache.org/licenses/LICENSE-2.0</span>
<span class="s0">#</span>
<span class="s0"># Unless required by applicable law or agreed to in writing, software</span>
<span class="s0"># distributed under the License is distributed on an &quot;AS IS&quot; BASIS,</span>
<span class="s0"># WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span>
<span class="s0"># See the License for the specific language governing permissions and</span>
<span class="s0"># limitations under the License.</span>
<span class="s2">&quot;&quot;&quot;Tests for backwards compatibility of custom calls. 
 
Since we have to guarantee 6 months of backward compatibility for the 
JAX serialized format, we need to guarantee that custom calls continue to 
work as before. We test this here. 
 
The tests in this file refer to the test data in ./back_compat_testdata. 
There is one test for each version of a custom call target, e.g., 
`test_ducc_fft` tests the FFT custom calls on CPU. 
Only custom call targets tested here should be listed in 
jax_export._CUSTOM_CALL_TARGETS_GUARANTEED_STABLE. All other custom 
call targets will result in an error when encountered during serialization. 
 
Once we stop using a custom call target in JAX, you can remove it from the 
_CUSTOM_CALL_TARGETS_GUARANTEED_STABLE and you can add a comment to the 
test here to remove it after 6 months. 
 
** To create a new test ** 
 
Write the JAX function `func` that exercises the custom call `foo_call` you 
want, then pick some inputs, and then add this to the new test to get started. 
 
  def test_foo_call(self): 
    def func(...): ... 
    inputs = (...,)  # Tuple of nd.array, keep it small, perhaps generate the 
                     # inputs in `func`. 
    data = dataclasses.replace(dummy_data, inputs=inputs, 
                               platform=default_jax_backend()) 
    self.run_one_test(func, data) 
 
The test will fail, but will save to a file the test data you will need. The 
file name will be printed in the logs. Create a new 
file ./back_compat_testdata/cuda_foo_call.py and paste the test data that 
you will see printed in the logs. You may want to 
edit the serialization string to remove any pathnames that may be included at 
the end, or gxxxxx3 at the beginning. 
 
Name the literal `data_YYYYY_MM_DD` to include the date of serializaton 
(for readability only). Then add here: 
 
  from jax.experimental.jax2tf.tests.back_compat_testdata import foo_call 
  def test_foo_call(self): 
    def func(...): ... 
    data = load_testdata(foo_call.data_YYYY_MM_DD) 
    self.run_one_test(func, data) 
 
&quot;&quot;&quot;</span>
<span class="s3">import </span><span class="s1">dataclasses</span>
<span class="s3">import </span><span class="s1">datetime</span>
<span class="s3">from </span><span class="s1">functools </span><span class="s3">import </span><span class="s1">partial</span>
<span class="s3">import </span><span class="s1">itertools</span>
<span class="s3">import </span><span class="s1">os</span>
<span class="s3">import </span><span class="s1">re</span>
<span class="s3">import </span><span class="s1">sys</span>
<span class="s3">from </span><span class="s1">typing </span><span class="s3">import </span><span class="s1">Any</span><span class="s3">, </span><span class="s1">Callable</span><span class="s3">, </span><span class="s1">Dict</span><span class="s3">, </span><span class="s1">Iterable</span><span class="s3">, </span><span class="s1">List</span><span class="s3">, </span><span class="s1">Sequence</span>

<span class="s0"># from absl import logging</span>
<span class="s3">from </span><span class="s1">absl.testing </span><span class="s3">import </span><span class="s1">absltest</span><span class="s3">, </span><span class="s1">parameterized</span>
<span class="s3">from </span><span class="s1">absl </span><span class="s3">import </span><span class="s1">logging</span>

<span class="s3">import </span><span class="s1">numpy </span><span class="s3">as </span><span class="s1">np</span>
<span class="s0"># Import some NumPy symbols so that we can parse repr(ndarray).</span>
<span class="s3">from </span><span class="s1">numpy </span><span class="s3">import </span><span class="s1">array</span><span class="s3">, </span><span class="s1">float32</span>

<span class="s3">import </span><span class="s1">jax</span>
<span class="s3">from </span><span class="s1">jax.config </span><span class="s3">import </span><span class="s1">config</span>
<span class="s3">from </span><span class="s1">jax </span><span class="s3">import </span><span class="s1">lax</span>
<span class="s3">from </span><span class="s1">jax.experimental </span><span class="s3">import </span><span class="s1">jax2tf</span>
<span class="s3">from </span><span class="s1">jax.experimental.jax2tf.tests.back_compat_testdata </span><span class="s3">import </span><span class="s1">cpu_ducc_fft</span>
<span class="s3">from </span><span class="s1">jax.experimental.jax2tf.tests.back_compat_testdata </span><span class="s3">import </span><span class="s1">cpu_lapack_geqrf</span>
<span class="s3">from </span><span class="s1">jax.experimental.jax2tf.tests.back_compat_testdata </span><span class="s3">import </span><span class="s1">cpu_lapack_syev</span>
<span class="s3">from </span><span class="s1">jax.experimental.jax2tf.tests.back_compat_testdata </span><span class="s3">import </span><span class="s1">cuda_cusolver_geqrf</span>
<span class="s3">from </span><span class="s1">jax.experimental.jax2tf.tests.back_compat_testdata </span><span class="s3">import </span><span class="s1">cuda_cusolver_syev</span>
<span class="s3">from </span><span class="s1">jax.experimental.jax2tf.tests.back_compat_testdata </span><span class="s3">import </span><span class="s1">cuda_threefry2x32</span>
<span class="s3">from </span><span class="s1">jax.experimental.jax2tf.tests.back_compat_testdata </span><span class="s3">import </span><span class="s1">tpu_Eigh</span>
<span class="s3">from </span><span class="s1">jax.experimental.jax2tf.tests.back_compat_testdata </span><span class="s3">import </span><span class="s1">tpu_Lu</span>
<span class="s3">from </span><span class="s1">jax.experimental.jax2tf.tests.back_compat_testdata </span><span class="s3">import </span><span class="s1">tpu_Qr</span>
<span class="s3">from </span><span class="s1">jax.experimental.jax2tf.tests.back_compat_testdata </span><span class="s3">import </span><span class="s1">tpu_Sharding</span>

<span class="s3">from </span><span class="s1">jax.experimental </span><span class="s3">import </span><span class="s1">pjit</span>
<span class="s3">from </span><span class="s1">jax.experimental.shard_map </span><span class="s3">import </span><span class="s1">shard_map</span>
<span class="s3">import </span><span class="s1">jax.numpy </span><span class="s3">as </span><span class="s1">jnp</span>

<span class="s3">from </span><span class="s1">jax.sharding </span><span class="s3">import </span><span class="s1">Mesh</span>
<span class="s3">from </span><span class="s1">jax.sharding </span><span class="s3">import </span><span class="s1">PartitionSpec </span><span class="s3">as </span><span class="s1">P</span>

<span class="s3">from </span><span class="s1">jax._src </span><span class="s3">import </span><span class="s1">core</span>
<span class="s3">from </span><span class="s1">jax._src </span><span class="s3">import </span><span class="s1">test_util </span><span class="s3">as </span><span class="s1">jtu</span>
<span class="s3">from </span><span class="s1">jax._src </span><span class="s3">import </span><span class="s1">xla_bridge </span><span class="s3">as </span><span class="s1">xb</span>

<span class="s3">import </span><span class="s1">tensorflow </span><span class="s3">as </span><span class="s1">tf  </span><span class="s0"># type: ignore[import]</span>

<span class="s0"># pylint: disable=g-direct-tensorflow-import</span>
<span class="s3">from </span><span class="s1">tensorflow.compiler.tf2xla.python </span><span class="s3">import </span><span class="s1">xla </span><span class="s3">as </span><span class="s1">tfxla  </span><span class="s0"># type: ignore[import]</span>
<span class="s0"># pylint: enable=g-direct-tensorflow-import</span>


<span class="s1">config.parse_flags_with_absl()</span>


<span class="s3">def </span><span class="s1">default_jax_backend() -&gt; str:</span>
  <span class="s0"># Canonicalize to turn into &quot;cuda&quot; or &quot;rocm&quot;</span>
  <span class="s3">return </span><span class="s1">xb.canonicalize_platform(jax.default_backend())</span>

<span class="s1">CURRENT_TESTDATA_VERSION = </span><span class="s4">1</span>

<span class="s1">@dataclasses.dataclass</span>
<span class="s3">class </span><span class="s1">CompatTestData:</span>
  <span class="s1">testdata_version: int</span>
  <span class="s1">platform: str  </span><span class="s0"># One of: &quot;cpu&quot;, &quot;tpu&quot;, &quot;cuda&quot;, &quot;rocm&quot;</span>
  <span class="s1">custom_call_targets: List[str]</span>
  <span class="s1">serialized_date: datetime.date  </span><span class="s0"># e.g., datetime.date(2023, 3, 9)</span>
  <span class="s1">inputs: Sequence[np.ndarray]</span>
  <span class="s1">expected_outputs: Sequence[np.ndarray]</span>
  <span class="s1">mlir_module_text: str</span>
  <span class="s1">mlir_module_serialized: bytes</span>
  <span class="s1">xla_call_module_version: int  </span><span class="s0"># The version of XlaCallModule to use for testing</span>


<span class="s0"># The dummy_data is used for getting started for adding a new test and for</span>
<span class="s0"># testing the helper functions.</span>

<span class="s0"># Pasted from the test output (see module docstring)</span>
<span class="s1">dummy_data_dict = dict(</span>
    <span class="s1">testdata_version = CURRENT_TESTDATA_VERSION</span><span class="s3">,</span>
    <span class="s1">platform=</span><span class="s5">&quot;cpu&quot;</span><span class="s3">,</span>
    <span class="s1">custom_call_targets=[]</span><span class="s3">,</span>
    <span class="s1">serialized_date=datetime.date(</span><span class="s4">2023</span><span class="s3">, </span><span class="s4">3</span><span class="s3">, </span><span class="s4">15</span><span class="s1">)</span><span class="s3">,</span>
    <span class="s1">inputs=(array(</span><span class="s4">0.0</span><span class="s3">, </span><span class="s1">dtype=float32)</span><span class="s3">,</span><span class="s1">)</span><span class="s3">,</span>
    <span class="s1">expected_outputs=(array(</span><span class="s4">0.0</span><span class="s3">, </span><span class="s1">dtype=float32)</span><span class="s3">,</span><span class="s1">)</span><span class="s3">,</span>
    <span class="s1">mlir_module_text=</span><span class="s5">&quot;&quot;&quot; 
  module @jit_sin { 
  func.func public @main(%arg0: tensor&lt;f32&gt;) -&gt; tensor&lt;f32&gt; { 
    %0 = stablehlo.sine %arg0 : tensor&lt;f32&gt; 
    return %0 : tensor&lt;f32&gt; 
  } 
} 
&quot;&quot;&quot;</span><span class="s3">,</span>
    <span class="s1">mlir_module_serialized=</span><span class="s6">b&quot;ML</span><span class="s3">\xef</span><span class="s6">R</span><span class="s3">\x03</span><span class="s6">MLIRxxx-trunk</span><span class="s3">\x00\x01\x17\x05\x01\x05\x01\x03\x05\x03\x07\x07\t\x0b\x03</span><span class="s6">K5</span><span class="s3">\x07\x01\x1b\x07\x0b\x13\x0b</span><span class="s6">3</span><span class="s3">\x0b\x0b\x0b\x0b\x0f\x0b\x13\x0b\x03\x1b\x0f\x1b\x0b\x0b\x0b\x0b\x0b\x0f\x13\x0b\x0b\x0b\x0b\x03\x07\x0f\x17\x07\x02\xa7\x1f\x05\r\x03\x03\x03\x07\x05\x0f\x03\x0b\x0b\x1b\r</span><span class="s6">'</span><span class="s3">\x0f</span><span class="s6">)</span><span class="s3">\x03</span><span class="s6">1</span><span class="s3">\x11</span><span class="s6">3</span><span class="s3">\x05\x11\x05\x13\x05\x15\x05\x17\x1d\x15\x17\x05\x19\x17\x19\xef\x01\x05\x1b\x03\x03\x1d\r\x05\x1f</span><span class="s6">!#%</span><span class="s3">\x1d\x1d\x1d\x1f\x1d</span><span class="s6">!</span><span class="s3">\x1d</span><span class="s6">##</span><span class="s3">\x03\x03\x03</span><span class="s6">+</span><span class="s3">\r\x03</span><span class="s6">-/</span><span class="s3">\x1d</span><span class="s6">%</span><span class="s3">\x1d</span><span class="s6">'</span><span class="s3">\x1d</span><span class="s6">)</span><span class="s3">\x1d</span><span class="s6">+)</span><span class="s3">\x01\x05\x11\x03\x01\x03\x01\t\x04</span><span class="s6">A</span><span class="s3">\x05\x01\x11\x01\x05\x07\x03\x01\x05\x03\x11\x01\t\x05\x03\x05\x0b\x03\x01\x01\x05\x06\x13\x03\x01\x03\x01\x07\x04\x01\x03\x03\x06\x03\x01\x05\x01\x00\x9a\x04</span><span class="s6">-</span><span class="s3">\x0f\x0b\x03</span><span class="s6">!</span><span class="s3">\x1b\x1d\x05\x1b\x83</span><span class="s6">/</span><span class="s3">\x1f\x15\x1d\x15\x11\x13\x15\x11\x11\x0f\x0b\x11</span><span class="s6">builtin</span><span class="s3">\x00</span><span class="s6">vhlo</span><span class="s3">\x00</span><span class="s6">module</span><span class="s3">\x00</span><span class="s6">func_v1</span><span class="s3">\x00</span><span class="s6">sine_v1</span><span class="s3">\x00</span><span class="s6">return_v1</span><span class="s3">\x00</span><span class="s6">sym_name</span><span class="s3">\x00</span><span class="s6">jit_sin</span><span class="s3">\x00</span><span class="s6">arg_attrs</span><span class="s3">\x00</span><span class="s6">function_type</span><span class="s3">\x00</span><span class="s6">res_attrs</span><span class="s3">\x00</span><span class="s6">sym_visibility</span><span class="s3">\x00</span><span class="s6">jit(sin)/jit(main)/sin</span><span class="s3">\x00</span><span class="s6">third_party/py/jax/experimental/jax2tf/tests/back_compat_test.py</span><span class="s3">\x00</span><span class="s6">jax.arg_info</span><span class="s3">\x00</span><span class="s6">x</span><span class="s3">\x00</span><span class="s6">mhlo.sharding</span><span class="s3">\x00</span><span class="s6">{replicated}</span><span class="s3">\x00</span><span class="s6">jax.result_info</span><span class="s3">\x00\x00</span><span class="s6">main</span><span class="s3">\x00</span><span class="s6">public</span><span class="s3">\x00</span><span class="s6">&quot;</span><span class="s3">,</span>
    <span class="s1">xla_call_module_version=</span><span class="s4">4</span><span class="s3">,</span>
<span class="s1">)  </span><span class="s0"># End paste</span>


<span class="s3">def </span><span class="s1">load_testdata(testdata_dict: Dict[str</span><span class="s3">, </span><span class="s1">Any]) -&gt; CompatTestData:</span>
  <span class="s3">if </span><span class="s1">testdata_dict[</span><span class="s5">&quot;testdata_version&quot;</span><span class="s1">] == CURRENT_TESTDATA_VERSION:</span>
    <span class="s3">return </span><span class="s1">CompatTestData(**testdata_dict)</span>
  <span class="s3">else</span><span class="s1">:</span>
    <span class="s3">raise </span><span class="s1">NotImplementedError(</span><span class="s5">&quot;testdata_version not recognized: &quot; </span><span class="s1">+</span>
                              <span class="s1">testdata_dict[</span><span class="s5">&quot;testdata_version&quot;</span><span class="s1">])</span>


<span class="s3">def </span><span class="s1">load_testdata_nested(testdata_nest) -&gt; Iterable[CompatTestData]:</span>
  <span class="s0"># Load all the CompatTestData in a Python nest.</span>
  <span class="s3">if </span><span class="s1">isinstance(testdata_nest</span><span class="s3">, </span><span class="s1">dict) </span><span class="s3">and </span><span class="s5">&quot;testdata_version&quot; </span><span class="s3">in </span><span class="s1">testdata_nest:</span>
    <span class="s3">yield </span><span class="s1">load_testdata(testdata_nest)</span>
  <span class="s3">elif </span><span class="s1">isinstance(testdata_nest</span><span class="s3">, </span><span class="s1">dict):</span>
    <span class="s3">for </span><span class="s1">e </span><span class="s3">in </span><span class="s1">testdata_nest.values():</span>
      <span class="s3">yield from </span><span class="s1">load_testdata_nested(e)</span>
  <span class="s3">elif </span><span class="s1">isinstance(testdata_nest</span><span class="s3">, </span><span class="s1">list):</span>
    <span class="s3">for </span><span class="s1">e </span><span class="s3">in </span><span class="s1">testdata_nest:</span>
      <span class="s3">yield from </span><span class="s1">load_testdata_nested(e)</span>
  <span class="s3">else</span><span class="s1">:</span>
    <span class="s3">assert False, </span><span class="s1">testdata_nest</span>

<span class="s1">dummy_data = load_testdata(dummy_data_dict)</span>


<span class="s3">class </span><span class="s1">CompatTest(jtu.JaxTestCase):</span>

  <span class="s3">def </span><span class="s1">run_one_test(self</span><span class="s3">, </span><span class="s1">func: Callable[...</span><span class="s3">, </span><span class="s1">jax.Array]</span><span class="s3">,</span>
                   <span class="s1">data: CompatTestData</span><span class="s3">,</span>
                   <span class="s1">run_tf=</span><span class="s3">None,</span>
                   <span class="s1">rtol=</span><span class="s3">None</span><span class="s1">):</span>
    <span class="s2">&quot;&quot;&quot;Run one compatibility test. 
 
    Args: 
      func: the JAX function to serialize and run 
      data: the test data 
      run_tf: (optional) a function to invoke the XlaCallModule TF op. Takes 
        a TensorFlow callable and the arguments. 
      rtol: relative tolerance for numerical comparisons 
    &quot;&quot;&quot;</span>
    <span class="s3">if </span><span class="s1">default_jax_backend() != data.platform:</span>
      <span class="s1">self.skipTest(</span><span class="s5">f&quot;Test enabled only for </span><span class="s3">{</span><span class="s1">data.platform</span><span class="s3">}</span><span class="s5">&quot;</span><span class="s1">)</span>

    <span class="s0"># Check that it runs in JAX native</span>
    <span class="s1">res_from_jax = jax.jit(func)(*data.inputs)</span>
    <span class="s3">if not </span><span class="s1">isinstance(res_from_jax</span><span class="s3">, </span><span class="s1">(list</span><span class="s3">, </span><span class="s1">tuple)):</span>
      <span class="s1">res_from_jax = (res_from_jax</span><span class="s3">,</span><span class="s1">)</span>
    <span class="s1">res_from_jax = tuple(np.array(a) </span><span class="s3">for </span><span class="s1">a </span><span class="s3">in </span><span class="s1">res_from_jax)</span>

    <span class="s0"># Use the native exporter, to make sure we get the proper serialized module.</span>
    <span class="s1">exported = jax2tf.jax_export.serialize_native(</span>
        <span class="s1">jax.jit(func)</span><span class="s3">,</span>
        <span class="s1">[core.ShapedArray(a.shape</span><span class="s3">, </span><span class="s1">a.dtype) </span><span class="s3">for </span><span class="s1">a </span><span class="s3">in </span><span class="s1">data.inputs]</span><span class="s3">,</span>
        <span class="s1">lowering_platform=default_jax_backend()</span><span class="s3">,</span>
        <span class="s0"># Must turn off strict checks because the custom calls may be unallowed.</span>
        <span class="s1">strict_checks=</span><span class="s3">False,</span>
    <span class="s1">)</span>

    <span class="s1">module_str = str(exported.mlir_module)</span>
    <span class="s1">custom_call_re = </span><span class="s5">r&quot;stablehlo.custom_call\s*@([^\(]+)\(&quot;</span>
    <span class="s1">custom_call_targets = sorted(</span>
        <span class="s1">list(set(re.findall(custom_call_re</span><span class="s3">, </span><span class="s1">module_str)))</span>
    <span class="s1">)</span>
    <span class="s1">np.set_printoptions(threshold=sys.maxsize</span><span class="s3">, </span><span class="s1">floatmode=</span><span class="s5">&quot;unique&quot;</span><span class="s1">)</span>
    <span class="s0"># Print the test data to simplify updating the test</span>
    <span class="s1">updated_testdata = </span><span class="s5">f&quot;&quot;&quot;</span>
<span class="s5"># Pasted from the test output (see back_compat_test.py module docstring)</span>
<span class="s5">data_</span><span class="s3">{</span><span class="s1">datetime.date.today().strftime(</span><span class="s5">'%Y_%m_%d'</span><span class="s1">)</span><span class="s3">} </span><span class="s5">= dict(</span>
    <span class="s5">testdata_version=</span><span class="s3">{</span><span class="s1">CURRENT_TESTDATA_VERSION</span><span class="s3">}</span><span class="s5">,</span>
    <span class="s5">platform=</span><span class="s3">{</span><span class="s1">repr(default_jax_backend())</span><span class="s3">}</span><span class="s5">,</span>
    <span class="s5">custom_call_targets=</span><span class="s3">{</span><span class="s1">repr(custom_call_targets)</span><span class="s3">}</span><span class="s5">,</span>
    <span class="s5">serialized_date=</span><span class="s3">{</span><span class="s1">repr(datetime.date.today())</span><span class="s3">}</span><span class="s5">,</span>
    <span class="s5">inputs=</span><span class="s3">{</span><span class="s1">repr(data.inputs)</span><span class="s3">}</span><span class="s5">,</span>
    <span class="s5">expected_outputs=</span><span class="s3">{</span><span class="s1">repr(res_from_jax)</span><span class="s3">}</span><span class="s5">,</span>
    <span class="s5">mlir_module_text=</span><span class="s3">\&quot;\&quot;\&quot;\n{</span><span class="s1">module_str</span><span class="s3">}\&quot;\&quot;\&quot;</span><span class="s5">,</span>
    <span class="s5">mlir_module_serialized=</span><span class="s3">{</span><span class="s1">repr(exported.mlir_module_serialized)</span><span class="s3">}</span><span class="s5">,</span>
    <span class="s5">xla_call_module_version=</span><span class="s3">{</span><span class="s1">exported.xla_call_module_version</span><span class="s3">}</span><span class="s5">,</span>
<span class="s5">)  # End paste</span>
<span class="s5">&quot;&quot;&quot;</span>
    <span class="s1">output_dir = os.getenv(</span><span class="s5">&quot;TEST_UNDECLARED_OUTPUTS_DIR&quot;</span><span class="s3">,</span>
                           <span class="s5">&quot;/tmp/back_compat_testdata&quot;</span><span class="s1">)</span>
    <span class="s1">output_file = os.path.join(output_dir</span><span class="s3">, </span><span class="s5">f&quot;</span><span class="s3">{</span><span class="s1">self._testMethodName</span><span class="s3">}</span><span class="s5">.py&quot;</span><span class="s1">)</span>
    <span class="s1">logging.info(</span><span class="s5">&quot;Writing the up-to-date testdata at %s&quot;</span><span class="s3">, </span><span class="s1">output_file)</span>
    <span class="s3">with </span><span class="s1">open(output_file</span><span class="s3">, </span><span class="s5">&quot;w&quot;</span><span class="s1">) </span><span class="s3">as </span><span class="s1">f:</span>
      <span class="s1">f.write(updated_testdata)</span>

    <span class="s3">if </span><span class="s1">rtol </span><span class="s3">is not None</span><span class="s1">:</span>
      <span class="s1">rtol = </span><span class="s4">1.e-7</span>
    <span class="s1">self.assertAllClose(res_from_jax</span><span class="s3">, </span><span class="s1">data.expected_outputs</span><span class="s3">, </span><span class="s1">rtol=rtol)</span>

    <span class="s1">res_serialized = self.run_serialized(data</span><span class="s3">, </span><span class="s1">run_tf=run_tf)</span>
    <span class="s1">logging.info(</span><span class="s5">&quot;Result of serialized run is %s&quot;</span><span class="s3">, </span><span class="s1">res_serialized)</span>
    <span class="s1">self.assertAllClose(res_serialized</span><span class="s3">, </span><span class="s1">data.expected_outputs)</span>
    <span class="s1">self.assertListEqual(custom_call_targets</span><span class="s3">, </span><span class="s1">data.custom_call_targets)</span>

  <span class="s3">def </span><span class="s1">run_serialized(self</span><span class="s3">, </span><span class="s1">data: CompatTestData</span><span class="s3">, </span><span class="s1">run_tf=</span><span class="s3">None</span><span class="s1">):</span>
    <span class="s0"># Run the serialized module. For now, use XlaCallModule. This has the</span>
    <span class="s0"># disadvantage that it brings TF and jax2tf in the picture, but has the</span>
    <span class="s0"># advantage that it is simple (e.g., XlaCallModule already has the</span>
    <span class="s0"># machinery to deserialize and run), and also it is the way users actually</span>
    <span class="s0"># run serialized modules today.</span>
    <span class="s0"># TODO(necula): come up with a JAX-native way of running serialized modules.</span>
    <span class="s1">tf_preferred_devices = (</span>
        <span class="s1">tf.config.list_logical_devices(</span><span class="s5">&quot;TPU&quot;</span><span class="s1">)</span>
        <span class="s1">+ tf.config.list_logical_devices(</span><span class="s5">&quot;GPU&quot;</span><span class="s1">)</span>
        <span class="s1">+ tf.config.list_logical_devices()</span>
    <span class="s1">)</span>
    <span class="s0"># We need --config=cuda build flag for TF to see the GPUs</span>
    <span class="s1">self.assertEqual(</span>
        <span class="s1">jtu.device_under_test().upper()</span><span class="s3">, </span><span class="s1">tf_preferred_devices[</span><span class="s4">0</span><span class="s1">].device_type</span>
    <span class="s1">)</span>

    <span class="s3">def </span><span class="s1">f_tf(*args_tf):</span>
      <span class="s3">return </span><span class="s1">tfxla.call_module(</span>
          <span class="s1">args_tf</span><span class="s3">,</span>
          <span class="s1">version=data.xla_call_module_version</span><span class="s3">,</span>
          <span class="s1">Tout=[r.dtype </span><span class="s3">for </span><span class="s1">r </span><span class="s3">in </span><span class="s1">res_tf]</span><span class="s3">,</span>
          <span class="s1">Sout=[r.shape </span><span class="s3">for </span><span class="s1">r </span><span class="s3">in </span><span class="s1">res_tf]</span><span class="s3">,</span>
          <span class="s1">module=data.mlir_module_serialized</span><span class="s3">,</span>
          <span class="s1">platforms=[data.platform.upper()])</span>

    <span class="s0"># We need this to run the TPU code on the TPU</span>
    <span class="s3">with </span><span class="s1">tf.device(tf_preferred_devices[</span><span class="s4">0</span><span class="s1">]):</span>
      <span class="s1">args_tf = [tf.constant(a) </span><span class="s3">for </span><span class="s1">a </span><span class="s3">in </span><span class="s1">data.inputs]</span>
      <span class="s1">res_tf = [tf.constant(r) </span><span class="s3">for </span><span class="s1">r </span><span class="s3">in </span><span class="s1">data.expected_outputs]</span>
      <span class="s3">if </span><span class="s1">run_tf </span><span class="s3">is not None</span><span class="s1">:</span>
        <span class="s1">res = run_tf(f_tf</span><span class="s3">, </span><span class="s1">*args_tf)</span>
      <span class="s3">else</span><span class="s1">:</span>
        <span class="s1">res = f_tf(*args_tf)</span>
      <span class="s3">return </span><span class="s1">tuple(r.numpy() </span><span class="s3">for </span><span class="s1">r </span><span class="s3">in </span><span class="s1">res)</span>

  <span class="s3">def </span><span class="s1">test_dummy(self):</span>
    <span class="s0"># Tests the test mechanism. Let this test run on all platforms</span>
    <span class="s1">platform_dummy_data = dataclasses.replace(</span>
        <span class="s1">dummy_data</span><span class="s3">, </span><span class="s1">platform=default_jax_backend())</span>
    <span class="s1">self.run_one_test(jnp.sin</span><span class="s3">, </span><span class="s1">platform_dummy_data)</span>

  <span class="s3">def </span><span class="s1">test_detect_different_output(self):</span>
    <span class="s0"># Test the detection mechanism. Let this test run on all platforms</span>
    <span class="s1">platform_dummy_data = dataclasses.replace(</span>
        <span class="s1">dummy_data</span><span class="s3">,</span>
        <span class="s1">platform=default_jax_backend()</span><span class="s3">,</span>
        <span class="s1">expected_outputs=(np.array(</span><span class="s4">2.0</span><span class="s3">, </span><span class="s1">dtype=np.float32)</span><span class="s3">,</span><span class="s1">))</span>
    <span class="s3">with </span><span class="s1">self.assertRaisesRegex(AssertionError</span><span class="s3">, </span><span class="s5">&quot;Not equal to tolerance&quot;</span><span class="s1">):</span>
      <span class="s1">self.run_one_test(jnp.sin</span><span class="s3">, </span><span class="s1">platform_dummy_data)</span>

  <span class="s3">def </span><span class="s1">test_detect_different_custom_calls(self):</span>
    <span class="s0"># Test the detection mechanism. Let this test run on all platforms</span>
    <span class="s1">platform_dummy_data = dataclasses.replace(</span>
        <span class="s1">dummy_data</span><span class="s3">,</span>
        <span class="s1">platform=default_jax_backend()</span><span class="s3">,</span>
        <span class="s1">custom_call_targets=[</span><span class="s5">&quot;missing&quot;</span><span class="s1">])</span>
    <span class="s3">with </span><span class="s1">self.assertRaisesRegex(AssertionError</span><span class="s3">, </span><span class="s5">&quot;Lists differ&quot;</span><span class="s1">):</span>
      <span class="s1">self.run_one_test(jnp.sin</span><span class="s3">, </span><span class="s1">platform_dummy_data)</span>

  <span class="s3">def </span><span class="s1">test_custom_call_coverage(self):</span>
    <span class="s1">targets_to_cover = set(jax2tf.jax_export._CUSTOM_CALL_TARGETS_GUARANTEED_STABLE)</span>
    <span class="s0"># Add here all the testdatas that should cover the targets guaranteed</span>
    <span class="s0"># stable</span>
    <span class="s1">covering_testdatas = [</span>
        <span class="s1">cpu_ducc_fft.data_2023_03_17</span><span class="s3">, </span><span class="s1">cpu_lapack_syev.data_2023_03_17</span><span class="s3">,</span>
        <span class="s1">cpu_lapack_geqrf.data_2023_03_17</span><span class="s3">, </span><span class="s1">cuda_threefry2x32.data_2023_03_15</span><span class="s3">,</span>
        <span class="s1">cuda_cusolver_geqrf.data_2023_03_18</span><span class="s3">, </span><span class="s1">cuda_cusolver_syev.data_2023_03_17</span><span class="s3">,</span>
        <span class="s1">tpu_Eigh.data</span><span class="s3">, </span><span class="s1">tpu_Lu.data_2023_03_21</span><span class="s3">, </span><span class="s1">tpu_Qr.data_2023_03_17</span><span class="s3">, </span><span class="s1">tpu_Sharding.data_2023_03_16]</span>
    <span class="s1">covering_testdatas = itertools.chain(</span>
        <span class="s1">*[load_testdata_nested(d) </span><span class="s3">for </span><span class="s1">d </span><span class="s3">in </span><span class="s1">covering_testdatas])</span>
    <span class="s1">covered_targets = set()</span>
    <span class="s3">for </span><span class="s1">data </span><span class="s3">in </span><span class="s1">covering_testdatas:</span>
      <span class="s1">self.assertIsInstance(data</span><span class="s3">, </span><span class="s1">CompatTestData)</span>
      <span class="s1">covered_targets = covered_targets.union(data.custom_call_targets)</span>

    <span class="s1">not_covered = targets_to_cover.difference(covered_targets)</span>
    <span class="s1">self.assertEmpty(not_covered)</span>

  <span class="s3">def </span><span class="s1">test_ducc_fft(self):</span>
    <span class="s3">def </span><span class="s1">func(x):</span>
      <span class="s3">return </span><span class="s1">lax.fft(x</span><span class="s3">, </span><span class="s1">fft_type=</span><span class="s5">&quot;fft&quot;</span><span class="s3">, </span><span class="s1">fft_lengths=(</span><span class="s4">4</span><span class="s3">,</span><span class="s1">))</span>

    <span class="s1">data = load_testdata(cpu_ducc_fft.data_2023_03_17)</span>
    <span class="s1">self.run_one_test(func</span><span class="s3">, </span><span class="s1">data)</span>

  <span class="s1">@staticmethod</span>
  <span class="s3">def </span><span class="s1">eigh_harness(shape</span><span class="s3">, </span><span class="s1">dtype):</span>
    <span class="s0"># In order to keep inputs small, we construct the input programmatically</span>
    <span class="s1">operand = jnp.reshape(jnp.arange(np.prod(shape)</span><span class="s3">, </span><span class="s1">dtype=dtype)</span><span class="s3">, </span><span class="s1">shape)</span>
    <span class="s0"># Make operand self-adjoint</span>
    <span class="s1">operand = (operand + jnp.conj(jnp.swapaxes(operand</span><span class="s3">, </span><span class="s1">-</span><span class="s4">1</span><span class="s3">, </span><span class="s1">-</span><span class="s4">2</span><span class="s1">))) / </span><span class="s4">2.</span>
    <span class="s3">return </span><span class="s1">lax.linalg.eigh(jnp.tril(operand)</span><span class="s3">, </span><span class="s1">lower=</span><span class="s3">True, </span><span class="s1">symmetrize_input=</span><span class="s3">False</span><span class="s1">)</span>

  <span class="s1">@parameterized.named_parameters(</span>
      <span class="s1">dict(testcase_name=</span><span class="s5">f&quot;_dtype=</span><span class="s3">{</span><span class="s1">dtype_name</span><span class="s3">}</span><span class="s5">&quot;</span><span class="s3">, </span><span class="s1">dtype_name=dtype_name)</span>
      <span class="s3">for </span><span class="s1">dtype_name </span><span class="s3">in </span><span class="s1">(</span><span class="s5">&quot;f32&quot;</span><span class="s3">, </span><span class="s5">&quot;f64&quot;</span><span class="s3">, </span><span class="s5">&quot;c64&quot;</span><span class="s3">, </span><span class="s5">&quot;c128&quot;</span><span class="s1">))</span>
  <span class="s3">def </span><span class="s1">test_cpu_lapack_syevd(self</span><span class="s3">, </span><span class="s1">dtype_name=</span><span class="s5">&quot;f32&quot;</span><span class="s1">):</span>
    <span class="s0"># For lax.linalg.eigh</span>
    <span class="s3">if not </span><span class="s1">config.jax_enable_x64 </span><span class="s3">and </span><span class="s1">dtype_name </span><span class="s3">in </span><span class="s1">[</span><span class="s5">&quot;f64&quot;</span><span class="s3">, </span><span class="s5">&quot;c128&quot;</span><span class="s1">]:</span>
      <span class="s1">self.skipTest(</span><span class="s5">&quot;Test disabled for x32 mode&quot;</span><span class="s1">)</span>

    <span class="s1">dtype = dict(f32=np.float32</span><span class="s3">, </span><span class="s1">f64=np.float64</span><span class="s3">,</span>
                 <span class="s1">c64=np.complex64</span><span class="s3">, </span><span class="s1">c128=np.complex128)[dtype_name]</span>
    <span class="s1">func = </span><span class="s3">lambda</span><span class="s1">: CompatTest.eigh_harness((</span><span class="s4">8</span><span class="s3">, </span><span class="s4">8</span><span class="s1">)</span><span class="s3">, </span><span class="s1">dtype)</span>
    <span class="s1">data = load_testdata(cpu_lapack_syev.data_2023_03_17[dtype_name])</span>
    <span class="s1">rtol = dict(f32=</span><span class="s4">1e-3</span><span class="s3">, </span><span class="s1">f64=</span><span class="s4">1e-5</span><span class="s3">, </span><span class="s1">c64=</span><span class="s4">1e-3</span><span class="s3">, </span><span class="s1">c128=</span><span class="s4">1e-5</span><span class="s1">)</span>
    <span class="s1">self.run_one_test(func</span><span class="s3">, </span><span class="s1">data</span><span class="s3">, </span><span class="s1">rtol=rtol)</span>

  <span class="s1">@parameterized.named_parameters(</span>
      <span class="s1">dict(testcase_name=</span><span class="s5">f&quot;_dtype=</span><span class="s3">{</span><span class="s1">dtype_name</span><span class="s3">}</span><span class="s5">_</span><span class="s3">{</span><span class="s1">variant</span><span class="s3">}</span><span class="s5">&quot;</span><span class="s3">,</span>
           <span class="s1">dtype_name=dtype_name</span><span class="s3">, </span><span class="s1">variant=variant)</span>
      <span class="s3">for </span><span class="s1">dtype_name </span><span class="s3">in </span><span class="s1">(</span><span class="s5">&quot;f32&quot;</span><span class="s3">, </span><span class="s5">&quot;f64&quot;</span><span class="s1">)</span>
      <span class="s0"># We use different custom calls for sizes &lt;= 32</span>
      <span class="s3">for </span><span class="s1">variant </span><span class="s3">in </span><span class="s1">[</span><span class="s5">&quot;syevj&quot;</span><span class="s3">, </span><span class="s5">&quot;syevd&quot;</span><span class="s1">])</span>
  <span class="s3">def </span><span class="s1">test_gpu_cusolver_syev(self</span><span class="s3">, </span><span class="s1">dtype_name=</span><span class="s5">&quot;f32&quot;</span><span class="s3">, </span><span class="s1">variant=</span><span class="s5">&quot;syevj&quot;</span><span class="s1">):</span>
    <span class="s0"># For lax.linalg.eigh</span>
    <span class="s1">dtype = dict(f32=np.float32</span><span class="s3">, </span><span class="s1">f64=np.float64)[dtype_name]</span>
    <span class="s1">size = dict(syevj=</span><span class="s4">8</span><span class="s3">, </span><span class="s1">syevd=</span><span class="s4">36</span><span class="s1">)[variant]</span>
    <span class="s1">rtol = dict(f32=</span><span class="s4">1e-3</span><span class="s3">, </span><span class="s1">f64=</span><span class="s4">1e-5</span><span class="s1">)</span>
    <span class="s1">func = </span><span class="s3">lambda</span><span class="s1">: CompatTest.eigh_harness((size</span><span class="s3">, </span><span class="s1">size)</span><span class="s3">, </span><span class="s1">dtype)</span>
    <span class="s1">data = load_testdata(cuda_cusolver_syev.data_2023_03_17[</span><span class="s5">f&quot;</span><span class="s3">{</span><span class="s1">dtype_name</span><span class="s3">}</span><span class="s5">_</span><span class="s3">{</span><span class="s1">variant</span><span class="s3">}</span><span class="s5">&quot;</span><span class="s1">])</span>
    <span class="s1">self.run_one_test(func</span><span class="s3">, </span><span class="s1">data</span><span class="s3">, </span><span class="s1">rtol=rtol)</span>

  <span class="s3">def </span><span class="s1">test_tpu_Eigh(self):</span>
    <span class="s0"># For lax.linalg.eigh</span>
    <span class="s1">func = </span><span class="s3">lambda</span><span class="s1">: CompatTest.eigh_harness((</span><span class="s4">8</span><span class="s3">, </span><span class="s4">8</span><span class="s1">)</span><span class="s3">, </span><span class="s1">np.float32)</span>
    <span class="s1">data = load_testdata(tpu_Eigh.data)</span>
    <span class="s1">self.run_one_test(func</span><span class="s3">, </span><span class="s1">data</span><span class="s3">, </span><span class="s1">rtol=</span><span class="s4">1e-3</span><span class="s1">)</span>

  <span class="s1">@staticmethod</span>
  <span class="s3">def </span><span class="s1">qr_harness(shape</span><span class="s3">, </span><span class="s1">dtype):</span>
    <span class="s0"># In order to keep inputs small, we construct the input programmatically</span>
    <span class="s1">operand = jnp.reshape(jnp.arange(np.prod(shape)</span><span class="s3">, </span><span class="s1">dtype=dtype)</span><span class="s3">, </span><span class="s1">shape)</span>
    <span class="s3">return </span><span class="s1">lax.linalg.qr(operand</span><span class="s3">, </span><span class="s1">full_matrices=</span><span class="s3">True</span><span class="s1">)</span>

  <span class="s1">@parameterized.named_parameters(</span>
      <span class="s1">dict(testcase_name=</span><span class="s5">f&quot;_dtype=</span><span class="s3">{</span><span class="s1">dtype_name</span><span class="s3">}</span><span class="s5">&quot;</span><span class="s3">, </span><span class="s1">dtype_name=dtype_name)</span>
      <span class="s3">for </span><span class="s1">dtype_name </span><span class="s3">in </span><span class="s1">(</span><span class="s5">&quot;f32&quot;</span><span class="s3">, </span><span class="s5">&quot;f64&quot;</span><span class="s3">, </span><span class="s5">&quot;c64&quot;</span><span class="s3">, </span><span class="s5">&quot;c128&quot;</span><span class="s1">))</span>
  <span class="s3">def </span><span class="s1">test_cpu_lapack_geqrf(self</span><span class="s3">, </span><span class="s1">dtype_name=</span><span class="s5">&quot;f32&quot;</span><span class="s1">):</span>
    <span class="s0"># For lax.linalg.qr</span>
    <span class="s3">if not </span><span class="s1">config.jax_enable_x64 </span><span class="s3">and </span><span class="s1">dtype_name </span><span class="s3">in </span><span class="s1">[</span><span class="s5">&quot;f64&quot;</span><span class="s3">, </span><span class="s5">&quot;c128&quot;</span><span class="s1">]:</span>
      <span class="s1">self.skipTest(</span><span class="s5">&quot;Test disabled for x32 mode&quot;</span><span class="s1">)</span>

    <span class="s1">dtype = dict(f32=np.float32</span><span class="s3">, </span><span class="s1">f64=np.float64</span><span class="s3">,</span>
                 <span class="s1">c64=np.complex64</span><span class="s3">, </span><span class="s1">c128=np.complex128)[dtype_name]</span>
    <span class="s1">func = </span><span class="s3">lambda</span><span class="s1">: CompatTest.qr_harness((</span><span class="s4">3</span><span class="s3">, </span><span class="s4">3</span><span class="s1">)</span><span class="s3">, </span><span class="s1">dtype)</span>
    <span class="s1">data = load_testdata(cpu_lapack_geqrf.data_2023_03_17[dtype_name])</span>
    <span class="s1">rtol = dict(f32=</span><span class="s4">1e-3</span><span class="s3">, </span><span class="s1">f64=</span><span class="s4">1e-5</span><span class="s3">, </span><span class="s1">c64=</span><span class="s4">1e-3</span><span class="s3">, </span><span class="s1">c128=</span><span class="s4">1e-5</span><span class="s1">)</span>
    <span class="s1">self.run_one_test(func</span><span class="s3">, </span><span class="s1">data</span><span class="s3">, </span><span class="s1">rtol=rtol)</span>

  <span class="s1">@parameterized.named_parameters(</span>
      <span class="s1">dict(testcase_name=</span><span class="s5">f&quot;_dtype=</span><span class="s3">{</span><span class="s1">dtype_name</span><span class="s3">}</span><span class="s5">_</span><span class="s3">{</span><span class="s1">batched</span><span class="s3">}</span><span class="s5">&quot;</span><span class="s3">,</span>
           <span class="s1">dtype_name=dtype_name</span><span class="s3">, </span><span class="s1">batched=batched)</span>
      <span class="s3">for </span><span class="s1">dtype_name </span><span class="s3">in </span><span class="s1">(</span><span class="s5">&quot;f32&quot;</span><span class="s3">,</span><span class="s1">)</span>
      <span class="s0"># For batched qr we use cublas_geqrf_batched</span>
      <span class="s3">for </span><span class="s1">batched </span><span class="s3">in </span><span class="s1">(</span><span class="s5">&quot;batched&quot;</span><span class="s3">, </span><span class="s5">&quot;unbatched&quot;</span><span class="s1">))</span>
  <span class="s3">def </span><span class="s1">test_gpu_cusolver_geqrf(self</span><span class="s3">, </span><span class="s1">dtype_name=</span><span class="s5">&quot;f32&quot;</span><span class="s3">, </span><span class="s1">batched=</span><span class="s5">&quot;unbatched&quot;</span><span class="s1">):</span>
    <span class="s0"># For lax.linalg.qr</span>
    <span class="s1">dtype = dict(f32=np.float32</span><span class="s3">, </span><span class="s1">f64=np.float64)[dtype_name]</span>
    <span class="s1">rtol = dict(f32=</span><span class="s4">1e-3</span><span class="s3">, </span><span class="s1">f64=</span><span class="s4">1e-5</span><span class="s1">)</span>
    <span class="s1">shape = dict(batched=(</span><span class="s4">2</span><span class="s3">, </span><span class="s4">3</span><span class="s3">, </span><span class="s4">3</span><span class="s1">)</span><span class="s3">, </span><span class="s1">unbatched=(</span><span class="s4">3</span><span class="s3">, </span><span class="s4">3</span><span class="s1">))[batched]</span>
    <span class="s1">func = </span><span class="s3">lambda</span><span class="s1">: CompatTest.qr_harness(shape</span><span class="s3">, </span><span class="s1">dtype)</span>
    <span class="s1">data = load_testdata(cuda_cusolver_geqrf.data_2023_03_18[batched])</span>
    <span class="s1">self.run_one_test(func</span><span class="s3">, </span><span class="s1">data</span><span class="s3">, </span><span class="s1">rtol=rtol)</span>

  <span class="s3">def </span><span class="s1">test_tpu_Qr(self):</span>
    <span class="s0"># For lax.linalg.qr</span>
    <span class="s1">func = </span><span class="s3">lambda</span><span class="s1">: CompatTest.qr_harness((</span><span class="s4">3</span><span class="s3">, </span><span class="s4">3</span><span class="s1">)</span><span class="s3">, </span><span class="s1">np.float32)</span>
    <span class="s1">data = load_testdata(tpu_Qr.data_2023_03_17)</span>
    <span class="s1">self.run_one_test(func</span><span class="s3">, </span><span class="s1">data</span><span class="s3">, </span><span class="s1">rtol=</span><span class="s4">1e-3</span><span class="s1">)</span>

  <span class="s1">@staticmethod</span>
  <span class="s3">def </span><span class="s1">lu_harness(shape</span><span class="s3">, </span><span class="s1">dtype):</span>
    <span class="s1">operand = jnp.reshape(jnp.arange(np.prod(shape)</span><span class="s3">, </span><span class="s1">dtype=dtype)</span><span class="s3">, </span><span class="s1">shape)</span>
    <span class="s3">return </span><span class="s1">lax.linalg.lu(operand)</span>

  <span class="s3">def </span><span class="s1">test_tpu_Lu(self):</span>
    <span class="s0"># For lax.linalg.lu</span>
    <span class="s1">func = </span><span class="s3">lambda</span><span class="s1">: CompatTest.lu_harness((</span><span class="s4">3</span><span class="s3">, </span><span class="s4">3</span><span class="s1">)</span><span class="s3">, </span><span class="s1">np.float32)</span>
    <span class="s1">data = load_testdata(tpu_Lu.data_2023_03_21)</span>
    <span class="s1">self.run_one_test(func</span><span class="s3">, </span><span class="s1">data</span><span class="s3">, </span><span class="s1">rtol=</span><span class="s4">1e-3</span><span class="s1">)</span>

  <span class="s3">def </span><span class="s1">test_cu_threefry2x32(self):</span>
    <span class="s3">def </span><span class="s1">func(x):</span>
      <span class="s3">return </span><span class="s1">jax.random.uniform(x</span><span class="s3">, </span><span class="s1">(</span><span class="s4">2</span><span class="s3">, </span><span class="s4">4</span><span class="s1">)</span><span class="s3">, </span><span class="s1">dtype=np.float32)</span>

    <span class="s1">data = load_testdata(cuda_threefry2x32.data_2023_03_15)</span>
    <span class="s1">self.run_one_test(func</span><span class="s3">, </span><span class="s1">data)</span>

  <span class="s3">def </span><span class="s1">test_sharding(self):</span>
    <span class="s0"># Tests &quot;Sharding&quot;, &quot;SPMDShardToFullShape&quot;, &quot;SPMDFullToShardShape&quot; on TPU</span>
    <span class="s3">if </span><span class="s1">jtu.device_under_test() != </span><span class="s5">&quot;tpu&quot; </span><span class="s3">or </span><span class="s1">len(jax.devices()) &lt; </span><span class="s4">2</span><span class="s1">:</span>
      <span class="s1">self.skipTest(</span><span class="s5">&quot;Test runs only on TPU with at least 2 devices&quot;</span><span class="s1">)</span>

    <span class="s0"># Must use exactly 2 devices for expected outputs from ppermute</span>
    <span class="s1">devices = jax.devices()[:</span><span class="s4">2</span><span class="s1">]</span>
    <span class="s1">mesh = Mesh(devices</span><span class="s3">, </span><span class="s1">axis_names=(</span><span class="s5">'a'</span><span class="s1">))</span>

    <span class="s1">@partial(pjit.pjit</span><span class="s3">,</span>
             <span class="s1">in_shardings=(P(</span><span class="s5">'a'</span><span class="s3">, None</span><span class="s1">)</span><span class="s3">,</span><span class="s1">)</span><span class="s3">, </span><span class="s1">out_shardings=P(</span><span class="s5">'a'</span><span class="s3">, None</span><span class="s1">))</span>
    <span class="s1">@partial(shard_map</span><span class="s3">, </span><span class="s1">mesh=mesh</span><span class="s3">,</span>
             <span class="s1">in_specs=(P(</span><span class="s5">'a'</span><span class="s3">, None</span><span class="s1">)</span><span class="s3">,</span><span class="s1">)</span><span class="s3">, </span><span class="s1">out_specs=P(</span><span class="s5">'a'</span><span class="s3">, None</span><span class="s1">))</span>
    <span class="s3">def </span><span class="s1">func(x):  </span><span class="s0"># b: f32[2, 4]</span>
      <span class="s1">axis_size = lax.psum(</span><span class="s4">1</span><span class="s3">, </span><span class="s5">'a'</span><span class="s1">)</span>
      <span class="s1">perm = [(j</span><span class="s3">, </span><span class="s1">(j + </span><span class="s4">1</span><span class="s1">) % axis_size) </span><span class="s3">for </span><span class="s1">j </span><span class="s3">in </span><span class="s1">range(axis_size)]</span>
      <span class="s3">return </span><span class="s1">lax.ppermute(x</span><span class="s3">, </span><span class="s5">'a'</span><span class="s3">, </span><span class="s1">perm=perm)</span>

    <span class="s0"># We need these only because for now we run the serialized module with TF</span>
    <span class="s1">tf_tpus = tf.config.list_logical_devices(</span><span class="s5">&quot;TPU&quot;</span><span class="s1">)</span>
    <span class="s1">self.assertNotEmpty(tf_tpus)</span>

    <span class="s1">resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu=</span><span class="s5">''</span><span class="s1">)</span>
    <span class="s1">tf.config.experimental_connect_to_cluster(resolver)</span>
    <span class="s1">topology = tf.tpu.experimental.initialize_tpu_system(resolver)</span>
    <span class="s1">device_assignment = tf.tpu.experimental.DeviceAssignment.build(</span>
        <span class="s1">topology</span><span class="s3">, </span><span class="s1">computation_shape=[</span><span class="s4">1</span><span class="s3">, </span><span class="s4">1</span><span class="s3">, </span><span class="s4">1</span><span class="s3">, </span><span class="s4">2</span><span class="s1">]</span><span class="s3">,</span>
        <span class="s1">num_replicas=</span><span class="s4">1</span><span class="s1">)</span>
    <span class="s3">def </span><span class="s1">run_tf(f_tf</span><span class="s3">, </span><span class="s1">x):</span>
      <span class="s3">def </span><span class="s1">wrapped_f_tf(x):</span>
        <span class="s3">return </span><span class="s1">tf.compat.v1.tpu.rewrite(</span>
            <span class="s1">f_tf</span><span class="s3">, </span><span class="s1">[tf.convert_to_tensor(x)]</span><span class="s3">,</span>
            <span class="s1">device_assignment=device_assignment)</span>
      <span class="s3">return </span><span class="s1">tf.function(wrapped_f_tf</span><span class="s3">, </span><span class="s1">autograph=</span><span class="s3">False, </span><span class="s1">jit_compile=</span><span class="s3">True</span><span class="s1">)(x)</span>

    <span class="s1">data = load_testdata(tpu_Sharding.data_2023_03_16)</span>
    <span class="s3">with </span><span class="s1">mesh:</span>
      <span class="s1">self.run_one_test(func</span><span class="s3">, </span><span class="s1">data</span><span class="s3">, </span><span class="s1">run_tf=run_tf)</span>


<span class="s3">if </span><span class="s1">__name__ == </span><span class="s5">&quot;__main__&quot;</span><span class="s1">:</span>
  <span class="s1">absltest.main(testLoader=jtu.JaxTestLoader())</span>
</pre>
</body>
</html>