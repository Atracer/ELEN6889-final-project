<html>
<head>
<title>other.py</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<style type="text/css">
.s0 { color: #808080;}
.s1 { color: #a9b7c6;}
.s2 { color: #cc7832;}
.s3 { color: #629755; font-style: italic;}
.s4 { color: #6897bb;}
</style>
</head>
<body bgcolor="#2b2b2b">
<table CELLSPACING=0 CELLPADDING=5 COLS=1 WIDTH="100%" BGCOLOR="#606060" >
<tr><td><center>
<font face="Arial, Helvetica" color="#000000">
other.py</font>
</center></td></tr></table>
<pre><span class="s0"># Copyright 2020 The JAX Authors.</span>
<span class="s0">#</span>
<span class="s0"># Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);</span>
<span class="s0"># you may not use this file except in compliance with the License.</span>
<span class="s0"># You may obtain a copy of the License at</span>
<span class="s0">#</span>
<span class="s0">#     https://www.apache.org/licenses/LICENSE-2.0</span>
<span class="s0">#</span>
<span class="s0"># Unless required by applicable law or agreed to in writing, software</span>
<span class="s0"># distributed under the License is distributed on an &quot;AS IS&quot; BASIS,</span>
<span class="s0"># WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span>
<span class="s0"># See the License for the specific language governing permissions and</span>
<span class="s0"># limitations under the License.</span>

<span class="s2">import </span><span class="s1">math</span>
<span class="s2">from </span><span class="s1">typing </span><span class="s2">import </span><span class="s1">Any</span><span class="s2">, </span><span class="s1">Optional</span><span class="s2">, </span><span class="s1">Sequence</span><span class="s2">, </span><span class="s1">Tuple</span><span class="s2">, </span><span class="s1">Union</span><span class="s2">, </span><span class="s1">cast </span><span class="s2">as </span><span class="s1">type_cast</span>

<span class="s2">import </span><span class="s1">jax</span>
<span class="s2">from </span><span class="s1">jax._src.numpy </span><span class="s2">import </span><span class="s1">lax_numpy </span><span class="s2">as </span><span class="s1">jnp</span>
<span class="s2">from </span><span class="s1">jax._src.lax </span><span class="s2">import </span><span class="s1">lax</span>
<span class="s2">from </span><span class="s1">jax._src.lax </span><span class="s2">import </span><span class="s1">convolution</span>

<span class="s1">DType = Any</span>

<span class="s2">def </span><span class="s1">conv_general_dilated_patches(</span>
    <span class="s1">lhs: jax.typing.ArrayLike</span><span class="s2">,</span>
    <span class="s1">filter_shape: Sequence[int]</span><span class="s2">,</span>
    <span class="s1">window_strides: Sequence[int]</span><span class="s2">,</span>
    <span class="s1">padding: Union[str</span><span class="s2">, </span><span class="s1">Sequence[Tuple[int</span><span class="s2">, </span><span class="s1">int]]]</span><span class="s2">,</span>
    <span class="s1">lhs_dilation: Optional[Sequence[int]] = </span><span class="s2">None,</span>
    <span class="s1">rhs_dilation: Optional[Sequence[int]] = </span><span class="s2">None,</span>
    <span class="s1">dimension_numbers: Optional[convolution.ConvGeneralDilatedDimensionNumbers] = </span><span class="s2">None,</span>
    <span class="s1">precision: Optional[lax.PrecisionType] = </span><span class="s2">None,</span>
    <span class="s1">preferred_element_type: Optional[DType] = </span><span class="s2">None,</span>
<span class="s1">) -&gt; jax.Array:</span>
  <span class="s3">&quot;&quot;&quot;Extract patches subject to the receptive field of `conv_general_dilated`. 
 
  Runs the input through a convolution with given parameters. The kernel of the 
  convolution is constructed such that the output channel dimension `&quot;C&quot;` 
  contains flattened image patches, so instead a single `&quot;C&quot;` dimension 
  represents, for example, three dimensions `&quot;chw&quot;` collapsed. The order of 
  these dimensions is `&quot;c&quot; + ''.join(c for c in rhs_spec if c not in 'OI')`, 
  where `rhs_spec == dimension_numbers[1]`, and the size of this `&quot;C&quot;` 
  dimension is therefore the size of each patch, i.e. 
  `np.prod(filter_shape) * lhs.shape[lhs_spec.index('C')]`, where 
  `lhs_spec == dimension_numbers[0]`. 
 
  Docstring below adapted from `jax.lax.conv_general_dilated`. 
 
  See Also: 
    https://www.tensorflow.org/xla/operation_semantics#conv_convolution 
 
  Args: 
    lhs: a rank `n+2` dimensional input array. 
    filter_shape: a sequence of `n` integers, representing the receptive window 
      spatial shape in the order as specified in 
      `rhs_spec = dimension_numbers[1]`. 
    window_strides: a sequence of `n` integers, representing the inter-window 
      strides. 
    padding: either the string `'SAME'`, the string `'VALID'`, or a sequence of 
      `n` `(low, high)` integer pairs that give the padding to apply before and 
      after each spatial dimension. 
    lhs_dilation: `None`, or a sequence of `n` integers, giving the 
      dilation factor to apply in each spatial dimension of `lhs`. LHS dilation 
      is also known as transposed convolution. 
    rhs_dilation: `None`, or a sequence of `n` integers, giving the 
      dilation factor to apply in each spatial dimension of `rhs`. RHS dilation 
      is also known as atrous convolution. 
    dimension_numbers: either `None`, or a 3-tuple 
      `(lhs_spec, rhs_spec, out_spec)`, where each element is a string 
      of length `n+2`. `None` defaults to `(&quot;NCHWD..., OIHWD..., NCHWD...&quot;)`. 
    precision: Optional. Either ``None``, which means the default precision for 
      the backend, or a :class:`~jax.lax.Precision` enum value (``Precision.DEFAULT``, 
      ``Precision.HIGH`` or ``Precision.HIGHEST``). 
    preferred_element_type: Optional. Either ``None``, which means the default 
      accumulation type for the input types, or a datatype, indicating to 
      accumulate results to and return a result with that datatype. 
 
  Returns: 
    A rank `n+2` array containing the flattened image patches in the output 
    channel (`&quot;C&quot;`) dimension. For example if 
    `dimension_numbers = (&quot;NcHW&quot;, &quot;OIwh&quot;, &quot;CNHW&quot;)`, the output has dimension 
    numbers `&quot;CNHW&quot; = &quot;{cwh}NHW&quot;`, with the size of dimension `&quot;C&quot;` equal to 
    the size of each patch 
    (`np.prod(filter_shape) * lhs.shape[lhs_spec.index('C')]`). 
 
  &quot;&quot;&quot;</span>
  <span class="s1">lhs_array = jnp.asarray(lhs)</span>
  <span class="s1">filter_shape = tuple(filter_shape)</span>
  <span class="s1">dimension_numbers = convolution.conv_dimension_numbers(</span>
      <span class="s1">lhs_array.shape</span><span class="s2">, </span><span class="s1">(</span><span class="s4">1</span><span class="s2">, </span><span class="s4">1</span><span class="s1">) + filter_shape</span><span class="s2">, </span><span class="s1">dimension_numbers)</span>

  <span class="s1">lhs_spec</span><span class="s2">, </span><span class="s1">rhs_spec</span><span class="s2">, </span><span class="s1">out_spec = dimension_numbers</span>

  <span class="s1">spatial_size = math.prod(filter_shape)</span>
  <span class="s1">n_channels = lhs_array.shape[lhs_spec[</span><span class="s4">1</span><span class="s1">]]</span>

  <span class="s0"># Move separate `lhs` spatial locations into separate `rhs` channels.</span>
  <span class="s1">rhs = jnp.eye(spatial_size</span><span class="s2">, </span><span class="s1">dtype=lhs_array.dtype).reshape(filter_shape * </span><span class="s4">2</span><span class="s1">)</span>

  <span class="s1">rhs = rhs.reshape((spatial_size</span><span class="s2">, </span><span class="s4">1</span><span class="s1">) + filter_shape)</span>
  <span class="s1">rhs = jnp.tile(rhs</span><span class="s2">, </span><span class="s1">(n_channels</span><span class="s2">,</span><span class="s1">) + (</span><span class="s4">1</span><span class="s2">,</span><span class="s1">) * (rhs.ndim - </span><span class="s4">1</span><span class="s1">))</span>
  <span class="s1">rhs = jnp.moveaxis(rhs</span><span class="s2">, </span><span class="s1">(</span><span class="s4">0</span><span class="s2">, </span><span class="s4">1</span><span class="s1">)</span><span class="s2">, </span><span class="s1">(rhs_spec[</span><span class="s4">0</span><span class="s1">]</span><span class="s2">, </span><span class="s1">rhs_spec[</span><span class="s4">1</span><span class="s1">]))</span>

  <span class="s1">out = convolution.conv_general_dilated(</span>
      <span class="s1">lhs=lhs_array</span><span class="s2">,</span>
      <span class="s1">rhs=rhs</span><span class="s2">,</span>
      <span class="s1">window_strides=window_strides</span><span class="s2">,</span>
      <span class="s1">padding=padding</span><span class="s2">,</span>
      <span class="s1">lhs_dilation=lhs_dilation</span><span class="s2">,</span>
      <span class="s1">rhs_dilation=rhs_dilation</span><span class="s2">,</span>
      <span class="s1">dimension_numbers=dimension_numbers</span><span class="s2">,</span>
      <span class="s1">precision=</span><span class="s2">None if </span><span class="s1">precision </span><span class="s2">is None else </span><span class="s1">(precision</span><span class="s2">,</span>
                                                <span class="s1">lax.Precision.DEFAULT)</span><span class="s2">,</span>
      <span class="s1">feature_group_count=n_channels</span><span class="s2">,</span>
      <span class="s1">preferred_element_type=preferred_element_type</span>
  <span class="s1">)</span>
  <span class="s2">return </span><span class="s1">out</span>


<span class="s2">def </span><span class="s1">conv_general_dilated_local(</span>
    <span class="s1">lhs: jax.typing.ArrayLike</span><span class="s2">,</span>
    <span class="s1">rhs: jax.typing.ArrayLike</span><span class="s2">,</span>
    <span class="s1">window_strides: Sequence[int]</span><span class="s2">,</span>
    <span class="s1">padding: Union[str</span><span class="s2">, </span><span class="s1">Sequence[Tuple[int</span><span class="s2">, </span><span class="s1">int]]]</span><span class="s2">,</span>
    <span class="s1">filter_shape: Sequence[int]</span><span class="s2">,</span>
    <span class="s1">lhs_dilation: Optional[Sequence[int]] = </span><span class="s2">None,</span>
    <span class="s1">rhs_dilation: Optional[Sequence[int]] = </span><span class="s2">None,</span>
    <span class="s1">dimension_numbers: Optional[convolution.ConvGeneralDilatedDimensionNumbers] = </span><span class="s2">None,</span>
    <span class="s1">precision: lax.PrecisionLike = </span><span class="s2">None</span>
<span class="s1">) -&gt; jax.Array:</span>
  <span class="s3">&quot;&quot;&quot;General n-dimensional unshared convolution operator with optional dilation. 
 
  Also known as locally connected layer, the operation is equivalent to 
  convolution with a separate (unshared) `rhs` kernel used at each output 
  spatial location. Docstring below adapted from `jax.lax.conv_general_dilated`. 
 
  See Also: 
    https://www.tensorflow.org/xla/operation_semantics#conv_convolution 
 
  Args: 
    lhs: a rank `n+2` dimensional input array. 
    rhs: a rank `n+2` dimensional array of kernel weights. Unlike in regular 
      CNNs, its spatial coordinates (`H`, `W`, ...) correspond to output spatial 
      locations, while input spatial locations are fused with the input channel 
      locations in the single `I` dimension, in the order of 
      `&quot;C&quot; + ''.join(c for c in rhs_spec if c not in 'OI')`, where 
      `rhs_spec = dimension_numbers[1]`. For example, if `rhs_spec == &quot;WHIO&quot;, 
      the unfolded kernel shape is 
      `&quot;[output W][output H]{I[receptive window W][receptive window H]}O&quot;`. 
    window_strides: a sequence of `n` integers, representing the inter-window 
      strides. 
    padding: either the string `'SAME'`, the string `'VALID'`, or a sequence of 
      `n` `(low, high)` integer pairs that give the padding to apply before and 
      after each spatial dimension. 
    filter_shape: a sequence of `n` integers, representing the receptive window 
      spatial shape in the order as specified in 
      `rhs_spec = dimension_numbers[1]`. 
    lhs_dilation: `None`, or a sequence of `n` integers, giving the 
      dilation factor to apply in each spatial dimension of `lhs`. LHS dilation 
      is also known as transposed convolution. 
    rhs_dilation: `None`, or a sequence of `n` integers, giving the 
      dilation factor to apply in each input spatial dimension of `rhs`. 
      RHS dilation is also known as atrous convolution. 
    dimension_numbers: either `None`, a `ConvDimensionNumbers` object, or 
      a 3-tuple `(lhs_spec, rhs_spec, out_spec)`, where each element is a string 
      of length `n+2`. 
    precision: Optional. Either ``None``, which means the default precision for 
      the backend, a ``lax.Precision`` enum value (``Precision.DEFAULT``, 
      ``Precision.HIGH`` or ``Precision.HIGHEST``) or a tuple of two 
      ``lax.Precision`` enums indicating precision of ``lhs``` and ``rhs``. 
 
  Returns: 
    An array containing the unshared convolution result. 
 
  In the string case of `dimension_numbers`, each character identifies by 
  position: 
 
  - the batch dimensions in `lhs`, `rhs`, and the output with the character 
    'N', 
  - the feature dimensions in `lhs` and the output with the character 'C', 
  - the input and output feature dimensions in rhs with the characters 'I' 
    and 'O' respectively, and 
  - spatial dimension correspondences between `lhs`, `rhs`, and the output using 
    any distinct characters. 
 
  For example, to indicate dimension numbers consistent with the `conv` function 
  with two spatial dimensions, one could use `('NCHW', 'OIHW', 'NCHW')`. As 
  another example, to indicate dimension numbers consistent with the TensorFlow 
  Conv2D operation, one could use `('NHWC', 'HWIO', 'NHWC')`. When using the 
  latter form of convolution dimension specification, window strides are 
  associated with spatial dimension character labels according to the order in 
  which the labels appear in the `rhs_spec` string, so that `window_strides[0]` 
  is matched with the dimension corresponding to the first character 
  appearing in rhs_spec that is not `'I'` or `'O'`. 
 
  If `dimension_numbers` is `None`, the default is `('NCHW', 'OIHW', 'NCHW')` 
  (for a 2D convolution). 
  &quot;&quot;&quot;</span>
  <span class="s1">lhs_array = jnp.asarray(lhs)</span>

  <span class="s1">c_precision = lax.canonicalize_precision(precision)</span>
  <span class="s1">lhs_precision = type_cast(</span>
      <span class="s1">Optional[lax.PrecisionType]</span><span class="s2">,</span>
      <span class="s1">(c_precision[</span><span class="s4">0</span><span class="s1">]</span>
       <span class="s2">if </span><span class="s1">(isinstance(c_precision</span><span class="s2">, </span><span class="s1">tuple) </span><span class="s2">and </span><span class="s1">len(c_precision) == </span><span class="s4">2</span><span class="s1">)</span>
       <span class="s2">else </span><span class="s1">c_precision))</span>

  <span class="s1">patches = conv_general_dilated_patches(</span>
      <span class="s1">lhs=lhs_array</span><span class="s2">,</span>
      <span class="s1">filter_shape=filter_shape</span><span class="s2">,</span>
      <span class="s1">window_strides=window_strides</span><span class="s2">,</span>
      <span class="s1">padding=padding</span><span class="s2">,</span>
      <span class="s1">lhs_dilation=lhs_dilation</span><span class="s2">,</span>
      <span class="s1">rhs_dilation=rhs_dilation</span><span class="s2">,</span>
      <span class="s1">dimension_numbers=dimension_numbers</span><span class="s2">,</span>
      <span class="s1">precision=lhs_precision</span>
  <span class="s1">)</span>

  <span class="s1">lhs_spec</span><span class="s2">, </span><span class="s1">rhs_spec</span><span class="s2">, </span><span class="s1">out_spec = convolution.conv_dimension_numbers(</span>
      <span class="s1">lhs_array.shape</span><span class="s2">, </span><span class="s1">(</span><span class="s4">1</span><span class="s2">, </span><span class="s4">1</span><span class="s1">) + tuple(filter_shape)</span><span class="s2">, </span><span class="s1">dimension_numbers)</span>

  <span class="s1">lhs_c_dims</span><span class="s2">, </span><span class="s1">rhs_c_dims = [out_spec[</span><span class="s4">1</span><span class="s1">]]</span><span class="s2">, </span><span class="s1">[rhs_spec[</span><span class="s4">1</span><span class="s1">]]</span>

  <span class="s1">lhs_b_dims = out_spec[</span><span class="s4">2</span><span class="s1">:]</span>
  <span class="s1">rhs_b_dims = rhs_spec[</span><span class="s4">2</span><span class="s1">:]</span>

  <span class="s1">rhs_b_dims = [rhs_b_dims[i] </span><span class="s2">for </span><span class="s1">i </span><span class="s2">in </span><span class="s1">sorted(range(len(rhs_b_dims))</span><span class="s2">,</span>
                                              <span class="s1">key=</span><span class="s2">lambda </span><span class="s1">k: lhs_b_dims[k])]</span>
  <span class="s1">lhs_b_dims = sorted(lhs_b_dims)</span>

  <span class="s1">dn = ((lhs_c_dims</span><span class="s2">, </span><span class="s1">rhs_c_dims)</span><span class="s2">, </span><span class="s1">(lhs_b_dims</span><span class="s2">, </span><span class="s1">rhs_b_dims))</span>
  <span class="s1">out = lax.dot_general(patches</span><span class="s2">, </span><span class="s1">rhs</span><span class="s2">, </span><span class="s1">dimension_numbers=dn</span><span class="s2">, </span><span class="s1">precision=precision)</span>
  <span class="s1">out = jnp.moveaxis(out</span><span class="s2">, </span><span class="s1">(-</span><span class="s4">2</span><span class="s2">, </span><span class="s1">-</span><span class="s4">1</span><span class="s1">)</span><span class="s2">, </span><span class="s1">(out_spec[</span><span class="s4">0</span><span class="s1">]</span><span class="s2">, </span><span class="s1">out_spec[</span><span class="s4">1</span><span class="s1">]))</span>
  <span class="s2">return </span><span class="s1">out</span>
</pre>
</body>
</html>