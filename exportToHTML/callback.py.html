<html>
<head>
<title>callback.py</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<style type="text/css">
.s0 { color: #808080;}
.s1 { color: #a9b7c6;}
.s2 { color: #629755; font-style: italic;}
.s3 { color: #cc7832;}
.s4 { color: #6a8759;}
.s5 { color: #6897bb;}
</style>
</head>
<body bgcolor="#2b2b2b">
<table CELLSPACING=0 CELLPADDING=5 COLS=1 WIDTH="100%" BGCOLOR="#606060" >
<tr><td><center>
<font face="Arial, Helvetica" color="#000000">
callback.py</font>
</center></td></tr></table>
<pre><span class="s0"># Copyright 2022 The JAX Authors.</span>
<span class="s0">#</span>
<span class="s0"># Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);</span>
<span class="s0"># you may not use this file except in compliance with the License.</span>
<span class="s0"># You may obtain a copy of the License at</span>
<span class="s0">#</span>
<span class="s0">#     https://www.apache.org/licenses/LICENSE-2.0</span>
<span class="s0">#</span>
<span class="s0"># Unless required by applicable law or agreed to in writing, software</span>
<span class="s0"># distributed under the License is distributed on an &quot;AS IS&quot; BASIS,</span>
<span class="s0"># WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span>
<span class="s0"># See the License for the specific language governing permissions and</span>
<span class="s0"># limitations under the License.</span>
<span class="s2">&quot;&quot;&quot;Module for JAX callbacks.&quot;&quot;&quot;</span>
<span class="s3">from </span><span class="s1">__future__ </span><span class="s3">import </span><span class="s1">annotations</span>

<span class="s3">import </span><span class="s1">functools</span>
<span class="s3">from </span><span class="s1">typing </span><span class="s3">import </span><span class="s1">Any</span><span class="s3">, </span><span class="s1">Callable</span><span class="s3">, </span><span class="s1">Sequence</span>

<span class="s3">import </span><span class="s1">numpy </span><span class="s3">as </span><span class="s1">np</span>

<span class="s3">from </span><span class="s1">jax </span><span class="s3">import </span><span class="s1">tree_util</span>
<span class="s3">from </span><span class="s1">jax.interpreters </span><span class="s3">import </span><span class="s1">mlir</span>

<span class="s3">from </span><span class="s1">jax._src </span><span class="s3">import </span><span class="s1">core</span>
<span class="s3">from </span><span class="s1">jax._src </span><span class="s3">import </span><span class="s1">dtypes</span>
<span class="s3">from </span><span class="s1">jax._src </span><span class="s3">import </span><span class="s1">effects</span>
<span class="s3">from </span><span class="s1">jax._src </span><span class="s3">import </span><span class="s1">util</span>
<span class="s3">from </span><span class="s1">jax._src </span><span class="s3">import </span><span class="s1">dispatch</span>
<span class="s3">from </span><span class="s1">jax._src.interpreters </span><span class="s3">import </span><span class="s1">ad</span>
<span class="s3">from </span><span class="s1">jax._src.interpreters </span><span class="s3">import </span><span class="s1">batching</span>
<span class="s3">from </span><span class="s1">jax._src.lib </span><span class="s3">import </span><span class="s1">xla_client </span><span class="s3">as </span><span class="s1">xc</span>

<span class="s0"># `pure_callback_p` is the main primitive for staging out Python pure callbacks.</span>
<span class="s1">pure_callback_p = core.Primitive(</span><span class="s4">&quot;pure_callback&quot;</span><span class="s1">)</span>
<span class="s1">pure_callback_p.multiple_results = </span><span class="s3">True</span>

<span class="s1">map</span><span class="s3">, </span><span class="s1">unsafe_map = util.safe_map</span><span class="s3">, </span><span class="s1">map</span>


<span class="s3">def </span><span class="s1">pure_callback_impl(*args</span><span class="s3">, </span><span class="s1">result_avals</span><span class="s3">, </span><span class="s1">callback: Callable[...</span><span class="s3">, </span><span class="s1">Any]</span><span class="s3">,</span>
                       <span class="s1">vectorized: bool):</span>
  <span class="s3">del </span><span class="s1">vectorized</span><span class="s3">, </span><span class="s1">result_avals</span>
  <span class="s3">return </span><span class="s1">callback(*args)</span>
<span class="s1">pure_callback_p.def_impl(functools.partial(dispatch.apply_primitive</span><span class="s3">,</span>
                                           <span class="s1">pure_callback_p))</span>


<span class="s1">@pure_callback_p.def_abstract_eval</span>
<span class="s3">def </span><span class="s1">pure_callback_abstract_eval(*avals</span><span class="s3">, </span><span class="s1">callback: Callable[...</span><span class="s3">, </span><span class="s1">Any]</span><span class="s3">,</span>
                                <span class="s1">result_avals</span><span class="s3">, </span><span class="s1">vectorized: bool):</span>
  <span class="s3">del </span><span class="s1">avals</span><span class="s3">, </span><span class="s1">callback</span><span class="s3">, </span><span class="s1">vectorized</span>
  <span class="s3">return </span><span class="s1">result_avals</span>


<span class="s3">def </span><span class="s1">pure_callback_jvp_rule(*args</span><span class="s3">, </span><span class="s1">**kwargs):</span>
  <span class="s3">del </span><span class="s1">args</span><span class="s3">, </span><span class="s1">kwargs</span>
  <span class="s3">raise </span><span class="s1">ValueError(</span>
      <span class="s4">&quot;Pure callbacks do not support JVP. &quot;</span>
      <span class="s4">&quot;Please use `jax.custom_jvp` to use callbacks while taking gradients.&quot;</span><span class="s1">)</span>


<span class="s1">ad.primitive_jvps[pure_callback_p] = pure_callback_jvp_rule</span>


<span class="s3">def </span><span class="s1">pure_callback_transpose_rule(*args</span><span class="s3">, </span><span class="s1">**kwargs):</span>
  <span class="s3">del </span><span class="s1">args</span><span class="s3">, </span><span class="s1">kwargs</span>
  <span class="s3">raise </span><span class="s1">ValueError(</span>
      <span class="s4">&quot;Pure callbacks do not support transpose. &quot;</span>
      <span class="s4">&quot;Please use `jax.custom_vjp` to use callbacks while taking gradients.&quot;</span><span class="s1">)</span>

<span class="s1">ad.primitive_transposes[pure_callback_p] = pure_callback_transpose_rule</span>


<span class="s3">def </span><span class="s1">pure_callback_batching_rule(args</span><span class="s3">, </span><span class="s1">dims</span><span class="s3">, </span><span class="s1">*</span><span class="s3">, </span><span class="s1">callback</span><span class="s3">, </span><span class="s1">vectorized: bool</span><span class="s3">,</span>
                                <span class="s1">result_avals: Sequence[core.ShapedArray]):</span>
  <span class="s1">axis_size = next(a.shape[</span><span class="s5">0</span><span class="s1">] </span><span class="s3">for </span><span class="s1">a</span><span class="s3">, </span><span class="s1">d </span><span class="s3">in </span><span class="s1">zip(args</span><span class="s3">, </span><span class="s1">dims)</span>
                   <span class="s3">if </span><span class="s1">d </span><span class="s3">is not </span><span class="s1">batching.not_mapped)</span>
  <span class="s1">new_args = [arg </span><span class="s3">if </span><span class="s1">dim </span><span class="s3">is </span><span class="s1">batching.not_mapped </span><span class="s3">else</span>
              <span class="s1">batching.moveaxis(arg</span><span class="s3">, </span><span class="s1">dim</span><span class="s3">, </span><span class="s5">0</span><span class="s1">) </span><span class="s3">for </span><span class="s1">arg</span><span class="s3">, </span><span class="s1">dim </span><span class="s3">in </span><span class="s1">zip(args</span><span class="s3">, </span><span class="s1">dims)]</span>
  <span class="s3">if </span><span class="s1">vectorized:</span>
    <span class="s1">result_avals = tuple(</span>
        <span class="s1">core.unmapped_aval(axis_size</span><span class="s3">, </span><span class="s1">core.no_axis_name</span><span class="s3">, </span><span class="s5">0</span><span class="s3">, </span><span class="s1">aval)  </span><span class="s0"># type: ignore</span>
        <span class="s3">for </span><span class="s1">aval </span><span class="s3">in </span><span class="s1">result_avals)</span>
    <span class="s1">outvals = pure_callback_p.bind(</span>
        <span class="s1">*new_args</span><span class="s3">, </span><span class="s1">callback=callback</span><span class="s3">, </span><span class="s1">vectorized=vectorized</span><span class="s3">,</span>
        <span class="s1">result_avals=result_avals)</span>
  <span class="s3">else</span><span class="s1">:</span>
    <span class="s1">is_batched = [d </span><span class="s3">is not </span><span class="s1">batching.not_mapped </span><span class="s3">for </span><span class="s1">d </span><span class="s3">in </span><span class="s1">dims]</span>
    <span class="s1">unbatched_args</span><span class="s3">, </span><span class="s1">batched_args = util.partition_list(is_batched</span><span class="s3">, </span><span class="s1">new_args)</span>
    <span class="s3">def </span><span class="s1">_batch_fun(batched_args):</span>
      <span class="s1">merged_args = util.merge_lists(is_batched</span><span class="s3">, </span><span class="s1">unbatched_args</span><span class="s3">, </span><span class="s1">batched_args)</span>
      <span class="s3">return </span><span class="s1">pure_callback_p.bind(</span>
          <span class="s1">*merged_args</span><span class="s3">, </span><span class="s1">callback=callback</span><span class="s3">, </span><span class="s1">result_avals=result_avals</span><span class="s3">,</span>
          <span class="s1">vectorized=vectorized)</span>
    <span class="s3">from </span><span class="s1">jax._src.lax.control_flow </span><span class="s3">import </span><span class="s1">map </span><span class="s3">as </span><span class="s1">lax_map</span>
    <span class="s1">outvals = lax_map(_batch_fun</span><span class="s3">, </span><span class="s1">batched_args)</span>
  <span class="s3">return </span><span class="s1">tuple(outvals)</span><span class="s3">, </span><span class="s1">(</span><span class="s5">0</span><span class="s3">,</span><span class="s1">) * len(outvals)</span>


<span class="s1">batching.primitive_batchers[pure_callback_p] = pure_callback_batching_rule</span>


<span class="s3">def </span><span class="s1">pure_callback_lowering(ctx</span><span class="s3">, </span><span class="s1">*args</span><span class="s3">, </span><span class="s1">callback</span><span class="s3">, </span><span class="s1">**params):</span>

  <span class="s3">def </span><span class="s1">_callback(*flat_args):</span>
    <span class="s3">return </span><span class="s1">tuple(pure_callback_impl(*flat_args</span><span class="s3">, </span><span class="s1">callback=callback</span><span class="s3">, </span><span class="s1">**params))</span>

  <span class="s1">sharding = </span><span class="s3">None</span>
  <span class="s1">axis_context = ctx.module_context.axis_context</span>
  <span class="s3">if </span><span class="s1">isinstance(axis_context</span><span class="s3">, </span><span class="s1">mlir.ShardingContext):</span>
    <span class="s3">if </span><span class="s1">len(axis_context.device_assignment) &gt; </span><span class="s5">1</span><span class="s1">:</span>
      <span class="s3">raise </span><span class="s1">NotImplementedError(</span>
          <span class="s4">&quot;pure_callback is only supported in spmd computations when all mesh&quot;</span>
          <span class="s4">&quot; axes are partitioned manually (no partial automatic sharding).&quot;</span>
      <span class="s1">)</span>
  <span class="s3">if </span><span class="s1">isinstance(axis_context</span><span class="s3">, </span><span class="s1">mlir.SPMDAxisContext):</span>
    <span class="s3">if </span><span class="s1">axis_context.manual_axes != frozenset(axis_context.mesh.axis_names):</span>
      <span class="s3">raise </span><span class="s1">NotImplementedError(</span>
          <span class="s4">&quot;pure_callback is only supported in spmd computations when all mesh&quot;</span>
          <span class="s4">&quot; axes are partitioned manually (no partial automatic sharding).&quot;</span>
      <span class="s1">)</span>
    <span class="s1">sharding = xc.OpSharding()</span>
    <span class="s1">sharding.type = xc.OpSharding.Type.MANUAL</span>

  <span class="s1">result</span><span class="s3">, </span><span class="s1">_</span><span class="s3">, </span><span class="s1">keepalive = mlir.emit_python_callback(</span>
      <span class="s1">ctx</span><span class="s3">, </span><span class="s1">_callback</span><span class="s3">, None, </span><span class="s1">list(args)</span><span class="s3">, </span><span class="s1">ctx.avals_in</span><span class="s3">, </span><span class="s1">ctx.avals_out</span><span class="s3">, False,</span>
      <span class="s1">sharding=sharding)</span>
  <span class="s1">ctx.module_context.add_keepalive(keepalive)</span>
  <span class="s3">return </span><span class="s1">result</span>

<span class="s1">mlir.register_lowering(pure_callback_p</span><span class="s3">, </span><span class="s1">pure_callback_lowering)</span>

<span class="s3">def </span><span class="s1">_check_shape_dtype(shape_dtype):</span>
  <span class="s1">dt = np.dtype(shape_dtype.dtype)</span>
  <span class="s3">if </span><span class="s1">dtypes.canonicalize_dtype(dt) != dt:</span>
    <span class="s3">raise </span><span class="s1">ValueError(</span>
        <span class="s4">&quot;Cannot return 64-bit values when `jax_enable_x64` is disabled&quot;</span><span class="s1">)</span>

<span class="s3">def </span><span class="s1">pure_callback(callback: Callable[...</span><span class="s3">, </span><span class="s1">Any]</span><span class="s3">, </span><span class="s1">result_shape_dtypes: Any</span><span class="s3">,</span>
                  <span class="s1">*args: Any</span><span class="s3">, </span><span class="s1">vectorized: bool = </span><span class="s3">False, </span><span class="s1">**kwargs: Any):</span>
  <span class="s3">def </span><span class="s1">_flat_callback(*flat_args):</span>
    <span class="s1">args</span><span class="s3">, </span><span class="s1">kwargs = tree_util.tree_unflatten(in_tree</span><span class="s3">, </span><span class="s1">flat_args)</span>
    <span class="s3">return </span><span class="s1">tree_util.tree_leaves(callback(*args</span><span class="s3">, </span><span class="s1">**kwargs))</span>

  <span class="s1">flat_args</span><span class="s3">, </span><span class="s1">in_tree = tree_util.tree_flatten((args</span><span class="s3">, </span><span class="s1">kwargs))</span>
  <span class="s1">tree_util.tree_map(_check_shape_dtype</span><span class="s3">, </span><span class="s1">result_shape_dtypes)</span>
  <span class="s1">result_avals = tree_util.tree_map(</span>
      <span class="s3">lambda </span><span class="s1">x: core.ShapedArray(x.shape</span><span class="s3">, </span><span class="s1">x.dtype)</span><span class="s3">, </span><span class="s1">result_shape_dtypes)</span>
  <span class="s1">flat_result_avals</span><span class="s3">, </span><span class="s1">out_tree = tree_util.tree_flatten(result_avals)</span>
  <span class="s1">out_flat = pure_callback_p.bind(</span>
      <span class="s1">*flat_args</span><span class="s3">, </span><span class="s1">callback=_flat_callback</span><span class="s3">,</span>
      <span class="s1">result_avals=tuple(flat_result_avals)</span><span class="s3">, </span><span class="s1">vectorized=vectorized)</span>
  <span class="s3">return </span><span class="s1">tree_util.tree_unflatten(out_tree</span><span class="s3">, </span><span class="s1">out_flat)</span>


<span class="s0"># IO Callback</span>

<span class="s1">io_callback_p = core.Primitive(</span><span class="s4">&quot;io_callback&quot;</span><span class="s1">)</span>
<span class="s1">io_callback_p.multiple_results = </span><span class="s3">True</span>

<span class="s3">class </span><span class="s1">IOEffect(effects.Effect):</span>
  <span class="s1">__str__ = </span><span class="s3">lambda </span><span class="s1">_: </span><span class="s4">&quot;IO&quot;</span>

<span class="s3">class </span><span class="s1">OrderedIOEffect(effects.Effect):</span>
  <span class="s1">__str__ = </span><span class="s3">lambda </span><span class="s1">_: </span><span class="s4">&quot;OrderedIO&quot;</span>

<span class="s1">_IOEffect = IOEffect()</span>
<span class="s1">_OrderedIOEffect = OrderedIOEffect()</span>
<span class="s1">effects.lowerable_effects.add_type(IOEffect)</span>
<span class="s1">effects.lowerable_effects.add_type(OrderedIOEffect)</span>
<span class="s1">effects.control_flow_allowed_effects.add_type(IOEffect)</span>
<span class="s1">effects.control_flow_allowed_effects.add_type(OrderedIOEffect)</span>
<span class="s1">effects.ordered_effects.add_type(OrderedIOEffect)</span>


<span class="s3">def </span><span class="s1">io_callback_impl(*args</span><span class="s3">, </span><span class="s1">result_avals</span><span class="s3">, </span><span class="s1">callback: Callable[...</span><span class="s3">, </span><span class="s1">Any]</span><span class="s3">,</span>
                     <span class="s1">ordered: bool):</span>
  <span class="s3">del </span><span class="s1">result_avals</span><span class="s3">, </span><span class="s1">ordered</span>
  <span class="s3">return </span><span class="s1">callback(*args)</span>
<span class="s1">io_callback_p.def_impl(functools.partial(dispatch.apply_primitive</span><span class="s3">,</span>
                                         <span class="s1">io_callback_p))</span>

<span class="s1">@io_callback_p.def_effectful_abstract_eval</span>
<span class="s3">def </span><span class="s1">io_callback_abstract_eval(*avals</span><span class="s3">, </span><span class="s1">callback: Callable[...</span><span class="s3">, </span><span class="s1">Any]</span><span class="s3">,</span>
                              <span class="s1">result_avals</span><span class="s3">, </span><span class="s1">ordered: bool):</span>
  <span class="s3">del </span><span class="s1">avals</span><span class="s3">, </span><span class="s1">callback</span>
  <span class="s1">effect = _OrderedIOEffect </span><span class="s3">if </span><span class="s1">ordered </span><span class="s3">else </span><span class="s1">_IOEffect</span>
  <span class="s3">return </span><span class="s1">result_avals</span><span class="s3">, </span><span class="s1">{effect}</span>

<span class="s3">def </span><span class="s1">io_callback_jvp_rule(*args</span><span class="s3">, </span><span class="s1">**kwargs):</span>
  <span class="s3">del </span><span class="s1">args</span><span class="s3">, </span><span class="s1">kwargs</span>
  <span class="s3">raise </span><span class="s1">ValueError(</span><span class="s4">&quot;IO callbacks do not support JVP.&quot;</span><span class="s1">)</span>
<span class="s1">ad.primitive_jvps[io_callback_p] = io_callback_jvp_rule</span>


<span class="s3">def </span><span class="s1">io_callback_transpose_rule(*args</span><span class="s3">, </span><span class="s1">**kwargs):</span>
  <span class="s3">del </span><span class="s1">args</span><span class="s3">, </span><span class="s1">kwargs</span>
  <span class="s3">raise </span><span class="s1">ValueError(</span><span class="s4">&quot;IO callbacks do not support transpose.&quot;</span><span class="s1">)</span>
<span class="s1">ad.primitive_transposes[io_callback_p] = io_callback_transpose_rule</span>


<span class="s3">def </span><span class="s1">io_callback_batching_rule(args</span><span class="s3">, </span><span class="s1">dims</span><span class="s3">, </span><span class="s1">callback</span><span class="s3">, </span><span class="s1">result_avals</span><span class="s3">, </span><span class="s1">ordered):</span>
  <span class="s3">if </span><span class="s1">ordered:</span>
    <span class="s3">raise </span><span class="s1">ValueError(</span><span class="s4">&quot;Cannot `vmap` ordered IO callback.&quot;</span><span class="s1">)</span>
  <span class="s3">return </span><span class="s1">pure_callback_batching_rule(args</span><span class="s3">, </span><span class="s1">dims</span><span class="s3">, </span><span class="s1">callback=callback</span><span class="s3">,</span>
      <span class="s1">vectorized=</span><span class="s3">False, </span><span class="s1">result_avals=result_avals)</span>
<span class="s1">batching.primitive_batchers[io_callback_p] = io_callback_batching_rule</span>

<span class="s3">def </span><span class="s1">io_callback_lowering(ctx</span><span class="s3">, </span><span class="s1">*args</span><span class="s3">, </span><span class="s1">callback</span><span class="s3">, </span><span class="s1">ordered</span><span class="s3">, </span><span class="s1">**params):</span>

  <span class="s3">def </span><span class="s1">_callback(*flat_args):</span>
    <span class="s3">return </span><span class="s1">tuple(io_callback_impl(*flat_args</span><span class="s3">, </span><span class="s1">callback=callback</span><span class="s3">,</span>
                                  <span class="s1">ordered=ordered</span><span class="s3">, </span><span class="s1">**params))</span>

  <span class="s0"># TODO(sharadmv): figure out the best API for sharding callbacks. For now, we</span>
  <span class="s0"># can only safely maximally shard. Should we allow device_index to be passed</span>
  <span class="s0"># in like host_callback?</span>
  <span class="s3">if </span><span class="s1">isinstance(ctx.module_context.axis_context</span><span class="s3">,</span>
                <span class="s1">(mlir.SPMDAxisContext</span><span class="s3">, </span><span class="s1">mlir.ShardingContext)):</span>
    <span class="s0"># Apply maximal sharding so pjit only executes the callback on device 0.</span>
    <span class="s1">sharding = xc.OpSharding()</span>
    <span class="s1">sharding.type = xc.OpSharding.Type.MAXIMAL</span>
    <span class="s1">sharding.tile_assignment_dimensions = [</span><span class="s5">1</span><span class="s1">]</span>
    <span class="s1">sharding.tile_assignment_devices = [</span><span class="s5">0</span><span class="s1">]</span>
  <span class="s3">else</span><span class="s1">:</span>
    <span class="s1">sharding = </span><span class="s3">None</span>

  <span class="s3">if </span><span class="s1">ordered:</span>
    <span class="s1">token = ctx.tokens_in.get(_OrderedIOEffect)[</span><span class="s5">0</span><span class="s1">]</span>
    <span class="s1">result</span><span class="s3">, </span><span class="s1">token</span><span class="s3">, </span><span class="s1">keepalive = mlir.emit_python_callback(</span>
        <span class="s1">ctx</span><span class="s3">, </span><span class="s1">_callback</span><span class="s3">, </span><span class="s1">token</span><span class="s3">, </span><span class="s1">list(args)</span><span class="s3">, </span><span class="s1">ctx.avals_in</span><span class="s3">, </span><span class="s1">ctx.avals_out</span><span class="s3">, True,</span>
        <span class="s1">sharding=sharding)</span>
    <span class="s1">ctx.set_tokens_out(mlir.TokenSet({_OrderedIOEffect: (token</span><span class="s3">,</span><span class="s1">)}))</span>
  <span class="s3">else</span><span class="s1">:</span>
    <span class="s1">result</span><span class="s3">, </span><span class="s1">token</span><span class="s3">, </span><span class="s1">keepalive = mlir.emit_python_callback(</span>
        <span class="s1">ctx</span><span class="s3">, </span><span class="s1">_callback</span><span class="s3">, None, </span><span class="s1">list(args)</span><span class="s3">, </span><span class="s1">ctx.avals_in</span><span class="s3">, </span><span class="s1">ctx.avals_out</span><span class="s3">, True,</span>
        <span class="s1">sharding=sharding)</span>
  <span class="s1">ctx.module_context.add_keepalive(keepalive)</span>
  <span class="s3">return </span><span class="s1">result</span>
<span class="s1">mlir.register_lowering(io_callback_p</span><span class="s3">, </span><span class="s1">io_callback_lowering)</span>

<span class="s3">def </span><span class="s1">io_callback(callback: Callable[...</span><span class="s3">, </span><span class="s1">Any]</span><span class="s3">, </span><span class="s1">result_shape_dtypes: Any</span><span class="s3">,</span>
                <span class="s1">*args: Any</span><span class="s3">, </span><span class="s1">ordered: bool = </span><span class="s3">False, </span><span class="s1">**kwargs: Any):</span>
  <span class="s3">def </span><span class="s1">_flat_callback(*flat_args):</span>
    <span class="s1">args</span><span class="s3">, </span><span class="s1">kwargs = tree_util.tree_unflatten(in_tree</span><span class="s3">, </span><span class="s1">flat_args)</span>
    <span class="s3">return </span><span class="s1">tree_util.tree_leaves(callback(*args</span><span class="s3">, </span><span class="s1">**kwargs))</span>

  <span class="s1">flat_args</span><span class="s3">, </span><span class="s1">in_tree = tree_util.tree_flatten((args</span><span class="s3">, </span><span class="s1">kwargs))</span>
  <span class="s1">tree_util.tree_map(_check_shape_dtype</span><span class="s3">, </span><span class="s1">result_shape_dtypes)</span>
  <span class="s1">flat_shape_dtypes</span><span class="s3">, </span><span class="s1">out_tree = tree_util.tree_flatten(result_shape_dtypes)</span>
  <span class="s1">flat_result_avals = map(</span><span class="s3">lambda </span><span class="s1">x: core.ShapedArray(x.shape</span><span class="s3">, </span><span class="s1">x.dtype)</span><span class="s3">,</span>
                          <span class="s1">flat_shape_dtypes)</span>
  <span class="s1">flat_args = map(core.raise_as_much_as_possible</span><span class="s3">, </span><span class="s1">flat_args)</span>
  <span class="s1">out_flat = io_callback_p.bind(</span>
      <span class="s1">*flat_args</span><span class="s3">, </span><span class="s1">callback=_flat_callback</span><span class="s3">,</span>
      <span class="s1">result_avals=tuple(flat_result_avals)</span><span class="s3">,</span>
      <span class="s1">ordered=ordered)</span>
  <span class="s3">return </span><span class="s1">tree_util.tree_unflatten(out_tree</span><span class="s3">, </span><span class="s1">out_flat)</span>
</pre>
</body>
</html>