<html>
<head>
<title>_lowerings.py</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<style type="text/css">
.s0 { color: #808080;}
.s1 { color: #a9b7c6;}
.s2 { color: #629755; font-style: italic;}
.s3 { color: #cc7832;}
.s4 { color: #6a8759;}
.s5 { color: #6897bb;}
</style>
</head>
<body bgcolor="#2b2b2b">
<table CELLSPACING=0 CELLPADDING=5 COLS=1 WIDTH="100%" BGCOLOR="#606060" >
<tr><td><center>
<font face="Arial, Helvetica" color="#000000">
_lowerings.py</font>
</center></td></tr></table>
<pre><span class="s0"># Copyright 2023 The JAX Authors.</span>
<span class="s0">#</span>
<span class="s0"># Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);</span>
<span class="s0"># you may not use this file except in compliance with the License.</span>
<span class="s0"># You may obtain a copy of the License at</span>
<span class="s0">#</span>
<span class="s0">#     https://www.apache.org/licenses/LICENSE-2.0</span>
<span class="s0">#</span>
<span class="s0"># Unless required by applicable law or agreed to in writing, software</span>
<span class="s0"># distributed under the License is distributed on an &quot;AS IS&quot; BASIS,</span>
<span class="s0"># WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span>
<span class="s0"># See the License for the specific language governing permissions and</span>
<span class="s0"># limitations under the License.</span>
<span class="s2">&quot;&quot;&quot;Primitives for calling out to cusparse. 
 
In general, these primitives are not meant to be used directly, but rather 
are used internally in GPU translation rules of higher-level primitives. 
&quot;&quot;&quot;</span>

<span class="s3">from </span><span class="s1">jax </span><span class="s3">import </span><span class="s1">core</span>
<span class="s3">from </span><span class="s1">jax._src </span><span class="s3">import </span><span class="s1">dispatch</span>
<span class="s3">from </span><span class="s1">jax._src.interpreters </span><span class="s3">import </span><span class="s1">mlir</span>
<span class="s3">from </span><span class="s1">jax._src.lib </span><span class="s3">import </span><span class="s1">gpu_sparse</span>
<span class="s3">import </span><span class="s1">numpy </span><span class="s3">as </span><span class="s1">np</span>


<span class="s1">SUPPORTED_DATA_DTYPES = [np.float32</span><span class="s3">, </span><span class="s1">np.float64</span><span class="s3">, </span><span class="s1">np.complex64</span><span class="s3">, </span><span class="s1">np.complex128]</span>
<span class="s1">SUPPORTED_INDEX_DTYPES = [np.int32]</span>

<span class="s0"># coo_spmv_p</span>
<span class="s0"># This is an internal-only primitive that calls into cusparse coo SpMV.</span>
<span class="s0"># This is a raw lowering that does no validation of inputs; the indices are</span>
<span class="s0"># assumed to be lexicographically sorted, deduplicated, and in-bounds.</span>
<span class="s1">coo_spmv_p = core.Primitive(</span><span class="s4">&quot;coo_spmv&quot;</span><span class="s1">)</span>

<span class="s3">def </span><span class="s1">_coo_spmv_abstract_eval(data</span><span class="s3">, </span><span class="s1">row</span><span class="s3">, </span><span class="s1">col</span><span class="s3">, </span><span class="s1">x</span><span class="s3">, </span><span class="s1">*</span><span class="s3">, </span><span class="s1">transpose</span><span class="s3">, </span><span class="s1">shape):</span>
  <span class="s0"># TODO(jakevdp) support for batched matvec.</span>
  <span class="s3">assert </span><span class="s1">data.shape == row.shape == col.shape</span>
  <span class="s3">assert </span><span class="s1">row.ndim == </span><span class="s5">1</span>
  <span class="s3">assert </span><span class="s1">x.ndim == </span><span class="s5">1</span>

  <span class="s3">assert </span><span class="s1">row.dtype == col.dtype</span>
  <span class="s3">assert </span><span class="s1">row.dtype </span><span class="s3">in </span><span class="s1">SUPPORTED_INDEX_DTYPES</span>

  <span class="s3">assert </span><span class="s1">data.dtype == x.dtype</span>
  <span class="s3">assert </span><span class="s1">x.dtype </span><span class="s3">in </span><span class="s1">SUPPORTED_DATA_DTYPES</span>

  <span class="s3">assert </span><span class="s1">len(shape) == </span><span class="s5">2</span>
  <span class="s3">assert </span><span class="s1">x.shape[</span><span class="s5">0</span><span class="s1">] == (shape[</span><span class="s5">0</span><span class="s1">] </span><span class="s3">if </span><span class="s1">transpose </span><span class="s3">else </span><span class="s1">shape[</span><span class="s5">1</span><span class="s1">])</span>

  <span class="s3">return </span><span class="s1">core.ShapedArray(</span>
    <span class="s1">shape=shape[</span><span class="s5">1</span><span class="s1">:] </span><span class="s3">if </span><span class="s1">transpose </span><span class="s3">else </span><span class="s1">shape[:</span><span class="s5">1</span><span class="s1">]</span><span class="s3">,</span>
    <span class="s1">dtype=x.dtype)</span>

<span class="s3">def </span><span class="s1">_coo_spmv_gpu_lowering(ctx</span><span class="s3">, </span><span class="s1">data</span><span class="s3">, </span><span class="s1">row</span><span class="s3">, </span><span class="s1">col</span><span class="s3">, </span><span class="s1">x</span><span class="s3">, </span><span class="s1">*</span><span class="s3">, </span><span class="s1">transpose</span><span class="s3">, </span><span class="s1">shape):</span>
  <span class="s1">data_aval</span><span class="s3">, </span><span class="s1">row_aval</span><span class="s3">, </span><span class="s1">_</span><span class="s3">, </span><span class="s1">x_aval = ctx.avals_in</span>
  <span class="s3">return </span><span class="s1">[gpu_sparse.cuda_coo_matvec(</span>
            <span class="s1">data</span><span class="s3">, </span><span class="s1">row</span><span class="s3">, </span><span class="s1">col</span><span class="s3">, </span><span class="s1">x</span><span class="s3">,</span>
            <span class="s1">shape=shape</span><span class="s3">,</span>
            <span class="s1">transpose=transpose</span><span class="s3">,</span>
            <span class="s1">data_dtype=data_aval.dtype</span><span class="s3">,</span>
            <span class="s1">index_dtype=row_aval.dtype</span><span class="s3">,</span>
            <span class="s1">x_dtype=x_aval.dtype)]</span>

<span class="s1">coo_spmv_p.def_abstract_eval(_coo_spmv_abstract_eval)</span>
<span class="s1">dispatch.simple_impl(coo_spmv_p)</span>
<span class="s3">if </span><span class="s1">gpu_sparse.cuda_is_supported:</span>
  <span class="s1">mlir.register_lowering(coo_spmv_p</span><span class="s3">, </span><span class="s1">_coo_spmv_gpu_lowering</span><span class="s3">, </span><span class="s1">platform=</span><span class="s4">'cuda'</span><span class="s1">)</span>
<span class="s3">if </span><span class="s1">gpu_sparse.rocm_is_supported:</span>
  <span class="s1">mlir.register_lowering(coo_spmv_p</span><span class="s3">, </span><span class="s1">_coo_spmv_gpu_lowering</span><span class="s3">, </span><span class="s1">platform=</span><span class="s4">'rocm'</span><span class="s1">)</span>


<span class="s0"># coo_spmm_p</span>
<span class="s0"># This is an internal-only primitive that calls into cusparse COO SpMM.</span>
<span class="s0"># This is a raw lowering that does no validation of inputs; the indices are</span>
<span class="s0"># assumed to be lexicographically sorted, deduplicated, and in-bounds.</span>
<span class="s1">coo_spmm_p = core.Primitive(</span><span class="s4">&quot;coo_spmm&quot;</span><span class="s1">)</span>

<span class="s3">def </span><span class="s1">_coo_spmm_abstract_eval(data</span><span class="s3">, </span><span class="s1">row</span><span class="s3">, </span><span class="s1">col</span><span class="s3">, </span><span class="s1">x</span><span class="s3">, </span><span class="s1">*</span><span class="s3">, </span><span class="s1">transpose</span><span class="s3">, </span><span class="s1">shape):</span>
  <span class="s0"># TODO(jakevdp) support for batched matmat.</span>
  <span class="s3">assert </span><span class="s1">data.shape == row.shape == col.shape</span>
  <span class="s3">assert </span><span class="s1">row.ndim == </span><span class="s5">1</span>
  <span class="s3">assert </span><span class="s1">x.ndim == </span><span class="s5">2</span>

  <span class="s3">assert </span><span class="s1">row.dtype == col.dtype</span>
  <span class="s3">assert </span><span class="s1">row.dtype </span><span class="s3">in </span><span class="s1">SUPPORTED_INDEX_DTYPES</span>

  <span class="s3">assert </span><span class="s1">data.dtype == x.dtype</span>
  <span class="s3">assert </span><span class="s1">x.dtype </span><span class="s3">in </span><span class="s1">SUPPORTED_DATA_DTYPES</span>

  <span class="s3">assert </span><span class="s1">len(shape) == </span><span class="s5">2</span>
  <span class="s3">assert </span><span class="s1">x.shape[</span><span class="s5">0</span><span class="s1">] == (shape[</span><span class="s5">0</span><span class="s1">] </span><span class="s3">if </span><span class="s1">transpose </span><span class="s3">else </span><span class="s1">shape[</span><span class="s5">1</span><span class="s1">])</span>

  <span class="s3">return </span><span class="s1">core.ShapedArray(</span>
    <span class="s1">shape=(shape[</span><span class="s5">1</span><span class="s1">] </span><span class="s3">if </span><span class="s1">transpose </span><span class="s3">else </span><span class="s1">shape[</span><span class="s5">0</span><span class="s1">]</span><span class="s3">, </span><span class="s1">x.shape[</span><span class="s5">1</span><span class="s1">])</span><span class="s3">,</span>
    <span class="s1">dtype=x.dtype)</span>

<span class="s3">def </span><span class="s1">_coo_spmm_gpu_lowering(ctx</span><span class="s3">, </span><span class="s1">data</span><span class="s3">, </span><span class="s1">row</span><span class="s3">, </span><span class="s1">col</span><span class="s3">, </span><span class="s1">x</span><span class="s3">, </span><span class="s1">*</span><span class="s3">, </span><span class="s1">transpose</span><span class="s3">, </span><span class="s1">shape):</span>
  <span class="s1">data_aval</span><span class="s3">, </span><span class="s1">row_aval</span><span class="s3">, </span><span class="s1">_</span><span class="s3">, </span><span class="s1">x_aval = ctx.avals_in</span>
  <span class="s3">return </span><span class="s1">[gpu_sparse.cuda_coo_matmat(</span>
            <span class="s1">data</span><span class="s3">, </span><span class="s1">row</span><span class="s3">, </span><span class="s1">col</span><span class="s3">, </span><span class="s1">x</span><span class="s3">,</span>
            <span class="s1">shape=shape</span><span class="s3">,</span>
            <span class="s1">transpose=transpose</span><span class="s3">,</span>
            <span class="s1">data_dtype=data_aval.dtype</span><span class="s3">,</span>
            <span class="s1">index_dtype=row_aval.dtype</span><span class="s3">,</span>
            <span class="s1">x_dtype=x_aval.dtype)]</span>

<span class="s1">coo_spmm_p.def_abstract_eval(_coo_spmm_abstract_eval)</span>
<span class="s1">dispatch.simple_impl(coo_spmm_p)</span>
<span class="s3">if </span><span class="s1">gpu_sparse.cuda_is_supported:</span>
  <span class="s1">mlir.register_lowering(coo_spmm_p</span><span class="s3">, </span><span class="s1">_coo_spmm_gpu_lowering</span><span class="s3">, </span><span class="s1">platform=</span><span class="s4">'cuda'</span><span class="s1">)</span>
<span class="s3">if </span><span class="s1">gpu_sparse.rocm_is_supported:</span>
  <span class="s1">mlir.register_lowering(coo_spmm_p</span><span class="s3">, </span><span class="s1">_coo_spmm_gpu_lowering</span><span class="s3">, </span><span class="s1">platform=</span><span class="s4">'rocm'</span><span class="s1">)</span>

<span class="s0"># csr_spmv_p</span>
<span class="s0"># This is an internal-only primitive that calls into cusparse csr SpMV.</span>
<span class="s0"># This is a raw lowering that does no validation of inputs; the indices are</span>
<span class="s0"># assumed to be lexicographically sorted, deduplicated, and in-bounds.</span>
<span class="s1">csr_spmv_p = core.Primitive(</span><span class="s4">&quot;csr_spmv&quot;</span><span class="s1">)</span>

<span class="s3">def </span><span class="s1">_csr_spmv_abstract_eval(data</span><span class="s3">, </span><span class="s1">indices</span><span class="s3">, </span><span class="s1">indptr</span><span class="s3">, </span><span class="s1">x</span><span class="s3">, </span><span class="s1">*</span><span class="s3">, </span><span class="s1">transpose</span><span class="s3">, </span><span class="s1">shape):</span>
  <span class="s0"># TODO(tianjianlu) support for batched matvec.</span>
  <span class="s3">assert </span><span class="s1">data.ndim == indices.ndim == indptr.ndim == </span><span class="s5">1</span>
  <span class="s3">assert </span><span class="s1">data.shape == indices.shape</span>
  <span class="s3">assert </span><span class="s1">indptr.shape[</span><span class="s5">0</span><span class="s1">] == shape[</span><span class="s5">0</span><span class="s1">] + </span><span class="s5">1</span>
  <span class="s3">assert </span><span class="s1">x.ndim == </span><span class="s5">1</span>

  <span class="s3">assert </span><span class="s1">indices.dtype == indptr.dtype</span>
  <span class="s3">assert </span><span class="s1">indices.dtype </span><span class="s3">in </span><span class="s1">SUPPORTED_INDEX_DTYPES</span>
  <span class="s3">assert </span><span class="s1">data.dtype == x.dtype</span>
  <span class="s3">assert </span><span class="s1">x.dtype </span><span class="s3">in </span><span class="s1">SUPPORTED_DATA_DTYPES</span>

  <span class="s3">assert </span><span class="s1">len(shape) == </span><span class="s5">2</span>
  <span class="s3">assert </span><span class="s1">x.shape[</span><span class="s5">0</span><span class="s1">] == (shape[</span><span class="s5">0</span><span class="s1">] </span><span class="s3">if </span><span class="s1">transpose </span><span class="s3">else </span><span class="s1">shape[</span><span class="s5">1</span><span class="s1">])</span>

  <span class="s3">return </span><span class="s1">core.ShapedArray(</span>
    <span class="s1">shape=shape[</span><span class="s5">1</span><span class="s1">:] </span><span class="s3">if </span><span class="s1">transpose </span><span class="s3">else </span><span class="s1">shape[:</span><span class="s5">1</span><span class="s1">]</span><span class="s3">,</span>
    <span class="s1">dtype=x.dtype)</span>

<span class="s3">def </span><span class="s1">_csr_spmv_gpu_lowering(ctx</span><span class="s3">, </span><span class="s1">data</span><span class="s3">, </span><span class="s1">indices</span><span class="s3">, </span><span class="s1">indptr</span><span class="s3">, </span><span class="s1">x</span><span class="s3">, </span><span class="s1">*</span><span class="s3">, </span><span class="s1">transpose</span><span class="s3">, </span><span class="s1">shape):</span>
  <span class="s1">data_aval</span><span class="s3">, </span><span class="s1">indices_aval</span><span class="s3">, </span><span class="s1">_</span><span class="s3">, </span><span class="s1">x_aval = ctx.avals_in</span>
  <span class="s3">return </span><span class="s1">[gpu_sparse.cuda_csr_matvec(</span>
            <span class="s1">data</span><span class="s3">, </span><span class="s1">indices</span><span class="s3">, </span><span class="s1">indptr</span><span class="s3">, </span><span class="s1">x</span><span class="s3">,</span>
            <span class="s1">shape=shape</span><span class="s3">,</span>
            <span class="s1">transpose=transpose</span><span class="s3">,</span>
            <span class="s1">data_dtype=data_aval.dtype</span><span class="s3">,</span>
            <span class="s1">index_dtype=indices_aval.dtype</span><span class="s3">,</span>
            <span class="s1">x_dtype=x_aval.dtype)]</span>

<span class="s1">csr_spmv_p.def_abstract_eval(_csr_spmv_abstract_eval)</span>
<span class="s1">dispatch.simple_impl(csr_spmv_p)</span>
<span class="s3">if </span><span class="s1">gpu_sparse.cuda_is_supported:</span>
  <span class="s1">mlir.register_lowering(csr_spmv_p</span><span class="s3">, </span><span class="s1">_csr_spmv_gpu_lowering</span><span class="s3">, </span><span class="s1">platform=</span><span class="s4">'cuda'</span><span class="s1">)</span>
<span class="s3">if </span><span class="s1">gpu_sparse.rocm_is_supported:</span>
  <span class="s1">mlir.register_lowering(csr_spmv_p</span><span class="s3">, </span><span class="s1">_csr_spmv_gpu_lowering</span><span class="s3">, </span><span class="s1">platform=</span><span class="s4">'rocm'</span><span class="s1">)</span>


<span class="s0"># csr_spmm_p</span>
<span class="s0"># This is an internal-only primitive that calls into cusparse CSR SpMM.</span>
<span class="s0"># This is a raw lowering that does no validation of inputs; the indices are</span>
<span class="s0"># assumed to be lexicographically sorted, deduplicated, and in-bounds.</span>
<span class="s1">csr_spmm_p = core.Primitive(</span><span class="s4">&quot;csr_spmm&quot;</span><span class="s1">)</span>

<span class="s3">def </span><span class="s1">_csr_spmm_abstract_eval(data</span><span class="s3">, </span><span class="s1">indices</span><span class="s3">, </span><span class="s1">indptr</span><span class="s3">, </span><span class="s1">x</span><span class="s3">, </span><span class="s1">*</span><span class="s3">, </span><span class="s1">transpose</span><span class="s3">, </span><span class="s1">shape):</span>
  <span class="s0"># TODO(tianjianlu) support for batched matmat.</span>
  <span class="s3">assert </span><span class="s1">data.ndim == indices.ndim == indptr.ndim == </span><span class="s5">1</span>
  <span class="s3">assert </span><span class="s1">data.shape == indices.shape</span>
  <span class="s3">assert </span><span class="s1">indptr.shape[</span><span class="s5">0</span><span class="s1">] == shape[</span><span class="s5">0</span><span class="s1">] + </span><span class="s5">1</span>
  <span class="s3">assert </span><span class="s1">x.ndim == </span><span class="s5">2</span>

  <span class="s3">assert </span><span class="s1">indices.dtype == indptr.dtype</span>
  <span class="s3">assert </span><span class="s1">indices.dtype </span><span class="s3">in </span><span class="s1">SUPPORTED_INDEX_DTYPES</span>
  <span class="s3">assert </span><span class="s1">data.dtype == x.dtype</span>
  <span class="s3">assert </span><span class="s1">x.dtype </span><span class="s3">in </span><span class="s1">SUPPORTED_DATA_DTYPES</span>

  <span class="s3">assert </span><span class="s1">len(shape) == </span><span class="s5">2</span>
  <span class="s3">assert </span><span class="s1">x.shape[</span><span class="s5">0</span><span class="s1">] == (shape[</span><span class="s5">0</span><span class="s1">] </span><span class="s3">if </span><span class="s1">transpose </span><span class="s3">else </span><span class="s1">shape[</span><span class="s5">1</span><span class="s1">])</span>

  <span class="s3">return </span><span class="s1">core.ShapedArray(</span>
    <span class="s1">shape=(shape[</span><span class="s5">1</span><span class="s1">] </span><span class="s3">if </span><span class="s1">transpose </span><span class="s3">else </span><span class="s1">shape[</span><span class="s5">0</span><span class="s1">]</span><span class="s3">, </span><span class="s1">x.shape[</span><span class="s5">1</span><span class="s1">])</span><span class="s3">,</span>
    <span class="s1">dtype=x.dtype)</span>

<span class="s3">def </span><span class="s1">_csr_spmm_gpu_lowering(ctx</span><span class="s3">, </span><span class="s1">data</span><span class="s3">, </span><span class="s1">indices</span><span class="s3">, </span><span class="s1">indptr</span><span class="s3">, </span><span class="s1">x</span><span class="s3">, </span><span class="s1">*</span><span class="s3">, </span><span class="s1">transpose</span><span class="s3">, </span><span class="s1">shape):</span>
  <span class="s1">data_aval</span><span class="s3">, </span><span class="s1">indices_aval</span><span class="s3">, </span><span class="s1">_</span><span class="s3">, </span><span class="s1">x_aval = ctx.avals_in</span>
  <span class="s3">return </span><span class="s1">[gpu_sparse.cuda_csr_matmat(</span>
            <span class="s1">data</span><span class="s3">, </span><span class="s1">indices</span><span class="s3">, </span><span class="s1">indptr</span><span class="s3">, </span><span class="s1">x</span><span class="s3">,</span>
            <span class="s1">shape=shape</span><span class="s3">,</span>
            <span class="s1">transpose=transpose</span><span class="s3">,</span>
            <span class="s1">data_dtype=data_aval.dtype</span><span class="s3">,</span>
            <span class="s1">index_dtype=indices_aval.dtype</span><span class="s3">,</span>
            <span class="s1">B_dtype=x_aval.dtype)]</span>

<span class="s1">csr_spmm_p.def_abstract_eval(_csr_spmm_abstract_eval)</span>
<span class="s1">dispatch.simple_impl(csr_spmm_p)</span>
<span class="s3">if </span><span class="s1">gpu_sparse.cuda_is_supported:</span>
  <span class="s1">mlir.register_lowering(csr_spmm_p</span><span class="s3">, </span><span class="s1">_csr_spmm_gpu_lowering</span><span class="s3">, </span><span class="s1">platform=</span><span class="s4">'cuda'</span><span class="s1">)</span>
<span class="s3">if </span><span class="s1">gpu_sparse.rocm_is_supported:</span>
  <span class="s1">mlir.register_lowering(csr_spmm_p</span><span class="s3">, </span><span class="s1">_csr_spmm_gpu_lowering</span><span class="s3">, </span><span class="s1">platform=</span><span class="s4">'rocm'</span><span class="s1">)</span>
</pre>
</body>
</html>