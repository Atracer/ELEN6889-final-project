<html>
<head>
<title>impl_no_xla.py</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<style type="text/css">
.s0 { color: #808080;}
.s1 { color: #a9b7c6;}
.s2 { color: #629755; font-style: italic;}
.s3 { color: #cc7832;}
.s4 { color: #6a8759;}
.s5 { color: #6897bb;}
</style>
</head>
<body bgcolor="#2b2b2b">
<table CELLSPACING=0 CELLPADDING=5 COLS=1 WIDTH="100%" BGCOLOR="#606060" >
<tr><td><center>
<font face="Arial, Helvetica" color="#000000">
impl_no_xla.py</font>
</center></td></tr></table>
<pre><span class="s0"># Copyright 2020 The JAX Authors.</span>
<span class="s0">#</span>
<span class="s0"># Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);</span>
<span class="s0"># you may not use this file except in compliance with the License.</span>
<span class="s0"># You may obtain a copy of the License at</span>
<span class="s0">#</span>
<span class="s0">#     https://www.apache.org/licenses/LICENSE-2.0</span>
<span class="s0">#</span>
<span class="s0"># Unless required by applicable law or agreed to in writing, software</span>
<span class="s0"># distributed under the License is distributed on an &quot;AS IS&quot; BASIS,</span>
<span class="s0"># WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span>
<span class="s0"># See the License for the specific language governing permissions and</span>
<span class="s0"># limitations under the License.</span>
<span class="s2">&quot;&quot;&quot;Workarounds for jax2tf transforms when XLA is not linked in.&quot;&quot;&quot;</span>
<span class="s3">import </span><span class="s1">builtins</span>
<span class="s3">import </span><span class="s1">dataclasses</span>
<span class="s3">from </span><span class="s1">functools </span><span class="s3">import </span><span class="s1">partial</span><span class="s3">, </span><span class="s1">wraps</span>
<span class="s3">import </span><span class="s1">string</span>
<span class="s3">from </span><span class="s1">typing </span><span class="s3">import </span><span class="s1">Any</span><span class="s3">, </span><span class="s1">Callable</span><span class="s3">, </span><span class="s1">Dict</span><span class="s3">, </span><span class="s1">Optional</span><span class="s3">, </span><span class="s1">Sequence</span><span class="s3">, </span><span class="s1">Tuple</span>

<span class="s3">from </span><span class="s1">jax._src </span><span class="s3">import </span><span class="s1">core</span>
<span class="s3">from </span><span class="s1">jax </span><span class="s3">import </span><span class="s1">lax</span>
<span class="s3">from </span><span class="s1">jax._src.lax </span><span class="s3">import </span><span class="s1">slicing </span><span class="s3">as </span><span class="s1">lax_slicing</span>
<span class="s3">from </span><span class="s1">jax._src </span><span class="s3">import </span><span class="s1">dtypes</span>
<span class="s3">from </span><span class="s1">jax._src </span><span class="s3">import </span><span class="s1">util</span>

<span class="s3">from </span><span class="s1">jax.experimental.jax2tf </span><span class="s3">import </span><span class="s1">jax2tf</span>

<span class="s3">import </span><span class="s1">numpy </span><span class="s3">as </span><span class="s1">np</span>
<span class="s3">import </span><span class="s1">tensorflow </span><span class="s3">as </span><span class="s1">tf  </span><span class="s0"># type: ignore[import]</span>


<span class="s0"># Implementation rules for primitives when XLA is not linked in. These</span>
<span class="s0"># implementations are workarounds, making use of TF ops that do work when XLA is</span>
<span class="s0"># not linked in. They are only used when the argument `enable_xla=False` when</span>
<span class="s0"># calling jax2tf.convert().</span>
<span class="s1">tf_impl_no_xla: Dict[core.Primitive</span><span class="s3">, </span><span class="s1">Callable[...</span><span class="s3">, </span><span class="s1">Any]] = {}</span>


<span class="s1">TfVal = Any</span>
<span class="s1">DType = Any</span>
<span class="s1">PrecisionType = Any</span>


<span class="s3">def </span><span class="s1">_error(primitive_name: str</span><span class="s3">, </span><span class="s1">suffix_msg: str = </span><span class="s4">&quot;&quot;</span><span class="s1">) -&gt; Exception:</span>
  <span class="s1">msg = </span><span class="s4">f&quot;Call to </span><span class="s3">{</span><span class="s1">primitive_name</span><span class="s3">} </span><span class="s4">cannot be converted with enable_xla=False.&quot;</span>
  <span class="s3">if </span><span class="s1">suffix_msg:</span>
    <span class="s1">msg += (</span><span class="s4">f&quot; </span><span class="s3">{</span><span class="s1">suffix_msg</span><span class="s3">} </span><span class="s4">- See source code for the precise conditions under &quot;</span>
             <span class="s4">&quot;which it can be converted without XLA.&quot;</span><span class="s1">)</span>
  <span class="s3">return </span><span class="s1">NotImplementedError(msg)</span>

<span class="s1">_conv_error = </span><span class="s3">lambda </span><span class="s1">msg: _error(</span><span class="s4">&quot;conv_general_dilated&quot;</span><span class="s3">, </span><span class="s1">msg)</span>
<span class="s1">_reduce_error = </span><span class="s3">lambda </span><span class="s1">msg: _error(</span><span class="s4">&quot;reduce_window&quot;</span><span class="s3">, </span><span class="s1">msg)</span>
<span class="s1">_scatter_error = </span><span class="s3">lambda </span><span class="s1">msg: _error(</span><span class="s4">&quot;scatter_(update/add/multiply/min/max)&quot;</span><span class="s3">, </span><span class="s1">msg</span>
                                   <span class="s1">)</span>

<span class="s3">def </span><span class="s1">_unimplemented(name):</span>

  <span class="s3">def </span><span class="s1">op(*arg</span><span class="s3">, </span><span class="s1">**kwargs):</span>
    <span class="s3">raise </span><span class="s1">_error(name)</span>

  <span class="s3">return </span><span class="s1">op</span>


<span class="s0"># TODO(marcvanzee): Remove this function and use `tf.math.invert_permutation`</span>
<span class="s0"># once it is implemented by TFjs:</span>
<span class="s0"># https://github.com/tensorflow/tfjs/issues/6395.</span>
<span class="s3">def </span><span class="s1">_invert_permutation(perm):</span>
  <span class="s3">return </span><span class="s1">tuple(perm.index(i) </span><span class="s3">for </span><span class="s1">i </span><span class="s3">in </span><span class="s1">range(len(perm)))</span>


<span class="s3">def </span><span class="s1">_transpose_with_shape(x: TfVal</span><span class="s3">, </span><span class="s1">x_shape: core.Shape</span><span class="s3">, </span><span class="s1">permutation) -&gt; Tuple[TfVal</span><span class="s3">, </span><span class="s1">core.Shape]:</span>
  <span class="s2">&quot;&quot;&quot;Computes transposition of x and its shape. 
 
  x_shape matches x.shape in the known dimensions, and it has dimension 
  polynomials elsewhere, while x.shape has None. 
  &quot;&quot;&quot;</span>
  <span class="s3">return </span><span class="s1">tf.transpose(x</span><span class="s3">, </span><span class="s1">perm=permutation)</span><span class="s3">, </span><span class="s1">tuple(x_shape[i] </span><span class="s3">for </span><span class="s1">i </span><span class="s3">in </span><span class="s1">permutation)</span>


<span class="s3">def </span><span class="s1">_transpose_for_tf_conv(lhs</span><span class="s3">, </span><span class="s1">lhs_shape: core.Shape</span><span class="s3">,</span>
                           <span class="s1">rhs</span><span class="s3">, </span><span class="s1">rhs_shape: core.Shape</span><span class="s3">, </span><span class="s1">dimension_numbers):</span>
  <span class="s2">&quot;&quot;&quot;Tranposes lhs and rhs to respectively NHWC and HWIO so they can be passed to TF functions. 
 
  The shapes passed in and returned may contain polynomials, and thus may 
  be different than lhs.shape and rhs.shape. 
  &quot;&quot;&quot;</span>
  <span class="s0"># TODO(marcvanzee): Add tests for this ops for shape polymorphism.</span>
  <span class="s1">lhs_perm</span><span class="s3">, </span><span class="s1">rhs_perm</span><span class="s3">, </span><span class="s1">_ = dimension_numbers</span>

  <span class="s0"># TODO(marcvanzee): Consider merging tranposes if we want to optimize.</span>
  <span class="s0"># For `lhs_perm` / `output_perm`, perm (0, 1, 2, 3) corresponds to &quot;NCHW&quot;.</span>
  <span class="s1">lhs</span><span class="s3">, </span><span class="s1">lhs_shape = _transpose_with_shape(lhs</span><span class="s3">, </span><span class="s1">lhs_shape</span><span class="s3">, </span><span class="s1">lhs_perm)  </span><span class="s0"># lhs --&gt; &quot;NCHW&quot;</span>
  <span class="s3">if </span><span class="s1">len(lhs_perm) == </span><span class="s5">3</span><span class="s1">:</span>
    <span class="s0"># For 1D convolution, we add a trivial &quot;W&quot; dimension, so that 2D Convolution</span>
    <span class="s0"># logic can be applied downstream.</span>
    <span class="s1">lhs = lhs[:</span><span class="s3">, </span><span class="s1">:</span><span class="s3">, </span><span class="s1">:</span><span class="s3">, </span><span class="s1">np.newaxis]</span>
    <span class="s1">lhs_shape = tuple(lhs_shape) + (</span><span class="s5">1</span><span class="s3">,</span><span class="s1">)</span>
  <span class="s0"># However, the TF ops only support &quot;NHWC&quot; on CPU, so we transpose again.</span>
  <span class="s1">lhs</span><span class="s3">, </span><span class="s1">lhs_shape = _transpose_with_shape(lhs</span><span class="s3">, </span><span class="s1">lhs_shape</span><span class="s3">, </span><span class="s1">(</span><span class="s5">0</span><span class="s3">, </span><span class="s5">2</span><span class="s3">, </span><span class="s5">3</span><span class="s3">, </span><span class="s5">1</span><span class="s1">))  </span><span class="s0"># &quot;NCHW&quot; --&gt; &quot;NHWC&quot;</span>

  <span class="s0"># For `rhs_perm`, perm (0, 1, 2, 3) corresponds to &quot;OIHW&quot;.</span>
  <span class="s1">rhs</span><span class="s3">, </span><span class="s1">rhs_shape = _transpose_with_shape(rhs</span><span class="s3">, </span><span class="s1">rhs_shape</span><span class="s3">, </span><span class="s1">rhs_perm)  </span><span class="s0"># rhs --&gt; &quot;OIHW&quot;</span>
  <span class="s0"># Handle conv1d case.</span>
  <span class="s3">if </span><span class="s1">len(rhs_perm) == </span><span class="s5">3</span><span class="s1">:</span>
    <span class="s1">rhs = rhs[:</span><span class="s3">, </span><span class="s1">:</span><span class="s3">, </span><span class="s1">:</span><span class="s3">, </span><span class="s1">np.newaxis]</span>
    <span class="s1">rhs_shape = tuple(rhs_shape) + (</span><span class="s5">1</span><span class="s3">,</span><span class="s1">)</span>
  <span class="s0"># For the tf ops, rhs is expected to be &quot;OIHW&quot;.</span>
  <span class="s1">rhs</span><span class="s3">, </span><span class="s1">rhs_shape = _transpose_with_shape(rhs</span><span class="s3">, </span><span class="s1">rhs_shape</span><span class="s3">, </span><span class="s1">(</span><span class="s5">2</span><span class="s3">, </span><span class="s5">3</span><span class="s3">, </span><span class="s5">1</span><span class="s3">, </span><span class="s5">0</span><span class="s1">))  </span><span class="s0"># &quot;OIHW&quot; --&gt; &quot;HWIO&quot;</span>
  <span class="s1">jax2tf._assert_matching_abstract_shape(lhs</span><span class="s3">, </span><span class="s1">lhs_shape)</span>
  <span class="s1">jax2tf._assert_matching_abstract_shape(rhs</span><span class="s3">, </span><span class="s1">rhs_shape)</span>
  <span class="s3">return </span><span class="s1">lhs</span><span class="s3">, </span><span class="s1">lhs_shape</span><span class="s3">, </span><span class="s1">rhs</span><span class="s3">, </span><span class="s1">rhs_shape</span>


<span class="s3">def </span><span class="s1">pads_to_padtype(in_shape</span><span class="s3">, </span><span class="s1">window_shape</span><span class="s3">, </span><span class="s1">window_strides</span><span class="s3">, </span><span class="s1">padding) -&gt; str:</span>
  <span class="s3">for </span><span class="s1">pad_str </span><span class="s3">in </span><span class="s1">[</span><span class="s4">&quot;VALID&quot;</span><span class="s3">, </span><span class="s4">&quot;SAME&quot;</span><span class="s1">]:</span>
    <span class="s1">pads = lax.padtype_to_pads(in_shape</span><span class="s3">, </span><span class="s1">window_shape</span><span class="s3">, </span><span class="s1">window_strides</span><span class="s3">, </span><span class="s1">pad_str)</span>
    <span class="s3">if </span><span class="s1">list(pads) == list(padding):</span>
      <span class="s3">return </span><span class="s1">pad_str</span>
  <span class="s3">return </span><span class="s4">&quot;EXPLICIT&quot;</span>


<span class="s3">def </span><span class="s1">_pad_spatial_dims(x</span><span class="s3">, </span><span class="s1">x_shape</span><span class="s3">, </span><span class="s1">padding):</span>
  <span class="s2">&quot;&quot;&quot;Pads `x` using `padding`, which specifies padding for the spatial dimensions.&quot;&quot;&quot;</span>
  <span class="s1">padding = tuple(padding)</span>
  <span class="s3">if </span><span class="s1">len(padding) == len(x_shape) - </span><span class="s5">2</span><span class="s1">:</span>
    <span class="s0"># If necessary, add empty padding for batch and feature dimensions.</span>
    <span class="s1">no_pad = ((</span><span class="s5">0</span><span class="s3">, </span><span class="s5">0</span><span class="s1">)</span><span class="s3">,</span><span class="s1">)</span>
    <span class="s1">padding = no_pad + padding + no_pad</span>
  <span class="s1">x = tf.pad(x</span><span class="s3">, </span><span class="s1">padding)</span>
  <span class="s3">assert </span><span class="s1">len(x.shape) == len(padding)</span>
  <span class="s1">x_shape = tuple(p0 + xs + p1 </span><span class="s3">for </span><span class="s1">xs</span><span class="s3">, </span><span class="s1">(p0</span><span class="s3">, </span><span class="s1">p1) </span><span class="s3">in </span><span class="s1">zip(x_shape</span><span class="s3">, </span><span class="s1">padding))</span>
  <span class="s1">jax2tf._assert_matching_abstract_shape(x</span><span class="s3">, </span><span class="s1">x_shape)</span>
  <span class="s3">return </span><span class="s1">x</span><span class="s3">, </span><span class="s1">x_shape</span>


<span class="s3">def </span><span class="s1">_conv_transpose_pads_to_padtype(kernel_sdims</span><span class="s3">, </span><span class="s1">lhs_dilation</span><span class="s3">, </span><span class="s1">padding):</span>
  <span class="s2">&quot;&quot;&quot;Finds the padding type for a transpose convolution.&quot;&quot;&quot;</span>
  <span class="s0"># This is simply checking agreement with lax._conv_transpose_padding.</span>
  <span class="s1">is_valid = </span><span class="s3">True</span>
  <span class="s1">is_same = </span><span class="s3">True</span>
  <span class="s3">if not </span><span class="s1">len(kernel_sdims) == len(lhs_dilation) == len(padding):</span>
    <span class="s3">raise </span><span class="s1">ValueError(</span><span class="s4">f'Found different lengths for '</span>
                     <span class="s4">f'kernel_sdims (</span><span class="s3">{</span><span class="s1">kernel_sdims</span><span class="s3">}</span><span class="s4">), '</span>
                     <span class="s4">f'lhs_dilation (</span><span class="s3">{</span><span class="s1">lhs_dilation</span><span class="s3">}</span><span class="s4">), '</span>
                     <span class="s4">f'and padding (</span><span class="s3">{</span><span class="s1">padding</span><span class="s3">}</span><span class="s4">).'</span><span class="s1">)</span>
  <span class="s3">for </span><span class="s1">k</span><span class="s3">, </span><span class="s1">s</span><span class="s3">, </span><span class="s1">(begin</span><span class="s3">, </span><span class="s1">end) </span><span class="s3">in </span><span class="s1">zip(kernel_sdims</span><span class="s3">, </span><span class="s1">lhs_dilation</span><span class="s3">, </span><span class="s1">padding):</span>
    <span class="s0"># Check for VALID padding.</span>
    <span class="s1">pad_len_valid = k + s - </span><span class="s5">2 </span><span class="s1">+ builtins.max(k - s</span><span class="s3">, </span><span class="s5">0</span><span class="s1">)</span>
    <span class="s1">pad_a = k - </span><span class="s5">1</span>
    <span class="s1">pad_b = pad_len_valid - pad_a</span>
    <span class="s3">if </span><span class="s1">begin != pad_a </span><span class="s3">or </span><span class="s1">end != pad_b:</span>
      <span class="s1">is_valid = </span><span class="s3">False</span>

    <span class="s0"># Check for SAME padding.</span>
    <span class="s1">pad_len_same = k + s - </span><span class="s5">2</span>
    <span class="s3">if </span><span class="s1">s &gt; k - </span><span class="s5">1</span><span class="s1">:</span>
      <span class="s1">pad_a = k - </span><span class="s5">1</span>
    <span class="s3">else</span><span class="s1">:</span>
      <span class="s1">pad_a = int(np.ceil(pad_len_same / </span><span class="s5">2</span><span class="s1">))</span>
    <span class="s1">pad_b = pad_len_same - pad_a</span>
    <span class="s3">if </span><span class="s1">begin != pad_a </span><span class="s3">or </span><span class="s1">end != pad_b:</span>
      <span class="s1">is_same = </span><span class="s3">False</span>

  <span class="s3">if </span><span class="s1">is_valid:</span>
    <span class="s3">return </span><span class="s4">'VALID'</span>
  <span class="s3">elif </span><span class="s1">is_same:</span>
    <span class="s3">return </span><span class="s4">'SAME'</span>
  <span class="s3">raise </span><span class="s1">ValueError(</span><span class="s4">'Transpose convolution padding mode must be '</span>
                   <span class="s4">'`SAME` or `VALID`.'</span><span class="s1">)</span>

<span class="s3">def </span><span class="s1">_validate_spatial_dimensions(lhs: TfVal</span><span class="s3">, </span><span class="s1">lhs_shape: core.Shape</span><span class="s3">,</span>
                                 <span class="s1">rhs: TfVal</span><span class="s3">, </span><span class="s1">rhs_shape: core.Shape):</span>
  <span class="s2">&quot;&quot;&quot;Check spatial dimension support.&quot;&quot;&quot;</span>
  <span class="s1">jax2tf._assert_matching_abstract_shape(lhs</span><span class="s3">, </span><span class="s1">lhs_shape)</span>
  <span class="s1">jax2tf._assert_matching_abstract_shape(rhs</span><span class="s3">, </span><span class="s1">rhs_shape)</span>

  <span class="s1">nr_spatial_dimensions = len(lhs_shape) - </span><span class="s5">2</span>
  <span class="s0"># Currently we only support 1D+2D convolutions because it keeps the code</span>
  <span class="s0"># relatively simple and covers most cases.</span>
  <span class="s3">if </span><span class="s1">nr_spatial_dimensions &gt; </span><span class="s5">2</span><span class="s1">:</span>
    <span class="s3">raise </span><span class="s1">_conv_error(</span>
        <span class="s4">&quot;We only support 1D or 2D convolutions, but found &quot;</span>
        <span class="s4">f&quot;</span><span class="s3">{</span><span class="s1">nr_spatial_dimensions</span><span class="s3">}</span><span class="s4">.&quot;</span><span class="s1">)</span>


<span class="s3">def </span><span class="s1">_normalize_padding_and_dilations(</span>
      <span class="s1">padding</span><span class="s3">, </span><span class="s1">lhs_dilation</span><span class="s3">, </span><span class="s1">rhs_dilation</span><span class="s3">, </span><span class="s1">is_conv1d):</span>
  <span class="s3">if </span><span class="s1">is_conv1d:</span>
    <span class="s1">lhs_dilation = list(lhs_dilation) + [</span><span class="s5">1</span><span class="s1">]</span>
    <span class="s1">rhs_dilation = list(rhs_dilation) + [</span><span class="s5">1</span><span class="s1">]</span>
    <span class="s0"># Empty padding in the dummy dimension.</span>
    <span class="s0"># Note that when kernel_size=stride=1, padding of (0, 0) is both 'VALID' and</span>
    <span class="s0"># 'SAME'. So the inferred padding type will still register according to the</span>
    <span class="s0"># first dimension padding.</span>
    <span class="s1">padding = list(padding) + [(</span><span class="s5">0</span><span class="s3">, </span><span class="s5">0</span><span class="s1">)]</span>
  <span class="s3">return </span><span class="s1">padding</span><span class="s3">, </span><span class="s1">lhs_dilation</span><span class="s3">, </span><span class="s1">rhs_dilation</span>


<span class="s3">def </span><span class="s1">_normalize_window_strides(window_strides):</span>
  <span class="s2">&quot;&quot;&quot;Ensure window_strides has length 4.&quot;&quot;&quot;</span>
  <span class="s0"># Some TF ops require len(window_strides) == 4 while others do not. We simply</span>
  <span class="s0"># ensure it always has len(4).</span>
  <span class="s3">if </span><span class="s1">len(window_strides) == </span><span class="s5">1</span><span class="s1">:</span>
    <span class="s0"># This is the Conv1D case. We add a dummy dimension to allow using 2D ops,</span>
    <span class="s0"># and use stride=1 on the dummy dimension.</span>
    <span class="s1">window_strides = list(window_strides) + [</span><span class="s5">1</span><span class="s1">]</span>
  <span class="s3">if </span><span class="s1">len(window_strides) == </span><span class="s5">2</span><span class="s1">:</span>
    <span class="s1">window_strides = [</span><span class="s5">1</span><span class="s1">] + list(window_strides) + [</span><span class="s5">1</span><span class="s1">]</span>
  <span class="s3">return </span><span class="s1">window_strides</span>


<span class="s3">def </span><span class="s1">_validate_conv_features(</span>
      <span class="s1">is_transpose</span><span class="s3">, </span><span class="s1">is_atrous</span><span class="s3">, </span><span class="s1">is_depthwise</span><span class="s3">, </span><span class="s1">feature_group_count</span><span class="s3">,</span>
      <span class="s1">batch_group_count</span><span class="s3">, </span><span class="s1">preferred_element_type</span><span class="s3">, </span><span class="s1">lhs_dtype):</span>
  <span class="s3">if </span><span class="s1">feature_group_count &gt; </span><span class="s5">1 </span><span class="s3">and not </span><span class="s1">is_depthwise:</span>
    <span class="s3">raise </span><span class="s1">_conv_error(</span><span class="s4">&quot;Grouped convolutions are unsupported&quot;</span><span class="s1">)</span>
  <span class="s3">if </span><span class="s1">(is_depthwise </span><span class="s3">and </span><span class="s1">is_atrous) </span><span class="s3">and not </span><span class="s1">is_transpose:</span>
    <span class="s0"># We allow dilated depthwise convolutions.</span>
    <span class="s3">pass</span>
  <span class="s3">elif </span><span class="s1">[is_depthwise</span><span class="s3">, </span><span class="s1">is_atrous</span><span class="s3">, </span><span class="s1">is_transpose].count(</span><span class="s3">True</span><span class="s1">) &gt; </span><span class="s5">1</span><span class="s1">:</span>
    <span class="s3">raise </span><span class="s1">_conv_error(</span>
        <span class="s4">f&quot;Can only do one of depthwise (</span><span class="s3">{</span><span class="s1">is_depthwise</span><span class="s3">}</span><span class="s4">), atrous (</span><span class="s3">{</span><span class="s1">is_atrous</span><span class="s3">}</span><span class="s4">) &quot;</span>
        <span class="s4">f&quot;and tranposed convolutions (</span><span class="s3">{</span><span class="s1">is_transpose</span><span class="s3">}</span><span class="s4">)&quot;</span><span class="s1">)</span>

  <span class="s0"># We can implement batch grouping when there is a need for it.</span>
  <span class="s3">if </span><span class="s1">batch_group_count != </span><span class="s5">1</span><span class="s1">:</span>
    <span class="s3">raise </span><span class="s1">_conv_error(</span><span class="s4">&quot;Unimplemented support for batch_group_count != 1 &quot;</span>
                <span class="s4">f&quot;(found </span><span class="s3">{</span><span class="s1">batch_group_count</span><span class="s3">}</span><span class="s4">)&quot;</span><span class="s1">)</span>

  <span class="s3">if </span><span class="s1">(preferred_element_type </span><span class="s3">is not None and</span>
      <span class="s1">preferred_element_type != lhs_dtype):</span>
    <span class="s3">raise </span><span class="s1">_conv_error(</span><span class="s4">&quot;Unimplemented support for preferred_element_type&quot;</span><span class="s1">)</span>


<span class="s3">def </span><span class="s1">_conv_general_dilated(</span>
    <span class="s1">lhs</span><span class="s3">, </span><span class="s1">rhs</span><span class="s3">, </span><span class="s1">*</span><span class="s3">, </span><span class="s1">window_strides</span><span class="s3">, </span><span class="s1">padding</span><span class="s3">, </span><span class="s1">lhs_dilation</span><span class="s3">, </span><span class="s1">rhs_dilation</span><span class="s3">,</span>
    <span class="s1">dimension_numbers: lax.ConvDimensionNumbers</span><span class="s3">, </span><span class="s1">feature_group_count: int</span><span class="s3">,</span>
    <span class="s1">batch_group_count: int</span><span class="s3">,</span>
    <span class="s1">precision: Optional[Tuple[PrecisionType</span><span class="s3">, </span><span class="s1">PrecisionType]]</span><span class="s3">,</span>
    <span class="s1">preferred_element_type: Optional[DType]</span><span class="s3">,</span>
    <span class="s1">_in_avals: Sequence[core.ShapedArray]</span><span class="s3">, </span><span class="s1">_out_aval: core.ShapedArray):</span>
  <span class="s2">&quot;&quot;&quot;Implementation of lax.conv_general_dilated_p using XlaConv.&quot;&quot;&quot;</span>
  <span class="s0"># In presence of shape polymorphism, lhs.shape and rhs.shape may contain</span>
  <span class="s0"># None. The actual dimension polynomial shapes are in _in_avals.</span>
  <span class="s3">del </span><span class="s1">precision  </span><span class="s0"># Unused arguments.</span>
  <span class="s1">lhs_shape</span><span class="s3">, </span><span class="s1">rhs_shape = _in_avals[</span><span class="s5">0</span><span class="s1">].shape</span><span class="s3">, </span><span class="s1">_in_avals[</span><span class="s5">1</span><span class="s1">].shape</span>
  <span class="s1">out_shape = _out_aval.shape</span>
  <span class="s1">_validate_spatial_dimensions(lhs</span><span class="s3">, </span><span class="s1">lhs_shape</span><span class="s3">, </span><span class="s1">rhs</span><span class="s3">, </span><span class="s1">rhs_shape)</span>
  <span class="s1">is_conv1d = len(lhs_shape) - </span><span class="s5">2 </span><span class="s1">== </span><span class="s5">1</span>

  <span class="s1">tf_window_strides = _normalize_window_strides(window_strides)</span>
  <span class="s1">padding</span><span class="s3">, </span><span class="s1">lhs_dilation</span><span class="s3">, </span><span class="s1">rhs_dilation = _normalize_padding_and_dilations(</span>
      <span class="s1">padding</span><span class="s3">, </span><span class="s1">lhs_dilation</span><span class="s3">, </span><span class="s1">rhs_dilation</span><span class="s3">, </span><span class="s1">is_conv1d)</span>

  <span class="s1">lhs</span><span class="s3">, </span><span class="s1">lhs_shape</span><span class="s3">, </span><span class="s1">rhs</span><span class="s3">, </span><span class="s1">rhs_shape = _transpose_for_tf_conv(lhs</span><span class="s3">, </span><span class="s1">lhs_shape</span><span class="s3">,</span>
                                                          <span class="s1">rhs</span><span class="s3">, </span><span class="s1">rhs_shape</span><span class="s3">,</span>
                                                          <span class="s1">dimension_numbers)</span>
  <span class="s1">in_channels = lhs_shape[-</span><span class="s5">1</span><span class="s1">]</span>
  <span class="s1">*rhs_spatial_shapes</span><span class="s3">, </span><span class="s1">_</span><span class="s3">, </span><span class="s1">rhs_out_channel = rhs_shape</span>

  <span class="s1">is_transpose = any([d != </span><span class="s5">1 </span><span class="s3">for </span><span class="s1">d </span><span class="s3">in </span><span class="s1">lhs_dilation])</span>
  <span class="s1">is_atrous = any([d != </span><span class="s5">1 </span><span class="s3">for </span><span class="s1">d </span><span class="s3">in </span><span class="s1">rhs_dilation])</span>
  <span class="s1">is_depthwise = in_channels == feature_group_count </span><span class="s3">and </span><span class="s1">feature_group_count &gt; </span><span class="s5">1</span>
  <span class="s1">_validate_conv_features(is_transpose</span><span class="s3">, </span><span class="s1">is_atrous</span><span class="s3">, </span><span class="s1">is_depthwise</span><span class="s3">,</span>
                          <span class="s1">feature_group_count</span><span class="s3">, </span><span class="s1">batch_group_count</span><span class="s3">,</span>
                          <span class="s1">preferred_element_type</span><span class="s3">, </span><span class="s1">lhs.dtype.as_numpy_dtype)</span>

  <span class="s1">rhs_dilated_shape = [</span>
      <span class="s1">(k - </span><span class="s5">1</span><span class="s1">) * r + </span><span class="s5">1 </span><span class="s3">for </span><span class="s1">k</span><span class="s3">, </span><span class="s1">r </span><span class="s3">in </span><span class="s1">zip(rhs_spatial_shapes</span><span class="s3">, </span><span class="s1">rhs_dilation)</span>
  <span class="s1">]</span>
  <span class="s1">output_perm = dimension_numbers[</span><span class="s5">2</span><span class="s1">]</span>

  <span class="s3">if </span><span class="s1">is_transpose:</span>
    <span class="s1">padding_type = _conv_transpose_pads_to_padtype(</span>
        <span class="s1">rhs_spatial_shapes</span><span class="s3">, </span><span class="s1">lhs_dilation</span><span class="s3">, </span><span class="s1">padding)</span>
  <span class="s3">else</span><span class="s1">:</span>
    <span class="s1">padding_type = pads_to_padtype(</span>
      <span class="s1">lhs_shape[</span><span class="s5">1</span><span class="s1">:</span><span class="s5">3</span><span class="s1">]</span><span class="s3">, </span><span class="s1">rhs_dilated_shape</span><span class="s3">, </span><span class="s1">window_strides</span><span class="s3">, </span><span class="s1">padding)</span>
    <span class="s0"># We only manually pad if we aren't using a tranposed convolutions.</span>
    <span class="s3">if </span><span class="s1">padding_type == </span><span class="s4">&quot;EXPLICIT&quot;</span><span class="s1">:</span>
      <span class="s1">lhs</span><span class="s3">, </span><span class="s1">lhs_shape = _pad_spatial_dims(lhs</span><span class="s3">, </span><span class="s1">lhs_shape</span><span class="s3">, </span><span class="s1">padding)</span>
      <span class="s1">padding_type = </span><span class="s4">&quot;VALID&quot;</span>

  <span class="s3">if </span><span class="s1">padding_type != </span><span class="s4">&quot;SAME&quot; </span><span class="s3">and </span><span class="s1">any(l &lt; r </span><span class="s3">for </span><span class="s1">l</span><span class="s3">, </span><span class="s1">r </span><span class="s3">in </span><span class="s1">zip(lhs_shape[</span><span class="s5">1</span><span class="s1">:</span><span class="s5">3</span><span class="s1">]</span><span class="s3">, </span><span class="s1">rhs_dilated_shape)):</span>
    <span class="s0"># If the input shape is smaller than the filter shape in a spatial dimension,</span>
    <span class="s0"># lax returns only zeros while tf.conv2d returns an error.</span>
    <span class="s0"># We thus return zeros to make sure the behavior is consistent.</span>
    <span class="s3">return </span><span class="s1">tf.broadcast_to(tf.constant(</span><span class="s5">0</span><span class="s3">, </span><span class="s1">dtype=tf.float32)</span><span class="s3">,</span>
                           <span class="s1">jax2tf._eval_shape(out_shape))</span>

  <span class="s3">if </span><span class="s1">is_depthwise:</span>
    <span class="s0"># Reshape filter from</span>
    <span class="s0"># [filter_height, filter_width, 1, in_channels * channel_multiplier] to</span>
    <span class="s0"># [filter_height, filter_width, in_channels, channel_multiplier].</span>
    <span class="s1">new_rhs_shape = tuple(rhs_spatial_shapes) + (in_channels</span><span class="s3">,</span>
                                                 <span class="s1">rhs_out_channel // in_channels)</span>
    <span class="s1">output = tf.nn.depthwise_conv2d(</span>
        <span class="s1">input=lhs</span><span class="s3">,</span>
        <span class="s1">filter=tf.reshape(rhs</span><span class="s3">, </span><span class="s1">jax2tf._eval_shape(new_rhs_shape))</span><span class="s3">,</span>
        <span class="s1">strides=tf_window_strides</span><span class="s3">,</span>
        <span class="s1">padding=padding_type</span><span class="s3">,</span>
        <span class="s1">dilations=rhs_dilation)</span>

  <span class="s3">elif </span><span class="s1">is_transpose:</span>
    <span class="s0"># tf.nn.conv2d_transpose requires a transposed filter.</span>
    <span class="s1">rhs_t = tf.reverse(rhs</span><span class="s3">, </span><span class="s1">[</span><span class="s5">0</span><span class="s3">, </span><span class="s5">1</span><span class="s1">])</span>
    <span class="s1">rhs_t = tf.transpose(rhs_t</span><span class="s3">, </span><span class="s1">(</span><span class="s5">0</span><span class="s3">, </span><span class="s5">1</span><span class="s3">, </span><span class="s5">3</span><span class="s3">, </span><span class="s5">2</span><span class="s1">))</span>

    <span class="s0"># We should transpose `out_shape` to &quot;NHWC&quot;, which is what TF expects.</span>
    <span class="s0"># First transpose to &quot;NCHW&quot;.</span>
    <span class="s3">if </span><span class="s1">is_conv1d:</span>
      <span class="s1">tf_out_shape = tuple(out_shape[i] </span><span class="s3">for </span><span class="s1">i </span><span class="s3">in </span><span class="s1">output_perm) + (</span><span class="s5">1</span><span class="s3">,</span><span class="s1">)</span>
    <span class="s3">else</span><span class="s1">:</span>
      <span class="s1">tf_out_shape = tuple(out_shape[i] </span><span class="s3">for </span><span class="s1">i </span><span class="s3">in </span><span class="s1">output_perm)</span>
    <span class="s0"># Then transpose &quot;NCHW&quot; to &quot;NHWC&quot;.</span>
    <span class="s1">tf_out_shape = tuple(tf_out_shape[i] </span><span class="s3">for </span><span class="s1">i </span><span class="s3">in </span><span class="s1">(</span><span class="s5">0</span><span class="s3">, </span><span class="s5">2</span><span class="s3">, </span><span class="s5">3</span><span class="s3">, </span><span class="s5">1</span><span class="s1">))</span>
    <span class="s1">output = tf.nn.conv2d_transpose(</span>
        <span class="s1">input=lhs</span><span class="s3">,</span>
        <span class="s1">filters=rhs_t</span><span class="s3">,</span>
        <span class="s1">output_shape=jax2tf._eval_shape(tf_out_shape)</span><span class="s3">,</span>
        <span class="s1">strides=lhs_dilation</span><span class="s3">,</span>
        <span class="s1">padding=padding_type)</span>

  <span class="s3">else</span><span class="s1">:</span>
    <span class="s1">output = tf.nn.conv2d(</span>
        <span class="s1">input=lhs</span><span class="s3">,</span>
        <span class="s1">filters=rhs</span><span class="s3">,</span>
        <span class="s1">strides=tf_window_strides</span><span class="s3">,</span>
        <span class="s1">padding=padding_type</span><span class="s3">,</span>
        <span class="s1">dilations=rhs_dilation)</span>

  <span class="s0"># TF outputs in format &quot;NHWC&quot;, so convert to &quot;NCHW&quot;, which is lax's default</span>
  <span class="s0"># format.</span>
  <span class="s1">output = tf.transpose(output</span><span class="s3">, </span><span class="s1">(</span><span class="s5">0</span><span class="s3">, </span><span class="s5">3</span><span class="s3">, </span><span class="s5">1</span><span class="s3">, </span><span class="s5">2</span><span class="s1">))  </span><span class="s0"># &quot;NHWC&quot; --&gt; &quot;NCHW&quot;</span>
  <span class="s3">if </span><span class="s1">is_conv1d:</span>
    <span class="s1">output = output[:</span><span class="s3">, </span><span class="s1">:</span><span class="s3">, </span><span class="s1">:</span><span class="s3">, </span><span class="s5">0</span><span class="s1">]</span>
  <span class="s0"># To determine the right permutation, we compute the inverse permutation of</span>
  <span class="s0"># `output_perm`, so that when `output_perm` is applied to `output`, we obtain</span>
  <span class="s0"># the outpt in NCHW format.</span>
  <span class="s1">inverse_perm = _invert_permutation(output_perm)</span>
  <span class="s1">output = tf.transpose(output</span><span class="s3">, </span><span class="s1">inverse_perm)  </span><span class="s0"># &quot;NCHW&quot; -&gt; desired output shape.</span>
  <span class="s3">return </span><span class="s1">output</span>


<span class="s1">tf_impl_no_xla[lax.conv_general_dilated_p] = _conv_general_dilated</span>


<span class="s3">def </span><span class="s1">_dot_general(lhs</span><span class="s3">, </span><span class="s1">rhs</span><span class="s3">, </span><span class="s1">*</span><span class="s3">, </span><span class="s1">dimension_numbers</span><span class="s3">,</span>
                 <span class="s1">precision: Optional[Tuple[PrecisionType</span><span class="s3">, </span><span class="s1">PrecisionType]]</span><span class="s3">,</span>
                 <span class="s1">preferred_element_type: Optional[DType]</span><span class="s3">,</span>
                 <span class="s1">_in_avals: Sequence[core.ShapedArray]</span><span class="s3">,</span>
                 <span class="s1">_out_aval: core.ShapedArray):</span>
  <span class="s2">&quot;&quot;&quot;Implementation of lax.dot_general_p in terms of tf.linalg.einsum.&quot;&quot;&quot;</span>
  <span class="s0"># Unused arguments.</span>
  <span class="s3">del </span><span class="s1">precision</span>
  <span class="s3">del </span><span class="s1">preferred_element_type</span>

  <span class="s1">(lhs_contracting</span><span class="s3">, </span><span class="s1">rhs_contracting)</span><span class="s3">, </span><span class="s1">(lhs_batch</span><span class="s3">, </span><span class="s1">rhs_batch) = dimension_numbers</span>
  <span class="s1">lhs_ndim</span><span class="s3">, </span><span class="s1">rhs_ndim = len(lhs.shape)</span><span class="s3">, </span><span class="s1">len(rhs.shape)</span>

  <span class="s0"># This condition ensures that:</span>
  <span class="s0"># 1) the batch dimensions are ordered in the same way in lhs and rhs (this is</span>
  <span class="s0">#    not strictly necessary, but we would have to reshape the array if that</span>
  <span class="s0">#    were not the case;</span>
  <span class="s0"># 2) lhs and rhs have the same number of dimensions +/- 1</span>
  <span class="s0"># 3) the number of non-batch dimensions in both tensors is either 1 or 2</span>
  <span class="s0"># 4) the contracting dimensions are consistent with those of a classic</span>
  <span class="s0">#    matrix/matrix, vector/matrix or matrix/vector multiplication.</span>
  <span class="s3">if </span><span class="s1">(lhs_batch == rhs_batch == tuple(range(len(lhs_batch))) </span><span class="s3">and</span>
      <span class="s1">lhs_ndim - rhs_ndim </span><span class="s3">in </span><span class="s1">[-</span><span class="s5">1</span><span class="s3">, </span><span class="s5">0</span><span class="s3">, </span><span class="s5">1</span><span class="s1">] </span><span class="s3">and</span>
      <span class="s5">1 </span><span class="s1">&lt;= lhs_ndim - len(lhs_batch) &lt;= </span><span class="s5">2 </span><span class="s3">and</span>
      <span class="s5">1 </span><span class="s1">&lt;= rhs_ndim - len(rhs_batch) &lt;= </span><span class="s5">2 </span><span class="s3">and</span>
      <span class="s1">lhs_contracting == (len(lhs.shape) - </span><span class="s5">1</span><span class="s3">,</span><span class="s1">) </span><span class="s3">and</span>
      <span class="s1">rhs_contracting == (len(lhs_batch)</span><span class="s3">,</span><span class="s1">)):</span>
    <span class="s0"># All the inputs to tf.linalg.matmul must have 2 inner dimensions,</span>
    <span class="s0"># after their batch dimensions, so we need to expand the dimensions</span>
    <span class="s0"># appropriately. We can get to this branch with three combinations of</span>
    <span class="s0"># inner shapes:</span>
    <span class="s0"># - lhs.inner_shape == [a, b], rhs.inner_shape == [b, c]</span>
    <span class="s0">#   - in this case, the resulting inner shape is [a, c];</span>
    <span class="s0"># - lhs.inner_shape == [b]   , rhs.inner_shape == [b, c]</span>
    <span class="s0">#   - in this case, we need to expand lhs to [1, b], and the resulting</span>
    <span class="s0">#     shape is [c]. We need to squeeze the result of tf.linalg.matmul</span>
    <span class="s0">#     as it will have shape [1, c];</span>
    <span class="s0"># - lhs.shape == [batch] + [a, b], rhs.shape == [batch] + [b]</span>
    <span class="s0">#   - in this case, we need to expand rhs to [b, 1], and the resulting</span>
    <span class="s0">#     shape is [a]. We need to squeeze the result of tf.linalg.matmul</span>
    <span class="s0">#     as it will have shape [a, 1];</span>
    <span class="s0"># - lhs.shape == [batch] + [b]   , rhs.shape == [batch] + [b]</span>
    <span class="s0">#   - in this case, we need to expand lhs to [1, b] and rhs to [b, 1],</span>
    <span class="s0">#     and the resulting shape is (). We need to squeeze the result of</span>
    <span class="s0">#     tf.linalg.matmul as it will have shape [1, 1].</span>
    <span class="s1">squeeze_idxs = []</span>
    <span class="s3">if </span><span class="s1">lhs_ndim - len(lhs_batch) == </span><span class="s5">1</span><span class="s1">:</span>
      <span class="s1">lhs = tf.expand_dims(lhs</span><span class="s3">, </span><span class="s1">lhs_ndim - </span><span class="s5">1</span><span class="s1">)</span>
      <span class="s1">squeeze_idxs.append(len(lhs.shape) - </span><span class="s5">2</span><span class="s1">)</span>
    <span class="s3">if </span><span class="s1">rhs_ndim - len(rhs_batch) == </span><span class="s5">1</span><span class="s1">:</span>
      <span class="s1">rhs = tf.expand_dims(rhs</span><span class="s3">, </span><span class="s1">rhs_ndim)</span>
      <span class="s1">squeeze_idxs.append(len(rhs.shape) - </span><span class="s5">1</span><span class="s1">)</span>
    <span class="s1">result = tf.linalg.matmul(lhs</span><span class="s3">, </span><span class="s1">rhs)</span>
    <span class="s3">if </span><span class="s1">len(squeeze_idxs) != </span><span class="s5">0</span><span class="s1">:</span>
      <span class="s3">assert </span><span class="s1">all([result.shape[i] == </span><span class="s5">1 </span><span class="s3">for </span><span class="s1">i </span><span class="s3">in </span><span class="s1">squeeze_idxs])</span>
      <span class="s1">result = tf.squeeze(result</span><span class="s3">, </span><span class="s1">squeeze_idxs)</span>
    <span class="s3">return </span><span class="s1">result</span>

  <span class="s1">new_id = iter(string.ascii_letters)</span>
  <span class="s1">lhs_axis_ids = [next(new_id) </span><span class="s3">for </span><span class="s1">_ </span><span class="s3">in </span><span class="s1">lhs.shape]</span>
  <span class="s1">rhs_axis_ids = [next(new_id) </span><span class="s3">for </span><span class="s1">_ </span><span class="s3">in </span><span class="s1">rhs.shape]</span>
  <span class="s1">lhs_out_axis_ids = lhs_axis_ids[:]</span>
  <span class="s1">rhs_out_axis_ids = rhs_axis_ids[:]</span>

  <span class="s3">for </span><span class="s1">lhs_axis</span><span class="s3">, </span><span class="s1">rhs_axis </span><span class="s3">in </span><span class="s1">zip(lhs_contracting</span><span class="s3">, </span><span class="s1">rhs_contracting):</span>
    <span class="s1">shared_id = next(new_id)</span>
    <span class="s1">lhs_axis_ids[lhs_axis] = shared_id</span>
    <span class="s1">rhs_axis_ids[rhs_axis] = shared_id</span>
    <span class="s1">lhs_out_axis_ids[lhs_axis] = </span><span class="s3">None  </span><span class="s0"># type: ignore[call-overload]</span>
    <span class="s1">rhs_out_axis_ids[rhs_axis] = </span><span class="s3">None  </span><span class="s0"># type: ignore[call-overload]</span>

  <span class="s1">batch_ids = []</span>
  <span class="s3">for </span><span class="s1">lhs_axis</span><span class="s3">, </span><span class="s1">rhs_axis </span><span class="s3">in </span><span class="s1">zip(lhs_batch</span><span class="s3">, </span><span class="s1">rhs_batch):</span>
    <span class="s1">shared_id = next(new_id)</span>
    <span class="s1">lhs_axis_ids[lhs_axis] = shared_id</span>
    <span class="s1">rhs_axis_ids[rhs_axis] = shared_id</span>
    <span class="s1">lhs_out_axis_ids[lhs_axis] = </span><span class="s3">None  </span><span class="s0"># type: ignore[call-overload]</span>
    <span class="s1">rhs_out_axis_ids[rhs_axis] = </span><span class="s3">None  </span><span class="s0"># type: ignore[call-overload]</span>
    <span class="s1">batch_ids.append(shared_id)</span>

  <span class="s1">not_none = </span><span class="s3">lambda </span><span class="s1">x: x </span><span class="s3">is not None</span>
  <span class="s1">out_axis_ids = list(</span>
      <span class="s1">filter(not_none</span><span class="s3">, </span><span class="s1">batch_ids + lhs_out_axis_ids + rhs_out_axis_ids))</span>
  <span class="s3">assert </span><span class="s1">lhs.dtype == rhs.dtype</span>
  <span class="s1">spec = </span><span class="s4">&quot;{},{}-&gt;{}&quot;</span><span class="s1">.format(</span><span class="s4">&quot;&quot;</span><span class="s1">.join(lhs_axis_ids)</span><span class="s3">, </span><span class="s4">&quot;&quot;</span><span class="s1">.join(rhs_axis_ids)</span><span class="s3">,</span>
                            <span class="s4">&quot;&quot;</span><span class="s1">.join(out_axis_ids))</span>
  <span class="s3">return </span><span class="s1">tf.linalg.einsum(spec</span><span class="s3">, </span><span class="s1">lhs</span><span class="s3">, </span><span class="s1">rhs)</span>


<span class="s1">tf_impl_no_xla[lax.dot_general_p] = _dot_general</span>


<span class="s3">def </span><span class="s1">_interior_padding(operand</span><span class="s3">, </span><span class="s1">padding_value</span><span class="s3">, </span><span class="s1">padding_config</span><span class="s3">, </span><span class="s1">operand_shape):</span>
  <span class="s0"># Used only when enable_xla=False</span>
  <span class="s0"># Applies only the interior padding from the padding_config.</span>
  <span class="s0"># We do this somewhat inefficiently, as as a scatter.</span>
  <span class="s0"># For each dimension we compute the indices_by_dim as [0, f, 2f, 3f, ...] where</span>
  <span class="s0"># f is the dilation factor for the dimension, i.e., 1 + interior_padding.</span>
  <span class="s0"># Then we compute the cartesian production of the indices (using broadcast</span>
  <span class="s0"># and concat).</span>

  <span class="s0"># We could make this code more complex and do all the padding at once, but</span>
  <span class="s0"># we prefer to keep it simple.</span>
  <span class="s1">indices_by_dim = []</span>
  <span class="s1">indices_shape = operand_shape + (</span><span class="s5">1</span><span class="s3">,</span><span class="s1">)</span>
  <span class="s1">output_shape = []  </span><span class="s0"># considering only interior padding</span>
  <span class="s3">for </span><span class="s1">d</span><span class="s3">, </span><span class="s1">(dsz</span><span class="s3">, </span><span class="s1">(_</span><span class="s3">, </span><span class="s1">_</span><span class="s3">, </span><span class="s1">i)) </span><span class="s3">in </span><span class="s1">enumerate(zip(operand_shape</span><span class="s3">, </span><span class="s1">padding_config)):</span>
    <span class="s1">dilation_factor = i + </span><span class="s5">1</span>
    <span class="s1">output_shape.append(dsz * dilation_factor - i)</span>
    <span class="s1">indices = tf.range(dsz) * dilation_factor</span>
    <span class="s1">expansion = [</span><span class="s3">None</span><span class="s1">] * (</span><span class="s5">1 </span><span class="s1">+ len(operand_shape))</span>
    <span class="s1">expansion[d] = slice(</span><span class="s3">None, None, None</span><span class="s1">)</span>
    <span class="s1">indices_by_dim.append(tf.broadcast_to(indices[expansion]</span><span class="s3">, </span><span class="s1">indices_shape))</span>

  <span class="s1">indices_cartesian = tf.concat(indices_by_dim</span><span class="s3">, </span><span class="s1">axis=len(operand_shape))</span>
  <span class="s1">scattered = tf.scatter_nd(indices_cartesian</span><span class="s3">, </span><span class="s1">operand</span><span class="s3">, </span><span class="s1">output_shape)</span>
  <span class="s0"># What elements from the output array we use from</span>
  <span class="s1">mask = tf.scatter_nd(indices_cartesian</span><span class="s3">, </span><span class="s1">tf.ones_like(operand</span><span class="s3">, </span><span class="s1">dtype=np.bool_)</span><span class="s3">,</span>
                       <span class="s1">output_shape)</span>
  <span class="s3">return </span><span class="s1">tf.where(mask</span><span class="s3">, </span><span class="s1">scattered</span><span class="s3">, </span><span class="s1">padding_value)</span>


<span class="s3">def </span><span class="s1">_pad(operand</span><span class="s3">, </span><span class="s1">padding_value</span><span class="s3">, </span><span class="s1">*</span><span class="s3">, </span><span class="s1">padding_config</span><span class="s3">,</span>
         <span class="s1">_in_avals: Sequence[core.ShapedArray]</span><span class="s3">, </span><span class="s1">_out_aval: core.ShapedArray):</span>
  <span class="s0"># Do only the interior padding first. This is rarely needed.</span>
  <span class="s3">if </span><span class="s1">any(i != </span><span class="s5">0 </span><span class="s3">for </span><span class="s1">_</span><span class="s3">, </span><span class="s1">_</span><span class="s3">, </span><span class="s1">i </span><span class="s3">in </span><span class="s1">padding_config):</span>
    <span class="s1">operand = _interior_padding(operand</span><span class="s3">, </span><span class="s1">padding_value</span><span class="s3">, </span><span class="s1">padding_config</span><span class="s3">,</span>
                                <span class="s1">jax2tf._eval_shape(_in_avals[</span><span class="s5">0</span><span class="s1">].shape))</span>

  <span class="s0"># Now do the non-negative edge padding. This is the common case, use tf.pad.</span>
  <span class="s1">non_negative_padding = [((lo </span><span class="s3">if </span><span class="s1">lo &gt;= </span><span class="s5">0 </span><span class="s3">else </span><span class="s5">0</span><span class="s1">)</span><span class="s3">, </span><span class="s1">(hi </span><span class="s3">if </span><span class="s1">hi &gt;= </span><span class="s5">0 </span><span class="s3">else </span><span class="s5">0</span><span class="s1">))</span>
                          <span class="s3">for </span><span class="s1">lo</span><span class="s3">, </span><span class="s1">hi</span><span class="s3">, </span><span class="s1">_ </span><span class="s3">in </span><span class="s1">padding_config]</span>
  <span class="s1">operand = tf.pad(</span>
      <span class="s1">operand</span><span class="s3">,</span>
      <span class="s1">non_negative_padding</span><span class="s3">,</span>
      <span class="s1">mode=</span><span class="s4">&quot;CONSTANT&quot;</span><span class="s3">,</span>
      <span class="s1">constant_values=padding_value)</span>
  <span class="s0"># Now the negative edge padding (this is also rare)</span>
  <span class="s3">if </span><span class="s1">any(lo &lt; </span><span class="s5">0 </span><span class="s3">or </span><span class="s1">hi &lt; </span><span class="s5">0 </span><span class="s3">for </span><span class="s1">lo</span><span class="s3">, </span><span class="s1">hi</span><span class="s3">, </span><span class="s1">_ </span><span class="s3">in </span><span class="s1">padding_config):</span>
    <span class="s1">output_shape = jax2tf._eval_shape(_out_aval.shape)</span>
    <span class="s1">begins = [(-lo </span><span class="s3">if </span><span class="s1">lo &lt; </span><span class="s5">0 </span><span class="s3">else </span><span class="s5">0</span><span class="s1">) </span><span class="s3">for </span><span class="s1">lo</span><span class="s3">, </span><span class="s1">_</span><span class="s3">, </span><span class="s1">_ </span><span class="s3">in </span><span class="s1">padding_config]</span>
    <span class="s1">operand = tf.slice(operand</span><span class="s3">, </span><span class="s1">begins</span><span class="s3">, </span><span class="s1">output_shape)</span>

  <span class="s3">return </span><span class="s1">operand</span>


<span class="s1">tf_impl_no_xla[lax.pad_p] = _pad</span>


<span class="s3">def </span><span class="s1">_argminmax(is_min: bool</span><span class="s3">, </span><span class="s1">operand: TfVal</span><span class="s3">, </span><span class="s1">axes: Sequence[int]</span><span class="s3">,</span>
               <span class="s1">index_dtype: DType</span><span class="s3">, </span><span class="s1">_in_avals: Sequence[core.ShapedArray]</span><span class="s3">,</span>
               <span class="s1">_out_aval: core.ShapedArray):</span>
  <span class="s0"># The following is known to diverge from JAX behavior for NaN.</span>
  <span class="s1">axis</span><span class="s3">, </span><span class="s1">= axes</span>
  <span class="s1">output_type = tf.int32</span>
  <span class="s3">if </span><span class="s1">dtypes.iinfo(index_dtype).bits &gt; </span><span class="s5">32</span><span class="s1">:</span>
    <span class="s1">output_type = tf.int64</span>
  <span class="s0"># TODO(phawkins): handle axes larger than 2^31.</span>
  <span class="s1">fn = tf.math.argmin </span><span class="s3">if </span><span class="s1">is_min </span><span class="s3">else </span><span class="s1">tf.math.argmax</span>
  <span class="s1">result = fn(operand</span><span class="s3">, </span><span class="s1">axis=axis</span><span class="s3">, </span><span class="s1">output_type=output_type)</span>
  <span class="s3">return </span><span class="s1">tf.cast(result</span><span class="s3">, </span><span class="s1">jax2tf._to_tf_dtype(index_dtype))</span>


<span class="s1">tf_impl_no_xla[lax.argmin_p] = partial(_argminmax</span><span class="s3">, True</span><span class="s1">)</span>
<span class="s1">tf_impl_no_xla[lax.argmax_p] = partial(_argminmax</span><span class="s3">, False</span><span class="s1">)</span>


<span class="s3">def </span><span class="s1">_validate_reduce_window_inputs(operand_shape</span><span class="s3">, </span><span class="s1">computation_name</span><span class="s3">, </span><span class="s1">dtype</span><span class="s3">,</span>
                                   <span class="s1">window_dimensions</span><span class="s3">, </span><span class="s1">window_strides</span><span class="s3">,</span>
                                   <span class="s1">base_dilation</span><span class="s3">, </span><span class="s1">window_dilation):</span>
  <span class="s3">if </span><span class="s1">computation_name </span><span class="s3">not in </span><span class="s1">[</span><span class="s4">&quot;min&quot;</span><span class="s3">, </span><span class="s4">&quot;max&quot;</span><span class="s3">, </span><span class="s4">&quot;add&quot;</span><span class="s1">]:</span>
    <span class="s3">raise </span><span class="s1">_reduce_error(</span><span class="s4">&quot;Reduction function should be either min, max, or add.&quot;</span><span class="s1">)</span>
  <span class="s3">if </span><span class="s1">computation_name </span><span class="s3">in </span><span class="s1">[</span><span class="s4">&quot;min&quot;</span><span class="s3">, </span><span class="s4">&quot;max&quot;</span><span class="s1">] </span><span class="s3">and </span><span class="s1">dtype </span><span class="s3">in </span><span class="s1">[</span>
      <span class="s1">tf.bool</span><span class="s3">, </span><span class="s1">tf.uint32</span><span class="s3">, </span><span class="s1">tf.uint64</span><span class="s3">, </span><span class="s1">tf.complex64</span><span class="s3">, </span><span class="s1">tf.complex128</span>
  <span class="s1">]:</span>
    <span class="s3">raise </span><span class="s1">_reduce_error(</span><span class="s4">&quot;Min/max pool does not support operands of type &quot;</span>
                        <span class="s4">f&quot;</span><span class="s3">{</span><span class="s1">dtype</span><span class="s3">}</span><span class="s4">&quot;</span><span class="s1">)</span>
  <span class="s3">if </span><span class="s1">computation_name == </span><span class="s4">&quot;min&quot; </span><span class="s3">and </span><span class="s1">dtype </span><span class="s3">in </span><span class="s1">[tf.uint8</span><span class="s3">, </span><span class="s1">tf.uint16]:</span>
    <span class="s0"># TODO(marcvanzee): We currently implement min pooling by negating the</span>
    <span class="s0"># input, but this doesn't work for uint. We could work around it using</span>
    <span class="s0"># tf.math.reduce_min.</span>
    <span class="s3">raise </span><span class="s1">_reduce_error(</span><span class="s4">f&quot;Min pool does not support operands of type </span><span class="s3">{</span><span class="s1">dtype</span><span class="s3">}</span><span class="s4">&quot;</span><span class="s1">)</span>
  <span class="s3">if </span><span class="s1">computation_name == </span><span class="s4">&quot;add&quot; </span><span class="s3">and </span><span class="s1">dtype </span><span class="s3">not in </span><span class="s1">[</span>
      <span class="s1">tf.float16</span><span class="s3">, </span><span class="s1">tf.float32</span><span class="s3">, </span><span class="s1">tf.float64</span>
  <span class="s1">]:</span>
    <span class="s3">raise </span><span class="s1">_reduce_error(</span><span class="s4">&quot;Add pooling does not support operands of type &quot;</span>
                        <span class="s4">f&quot;</span><span class="s3">{</span><span class="s1">dtype</span><span class="s3">}</span><span class="s4">&quot;</span><span class="s1">)</span>

  <span class="s3">if </span><span class="s1">(len(operand_shape) != len(window_dimensions) != len(window_strides) !=</span>
      <span class="s1">len(window_dilation)):</span>
    <span class="s3">raise </span><span class="s1">_reduce_error(</span><span class="s4">&quot;Input shapes, window dimensions, window stride &quot;</span>
                        <span class="s4">&quot;dimensions, and window dilation dimensions should &quot;</span>
                        <span class="s4">&quot;match.&quot;</span><span class="s1">)</span>

  <span class="s1">has_only_spatial_dims = </span><span class="s3">True</span>
  <span class="s3">if </span><span class="s1">len(operand_shape) &gt; </span><span class="s5">4</span><span class="s1">:</span>
    <span class="s3">raise </span><span class="s1">_reduce_error(</span><span class="s4">&quot;Only 1D or 2D input are supported.&quot;</span><span class="s1">)</span>
  <span class="s3">if </span><span class="s1">len(operand_shape) &gt; </span><span class="s5">2</span><span class="s1">:</span>
    <span class="s0"># operand_shape = (batch, spatial_dims, ..., channel).</span>
    <span class="s1">has_only_spatial_dims = </span><span class="s3">False</span>

    <span class="s3">for </span><span class="s1">name</span><span class="s3">, </span><span class="s1">value </span><span class="s3">in </span><span class="s1">[(</span><span class="s4">&quot;window_dimensions&quot;</span><span class="s3">, </span><span class="s1">window_dimensions)</span><span class="s3">,</span>
                        <span class="s1">(</span><span class="s4">&quot;window_strides&quot;</span><span class="s3">, </span><span class="s1">window_strides)</span><span class="s3">,</span>
                        <span class="s1">(</span><span class="s4">&quot;window_dilation&quot;</span><span class="s3">, </span><span class="s1">window_dilation)]:</span>
      <span class="s3">if </span><span class="s1">value[</span><span class="s5">0</span><span class="s1">] != value[-</span><span class="s5">1</span><span class="s1">] != </span><span class="s5">1</span><span class="s1">:</span>
        <span class="s3">raise </span><span class="s1">_reduce_error(</span><span class="s4">&quot;Only 1D or 2D input are supported, expected &quot;</span>
                            <span class="s4">f&quot;</span><span class="s3">{</span><span class="s1">name</span><span class="s3">}</span><span class="s4">=(1, spatial_dims, ..., 1), but got &quot;</span>
                            <span class="s4">f&quot;</span><span class="s3">{</span><span class="s1">value</span><span class="s3">}</span><span class="s4">&quot;</span><span class="s1">)</span>

  <span class="s3">if </span><span class="s1">list(base_dilation) != [</span><span class="s5">1</span><span class="s1">] * len(operand_shape):</span>
    <span class="s0"># TODO(marcvanzee): Add support for base dilations. We can do this using</span>
    <span class="s0"># a scatter on operand.</span>
    <span class="s3">raise </span><span class="s1">_reduce_error(</span><span class="s4">&quot;Unimplemented support for base dilation.&quot;</span><span class="s1">)</span>

  <span class="s3">return </span><span class="s1">has_only_spatial_dims</span>


<span class="s3">def </span><span class="s1">_padding_reduce_window(operand</span><span class="s3">, </span><span class="s1">operand_shape</span><span class="s3">, </span><span class="s1">computation_name</span><span class="s3">,</span>
                           <span class="s1">window_dimensions</span><span class="s3">, </span><span class="s1">window_strides</span><span class="s3">, </span><span class="s1">padding):</span>
  <span class="s1">padding_type = pads_to_padtype(operand_shape</span><span class="s3">, </span><span class="s1">window_dimensions</span><span class="s3">,</span>
                                 <span class="s1">window_strides</span><span class="s3">, </span><span class="s1">padding)</span>

  <span class="s0"># https://github.com/google/jax/issues/11874.</span>
  <span class="s1">needs_manual_padding = (</span>
      <span class="s1">padding_type == </span><span class="s4">&quot;SAME&quot; </span><span class="s3">and </span><span class="s1">computation_name == </span><span class="s4">&quot;add&quot; </span><span class="s3">and</span>
      <span class="s1">window_dimensions != [</span><span class="s5">1</span><span class="s1">] * len(operand_shape))</span>

  <span class="s3">if </span><span class="s1">needs_manual_padding </span><span class="s3">or </span><span class="s1">padding_type == </span><span class="s4">&quot;EXPLICIT&quot;</span><span class="s1">:</span>
    <span class="s1">operand</span><span class="s3">, </span><span class="s1">operand_shape = _pad_spatial_dims(operand</span><span class="s3">, </span><span class="s1">operand_shape</span><span class="s3">, </span><span class="s1">padding)</span>
    <span class="s1">padding_type = </span><span class="s4">&quot;VALID&quot;</span>

  <span class="s3">return </span><span class="s1">operand</span><span class="s3">, </span><span class="s1">operand_shape</span><span class="s3">, </span><span class="s1">padding_type</span>


<span class="s3">def </span><span class="s1">_reshape_reduce_window(operand</span><span class="s3">, </span><span class="s1">operand_shape</span><span class="s3">, </span><span class="s1">window_dimensions</span><span class="s3">,</span>
                           <span class="s1">window_strides</span><span class="s3">, </span><span class="s1">window_dilation</span><span class="s3">, </span><span class="s1">*</span><span class="s3">,</span>
                           <span class="s1">has_only_spatial_dims):</span>
  <span class="s0"># Reshape inputs so they are accepted by tf.nn.pool, which expects batch and</span>
  <span class="s0"># channel dimensions for operand but not for any of the other inputs.</span>
  <span class="s3">if </span><span class="s1">has_only_spatial_dims:  </span><span class="s0"># len(operand_shape) &lt;= 2</span>
    <span class="s0"># Call eval_shape on a shape that may contain polynomials, otherwise TF does</span>
    <span class="s0"># not know what to do with polynomials in the shape.</span>
    <span class="s1">operand_shape = jax2tf._eval_shape(operand_shape)</span>
    <span class="s0"># Add batch and channel dimensions to operand.</span>
    <span class="s1">operand = tf.reshape(operand</span><span class="s3">, </span><span class="s1">(</span><span class="s5">1</span><span class="s3">,</span><span class="s1">) + operand_shape + (</span><span class="s5">1</span><span class="s3">,</span><span class="s1">))</span>
  <span class="s3">else</span><span class="s1">:</span>
    <span class="s0"># This branch assumes operand.shape = (batch, spatial_dims, ..., channel),</span>
    <span class="s0"># and dimensions, strides, dilation are all (1, spatial_values, ..., 1).</span>
    <span class="s0"># Input validation for this is done in _validate_reduce_window_inputs.</span>
    <span class="s1">window_dimensions = window_dimensions[</span><span class="s5">1</span><span class="s1">:-</span><span class="s5">1</span><span class="s1">]</span>
    <span class="s1">window_strides = window_strides[</span><span class="s5">1</span><span class="s1">:-</span><span class="s5">1</span><span class="s1">]</span>
    <span class="s1">window_dilation = window_dilation[</span><span class="s5">1</span><span class="s1">:-</span><span class="s5">1</span><span class="s1">]</span>

  <span class="s3">return </span><span class="s1">operand</span><span class="s3">, </span><span class="s1">window_dimensions</span><span class="s3">, </span><span class="s1">window_strides</span><span class="s3">, </span><span class="s1">window_dilation</span>


<span class="s3">def </span><span class="s1">_reduce_monoid(operand</span><span class="s3">, </span><span class="s1">window_dimensions</span><span class="s3">, </span><span class="s1">window_strides</span><span class="s3">, </span><span class="s1">padding</span><span class="s3">,</span>
                   <span class="s1">base_dilation</span><span class="s3">, </span><span class="s1">window_dilation</span><span class="s3">, </span><span class="s1">computation_name</span><span class="s3">,</span>
                   <span class="s1">_in_avals: Sequence[core.ShapedArray]</span><span class="s3">,</span>
                   <span class="s1">_out_aval: core.ShapedArray):</span>
  <span class="s1">dtype = operand.dtype</span>
  <span class="s0"># In presence of shape polymorphism, operand.shape may contain None. The</span>
  <span class="s0"># actual dimension polynomial shapes are in _in_avals.</span>
  <span class="s1">operand_shape = _in_avals[</span><span class="s5">0</span><span class="s1">].shape</span>

  <span class="s0"># TODO(marcvanzee): Put reduce_window arguments into dataclass, similar to</span>
  <span class="s0"># Gather, to simplify function calls.</span>
  <span class="s1">has_only_spatial_dims = _validate_reduce_window_inputs(</span>
      <span class="s1">operand_shape</span><span class="s3">, </span><span class="s1">computation_name</span><span class="s3">, </span><span class="s1">dtype</span><span class="s3">, </span><span class="s1">window_dimensions</span><span class="s3">, </span><span class="s1">window_strides</span><span class="s3">,</span>
      <span class="s1">base_dilation</span><span class="s3">, </span><span class="s1">window_dilation)</span>

  <span class="s1">operand</span><span class="s3">, </span><span class="s1">operand_shape</span><span class="s3">, </span><span class="s1">padding_type = _padding_reduce_window(</span>
      <span class="s1">operand</span><span class="s3">, </span><span class="s1">operand_shape</span><span class="s3">, </span><span class="s1">computation_name</span><span class="s3">, </span><span class="s1">window_dimensions</span><span class="s3">,</span>
      <span class="s1">window_strides</span><span class="s3">, </span><span class="s1">padding)</span>

  <span class="s1">operand</span><span class="s3">, </span><span class="s1">window_dimensions</span><span class="s3">, </span><span class="s1">window_strides</span><span class="s3">, </span><span class="s1">dilations = _reshape_reduce_window(</span>
      <span class="s1">operand</span><span class="s3">,</span>
      <span class="s1">operand_shape</span><span class="s3">,</span>
      <span class="s1">window_dimensions</span><span class="s3">,</span>
      <span class="s1">window_strides</span><span class="s3">,</span>
      <span class="s1">window_dilation</span><span class="s3">,</span>
      <span class="s1">has_only_spatial_dims=has_only_spatial_dims)</span>

  <span class="s3">def </span><span class="s1">tf_pool(inputs</span><span class="s3">, </span><span class="s1">pooling_type):</span>
    <span class="s1">result = tf.nn.pool(</span>
        <span class="s1">inputs</span><span class="s3">,</span>
        <span class="s1">window_shape=window_dimensions</span><span class="s3">,</span>
        <span class="s1">pooling_type=pooling_type</span><span class="s3">,</span>
        <span class="s1">padding=padding_type</span><span class="s3">,</span>
        <span class="s1">strides=window_strides</span><span class="s3">,</span>
        <span class="s1">dilations=dilations)</span>

    <span class="s3">if </span><span class="s1">has_only_spatial_dims:</span>
      <span class="s0"># If the input only had spatial dimensions we need to contract the batch</span>
      <span class="s0"># and channel dimensions before returning the output.</span>
      <span class="s1">result = tf.squeeze(result</span><span class="s3">, </span><span class="s1">[</span><span class="s5">0</span><span class="s3">, </span><span class="s1">-</span><span class="s5">1</span><span class="s1">])</span>

    <span class="s1">jax2tf._assert_matching_abstract_shape(result</span><span class="s3">, </span><span class="s1">_out_aval.shape)</span>
    <span class="s3">return </span><span class="s1">result</span>

  <span class="s1">negate = </span><span class="s3">lambda </span><span class="s1">x: tf.multiply(x</span><span class="s3">, </span><span class="s1">tf.constant(-</span><span class="s5">1</span><span class="s3">, </span><span class="s1">dtype))</span>
  <span class="s3">if </span><span class="s1">computation_name == </span><span class="s4">&quot;max&quot;</span><span class="s1">:</span>
    <span class="s3">return </span><span class="s1">tf_pool(operand</span><span class="s3">, </span><span class="s4">&quot;MAX&quot;</span><span class="s1">)</span>
  <span class="s3">elif </span><span class="s1">computation_name == </span><span class="s4">&quot;min&quot;</span><span class="s1">:</span>
    <span class="s3">return </span><span class="s1">negate(tf_pool(negate(operand)</span><span class="s3">, </span><span class="s4">&quot;MAX&quot;</span><span class="s1">))</span>
  <span class="s3">elif </span><span class="s1">computation_name == </span><span class="s4">&quot;add&quot;</span><span class="s1">:</span>
    <span class="s0"># TODO(marcvanzee): This may give very large deviations on TPU when using</span>
    <span class="s0"># floats as inputs. Alternatively, we could implement this using a</span>
    <span class="s0"># convolution with an all-1's kernel.</span>
    <span class="s3">return </span><span class="s1">tf.multiply(tf_pool(operand</span><span class="s3">, </span><span class="s4">&quot;AVG&quot;</span><span class="s1">)</span><span class="s3">, </span><span class="s1">np.prod(window_dimensions))</span>


<span class="s3">def </span><span class="s1">_reduce_window(*args</span><span class="s3">, </span><span class="s1">jaxpr</span><span class="s3">, </span><span class="s1">consts</span><span class="s3">, </span><span class="s1">window_dimensions</span><span class="s3">,</span>
                   <span class="s1">window_strides</span><span class="s3">, </span><span class="s1">padding</span><span class="s3">, </span><span class="s1">base_dilation</span><span class="s3">, </span><span class="s1">window_dilation</span><span class="s3">,</span>
                   <span class="s1">_in_avals: Sequence[core.ShapedArray]</span><span class="s3">,</span>
                   <span class="s1">_out_aval: Tuple[core.ShapedArray</span><span class="s3">, </span><span class="s1">...]</span>
                   <span class="s1">) -&gt; Tuple[TfVal</span><span class="s3">, </span><span class="s1">...]:</span>
  <span class="s3">assert </span><span class="s1">len(consts) == </span><span class="s5">0</span><span class="s3">, </span><span class="s4">&quot;Reduction computation cannot have constants&quot;</span>
  <span class="s1">operands</span><span class="s3">, </span><span class="s1">init_values = util.split_list(args</span><span class="s3">, </span><span class="s1">[len(args) // </span><span class="s5">2</span><span class="s1">])</span>

  <span class="s3">if </span><span class="s1">len(operands) != </span><span class="s5">1 </span><span class="s3">or </span><span class="s1">len(init_values) != </span><span class="s5">1</span><span class="s1">:</span>
    <span class="s3">raise </span><span class="s1">_reduce_error(</span><span class="s4">&quot;jax2tf does not support variadic reduce_window&quot;</span><span class="s1">)</span>

  <span class="s1">operand</span><span class="s3">, </span><span class="s1">init_value = operands[</span><span class="s5">0</span><span class="s1">]</span><span class="s3">, </span><span class="s1">init_values[</span><span class="s5">0</span><span class="s1">]</span>
  <span class="s0"># Infer operation type from jaxpr.</span>
  <span class="s3">if </span><span class="s1">(len(jaxpr.eqns) != </span><span class="s5">1 </span><span class="s3">or</span>
      <span class="s1">len(jaxpr.eqns[</span><span class="s5">0</span><span class="s1">].invars) != </span><span class="s5">2 </span><span class="s3">or</span>
      <span class="s1">len(jaxpr.eqns[</span><span class="s5">0</span><span class="s1">].outvars) != </span><span class="s5">1 </span><span class="s3">or</span>
      <span class="s1">jaxpr.eqns[</span><span class="s5">0</span><span class="s1">].primitive.name </span><span class="s3">not in </span><span class="s1">[</span><span class="s4">&quot;min&quot;</span><span class="s3">, </span><span class="s4">&quot;max&quot;</span><span class="s3">, </span><span class="s4">&quot;add&quot;</span><span class="s1">]):</span>
    <span class="s3">raise </span><span class="s1">_reduce_error(</span><span class="s4">&quot;Reduction function should be either min, max, or add.&quot;</span><span class="s1">)</span>

  <span class="s1">computation_name = jaxpr.eqns[</span><span class="s5">0</span><span class="s1">].primitive.name</span>
  <span class="s1">result = _reduce_monoid(operand</span><span class="s3">,</span>
                          <span class="s1">window_dimensions=window_dimensions</span><span class="s3">,</span>
                          <span class="s1">window_strides=window_strides</span><span class="s3">,</span>
                          <span class="s1">padding=padding</span><span class="s3">,</span>
                          <span class="s1">base_dilation=base_dilation</span><span class="s3">,</span>
                          <span class="s1">window_dilation=window_dilation</span><span class="s3">,</span>
                          <span class="s1">computation_name=computation_name</span><span class="s3">,</span>
                          <span class="s1">_in_avals=(_in_avals[</span><span class="s5">0</span><span class="s1">]</span><span class="s3">,</span><span class="s1">)</span><span class="s3">,  </span><span class="s0"># Don't pass init_value.</span>
                          <span class="s1">_out_aval=_out_aval[</span><span class="s5">0</span><span class="s1">])     </span><span class="s0"># Returns single value.</span>

  <span class="s1">reduce_fn = {</span>
      <span class="s4">&quot;min&quot;</span><span class="s1">: tf.minimum</span><span class="s3">,</span>
      <span class="s4">&quot;max&quot;</span><span class="s1">: tf.maximum</span><span class="s3">,</span>
      <span class="s4">&quot;add&quot;</span><span class="s1">: tf.add</span><span class="s3">,</span>
  <span class="s1">}[computation_name]</span>
  <span class="s1">result = reduce_fn(result</span><span class="s3">, </span><span class="s1">init_value)</span>

  <span class="s0"># The outut is expected to be wrapped in a tuple, and since we don't use</span>
  <span class="s0"># variadic reductions, this tuple always contains a single element.</span>
  <span class="s3">return </span><span class="s1">(result</span><span class="s3">,</span><span class="s1">)</span>


<span class="s1">tf_impl_no_xla[lax.reduce_window_min_p] = (</span>
    <span class="s1">partial(_reduce_monoid</span><span class="s3">, </span><span class="s1">computation_name=</span><span class="s4">&quot;min&quot;</span><span class="s1">))</span>
<span class="s1">tf_impl_no_xla[lax.reduce_window_max_p] = (</span>
    <span class="s1">partial(_reduce_monoid</span><span class="s3">, </span><span class="s1">computation_name=</span><span class="s4">&quot;max&quot;</span><span class="s1">))</span>
<span class="s1">tf_impl_no_xla[lax.reduce_window_sum_p] = (</span>
    <span class="s1">partial(_reduce_monoid</span><span class="s3">, </span><span class="s1">computation_name=</span><span class="s4">&quot;add&quot;</span><span class="s1">))</span>

<span class="s1">tf_impl_no_xla[lax.reduce_window_p] = _reduce_window</span>

<span class="s1">tf_impl_no_xla[lax.reduce_p] = _unimplemented(</span><span class="s4">&quot;reduce&quot;</span><span class="s1">)</span>

<span class="s1">tf_impl_no_xla[lax.select_and_scatter_add_p] = _unimplemented(</span>
    <span class="s4">&quot;select_and_scatter_add&quot;</span><span class="s1">)</span>

<span class="s1">tf_impl_no_xla[lax.rng_bit_generator_p] = _unimplemented(</span><span class="s4">&quot;rng_bit_generator&quot;</span><span class="s1">)</span>


<span class="s3">def </span><span class="s1">_clip(max_indices: Sequence[TfVal]</span><span class="s3">, </span><span class="s1">start_indices: Sequence[TfVal]</span><span class="s3">,</span>
          <span class="s1">slice_sizes: Sequence[TfVal]):</span>
  <span class="s2">&quot;&quot;&quot;Simulates XLA clipping behavior with TF ops. 
 
  Various TF ops have different clipping behavior than XLA: 
  * If `start_indices` is out-of-bounds, then TF fails but XLA clips the indices 
  to 
    [0, max_len]. 
  * If `start_indices + slice_size` is out-of-bounds, then TF fails, but XLA 
  adjust 
    `start_indices` so that a full slice is returned. 
  This function clips the start indices correctly. 
  &quot;&quot;&quot;</span>
  <span class="s0"># We cast both arguments to `tf.clip_by_value` to int32. Otherwise, this</span>
  <span class="s0"># function may return uint32 which is not always compatible with TF ops, so</span>
  <span class="s0"># this may result in type errors.</span>
  <span class="s1">max_start = tf.cast(tf.subtract(max_indices</span><span class="s3">, </span><span class="s1">slice_sizes)</span><span class="s3">, </span><span class="s1">dtype=tf.int32)</span>
  <span class="s3">return </span><span class="s1">tf.clip_by_value(tf.cast(start_indices</span><span class="s3">, </span><span class="s1">dtype=tf.int32)</span><span class="s3">, </span><span class="s5">0</span><span class="s3">, </span><span class="s1">max_start)</span>


<span class="s1">@dataclasses.dataclass</span>
<span class="s3">class </span><span class="s1">GatherArgs:</span>
  <span class="s1">operand: TfVal</span>
  <span class="s1">start_indices: TfVal</span>
  <span class="s1">dnums: lax.GatherDimensionNumbers</span>
  <span class="s1">slice_sizes: TfVal</span>
  <span class="s1">op_shape: core.Shape</span>
  <span class="s1">start_indices_shape: core.Shape</span>
  <span class="s1">out_aval: core.ShapedArray</span>

  <span class="s3">def </span><span class="s1">__post_init__(self):</span>
    <span class="s3">assert </span><span class="s1">len(self.op_shape) == len(self.slice_sizes)</span>

  <span class="s3">def </span><span class="s1">__repr__(self):</span>
    <span class="s3">return </span><span class="s1">(</span><span class="s4">f&quot;operand shape=</span><span class="s3">{</span><span class="s1">self.op_shape</span><span class="s3">}</span><span class="s4">, &quot;</span>
            <span class="s4">f&quot;start_indices=</span><span class="s3">{</span><span class="s1">self.start_indices</span><span class="s3">}</span><span class="s4">, &quot;</span>
            <span class="s4">f&quot;dimension_numbes=</span><span class="s3">{</span><span class="s1">self.dnums</span><span class="s3">}</span><span class="s4">, &quot;</span>
            <span class="s4">f&quot;slice_sizes=</span><span class="s3">{</span><span class="s1">self.slice_sizes</span><span class="s3">}</span><span class="s4">&quot;</span><span class="s1">)</span>
  <span class="s1">@property</span>
  <span class="s3">def </span><span class="s1">batch_dims(self):</span>
    <span class="s3">return </span><span class="s1">tuple(x </span><span class="s3">for </span><span class="s1">x </span><span class="s3">in </span><span class="s1">range(len(self.out_aval.shape))</span>
                <span class="s3">if </span><span class="s1">x </span><span class="s3">not in </span><span class="s1">self.dnums.offset_dims)</span>

<span class="s3">def </span><span class="s1">gather_precondition(precondition_fn: Callable[[GatherArgs]</span><span class="s3">, None</span><span class="s1">]):</span>
  <span class="s2">&quot;&quot;&quot;Decorator for specifying a precondition function. 
 
  This decorator should be put on a function with argument `arg` of type 
  `GatherArgs`. It will first call `precondition_fn` with `arg` (which may throw 
  an exception), and then call the function it is decorating with `arg` as well. 
  &quot;&quot;&quot;</span>

  <span class="s3">def </span><span class="s1">decorator(gather_fn: Callable[[GatherArgs]</span><span class="s3">, </span><span class="s1">Any]):</span>

    <span class="s1">@wraps(gather_fn)</span>
    <span class="s3">def </span><span class="s1">wrapper(args: GatherArgs):</span>
      <span class="s0"># Call `precondition_fn`; we assume it may throw an exception.</span>
      <span class="s1">precondition_fn(args)</span>
      <span class="s3">return </span><span class="s1">gather_fn(args)</span>

    <span class="s3">return </span><span class="s1">wrapper</span>

  <span class="s3">return </span><span class="s1">decorator</span>


<span class="s3">def </span><span class="s1">_pre_gather_for_scalar_indexing(args: GatherArgs):</span>
  <span class="s2">&quot;&quot;&quot;Returns True if this call to gather represents scalar indexing into arrays. 
 
  E.g., op[2], op[:, :5, :], jnp.take(op, 0, axis=0). 
  &quot;&quot;&quot;</span>
  <span class="s0"># TODO(marcvanzee): Add more assumptions here, because this is currently too</span>
  <span class="s0"># permissive.</span>
  <span class="s3">if </span><span class="s1">len(args.start_indices_shape) != </span><span class="s5">1</span><span class="s1">:</span>
    <span class="s3">raise </span><span class="s1">ValueError(</span><span class="s4">&quot;start_indices shape should be 1&quot;</span><span class="s1">)</span>


<span class="s1">@gather_precondition(_pre_gather_for_scalar_indexing)</span>
<span class="s3">def </span><span class="s1">_gather_for_scalar_indexing(args: GatherArgs):</span>
  <span class="s2">&quot;&quot;&quot;Implements 'scalar indexing into arrays' cases of lax.gather using tf.slice. 
 
  E.g., op[2], op[:, :5, :], jnp.take(op, 0, axis=0). 
  &quot;&quot;&quot;</span>
  <span class="s1">indices = tf.expand_dims(args.dnums.start_index_map</span><span class="s3">, </span><span class="s5">1</span><span class="s1">)</span>
  <span class="s0"># lax.gather uses an &quot;index map&quot; which maps `start_indices` to the right axes</span>
  <span class="s0"># in `operand`. Since tf.strided_slice uses a single array for specifying the</span>
  <span class="s0"># start indices, we use a scatter to map the start indices to the right axes.</span>
  <span class="s1">op_shape = jax2tf._eval_shape(args.op_shape)</span>
  <span class="s1">slice_sizes_tf = jax2tf._eval_shape(args.slice_sizes)</span>
  <span class="s0"># TODO(marcvanzee): Consider transposing `operand`, which is probably more</span>
  <span class="s0"># optimization friendly.</span>
  <span class="s1">begin = tf.scatter_nd(indices</span><span class="s3">, </span><span class="s1">args.start_indices</span><span class="s3">, </span><span class="s1">[len(op_shape)])</span>
  <span class="s1">begin = _clip(op_shape</span><span class="s3">, </span><span class="s1">begin</span><span class="s3">, </span><span class="s1">slice_sizes_tf)</span>
  <span class="s1">end = slice_sizes_tf + begin</span>

  <span class="s0"># `collapsed_slice_dims` is a tuple of dimensions to collapse, e.g. (0, 2).</span>
  <span class="s0"># `tf.strided_slice` expects a binary mask to specify the shrink axes, i.e.,</span>
  <span class="s0"># if we want to shrink axis 0 and 2, this corresponds to binary mask 101,</span>
  <span class="s0"># which is 5 in decimals. The following line converts the lax representation</span>
  <span class="s0"># to the one used by `tf.strided_slice`.</span>
  <span class="s1">shrink_mask = sum(</span><span class="s5">2</span><span class="s1">**x </span><span class="s3">for </span><span class="s1">x </span><span class="s3">in </span><span class="s1">args.dnums.collapsed_slice_dims)</span>
  <span class="s1">res = tf.strided_slice(args.operand</span><span class="s3">, </span><span class="s1">begin</span><span class="s3">, </span><span class="s1">end</span><span class="s3">, </span><span class="s1">shrink_axis_mask=shrink_mask)</span>
  <span class="s0"># Shape inference doesn't work for tf.strided_slice.</span>
  <span class="s1">res = jax2tf._ensure_tf_shape_if_dynamic(</span>
      <span class="s1">res</span><span class="s3">, </span><span class="s1">jax2tf._aval_to_tf_shape(args.out_aval)</span>
  <span class="s1">)</span>
  <span class="s3">return </span><span class="s1">res</span>


<span class="s3">def </span><span class="s1">_pre_gather_for_multidim_indexing(args: GatherArgs):</span>
  <span class="s2">&quot;&quot;&quot;Returns True if this call to gather represents multi-dimensional indexing. 
 
  E.g., jnp.take(op, [[0], [1]], axis=0). 
  Note we currently only support multi-dimensional indexing if the last 
  dimension is 1. 
  &quot;&quot;&quot;</span>
  <span class="s0"># Handle only the case when tf.gather argument batch_dims=0.</span>
  <span class="s0"># Find axis to match the tf.gather semantics</span>
  <span class="s0"># Let I = len(start_indices_shape)</span>
  <span class="s0"># let O = len(op_shape)</span>
  <span class="s0"># slice_sizes == op_shape[:axis] + (1,) + op_shape[axis+1:]</span>
  <span class="s0"># collapsed_slice_dims == (axis,)</span>
  <span class="s0"># start_index_map == (axis,)</span>
  <span class="s0"># offset_dims == (0, 1, ..., axis - 1, axis + I, ..., O + I - 1)</span>
  <span class="s0"># We added a trailing dimension of size 1</span>
  <span class="s1">op_shape = args.op_shape</span>
  <span class="s1">start_index_map = args.dnums.start_index_map</span>
  <span class="s1">collapsed_slice_dims = args.dnums.collapsed_slice_dims</span>
  <span class="s1">offset_dims = args.dnums.offset_dims</span>
  <span class="s3">if not </span><span class="s1">(len(op_shape) &gt;= </span><span class="s5">1 </span><span class="s3">and </span><span class="s1">len(start_index_map) == </span><span class="s5">1 </span><span class="s3">and</span>
          <span class="s1">len(collapsed_slice_dims) == </span><span class="s5">1 </span><span class="s3">and </span><span class="s1">collapsed_slice_dims[</span><span class="s5">0</span><span class="s1">]</span>
          <span class="s1">== start_index_map[</span><span class="s5">0</span><span class="s1">] </span><span class="s3">and </span><span class="s1">len(offset_dims) == len(op_shape) - </span><span class="s5">1</span><span class="s1">):</span>
    <span class="s3">raise </span><span class="s1">ValueError(</span><span class="s4">&quot;unsupported dimension numbers&quot;</span><span class="s1">)</span>
  <span class="s0"># We added a trailing dimension of size 1</span>
  <span class="s3">if not </span><span class="s1">core.symbolic_equal_dim(args.start_indices_shape[-</span><span class="s5">1</span><span class="s1">]</span><span class="s3">, </span><span class="s5">1</span><span class="s1">):</span>
    <span class="s3">raise </span><span class="s1">ValueError(</span><span class="s4">&quot;start_indices shape[-1] should be 1&quot;</span><span class="s1">)</span>
  <span class="s0"># Guess the axis</span>
  <span class="s1">axis = collapsed_slice_dims[</span><span class="s5">0</span><span class="s1">]</span>
  <span class="s1">index_dims = len(args.start_indices_shape) - </span><span class="s5">1</span>
  <span class="s1">expected_offset_dims = tuple(</span>
      <span class="s1">list(range(axis)) +</span>
      <span class="s1">list(range(axis + index_dims</span><span class="s3">,</span>
                 <span class="s1">len(op_shape) + index_dims - </span><span class="s5">1</span><span class="s1">)))</span>
  <span class="s3">if </span><span class="s1">offset_dims != expected_offset_dims:</span>
    <span class="s3">raise </span><span class="s1">ValueError(</span><span class="s4">&quot;unsupported offset_dims&quot;</span><span class="s1">)</span>
  <span class="s1">expected_slice_sizes = op_shape[:axis] + (</span><span class="s5">1</span><span class="s3">,</span><span class="s1">) + op_shape[axis + </span><span class="s5">1</span><span class="s1">:]  </span><span class="s0"># type: ignore</span>
  <span class="s3">if not </span><span class="s1">core.symbolic_equal_shape(args.slice_sizes</span><span class="s3">, </span><span class="s1">expected_slice_sizes):</span>
    <span class="s3">raise </span><span class="s1">ValueError(</span><span class="s4">&quot;unsupported slice_sizes&quot;</span><span class="s1">)</span>


<span class="s1">@gather_precondition(_pre_gather_for_multidim_indexing)</span>
<span class="s3">def </span><span class="s1">_gather_for_multidim_indexing(args: GatherArgs):</span>
  <span class="s2">&quot;&quot;&quot;Implements 'multi-dimensional indexing into arrays' cases of lax.gather using tf.gather. 
 
  E.g., jnp.take(op, [[0], [1]], axis=0). 
  &quot;&quot;&quot;</span>
  <span class="s0"># Guess the axis.</span>
  <span class="s1">axis = args.dnums.collapsed_slice_dims[</span><span class="s5">0</span><span class="s1">]</span>
  <span class="s1">squeezed_indices = tf.squeeze(args.start_indices</span><span class="s3">, </span><span class="s1">-</span><span class="s5">1</span><span class="s1">)</span>
  <span class="s1">op_shape = jax2tf._eval_shape(args.op_shape)</span>
  <span class="s1">start_indices = _clip((op_shape[axis]</span><span class="s3">,</span><span class="s1">)</span><span class="s3">, </span><span class="s1">squeezed_indices</span><span class="s3">, </span><span class="s1">(</span><span class="s5">1</span><span class="s3">,</span><span class="s1">))</span>
  <span class="s3">return </span><span class="s1">tf.gather(args.operand</span><span class="s3">, </span><span class="s1">start_indices</span><span class="s3">, </span><span class="s1">axis=axis</span><span class="s3">, </span><span class="s1">batch_dims=</span><span class="s5">0</span><span class="s1">)</span>


<span class="s3">def </span><span class="s1">_pre_gather_with_batch_dim(args: GatherArgs):</span>
  <span class="s2">&quot;&quot;&quot;Returns True if this call to gather has non-empty batch dimensions. 
 
  This is for instance triggered when doing jax.vmap(lax.dynamic_slice). 
  &quot;&quot;&quot;</span>
  <span class="s0"># We assume exactly one batch (and one or more non-batch dimensions).</span>
  <span class="s3">if </span><span class="s1">len(args.batch_dims) != </span><span class="s5">1</span><span class="s1">:</span>
    <span class="s3">raise </span><span class="s1">ValueError(</span><span class="s4">f&quot;batch_dims is </span><span class="s3">{</span><span class="s1">len(args.batch_dims)</span><span class="s3">} </span><span class="s4">but should be 1&quot;</span><span class="s1">)</span>

  <span class="s0"># `start_index_map` maps indices in `start_indices` to indices in `operand`.</span>
  <span class="s0"># For simplicity, we currently only consider the case where this mapping is</span>
  <span class="s0"># the identity function, i.e., [2, 3] in `start_indices` maps to</span>
  <span class="s0"># `operand[2, 3]`.</span>
  <span class="s3">if </span><span class="s1">args.dnums.start_index_map != tuple(range(args.start_indices_shape[-</span><span class="s5">1</span><span class="s1">])):</span>
    <span class="s3">raise </span><span class="s1">ValueError(</span><span class="s4">&quot;unsupported start_index_map&quot;</span><span class="s1">)</span>

  <span class="s0"># The batch dims in `start_indices` and `operand` should agree.</span>
  <span class="s3">if not </span><span class="s1">core.symbolic_equal_dim(args.op_shape[</span><span class="s5">0</span><span class="s1">]</span><span class="s3">, </span><span class="s1">args.start_indices_shape[</span><span class="s5">0</span><span class="s1">]):</span>
    <span class="s3">raise </span><span class="s1">ValueError(</span><span class="s4">&quot;Batch dimensions in operand and start_indices don't &quot;</span>
                     <span class="s4">&quot;agree&quot;</span><span class="s1">)</span>


<span class="s3">def </span><span class="s1">_pre_gather_with_batch_dims(args: GatherArgs):</span>
  <span class="s2">&quot;&quot;&quot;Returns True if this call to gather has non-empty 2D batch dimensions. 
 
  This is for instance triggered when doing 
  jax.vmap(jax.vmap(lax.dynamic_slice)). 
  &quot;&quot;&quot;</span>
  <span class="s3">if </span><span class="s1">len(args.dnums.collapsed_slice_dims) != </span><span class="s5">0</span><span class="s1">:</span>
    <span class="s0"># NOTE: this can be relaxed in _gather_with_batch_dims but we might</span>
    <span class="s0">#   also need to re-work the output reshaping</span>
    <span class="s3">raise </span><span class="s1">ValueError(</span><span class="s4">&quot;only len(collapsed_slice_dims) == 0 is supported&quot;</span><span class="s1">)</span>

  <span class="s0"># NOTE: This supports higher dimensions than listed (the highest dimenison</span>
  <span class="s0"># in the tests is 3D so it is limited to that, but the implementation is</span>
  <span class="s0"># designed to handle higher dimensions (N-Dimensional)).</span>
  <span class="s3">if </span><span class="s1">len(args.batch_dims) </span><span class="s3">not in </span><span class="s1">[</span><span class="s5">1</span><span class="s3">, </span><span class="s5">2</span><span class="s3">, </span><span class="s5">3</span><span class="s1">]:</span>
    <span class="s3">raise </span><span class="s1">ValueError(</span>
        <span class="s4">f&quot;Size of batch_dims is </span><span class="s3">{</span><span class="s1">len(args.batch_dims)</span><span class="s3">} </span><span class="s4">but should be up to 3&quot;</span>
    <span class="s1">)</span>

<span class="s1">@gather_precondition(_pre_gather_with_batch_dim)</span>
<span class="s3">def </span><span class="s1">_gather_with_batch_dim(args: GatherArgs):</span>
  <span class="s2">&quot;&quot;&quot;Implements call to gather with non-empty batch dimensions. 
 
  E.g., when doing `jax.vmap(lax.dynamic_slice). 
  &quot;&quot;&quot;</span>
  <span class="s1">op_shape = jax2tf._eval_shape(args.op_shape)</span>
  <span class="s1">start_indices = _clip(op_shape</span><span class="s3">, </span><span class="s1">args.start_indices</span><span class="s3">, </span><span class="s1">args.slice_sizes)</span>
  <span class="s1">result = tf.map_fn(</span>
    <span class="s3">lambda </span><span class="s1">idxs: tf.slice(args.operand</span><span class="s3">, </span><span class="s1">begin=idxs</span><span class="s3">, </span><span class="s1">size=args.slice_sizes)</span><span class="s3">,</span>
    <span class="s1">start_indices</span><span class="s3">,</span>
    <span class="s1">fn_output_signature=jax2tf._to_tf_dtype(args.operand.dtype)</span>
  <span class="s1">)</span>
  <span class="s1">result = tf.reshape(result</span><span class="s3">, </span><span class="s1">jax2tf._eval_shape(args.out_aval.shape))</span>
  <span class="s3">return </span><span class="s1">result</span>


<span class="s3">def </span><span class="s1">_gather_generate_indices(shape: Tuple[int</span><span class="s3">, </span><span class="s1">...]):</span>
  <span class="s2">&quot;&quot;&quot; 
  Returns the indices of the according to `shape`: 
    each element in the output is the index of an element of an array 
    of the provided shape. The result's shape is (np.prod(shape), len(shape)) 
 
  For example, given shape (2,2) it returns (0,0),(0,1),(1,0),(1,1) 
  &quot;&quot;&quot;</span>
  <span class="s3">return </span><span class="s1">tf.reshape(</span>
      <span class="s1">tf.stack(</span>
          <span class="s1">tf.meshgrid(</span>
              <span class="s1">*[tf.range(start=</span><span class="s5">0</span><span class="s3">, </span><span class="s1">limit=x) </span><span class="s3">for </span><span class="s1">x </span><span class="s3">in </span><span class="s1">shape]</span><span class="s3">, </span><span class="s1">indexing=</span><span class="s4">&quot;ij&quot;</span>
          <span class="s1">)</span><span class="s3">,</span>
          <span class="s1">axis=-</span><span class="s5">1</span><span class="s3">,</span>
      <span class="s1">)</span><span class="s3">,</span>
      <span class="s1">(-</span><span class="s5">1</span><span class="s3">, </span><span class="s1">len(shape))</span><span class="s3">,</span>
  <span class="s1">)</span>


<span class="s1">@gather_precondition(_pre_gather_with_batch_dims)</span>
<span class="s3">def </span><span class="s1">_gather_with_batch_dims(args: GatherArgs):</span>
  <span class="s2">&quot;&quot;&quot;Implements call to gather with non-empty 2D batch dimensions.&quot;&quot;&quot;</span>
  <span class="s1">op_shape = jax2tf._eval_shape(args.op_shape)</span>
  <span class="s1">output_shape = jax2tf._eval_shape(args.out_aval.shape)</span>
  <span class="s0"># Used to map the start_indices w.r.t start_index_map</span>
  <span class="s1">indices = tf.expand_dims(args.dnums.start_index_map</span><span class="s3">, </span><span class="s5">1</span><span class="s1">)</span>

  <span class="s0"># batch_indices is shaped (N,d) where N is the number of slices and d is</span>
  <span class="s0"># the number of batch_dims; batch_indices_size equals to N</span>
  <span class="s1">batch_indices = _gather_generate_indices(</span>
      <span class="s1">tuple(output_shape[i] </span><span class="s3">for </span><span class="s1">i </span><span class="s3">in </span><span class="s1">args.batch_dims)</span>
  <span class="s1">)</span>
  <span class="s1">batch_indices_size = jax2tf._eval_shape(batch_indices.shape)[</span><span class="s5">0</span><span class="s1">]</span>
  <span class="s0"># offset_indices is shaped (K,d) where K is the number of elements in each</span>
  <span class="s0"># slice and d is the number of offset_dims; offset_indices_size equals to K</span>
  <span class="s1">offset_indices = _gather_generate_indices(</span>
      <span class="s1">tuple(output_shape[i] </span><span class="s3">for </span><span class="s1">i </span><span class="s3">in </span><span class="s1">args.dnums.offset_dims)</span>
  <span class="s1">)</span>
  <span class="s1">offset_indices_size = jax2tf._eval_shape(offset_indices.shape)[</span><span class="s5">0</span><span class="s1">]</span>

  <span class="s0"># After we compute the result we need to reshape the axes with respect to</span>
  <span class="s0"># the output batch_dims and offset_dims.</span>
  <span class="s1">dim_mask = args.batch_dims + args.dnums.offset_dims</span>
  <span class="s1">mask_output_shape = tuple(output_shape[x] </span><span class="s3">for </span><span class="s1">x </span><span class="s3">in </span><span class="s1">dim_mask)</span>

  <span class="s3">def </span><span class="s1">get_scatter_indices(indices</span><span class="s3">, </span><span class="s1">batch_indices_size</span><span class="s3">, </span><span class="s1">size_of_index_map):</span>
    <span class="s2">&quot;&quot;&quot;Generate the start indices of each slice, which index into the operand.&quot;&quot;&quot;</span>
    <span class="s0"># Tile indices batch_indices_size times</span>
    <span class="s1">tiled_indices = tf.tile(</span>
        <span class="s1">tf.expand_dims(indices</span><span class="s3">, </span><span class="s5">0</span><span class="s1">)</span><span class="s3">, </span><span class="s1">[batch_indices_size</span><span class="s3">, </span><span class="s5">1</span><span class="s3">, </span><span class="s5">1</span><span class="s1">]</span>
    <span class="s1">)</span>
    <span class="s0"># The above tiles need to index the proper element of batch_indices</span>
    <span class="s0"># To do this generate a repeated sequence of numbers</span>
    <span class="s1">temp_batch_indices = tf.repeat(</span>
        <span class="s1">tf.range(start=</span><span class="s5">0</span><span class="s3">, </span><span class="s1">limit=batch_indices_size)</span><span class="s3">, </span><span class="s1">size_of_index_map</span>
    <span class="s1">)</span>
    <span class="s0"># Reshape the above sequence so it follows the same shape of tiled_indices</span>
    <span class="s1">temp_batch_indices = tf.reshape(</span>
        <span class="s1">temp_batch_indices</span><span class="s3">, </span><span class="s1">(batch_indices_size</span><span class="s3">, </span><span class="s1">size_of_index_map</span><span class="s3">, </span><span class="s5">1</span><span class="s1">)</span>
    <span class="s1">)</span>
    <span class="s0"># Now we concatenate to create indices offset by the temp_batch_indices</span>
    <span class="s3">return </span><span class="s1">tf.concat([temp_batch_indices</span><span class="s3">, </span><span class="s1">tiled_indices]</span><span class="s3">, </span><span class="s1">axis=-</span><span class="s5">1</span><span class="s1">)</span>

  <span class="s1">slice_start_indices = tf.gather_nd(args.start_indices</span><span class="s3">, </span><span class="s1">batch_indices)</span>
  <span class="s0"># TODO: In the case where start_index_map is the identity we can skip this.</span>
  <span class="s1">scatter_indices = get_scatter_indices(</span>
      <span class="s1">indices</span><span class="s3">, </span><span class="s1">batch_indices_size</span><span class="s3">, </span><span class="s1">len(args.dnums.start_index_map)</span>
  <span class="s1">)</span>
  <span class="s0"># We map the scatter_indices w.r.t start_index_map</span>
  <span class="s1">indices_in_operand = tf.scatter_nd(</span>
        <span class="s1">scatter_indices</span><span class="s3">, </span><span class="s1">slice_start_indices</span><span class="s3">, </span><span class="s1">[batch_indices_size</span><span class="s3">, </span><span class="s1">len(op_shape)]</span>
  <span class="s1">)</span>

  <span class="s0"># We clip the indices as OOB cases are possible when offsetting past</span>
  <span class="s0"># the operand boundaries</span>
  <span class="s1">clipped_start_indices = _clip(op_shape</span><span class="s3">, </span><span class="s1">indices_in_operand</span><span class="s3">, </span><span class="s1">args.slice_sizes)</span>
  <span class="s0"># Here we need to broadcast clipped_start_indices and add each of the offsets</span>
  <span class="s0"># which will generate a large index tensor of shape (T,d) where T is the</span>
  <span class="s0"># number of slices times the size of each slice (i.e total number of items</span>
  <span class="s0"># across all sices); d is rank(operand)</span>
  <span class="s1">slice_element_indices = tf.add(</span>
      <span class="s1">tf.repeat(clipped_start_indices</span><span class="s3">, </span><span class="s1">offset_indices_size</span><span class="s3">, </span><span class="s1">axis=</span><span class="s5">0</span><span class="s1">)</span><span class="s3">,</span>
      <span class="s1">tf.tile(offset_indices</span><span class="s3">, </span><span class="s1">(batch_indices_size</span><span class="s3">, </span><span class="s5">1</span><span class="s1">))</span><span class="s3">,</span>
  <span class="s1">)</span>
  <span class="s1">results = tf.gather_nd(args.operand</span><span class="s3">, </span><span class="s1">slice_element_indices)</span>

  <span class="s0"># Here results comes shaped as (N,1). Because collapsed_slice_dims is 0,</span>
  <span class="s0"># offset_dims is effectviely slice_sizes.</span>
  <span class="s0"># We reshape to mask_output_shape because if we directly reshape to the</span>
  <span class="s0"># output shape and our batch_dims are non-contiguous we will produce the</span>
  <span class="s0"># wrong shape. Reshaping to mask_output_shape gives (...,*slice_sizes),</span>
  <span class="s0"># which we then transpose to permute the axes in the proper way.</span>
  <span class="s0"># Note that if the batch_dims are contiguous this won't change the output.</span>
  <span class="s1">temp = tf.reshape(results</span><span class="s3">, </span><span class="s1">shape=mask_output_shape)</span>
  <span class="s3">return </span><span class="s1">tf.transpose(temp</span><span class="s3">, </span><span class="s1">perm=tf.math.invert_permutation(dim_mask))</span>

<span class="s3">def </span><span class="s1">_gather(operand</span><span class="s3">, </span><span class="s1">start_indices</span><span class="s3">, </span><span class="s1">*</span><span class="s3">, </span><span class="s1">dimension_numbers</span><span class="s3">,</span>
            <span class="s1">slice_sizes: core.Shape</span><span class="s3">, </span><span class="s1">indices_are_sorted</span><span class="s3">, </span><span class="s1">unique_indices</span><span class="s3">, </span><span class="s1">mode</span><span class="s3">,</span>
            <span class="s1">fill_value</span><span class="s3">, </span><span class="s1">_in_avals: Sequence[core.ShapedArray]</span><span class="s3">,</span>
            <span class="s1">_out_aval: core.ShapedArray):</span>
  <span class="s2">&quot;&quot;&quot;Tensorflow implementation of gather.&quot;&quot;&quot;</span>
  <span class="s3">if </span><span class="s1">mode == lax.GatherScatterMode.FILL_OR_DROP:</span>
    <span class="s1">gather_fill_fn = jax2tf._convert_jax_impl(lax_slicing._gather_fill</span><span class="s3">,</span>
                                              <span class="s1">multiple_results=</span><span class="s3">False</span><span class="s1">)</span>
    <span class="s3">return </span><span class="s1">gather_fill_fn(</span>
        <span class="s1">operand</span><span class="s3">, </span><span class="s1">start_indices</span><span class="s3">, </span><span class="s1">dimension_numbers=dimension_numbers</span><span class="s3">,</span>
        <span class="s1">slice_sizes=slice_sizes</span><span class="s3">, </span><span class="s1">unique_indices=unique_indices</span><span class="s3">,</span>
        <span class="s1">indices_are_sorted=indices_are_sorted</span><span class="s3">, </span><span class="s1">fill_value=fill_value</span><span class="s3">,</span>
        <span class="s1">output_shape=_out_aval.shape</span><span class="s3">, </span><span class="s1">_in_avals=_in_avals</span><span class="s3">, </span><span class="s1">_out_aval=_out_aval)</span>

  <span class="s0"># TODO(marcvanzee): Check if we need more tests in shape_poly for gather with</span>
  <span class="s0"># enable_xla=False.</span>
  <span class="s1">gather_args = GatherArgs(</span>
      <span class="s1">operand=operand</span><span class="s3">,</span>
      <span class="s1">start_indices=start_indices</span><span class="s3">,</span>
      <span class="s1">dnums=dimension_numbers</span><span class="s3">,</span>
      <span class="s1">slice_sizes=slice_sizes</span><span class="s3">,</span>
      <span class="s1">op_shape=_in_avals[</span><span class="s5">0</span><span class="s1">].shape</span><span class="s3">,</span>
      <span class="s1">start_indices_shape=_in_avals[</span><span class="s5">1</span><span class="s1">].shape</span><span class="s3">,</span>
      <span class="s1">out_aval=_out_aval)</span>

  <span class="s1">errors = []</span>

  <span class="s3">for </span><span class="s1">gather_fn </span><span class="s3">in </span><span class="s1">[</span>
      <span class="s1">_gather_for_scalar_indexing</span><span class="s3">,</span>
      <span class="s1">_gather_for_multidim_indexing</span><span class="s3">,</span>
      <span class="s1">_gather_with_batch_dim</span><span class="s3">,</span>
      <span class="s1">_gather_with_batch_dims</span><span class="s3">,</span>
  <span class="s1">]:</span>
    <span class="s3">try</span><span class="s1">:</span>
      <span class="s3">return </span><span class="s1">gather_fn(gather_args)</span>
    <span class="s3">except </span><span class="s1">ValueError </span><span class="s3">as </span><span class="s1">e:</span>
      <span class="s1">errors.append(</span><span class="s4">f&quot;</span><span class="s3">{</span><span class="s1">gather_fn</span><span class="s3">}</span><span class="s4">: </span><span class="s3">{</span><span class="s1">repr(e)</span><span class="s3">}</span><span class="s4">&quot;</span><span class="s1">)</span>

  <span class="s1">error_msg = (</span><span class="s4">f&quot;Unsupported arguments for gather: </span><span class="s3">{</span><span class="s1">gather_args</span><span class="s3">}</span><span class="s4">, errors:</span><span class="s3">\n</span><span class="s4">&quot; </span><span class="s1">+</span>
               <span class="s4">&quot;</span><span class="s3">\n</span><span class="s4">&quot;</span><span class="s1">.join(errors))</span>

  <span class="s3">raise </span><span class="s1">_error(</span><span class="s4">&quot;gather&quot;</span><span class="s3">, </span><span class="s1">error_msg)</span>


<span class="s1">tf_impl_no_xla[lax.gather_p] = _gather</span>


<span class="s3">def </span><span class="s1">_dynamic_slice(operand</span><span class="s3">, </span><span class="s1">*start_indices</span><span class="s3">, </span><span class="s1">slice_sizes: core.Shape</span><span class="s3">,</span>
                   <span class="s1">_in_avals: Sequence[core.ShapedArray]</span><span class="s3">,</span>
                   <span class="s1">_out_aval: core.ShapedArray):</span>
  <span class="s1">start_indices = tf.stack(start_indices)</span>
  <span class="s1">slice_sizes_tf = jax2tf._eval_shape(slice_sizes)</span>

  <span class="s1">operand_shape = jax2tf._eval_shape(_in_avals[</span><span class="s5">0</span><span class="s1">].shape)</span>
  <span class="s1">start_indices = _clip(operand_shape</span><span class="s3">, </span><span class="s1">start_indices</span><span class="s3">, </span><span class="s1">slice_sizes_tf)</span>
  <span class="s3">return </span><span class="s1">tf.slice(operand</span><span class="s3">, </span><span class="s1">start_indices</span><span class="s3">, </span><span class="s1">size=slice_sizes_tf)</span>


<span class="s1">tf_impl_no_xla[lax.dynamic_slice_p] = _dynamic_slice</span>


<span class="s3">def </span><span class="s1">_dynamic_update_slice(operand</span><span class="s3">, </span><span class="s1">update</span><span class="s3">, </span><span class="s1">*start_indices</span><span class="s3">,</span>
                          <span class="s1">_in_avals: Sequence[core.ShapedArray]</span><span class="s3">,</span>
                          <span class="s1">_out_aval: core.ShapedArray):</span>
  <span class="s1">start_indices = tf.stack(start_indices)</span>

  <span class="s1">op_shape = jax2tf._eval_shape(_in_avals[</span><span class="s5">0</span><span class="s1">].shape)</span>
  <span class="s1">op_size = tf.size(operand)</span>
  <span class="s1">update_shape_tf = jax2tf._eval_shape(_in_avals[</span><span class="s5">1</span><span class="s1">].shape)</span>

  <span class="s1">start_indices = _clip(op_shape</span><span class="s3">, </span><span class="s1">start_indices</span><span class="s3">, </span><span class="s1">update_shape_tf)</span>
  <span class="s1">end_indices = tf.add(start_indices</span><span class="s3">, </span><span class="s1">update_shape_tf)</span>

  <span class="s0"># Get the cells to update in `operand` as an array of ids.</span>
  <span class="s1">id_tensor = tf.reshape(tf.range(op_size)</span><span class="s3">, </span><span class="s1">op_shape)</span>
  <span class="s1">scattered_indices = tf.strided_slice(id_tensor</span><span class="s3">, </span><span class="s1">start_indices</span><span class="s3">, </span><span class="s1">end_indices)</span>

  <span class="s0"># Create an array containing updates at scattered_indices and zeros otherwise.</span>
  <span class="s1">flat_indices = tf.expand_dims(tf.nest.flatten(scattered_indices)</span><span class="s3">, </span><span class="s1">-</span><span class="s5">1</span><span class="s1">)</span>
  <span class="s1">flat_update = tf.nest.flatten(update)</span>
  <span class="s1">update = tf.scatter_nd(flat_indices</span><span class="s3">, </span><span class="s1">flat_update</span><span class="s3">, </span><span class="s1">(op_size</span><span class="s3">,</span><span class="s1">))</span>
  <span class="s1">update = tf.reshape(update</span><span class="s3">, </span><span class="s1">op_shape)</span>

  <span class="s0"># Create a bool mask that is True only where `operand` should be updated.</span>
  <span class="s1">update_mask = tf.ones_like(flat_update</span><span class="s3">, </span><span class="s1">dtype=tf.bool)</span>
  <span class="s1">update_mask = tf.scatter_nd(flat_indices</span><span class="s3">, </span><span class="s1">update_mask</span><span class="s3">, </span><span class="s1">(op_size</span><span class="s3">,</span><span class="s1">))</span>
  <span class="s1">update_mask = tf.reshape(update_mask</span><span class="s3">, </span><span class="s1">op_shape)</span>

  <span class="s0"># Use the mask to only update `operand` with `update`.</span>
  <span class="s3">return </span><span class="s1">tf.where(update_mask</span><span class="s3">, </span><span class="s1">update</span><span class="s3">, </span><span class="s1">operand)</span>


<span class="s1">tf_impl_no_xla[lax.dynamic_update_slice_p] = _dynamic_update_slice</span>


<span class="s3">def </span><span class="s1">shift_axes_forward(operand</span><span class="s3">,</span>
                       <span class="s1">axes: Tuple[int</span><span class="s3">, </span><span class="s1">...]</span><span class="s3">,</span>
                       <span class="s1">inverse: bool = </span><span class="s3">False,</span>
                       <span class="s1">forward: bool = </span><span class="s3">True</span><span class="s1">):</span>
  <span class="s2">&quot;&quot;&quot;Shifts the tuple of axes to the front of an array&quot;&quot;&quot;</span>
  <span class="s1">other_axes = tuple([i </span><span class="s3">for </span><span class="s1">i </span><span class="s3">in </span><span class="s1">range(len(operand.shape)) </span><span class="s3">if </span><span class="s1">i </span><span class="s3">not in </span><span class="s1">axes])</span>
  <span class="s1">fwd_order = axes + other_axes </span><span class="s3">if </span><span class="s1">forward </span><span class="s3">else </span><span class="s1">other_axes + axes</span>
  <span class="s1">order = fwd_order </span><span class="s3">if not </span><span class="s1">inverse </span><span class="s3">else </span><span class="s1">_invert_permutation(fwd_order)</span>
  <span class="s3">return </span><span class="s1">tf.transpose(operand</span><span class="s3">, </span><span class="s1">order)</span>

<span class="s3">def </span><span class="s1">convert_scatter_jax_to_tf(update_op</span><span class="s3">, </span><span class="s1">unsorted_segment_op=</span><span class="s3">None</span><span class="s1">):</span>

  <span class="s3">def </span><span class="s1">_sparse_scatter(operand</span><span class="s3">, </span><span class="s1">scatter_indices</span><span class="s3">, </span><span class="s1">updates</span><span class="s3">, </span><span class="s1">unique_indices</span><span class="s3">, </span><span class="s1">mode</span><span class="s3">,</span>
                      <span class="s1">_in_avals: Sequence[core.ShapedArray]</span><span class="s3">,</span>
                      <span class="s1">_out_aval: core.ShapedArray):</span>
    <span class="s2">&quot;&quot;&quot;Implementation of scatter specialised to indexing from the front axes. 
 
    This covers unique indices and non-unique indices of single depth. 
    Note on unique indices: `tf.tensor_scatter_nd_update` interprets indices 
    thusly: every axis except the final one encodes a batch dimension, the final 
    axis encoding the actual indices to scatter in to. It enforces, at least 
    one, batch dimension so we add an empty dimension to indices and updates if 
    lacking. 
 
    Note on non-unique indices: There is no tf op for non-single depth indexing, 
    but if indexing is single depth, this can be viewed as a segment op. 
    &quot;&quot;&quot;</span>
    <span class="s0"># Infer unique indices from lack of batch dimension</span>
    <span class="s1">unique_indices = unique_indices </span><span class="s3">or </span><span class="s1">(len(scatter_indices.shape) == </span><span class="s5">1</span><span class="s1">)</span>
    <span class="s3">if </span><span class="s1">unique_indices:</span>
      <span class="s1">suboperand = tf.gather_nd(operand</span><span class="s3">, </span><span class="s1">scatter_indices)</span>
      <span class="s1">updated_suboperand = update_op(suboperand</span><span class="s3">, </span><span class="s1">updates)</span>
      <span class="s0"># add a batch dim if none exist</span>
      <span class="s3">if </span><span class="s1">len(scatter_indices.shape) == </span><span class="s5">1</span><span class="s1">:</span>
        <span class="s1">scatter_indices = scatter_indices[</span><span class="s3">None</span><span class="s1">]</span>
        <span class="s1">updated_suboperand = updated_suboperand[</span><span class="s3">None</span><span class="s1">]</span>
      <span class="s1">y = tf.tensor_scatter_nd_update(operand</span><span class="s3">, </span><span class="s1">scatter_indices</span><span class="s3">, </span><span class="s1">updated_suboperand)</span>
    <span class="s3">else</span><span class="s1">:</span>
      <span class="s3">if </span><span class="s1">(scatter_indices.shape[-</span><span class="s5">1</span><span class="s1">] == </span><span class="s5">1</span><span class="s1">) </span><span class="s3">and </span><span class="s1">unsorted_segment_op:</span>
        <span class="s0"># If only indexing into the first dimension, it's a segment op</span>
        <span class="s1">operand_update = unsorted_segment_op(updates</span><span class="s3">,</span>
                                             <span class="s1">tf.squeeze(scatter_indices</span><span class="s3">, </span><span class="s1">-</span><span class="s5">1</span><span class="s1">)</span><span class="s3">,</span>
                                             <span class="s1">operand.shape[</span><span class="s5">0</span><span class="s1">])</span>
        <span class="s1">y = update_op(operand</span><span class="s3">, </span><span class="s1">operand_update)</span>
      <span class="s3">else</span><span class="s1">:</span>
        <span class="s3">raise </span><span class="s1">_scatter_error(</span>
            <span class="s4">&quot;Scatter only supports non-unique &quot;</span>
            <span class="s4">&quot;indices with indexing into only one dimension for (add, mul, min, &quot;</span>
            <span class="s4">&quot;max)&quot;</span><span class="s1">)</span>
    <span class="s3">return </span><span class="s1">y</span>

  <span class="s3">def </span><span class="s1">sparse_scatter(operand</span><span class="s3">, </span><span class="s1">scatter_indices</span><span class="s3">, </span><span class="s1">updates</span><span class="s3">, </span><span class="s1">update_jaxpr</span><span class="s3">,</span>
                     <span class="s1">update_consts</span><span class="s3">, </span><span class="s1">dimension_numbers</span><span class="s3">, </span><span class="s1">indices_are_sorted: bool</span><span class="s3">,</span>
                     <span class="s1">unique_indices: bool</span><span class="s3">, </span><span class="s1">mode</span><span class="s3">,</span>
                     <span class="s1">_in_avals: Sequence[core.ShapedArray]</span><span class="s3">,</span>
                     <span class="s1">_out_aval: core.ShapedArray):</span>
    <span class="s2">&quot;&quot;&quot; 
    Wrapper around the scatter function. 
    The underlying tf ops `tf.tensor_scatter_nd_update` and 
    `tf.math.unsorted_segment_*` index from the front dimensions. 
    `tf.math.unsorted_segment_*` indexs to a depth 1 from the front. 
    `tf.tensor_scatter_nd_update` indexs from the front dimensions onwards, 
    with no ability to skip a dimension. This function shifts the axes to be 
    indexed to the front then calls a front-specific implementation, then 
    inverse-shifts the output. 
 
    scatter_dims_to_operand_dims: dimensions which the scatter indexes in to. 
      We shift these to the front to match tf syntax. All other dims are batch 
    update_window_dims: dimensions which are not batch dimensions. We shift 
      these to the back as the remaining dimensions are batch dimensions. 
    &quot;&quot;&quot;</span>
    <span class="s3">del </span><span class="s1">update_jaxpr</span><span class="s3">, </span><span class="s1">update_consts</span><span class="s3">, </span><span class="s1">indices_are_sorted  </span><span class="s0"># Unused arguments</span>

    <span class="s1">update_window_dims = dimension_numbers.update_window_dims</span>
    <span class="s1">inserted_window_dims = dimension_numbers.inserted_window_dims</span>
    <span class="s1">scatter_to_operand_dims = dimension_numbers.scatter_dims_to_operand_dims</span>

    <span class="s1">dtype = operand.dtype  </span><span class="s0"># assume updates has same dtype as operand</span>
    <span class="s3">if </span><span class="s1">dtype </span><span class="s3">in </span><span class="s1">[tf.bool</span><span class="s3">, </span><span class="s1">tf.complex64]:</span>
      <span class="s3">raise </span><span class="s1">_scatter_error(</span><span class="s4">f&quot;Scatter does not support operands of type </span><span class="s3">{</span><span class="s1">dtype</span><span class="s3">}</span><span class="s4">&quot;</span><span class="s1">)</span>

    <span class="s3">if </span><span class="s1">inserted_window_dims != scatter_to_operand_dims:</span>
      <span class="s3">raise </span><span class="s1">_scatter_error(</span><span class="s4">&quot;Complex scatters are not supported&quot;</span><span class="s1">)</span>

    <span class="s3">if </span><span class="s1">(mode != lax.GatherScatterMode.FILL_OR_DROP </span><span class="s3">and</span>
        <span class="s1">mode != lax.GatherScatterMode.PROMISE_IN_BOUNDS):</span>
      <span class="s0"># The OOB behavior for tf.scatter is as follows:</span>
      <span class="s0"># - When running in eager or graph mode, it throws an error.</span>
      <span class="s0">#   TODO(marcvanzee): Fix this case by removing the OOB indices.</span>
      <span class="s0"># - When running in compile mode, the OOB indices are dropped, which is</span>
      <span class="s0">#   the same behavior as FILL_OR_DROP and PROMISE_IN_BOUNDS.</span>
      <span class="s0"># To ensure correctness, we disallow CLIP mode for now.</span>
      <span class="s3">raise </span><span class="s1">_scatter_error(</span><span class="s4">&quot;Only scatter modes `FILL_OR_DROP` and &quot;</span>
                           <span class="s4">&quot;`PROMISE_IN_BOUNDS` are supported.&quot;</span><span class="s1">)</span>

    <span class="s0"># Shift axes to the front to match tf syntax, inverse afterwards</span>
    <span class="s1">fwd = partial(shift_axes_forward</span><span class="s3">, </span><span class="s1">axes=scatter_to_operand_dims)</span>
    <span class="s1">inv = partial(fwd</span><span class="s3">, </span><span class="s1">inverse=</span><span class="s3">True</span><span class="s1">)</span>

    <span class="s0"># Shift update value axes to the back, so batch are at the front</span>
    <span class="s1">updates_shifted = shift_axes_forward(</span>
        <span class="s1">updates</span><span class="s3">, </span><span class="s1">axes=update_window_dims</span><span class="s3">, </span><span class="s1">forward=</span><span class="s3">False</span><span class="s1">)</span>

    <span class="s3">return </span><span class="s1">inv(</span>
        <span class="s1">_sparse_scatter(</span>
            <span class="s1">fwd(operand)</span><span class="s3">, </span><span class="s1">scatter_indices</span><span class="s3">, </span><span class="s1">updates_shifted</span><span class="s3">, </span><span class="s1">unique_indices</span><span class="s3">,</span>
            <span class="s1">mode</span><span class="s3">, </span><span class="s1">_in_avals</span><span class="s3">, </span><span class="s1">_out_aval))</span>
  <span class="s3">return </span><span class="s1">sparse_scatter</span>


<span class="s1">tf_impl_no_xla[lax.scatter_p] = convert_scatter_jax_to_tf(</span>
    <span class="s3">lambda </span><span class="s1">x</span><span class="s3">, </span><span class="s1">y: y)  </span><span class="s0"># just replace with the update</span>
<span class="s1">tf_impl_no_xla[lax.scatter_add_p] = convert_scatter_jax_to_tf(tf.add</span><span class="s3">,      </span><span class="s1">tf.math.unsorted_segment_sum)</span>
<span class="s1">tf_impl_no_xla[lax.scatter_mul_p] = convert_scatter_jax_to_tf(tf.multiply</span><span class="s3">, </span><span class="s1">tf.math.unsorted_segment_prod)</span>
<span class="s1">tf_impl_no_xla[lax.scatter_min_p] = convert_scatter_jax_to_tf(tf.minimum</span><span class="s3">,  </span><span class="s1">tf.math.unsorted_segment_min)</span>
<span class="s1">tf_impl_no_xla[lax.scatter_max_p] = convert_scatter_jax_to_tf(tf.maximum</span><span class="s3">,  </span><span class="s1">tf.math.unsorted_segment_max)</span>

<span class="s1">tf_impl_no_xla[lax.sort_p] = _unimplemented(</span><span class="s4">&quot;sort&quot;</span><span class="s1">)</span>

<span class="s1">tf_impl_no_xla[lax.reduce_precision_p] = _unimplemented(</span><span class="s4">&quot;reduce_precision&quot;</span><span class="s1">)</span>
</pre>
</body>
</html>