<html>
<head>
<title>qdwh.py</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<style type="text/css">
.s0 { color: #808080;}
.s1 { color: #a9b7c6;}
.s2 { color: #629755; font-style: italic;}
.s3 { color: #cc7832;}
.s4 { color: #6897bb;}
.s5 { color: #6a8759;}
</style>
</head>
<body bgcolor="#2b2b2b">
<table CELLSPACING=0 CELLPADDING=5 COLS=1 WIDTH="100%" BGCOLOR="#606060" >
<tr><td><center>
<font face="Arial, Helvetica" color="#000000">
qdwh.py</font>
</center></td></tr></table>
<pre><span class="s0"># Copyright 2021 The JAX Authors.</span>
<span class="s0">#</span>
<span class="s0"># Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);</span>
<span class="s0"># you may not use this file except in compliance with the License.</span>
<span class="s0"># You may obtain a copy of the License at</span>
<span class="s0">#</span>
<span class="s0">#     https://www.apache.org/licenses/LICENSE-2.0</span>
<span class="s0">#</span>
<span class="s0"># Unless required by applicable law or agreed to in writing, software</span>
<span class="s0"># distributed under the License is distributed on an &quot;AS IS&quot; BASIS,</span>
<span class="s0"># WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span>
<span class="s0"># See the License for the specific language governing permissions and</span>
<span class="s0"># limitations under the License</span>

<span class="s2">&quot;&quot;&quot;A JIT-compatible library for QDWH-based polar decomposition. 
 
QDWH is short for QR-based dynamically weighted Halley iteration. The Halley 
iteration implemented through QR decmopositions does not require matrix 
inversion. This is desirable for multicore and heterogeneous computing systems. 
 
Reference: Nakatsukasa, Yuji, Zhaojun Bai, and Fran√ßois Gygi. 
&quot;Optimizing Halley's iteration for computing the matrix polar decomposition.&quot; 
SIAM Journal on Matrix Analysis and Applications 31, no. 5 (2010): 2700-2720. 
https://epubs.siam.org/doi/abs/10.1137/090774999 
&quot;&quot;&quot;</span>

<span class="s3">import </span><span class="s1">functools</span>
<span class="s3">from </span><span class="s1">typing </span><span class="s3">import </span><span class="s1">Optional</span><span class="s3">, </span><span class="s1">Tuple</span>

<span class="s3">import </span><span class="s1">jax</span>
<span class="s3">import </span><span class="s1">jax.numpy </span><span class="s3">as </span><span class="s1">jnp</span>
<span class="s3">from </span><span class="s1">jax </span><span class="s3">import </span><span class="s1">lax</span>
<span class="s3">from </span><span class="s1">jax._src </span><span class="s3">import </span><span class="s1">core</span>
<span class="s3">from </span><span class="s1">jax._src.lax </span><span class="s3">import </span><span class="s1">linalg </span><span class="s3">as </span><span class="s1">lax_linalg</span>


<span class="s0"># Helpers for working with padded shapes</span>
<span class="s3">def </span><span class="s1">_mask(x</span><span class="s3">, </span><span class="s1">dims</span><span class="s3">, </span><span class="s1">alternative=</span><span class="s4">0</span><span class="s1">):</span>
  <span class="s2">&quot;&quot;&quot;Masks `x` up to the dynamic shape `dims`. 
 
  Replaces values outside those dimensions with `alternative`. `alternative` is 
  broadcast with `x`. 
  &quot;&quot;&quot;</span>
  <span class="s3">assert </span><span class="s1">jnp.ndim(x) == len(dims)</span>
  <span class="s1">mask = </span><span class="s3">None</span>
  <span class="s3">for </span><span class="s1">i</span><span class="s3">, </span><span class="s1">d </span><span class="s3">in </span><span class="s1">enumerate(dims):</span>
    <span class="s3">if </span><span class="s1">d </span><span class="s3">is not None</span><span class="s1">:</span>
      <span class="s1">mask_dim_i = lax.broadcasted_iota(jnp.int32</span><span class="s3">, </span><span class="s1">x.shape</span><span class="s3">, </span><span class="s1">i) &lt; d</span>
      <span class="s1">mask = mask_dim_i </span><span class="s3">if </span><span class="s1">mask </span><span class="s3">is None else </span><span class="s1">(mask &amp; mask_dim_i)</span>
  <span class="s3">return </span><span class="s1">x </span><span class="s3">if </span><span class="s1">mask </span><span class="s3">is None else </span><span class="s1">jnp.where(mask</span><span class="s3">, </span><span class="s1">x</span><span class="s3">, </span><span class="s1">alternative)</span>

<span class="s3">def </span><span class="s1">_pad_in_dim(x</span><span class="s3">, </span><span class="s1">low=</span><span class="s4">0</span><span class="s3">, </span><span class="s1">high=</span><span class="s4">0</span><span class="s3">, </span><span class="s1">interior=</span><span class="s4">0</span><span class="s3">, </span><span class="s1">fill_value=</span><span class="s4">0</span><span class="s3">, </span><span class="s1">axis=</span><span class="s4">0</span><span class="s1">):</span>
  <span class="s1">pads = [(</span><span class="s4">0</span><span class="s3">, </span><span class="s4">0</span><span class="s3">, </span><span class="s4">0</span><span class="s1">)] * x.ndim</span>
  <span class="s1">pads[axis] = (low</span><span class="s3">, </span><span class="s1">high</span><span class="s3">, </span><span class="s1">interior)</span>
  <span class="s3">return </span><span class="s1">lax.pad(x</span><span class="s3">, </span><span class="s1">jnp.array(fill_value</span><span class="s3">, </span><span class="s1">x.dtype)</span><span class="s3">, </span><span class="s1">pads)</span>

<span class="s3">def </span><span class="s1">_dynamic_concat(a</span><span class="s3">, </span><span class="s1">b</span><span class="s3">, </span><span class="s1">m</span><span class="s3">, </span><span class="s1">axis=</span><span class="s4">0</span><span class="s1">):</span>
  <span class="s2">&quot;Concatenates padded arrays `a` and `b` where the true size of `a` is `m`.&quot;</span>
  <span class="s3">if </span><span class="s1">m </span><span class="s3">is None</span><span class="s1">:</span>
    <span class="s3">return </span><span class="s1">jnp.concatenate([a</span><span class="s3">, </span><span class="s1">b]</span><span class="s3">, </span><span class="s1">axis=axis)</span>
  <span class="s3">return </span><span class="s1">lax.dynamic_update_slice_in_dim(</span>
      <span class="s1">_pad_in_dim(a</span><span class="s3">, </span><span class="s1">high=b.shape[axis]</span><span class="s3">, </span><span class="s1">axis=axis)</span><span class="s3">, </span><span class="s1">b</span><span class="s3">, </span><span class="s1">m</span><span class="s3">, </span><span class="s1">axis)</span>


<span class="s3">def </span><span class="s1">_use_qr(u</span><span class="s3">, </span><span class="s1">m</span><span class="s3">, </span><span class="s1">n</span><span class="s3">, </span><span class="s1">params):</span>
  <span class="s2">&quot;&quot;&quot;QDWH iteration using QR decomposition. 
 
  Args: 
  u: a matrix, with static (padded) shape M x N. 
  m, n: the dynamic shape of the matrix, where m &lt;= M and n &lt;= N. 
  params: the QDWH parameters. 
  &quot;&quot;&quot;</span>
  <span class="s1">a</span><span class="s3">, </span><span class="s1">b</span><span class="s3">, </span><span class="s1">c = params</span>
  <span class="s1">M</span><span class="s3">, </span><span class="s1">N = u.shape</span>

  <span class="s1">y = _dynamic_concat(jnp.sqrt(c) * u</span><span class="s3">, </span><span class="s1">jnp.eye(N</span><span class="s3">, </span><span class="s1">dtype=jnp.dtype(u))</span><span class="s3">, </span><span class="s1">m)</span>
  <span class="s1">q</span><span class="s3">, </span><span class="s1">_ = lax_linalg.qr(y</span><span class="s3">, </span><span class="s1">full_matrices=</span><span class="s3">False</span><span class="s1">)</span>
  <span class="s0"># q1 = q[:m, :]</span>
  <span class="s1">q1 = _mask(lax.slice(q</span><span class="s3">, </span><span class="s1">(</span><span class="s4">0</span><span class="s3">, </span><span class="s4">0</span><span class="s1">)</span><span class="s3">, </span><span class="s1">(M</span><span class="s3">, </span><span class="s1">N))</span><span class="s3">, </span><span class="s1">(m</span><span class="s3">, </span><span class="s1">n))</span>
  <span class="s0"># q2 = (q[m:, :]).T.conj()</span>
  <span class="s1">q2 = lax.dynamic_slice_in_dim(q</span><span class="s3">, </span><span class="s1">m</span><span class="s3">, </span><span class="s1">N</span><span class="s3">, </span><span class="s1">axis=</span><span class="s4">0</span><span class="s1">)</span>
  <span class="s1">q2 = _mask(q2</span><span class="s3">, </span><span class="s1">(n</span><span class="s3">, </span><span class="s1">n)).T.conj()</span>
  <span class="s1">e = b / c</span>
  <span class="s1">u = (e * u + (a - e) / jnp.sqrt(c) * jnp.einsum(</span><span class="s5">'ij,jk-&gt;ik'</span><span class="s3">, </span><span class="s1">q1</span><span class="s3">, </span><span class="s1">q2))</span>
  <span class="s3">return </span><span class="s1">u</span>


<span class="s3">def </span><span class="s1">_use_cholesky(u</span><span class="s3">, </span><span class="s1">m</span><span class="s3">, </span><span class="s1">n</span><span class="s3">, </span><span class="s1">params):</span>
  <span class="s2">&quot;&quot;&quot;QDWH iteration using Cholesky decomposition. 
 
  Args: 
  u: a matrix, with static (padded) shape M x N 
  m, n: the dynamic shape of the matrix, where m &lt;= M and n &lt;= N. 
  params: the QDWH parameters. 
  &quot;&quot;&quot;</span>
  <span class="s1">a</span><span class="s3">, </span><span class="s1">b</span><span class="s3">, </span><span class="s1">c = params</span>
  <span class="s1">_</span><span class="s3">, </span><span class="s1">N = u.shape</span>
  <span class="s1">x = c * (u.T.conj() @ u) + jnp.eye(N</span><span class="s3">, </span><span class="s1">dtype=jnp.dtype(u))</span>
  <span class="s0"># Pads the lower-right corner with the identity matrix to prevent the Cholesky</span>
  <span class="s0"># decomposition from failing due to the matrix not being PSD if padded with</span>
  <span class="s0"># zeros.</span>
  <span class="s1">x = _mask(x</span><span class="s3">, </span><span class="s1">(n</span><span class="s3">, </span><span class="s1">n)</span><span class="s3">, </span><span class="s1">jnp.eye(N</span><span class="s3">, </span><span class="s1">dtype=x.dtype))</span>

  <span class="s0"># `y` is lower triangular.</span>
  <span class="s1">y = lax_linalg.cholesky(x</span><span class="s3">, </span><span class="s1">symmetrize_input=</span><span class="s3">False</span><span class="s1">)</span>

  <span class="s1">z = lax_linalg.triangular_solve(</span>
      <span class="s1">y</span><span class="s3">, </span><span class="s1">u.T</span><span class="s3">, </span><span class="s1">left_side=</span><span class="s3">True, </span><span class="s1">lower=</span><span class="s3">True, </span><span class="s1">conjugate_a=</span><span class="s3">True</span><span class="s1">).conj()</span>

  <span class="s1">z = lax_linalg.triangular_solve(y</span><span class="s3">, </span><span class="s1">z</span><span class="s3">, </span><span class="s1">left_side=</span><span class="s3">True, </span><span class="s1">lower=</span><span class="s3">True,</span>
                                  <span class="s1">transpose_a=</span><span class="s3">True, </span><span class="s1">conjugate_a=</span><span class="s3">True</span><span class="s1">).T.conj()</span>

  <span class="s1">e = b / c</span>
  <span class="s1">u = e * u + (a - e) * z</span>
  <span class="s3">return </span><span class="s1">u</span>

<span class="s3">def </span><span class="s1">_qdwh(x</span><span class="s3">, </span><span class="s1">m</span><span class="s3">, </span><span class="s1">n</span><span class="s3">, </span><span class="s1">is_hermitian</span><span class="s3">, </span><span class="s1">max_iterations</span><span class="s3">, </span><span class="s1">eps):</span>
  <span class="s2">&quot;&quot;&quot;QR-based dynamically weighted Halley iteration for polar decomposition.&quot;&quot;&quot;</span>

  <span class="s0"># Estimates `alpha` and `beta = alpha * l`, where `alpha` is an estimate of</span>
  <span class="s0"># norm(x, 2) such that `alpha &gt;= norm(x, 2)` and `beta` is a lower bound for</span>
  <span class="s0"># the smallest singular value of x.</span>
  <span class="s3">if </span><span class="s1">eps </span><span class="s3">is None</span><span class="s1">:</span>
    <span class="s1">eps = float(jnp.finfo(x.dtype).eps)</span>
  <span class="s1">alpha = (jnp.sqrt(jnp.linalg.norm(x</span><span class="s3">, </span><span class="s1">ord=</span><span class="s4">1</span><span class="s1">)) *</span>
           <span class="s1">jnp.sqrt(jnp.linalg.norm(x</span><span class="s3">, </span><span class="s1">ord=jnp.inf))).astype(x.dtype)</span>
  <span class="s1">l = eps</span>

  <span class="s1">u = x / alpha</span>

  <span class="s0"># Iteration tolerances.</span>
  <span class="s1">tol_l = </span><span class="s4">10.0 </span><span class="s1">* eps / </span><span class="s4">2.0</span>
  <span class="s1">tol_norm = jnp.cbrt(tol_l)</span>

  <span class="s3">def </span><span class="s1">cond_fun(state):</span>
    <span class="s1">_</span><span class="s3">, </span><span class="s1">_</span><span class="s3">, </span><span class="s1">_</span><span class="s3">, </span><span class="s1">is_unconverged</span><span class="s3">, </span><span class="s1">is_not_max_iteration = state</span>
    <span class="s3">return </span><span class="s1">jnp.logical_and(is_unconverged</span><span class="s3">, </span><span class="s1">is_not_max_iteration)</span>

  <span class="s3">def </span><span class="s1">body_fun(state):</span>
    <span class="s1">u</span><span class="s3">, </span><span class="s1">l</span><span class="s3">, </span><span class="s1">iter_idx</span><span class="s3">, </span><span class="s1">_</span><span class="s3">, </span><span class="s1">_ = state</span>

    <span class="s1">u_prev = u</span>

    <span class="s0"># Computes parameters.</span>
    <span class="s1">l2 = l**</span><span class="s4">2</span>
    <span class="s1">dd = jnp.cbrt(</span><span class="s4">4.0 </span><span class="s1">* (</span><span class="s4">1.0 </span><span class="s1">/ l2 - </span><span class="s4">1.0</span><span class="s1">) / l2)</span>
    <span class="s1">sqd = jnp.sqrt(</span><span class="s4">1.0 </span><span class="s1">+ dd)</span>
    <span class="s1">a = (sqd + jnp.sqrt(</span><span class="s4">8.0 </span><span class="s1">- </span><span class="s4">4.0 </span><span class="s1">* dd + </span><span class="s4">8.0 </span><span class="s1">* (</span><span class="s4">2.0 </span><span class="s1">- l2) / (l2 * sqd)) / </span><span class="s4">2</span><span class="s1">)</span>
    <span class="s1">a = jnp.real(a)</span>
    <span class="s1">b = (a - </span><span class="s4">1.0</span><span class="s1">)**</span><span class="s4">2 </span><span class="s1">/ </span><span class="s4">4.0</span>
    <span class="s1">c = a + b - </span><span class="s4">1.0</span>

    <span class="s0"># Updates l.</span>
    <span class="s1">l = l * (a + b * l2) / (</span><span class="s4">1.0 </span><span class="s1">+ c * l2)</span>

    <span class="s0"># Uses QR or Cholesky decomposition.</span>
    <span class="s3">def </span><span class="s1">true_fn(u):</span>
      <span class="s3">return </span><span class="s1">_use_qr(u</span><span class="s3">, </span><span class="s1">m</span><span class="s3">, </span><span class="s1">n</span><span class="s3">, </span><span class="s1">params=(a</span><span class="s3">, </span><span class="s1">b</span><span class="s3">, </span><span class="s1">c))</span>

    <span class="s3">def </span><span class="s1">false_fn(u):</span>
      <span class="s3">return </span><span class="s1">_use_cholesky(u</span><span class="s3">, </span><span class="s1">m</span><span class="s3">, </span><span class="s1">n</span><span class="s3">, </span><span class="s1">params=(a</span><span class="s3">, </span><span class="s1">b</span><span class="s3">, </span><span class="s1">c))</span>

    <span class="s1">u = jax.lax.cond(c &gt; </span><span class="s4">100</span><span class="s3">, </span><span class="s1">true_fn</span><span class="s3">, </span><span class="s1">false_fn</span><span class="s3">, </span><span class="s1">operand=(u))</span>

    <span class="s3">if </span><span class="s1">is_hermitian:</span>
      <span class="s1">u = (u + u.T.conj()) / </span><span class="s4">2.0</span>

    <span class="s0"># Checks convergence.</span>
    <span class="s1">iterating_l = jnp.abs(</span><span class="s4">1.0 </span><span class="s1">- l) &gt; tol_l</span>
    <span class="s1">iterating_u = jnp.linalg.norm(u-u_prev) &gt; tol_norm</span>
    <span class="s1">is_unconverged = jnp.logical_or(iterating_l</span><span class="s3">, </span><span class="s1">iterating_u)</span>

    <span class="s1">is_not_max_iteration = iter_idx &lt; max_iterations</span>

    <span class="s3">return </span><span class="s1">u</span><span class="s3">, </span><span class="s1">l</span><span class="s3">, </span><span class="s1">iter_idx + </span><span class="s4">1</span><span class="s3">, </span><span class="s1">is_unconverged</span><span class="s3">, </span><span class="s1">is_not_max_iteration</span>

  <span class="s1">iter_idx = </span><span class="s4">1</span>
  <span class="s1">is_unconverged = </span><span class="s3">True</span>
  <span class="s1">is_not_max_iteration = </span><span class="s3">True</span>
  <span class="s1">u</span><span class="s3">, </span><span class="s1">_</span><span class="s3">, </span><span class="s1">num_iters</span><span class="s3">, </span><span class="s1">is_unconverged</span><span class="s3">, </span><span class="s1">_ = jax.lax.while_loop(</span>
      <span class="s1">cond_fun=cond_fun</span><span class="s3">, </span><span class="s1">body_fun=body_fun</span><span class="s3">,</span>
      <span class="s1">init_val=(u</span><span class="s3">, </span><span class="s1">l</span><span class="s3">, </span><span class="s1">iter_idx</span><span class="s3">, </span><span class="s1">is_unconverged</span><span class="s3">, </span><span class="s1">is_not_max_iteration))</span>

  <span class="s0"># Applies Newton-Schulz refinement for better accuracy.</span>
  <span class="s1">u = </span><span class="s4">1.5 </span><span class="s1">* u - </span><span class="s4">0.5 </span><span class="s1">* u @ (u.T.conj() @ u)</span>

  <span class="s1">h = u.T.conj() @ x</span>
  <span class="s1">h = (h + h.T.conj()) / </span><span class="s4">2.0</span>

  <span class="s0"># Converged within the maximum number of iterations.</span>
  <span class="s1">is_converged = jnp.logical_not(is_unconverged)</span>

  <span class="s3">return </span><span class="s1">u</span><span class="s3">, </span><span class="s1">h</span><span class="s3">, </span><span class="s1">num_iters - </span><span class="s4">1</span><span class="s3">, </span><span class="s1">is_converged</span>


<span class="s0"># TODO: Add pivoting.</span>
<span class="s1">@functools.partial(jax.jit</span><span class="s3">, </span><span class="s1">static_argnames=(</span><span class="s5">'is_hermitian'</span><span class="s3">,</span><span class="s1">))</span>
<span class="s3">def </span><span class="s1">qdwh(x</span><span class="s3">, </span><span class="s1">*</span><span class="s3">, </span><span class="s1">is_hermitian=</span><span class="s3">False, </span><span class="s1">max_iterations=</span><span class="s3">None, </span><span class="s1">eps=</span><span class="s3">None,</span>
         <span class="s1">dynamic_shape: Optional[Tuple[int</span><span class="s3">, </span><span class="s1">int]] = </span><span class="s3">None</span><span class="s1">):</span>
  <span class="s2">&quot;&quot;&quot;QR-based dynamically weighted Halley iteration for polar decomposition. 
 
  Args: 
    x: A full-rank matrix, with shape `M x N`. The matrix may be 
      padded up to that size from a smaller true shape (``dynamic_shape``). 
    is_hermitian: True if `x` is Hermitian. Default to `False`. 
    eps: The final result will satisfy 
      ``|x_k - x_k-1| &lt; |x_k| * (4*eps)**(1/3)`` where `x_k` is the iterate. 
    max_iterations: Iterations will terminate after this many steps even if the 
      above is unsatisfied. 
    dynamic_shape: the unpadded shape as an ``(m, n)`` tuple; optional. 
 
  Returns: 
    A four-tuple of (u, h, num_iters, is_converged) containing the 
    polar decomposition of `x = u * h`, the number of iterations to compute `u`, 
    and `is_converged`, whose value is `True` when the convergence is achieved 
    within the maximum number of iterations. 
  &quot;&quot;&quot;</span>
  <span class="s1">is_hermitian = core.concrete_or_error(</span>
      <span class="s1">bool</span><span class="s3">, </span><span class="s1">is_hermitian</span><span class="s3">, </span><span class="s5">'The `is_hermitian` argument must be statically '</span>
      <span class="s5">'specified to use `qdwh` within JAX transformations.'</span><span class="s1">)</span>

  <span class="s3">if </span><span class="s1">max_iterations </span><span class="s3">is None</span><span class="s1">:</span>
    <span class="s1">max_iterations = </span><span class="s4">10</span>

  <span class="s1">M</span><span class="s3">, </span><span class="s1">N = x.shape</span>
  <span class="s3">if </span><span class="s1">M &lt; N:</span>
    <span class="s3">raise </span><span class="s1">ValueError(</span><span class="s5">'The input matrix of shape M x N must have M &gt;= N.'</span><span class="s1">)</span>
  <span class="s3">if </span><span class="s1">dynamic_shape </span><span class="s3">is not None</span><span class="s1">:</span>
    <span class="s1">m</span><span class="s3">, </span><span class="s1">n = dynamic_shape</span>
    <span class="s1">x = _mask(x</span><span class="s3">, </span><span class="s1">(m</span><span class="s3">, </span><span class="s1">n))</span>
  <span class="s3">else</span><span class="s1">:</span>
    <span class="s1">m</span><span class="s3">, </span><span class="s1">n = M</span><span class="s3">, </span><span class="s1">N</span>

  <span class="s3">with </span><span class="s1">jax.default_matmul_precision(</span><span class="s5">'float32'</span><span class="s1">):</span>
    <span class="s1">u</span><span class="s3">, </span><span class="s1">h</span><span class="s3">, </span><span class="s1">num_iters</span><span class="s3">, </span><span class="s1">is_converged = _qdwh(x</span><span class="s3">, </span><span class="s1">m</span><span class="s3">, </span><span class="s1">n</span><span class="s3">, </span><span class="s1">is_hermitian</span><span class="s3">, </span><span class="s1">max_iterations</span><span class="s3">,</span>
                                          <span class="s1">eps)</span>


  <span class="s3">return </span><span class="s1">u</span><span class="s3">, </span><span class="s1">h</span><span class="s3">, </span><span class="s1">num_iters</span><span class="s3">, </span><span class="s1">is_converged</span>
</pre>
</body>
</html>