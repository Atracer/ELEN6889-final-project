<html>
<head>
<title>loops.py</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<style type="text/css">
.s0 { color: #808080;}
.s1 { color: #a9b7c6;}
.s2 { color: #629755; font-style: italic;}
.s3 { color: #cc7832;}
.s4 { color: #6a8759;}
.s5 { color: #6897bb;}
</style>
</head>
<body bgcolor="#2b2b2b">
<table CELLSPACING=0 CELLPADDING=5 COLS=1 WIDTH="100%" BGCOLOR="#606060" >
<tr><td><center>
<font face="Arial, Helvetica" color="#000000">
loops.py</font>
</center></td></tr></table>
<pre><span class="s0"># Copyright 2022 The JAX Authors.</span>
<span class="s0">#</span>
<span class="s0"># Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);</span>
<span class="s0"># you may not use this file except in compliance with the License.</span>
<span class="s0"># You may obtain a copy of the License at</span>
<span class="s0">#</span>
<span class="s0">#     https://www.apache.org/licenses/LICENSE-2.0</span>
<span class="s0">#</span>
<span class="s0"># Unless required by applicable law or agreed to in writing, software</span>
<span class="s0"># distributed under the License is distributed on an &quot;AS IS&quot; BASIS,</span>
<span class="s0"># WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span>
<span class="s0"># See the License for the specific language governing permissions and</span>
<span class="s0"># limitations under the License.</span>
<span class="s2">&quot;&quot;&quot;Module for the loop primitives.&quot;&quot;&quot;</span>
<span class="s3">from </span><span class="s1">functools </span><span class="s3">import </span><span class="s1">partial</span>
<span class="s3">import </span><span class="s1">inspect</span>
<span class="s3">import </span><span class="s1">itertools</span>
<span class="s3">import </span><span class="s1">operator</span>
<span class="s3">from </span><span class="s1">typing </span><span class="s3">import </span><span class="s1">Any</span><span class="s3">, </span><span class="s1">Callable</span><span class="s3">, </span><span class="s1">List</span><span class="s3">, </span><span class="s1">Optional</span><span class="s3">, </span><span class="s1">Sequence</span><span class="s3">, </span><span class="s1">Tuple</span><span class="s3">, </span><span class="s1">TypeVar</span>

<span class="s3">import </span><span class="s1">jax</span>
<span class="s3">import </span><span class="s1">weakref</span>
<span class="s3">from </span><span class="s1">jax._src </span><span class="s3">import </span><span class="s1">core</span>
<span class="s3">from </span><span class="s1">jax._src </span><span class="s3">import </span><span class="s1">linear_util </span><span class="s3">as </span><span class="s1">lu</span>
<span class="s3">from </span><span class="s1">jax.config </span><span class="s3">import </span><span class="s1">config</span>
<span class="s3">from </span><span class="s1">jax._src.core </span><span class="s3">import </span><span class="s1">ConcreteArray</span><span class="s3">, </span><span class="s1">ShapedArray</span><span class="s3">, </span><span class="s1">raise_to_shaped</span>
<span class="s3">from </span><span class="s1">jax.interpreters </span><span class="s3">import </span><span class="s1">ad</span>
<span class="s3">from </span><span class="s1">jax.interpreters </span><span class="s3">import </span><span class="s1">batching</span>
<span class="s3">from </span><span class="s1">jax._src.interpreters </span><span class="s3">import </span><span class="s1">mlir</span>
<span class="s3">from </span><span class="s1">jax._src.interpreters </span><span class="s3">import </span><span class="s1">partial_eval </span><span class="s3">as </span><span class="s1">pe</span>
<span class="s3">from </span><span class="s1">jax._src.interpreters </span><span class="s3">import </span><span class="s1">xla</span>
<span class="s3">from </span><span class="s1">jax.tree_util </span><span class="s3">import </span><span class="s1">(tree_flatten</span><span class="s3">, </span><span class="s1">tree_unflatten</span><span class="s3">, </span><span class="s1">treedef_is_leaf</span><span class="s3">,</span>
                           <span class="s1">tree_map</span><span class="s3">, </span><span class="s1">tree_flatten_with_path</span><span class="s3">, </span><span class="s1">keystr)</span>
<span class="s3">from </span><span class="s1">jax._src.tree_util </span><span class="s3">import </span><span class="s1">equality_errors</span>
<span class="s3">from </span><span class="s1">jax._src </span><span class="s3">import </span><span class="s1">ad_checkpoint</span>
<span class="s3">from </span><span class="s1">jax._src </span><span class="s3">import </span><span class="s1">ad_util</span>
<span class="s3">from </span><span class="s1">jax._src </span><span class="s3">import </span><span class="s1">api</span>
<span class="s3">from </span><span class="s1">jax._src </span><span class="s3">import </span><span class="s1">dispatch</span>
<span class="s3">from </span><span class="s1">jax._src </span><span class="s3">import </span><span class="s1">dtypes</span>
<span class="s3">from </span><span class="s1">jax._src </span><span class="s3">import </span><span class="s1">effects</span>
<span class="s3">from </span><span class="s1">jax._src </span><span class="s3">import </span><span class="s1">source_info_util</span>
<span class="s3">from </span><span class="s1">jax._src </span><span class="s3">import </span><span class="s1">util</span>
<span class="s3">from </span><span class="s1">jax._src.lax </span><span class="s3">import </span><span class="s1">lax</span>
<span class="s3">from </span><span class="s1">jax._src.lax </span><span class="s3">import </span><span class="s1">slicing</span>
<span class="s3">from </span><span class="s1">jax._src.lax </span><span class="s3">import </span><span class="s1">windowed_reductions</span>
<span class="s3">from </span><span class="s1">jax._src.lib.mlir </span><span class="s3">import </span><span class="s1">ir</span>
<span class="s3">from </span><span class="s1">jax._src.lib.mlir.dialects </span><span class="s3">import </span><span class="s1">hlo</span>
<span class="s3">from </span><span class="s1">jax._src.numpy.ufuncs </span><span class="s3">import </span><span class="s1">logaddexp</span>
<span class="s3">from </span><span class="s1">jax._src.traceback_util </span><span class="s3">import </span><span class="s1">api_boundary</span>
<span class="s3">from </span><span class="s1">jax._src.util </span><span class="s3">import </span><span class="s1">(partition_list</span><span class="s3">, </span><span class="s1">safe_map</span><span class="s3">, </span><span class="s1">safe_zip</span><span class="s3">, </span><span class="s1">split_list</span><span class="s3">,</span>
                           <span class="s1">unzip2</span><span class="s3">, </span><span class="s1">weakref_lru_cache)</span>
<span class="s3">import </span><span class="s1">numpy </span><span class="s3">as </span><span class="s1">np</span>

<span class="s3">from </span><span class="s1">jax._src.lax.control_flow.common </span><span class="s3">import </span><span class="s1">(</span>
    <span class="s1">_abstractify</span><span class="s3">, </span><span class="s1">_avals_short</span><span class="s3">, </span><span class="s1">_check_tree_and_avals</span><span class="s3">, </span><span class="s1">_initial_style_jaxpr</span><span class="s3">,</span>
    <span class="s1">_make_closed_jaxpr</span><span class="s3">, </span><span class="s1">_prune_zeros</span><span class="s3">, </span><span class="s1">_typecheck_param</span><span class="s3">, </span><span class="s1">allowed_effects)</span>

<span class="s1">_map = safe_map</span>
<span class="s1">zip = safe_zip</span>

<span class="s1">T = TypeVar(</span><span class="s4">'T'</span><span class="s1">)</span>
<span class="s1">Array = Any</span>
<span class="s1">BooleanNumeric = Any  </span><span class="s0"># A bool, or a Boolean array.</span>

<span class="s0">### Helper functions</span>

<span class="s3">def </span><span class="s1">_promote_weak_typed_inputs(in_vals</span><span class="s3">, </span><span class="s1">in_avals</span><span class="s3">, </span><span class="s1">out_avals):</span>
  <span class="s2">&quot;&quot;&quot;Promote weakly-typed in_vals to be compatible with out_avals. 
 
  Args: 
    in_vals : flattened list of input values. 
    in_avals : corresponding list of avals. 
    out_avals : list of target output avals. 
  Returns: 
    in_vals_new : flattened list of modified in_vals with no weak types. 
    changed : bool; true if in_vals required modification. 
  &quot;&quot;&quot;</span>
  <span class="s3">if </span><span class="s1">len(in_vals) != len(in_avals) </span><span class="s3">or </span><span class="s1">len(in_avals) != len(out_avals):</span>
    <span class="s0"># Calling function is responsible for catching this.</span>
    <span class="s3">return </span><span class="s1">in_vals</span><span class="s3">, False</span>
  <span class="s1">weak_mismatches = [i </span><span class="s3">for </span><span class="s1">i</span><span class="s3">, </span><span class="s1">(a1</span><span class="s3">, </span><span class="s1">a2) </span><span class="s3">in </span><span class="s1">enumerate(zip(in_avals</span><span class="s3">, </span><span class="s1">out_avals))</span>
                    <span class="s3">if </span><span class="s1">getattr(a1</span><span class="s3">, </span><span class="s4">'weak_type'</span><span class="s3">, False</span><span class="s1">) </span><span class="s3">and not </span><span class="s1">core.typematch(a1</span><span class="s3">, </span><span class="s1">a2)]</span>
  <span class="s3">if not </span><span class="s1">weak_mismatches:</span>
    <span class="s3">return </span><span class="s1">in_vals</span><span class="s3">, False</span>
  <span class="s3">for </span><span class="s1">i </span><span class="s3">in </span><span class="s1">weak_mismatches:</span>
    <span class="s1">new_dtype = dtypes.result_type(in_vals[i]</span><span class="s3">, </span><span class="s1">out_avals[i])</span>
    <span class="s1">in_vals[i] = lax.convert_element_type(in_vals[i]</span><span class="s3">, </span><span class="s1">new_dtype)</span>
  <span class="s3">return </span><span class="s1">in_vals</span><span class="s3">, True</span>


<span class="s0">### scan</span>

<span class="s1">Carry = TypeVar(</span><span class="s4">'Carry'</span><span class="s1">)</span>
<span class="s1">X = TypeVar(</span><span class="s4">'X'</span><span class="s1">)</span>
<span class="s1">Y = TypeVar(</span><span class="s4">'Y'</span><span class="s1">)</span>

<span class="s1">@api_boundary</span>
<span class="s3">def </span><span class="s1">scan(f: Callable[[Carry</span><span class="s3">, </span><span class="s1">X]</span><span class="s3">, </span><span class="s1">Tuple[Carry</span><span class="s3">, </span><span class="s1">Y]]</span><span class="s3">,</span>
         <span class="s1">init: Carry</span><span class="s3">,</span>
         <span class="s1">xs: X</span><span class="s3">,</span>
         <span class="s1">length: Optional[int] = </span><span class="s3">None,</span>
         <span class="s1">reverse: bool = </span><span class="s3">False,</span>
         <span class="s1">unroll: int = </span><span class="s5">1</span><span class="s1">) -&gt; Tuple[Carry</span><span class="s3">, </span><span class="s1">Y]:</span>
  <span class="s2">&quot;&quot;&quot;Scan a function over leading array axes while carrying along state. 
 
  The `Haskell-like type signature`_ in brief is 
 
  .. code-block:: haskell 
 
    scan :: (c -&gt; a -&gt; (c, b)) -&gt; c -&gt; [a] -&gt; (c, [b]) 
 
  where we use [t] here to denote the type t with an additional leading axis. 
  That is, if t is an array type then [t] represents the type with an additional 
  leading axis, and if t is a pytree (container) type with array leaves then [t] 
  represents the type with the same pytree structure and corresponding leaves 
  each with an additional leading axis. 
 
  When the type of ``xs`` (denoted `a` above) is an array type or None, and the type 
  of ``ys`` (denoted `b` above) is an array type, the semantics of :func:`~scan` are 
  given roughly by this Python implementation:: 
 
    def scan(f, init, xs, length=None): 
      if xs is None: 
        xs = [None] * length 
      carry = init 
      ys = [] 
      for x in xs: 
        carry, y = f(carry, x) 
        ys.append(y) 
      return carry, np.stack(ys) 
 
  Unlike that Python version, both ``xs`` and ``ys`` may be arbitrary pytree 
  values, and so multiple arrays can be scanned over at once and produce multiple 
  output arrays. ``None`` is actually a special case of this, as it represents an 
  empty pytree. 
 
  Also unlike that Python version, :func:`~scan` is a JAX primitive and is 
  lowered to a single WhileOp. That makes it useful for reducing 
  compilation times for JIT-compiled functions, since native Python 
  loop constructs in an :func:`~jax.jit` function are unrolled, leading to large 
  XLA computations. 
 
  Finally, the loop-carried value ``carry`` must hold a fixed shape and dtype 
  across all iterations (and not just be consistent up to NumPy rank/shape 
  broadcasting and dtype promotion rules, for example). In other words, the type 
  ``c`` in the type signature above represents an array with a fixed shape and 
  dtype (or a nested tuple/list/dict container data structure with a fixed 
  structure and arrays with fixed shape and dtype at the leaves). 
 
  .. note:: 
    :py:func:`scan` compiles ``f``, so while it can be combined with 
    :py:func:`jit`, it's usually unnecessary. 
 
  Args: 
    f: a Python function to be scanned of type ``c -&gt; a -&gt; (c, b)``, meaning 
      that ``f`` accepts two arguments where the first is a value of the loop 
      carry and the second is a slice of ``xs`` along its leading axis, and that 
      ``f`` returns a pair where the first element represents a new value for 
      the loop carry and the second represents a slice of the output. 
    init: an initial loop carry value of type ``c``, which can be a scalar, 
      array, or any pytree (nested Python tuple/list/dict) thereof, representing 
      the initial loop carry value. This value must have the same structure as 
      the first element of the pair returned by ``f``. 
    xs: the value of type ``[a]`` over which to scan along the leading axis, 
      where ``[a]`` can be an array or any pytree (nested Python 
      tuple/list/dict) thereof with consistent leading axis sizes. 
    length: optional integer specifying the number of loop iterations, which 
      must agree with the sizes of leading axes of the arrays in ``xs`` (but can 
      be used to perform scans where no input ``xs`` are needed). 
    reverse: optional boolean specifying whether to run the scan iteration 
      forward (the default) or in reverse, equivalent to reversing the leading 
      axes of the arrays in both ``xs`` and in ``ys``. 
    unroll: optional positive int specifying, in the underlying operation of the 
      scan primitive, how many scan iterations to unroll within a single 
      iteration of a loop. 
 
  Returns: 
    A pair of type ``(c, [b])`` where the first element represents the final 
    loop carry value and the second element represents the stacked outputs of 
    the second output of ``f`` when scanned over the leading axis of the inputs. 
 
  .. _Haskell-like type signature: https://wiki.haskell.org/Type_signature 
  &quot;&quot;&quot;</span>
  <span class="s3">if not </span><span class="s1">callable(f):</span>
    <span class="s3">raise </span><span class="s1">TypeError(</span><span class="s4">&quot;lax.scan: f argument should be a callable.&quot;</span><span class="s1">)</span>
  <span class="s1">xs_flat</span><span class="s3">, </span><span class="s1">xs_tree = tree_flatten(xs)</span>

  <span class="s3">try</span><span class="s1">:</span>
    <span class="s1">lengths = [x.shape[</span><span class="s5">0</span><span class="s1">] </span><span class="s3">for </span><span class="s1">x </span><span class="s3">in </span><span class="s1">xs_flat]</span>
  <span class="s3">except </span><span class="s1">AttributeError </span><span class="s3">as </span><span class="s1">err:</span>
    <span class="s1">msg = </span><span class="s4">&quot;scan got value with no leading axis to scan over: {}.&quot;</span>
    <span class="s3">raise </span><span class="s1">ValueError(</span>
      <span class="s1">msg.format(</span><span class="s4">', '</span><span class="s1">.join(str(x) </span><span class="s3">for </span><span class="s1">x </span><span class="s3">in </span><span class="s1">xs_flat</span>
                           <span class="s3">if not </span><span class="s1">hasattr(x</span><span class="s3">, </span><span class="s4">'shape'</span><span class="s1">)))) </span><span class="s3">from </span><span class="s1">err</span>

  <span class="s3">if </span><span class="s1">length </span><span class="s3">is not None</span><span class="s1">:</span>
    <span class="s1">length = int(length)</span>
    <span class="s3">if not </span><span class="s1">all(length == l </span><span class="s3">for </span><span class="s1">l </span><span class="s3">in </span><span class="s1">lengths):</span>
      <span class="s1">msg = (</span><span class="s4">&quot;scan got `length` argument of {} which disagrees with &quot;</span>
             <span class="s4">&quot;leading axis sizes {}.&quot;</span><span class="s1">)</span>
      <span class="s3">raise </span><span class="s1">ValueError(msg.format(length</span><span class="s3">, </span><span class="s1">[x.shape[</span><span class="s5">0</span><span class="s1">] </span><span class="s3">for </span><span class="s1">x </span><span class="s3">in </span><span class="s1">xs_flat]))</span>
  <span class="s3">else</span><span class="s1">:</span>
    <span class="s1">unique_lengths = set(lengths)</span>
    <span class="s3">if </span><span class="s1">len(unique_lengths) &gt; </span><span class="s5">1</span><span class="s1">:</span>
      <span class="s1">msg = </span><span class="s4">&quot;scan got values with different leading axis sizes: {}.&quot;</span>
      <span class="s3">raise </span><span class="s1">ValueError(msg.format(</span><span class="s4">', '</span><span class="s1">.join(str(x.shape[</span><span class="s5">0</span><span class="s1">]) </span><span class="s3">for </span><span class="s1">x </span><span class="s3">in </span><span class="s1">xs_flat)))</span>
    <span class="s3">elif </span><span class="s1">len(unique_lengths) == </span><span class="s5">0</span><span class="s1">:</span>
      <span class="s1">msg = </span><span class="s4">&quot;scan got no values to scan over and `length` not provided.&quot;</span>
      <span class="s3">raise </span><span class="s1">ValueError(msg)</span>
    <span class="s3">else</span><span class="s1">:</span>
      <span class="s1">length</span><span class="s3">, </span><span class="s1">= unique_lengths</span>

  <span class="s3">if </span><span class="s1">config.jax_disable_jit:</span>
    <span class="s3">if </span><span class="s1">length == </span><span class="s5">0</span><span class="s1">:</span>
      <span class="s3">raise </span><span class="s1">ValueError(</span><span class="s4">&quot;zero-length scan is not supported in disable_jit() mode because the output type is unknown.&quot;</span><span class="s1">)</span>
    <span class="s1">carry = init</span>
    <span class="s1">ys = []</span>
    <span class="s1">maybe_reversed = reversed </span><span class="s3">if </span><span class="s1">reverse </span><span class="s3">else lambda </span><span class="s1">x: x</span>
    <span class="s3">for </span><span class="s1">i </span><span class="s3">in </span><span class="s1">maybe_reversed(range(length)):</span>
      <span class="s1">xs_slice = [_index_array(i</span><span class="s3">, </span><span class="s1">core.get_aval(x)</span><span class="s3">, </span><span class="s1">x) </span><span class="s3">for </span><span class="s1">x </span><span class="s3">in </span><span class="s1">xs_flat]</span>
      <span class="s1">carry</span><span class="s3">, </span><span class="s1">y = f(carry</span><span class="s3">, </span><span class="s1">tree_unflatten(xs_tree</span><span class="s3">, </span><span class="s1">xs_slice))</span>
      <span class="s1">ys.append(y)</span>
    <span class="s1">stack = </span><span class="s3">lambda </span><span class="s1">*ys: jax.numpy.stack(ys)</span>
    <span class="s1">stacked_y = tree_map(stack</span><span class="s3">, </span><span class="s1">*maybe_reversed(ys))</span>
    <span class="s3">return </span><span class="s1">carry</span><span class="s3">, </span><span class="s1">stacked_y</span>

  <span class="s1">xs_avals = [core.raise_to_shaped(core.get_aval(x)) </span><span class="s3">for </span><span class="s1">x </span><span class="s3">in </span><span class="s1">xs_flat]</span>
  <span class="s1">x_avals = [core.mapped_aval(length</span><span class="s3">, </span><span class="s5">0</span><span class="s3">, </span><span class="s1">aval) </span><span class="s3">for </span><span class="s1">aval </span><span class="s3">in </span><span class="s1">xs_avals]</span>

  <span class="s3">def </span><span class="s1">_create_jaxpr(init):</span>
    <span class="s1">init_flat</span><span class="s3">, </span><span class="s1">init_tree = tree_flatten(init)</span>
    <span class="s1">in_flat</span><span class="s3">, </span><span class="s1">in_tree = tree_flatten((init</span><span class="s3">, </span><span class="s1">xs))</span>

    <span class="s1">carry_avals = tuple(_map(_abstractify</span><span class="s3">, </span><span class="s1">init_flat))</span>
    <span class="s1">jaxpr</span><span class="s3">, </span><span class="s1">consts</span><span class="s3">, </span><span class="s1">out_tree = _initial_style_jaxpr(</span>
        <span class="s1">f</span><span class="s3">, </span><span class="s1">in_tree</span><span class="s3">, </span><span class="s1">(*carry_avals</span><span class="s3">, </span><span class="s1">*x_avals)</span><span class="s3">, </span><span class="s4">&quot;scan&quot;</span><span class="s1">)</span>
    <span class="s1">out_tree_children = out_tree.children()</span>
    <span class="s3">if </span><span class="s1">len(out_tree_children) != </span><span class="s5">2</span><span class="s1">:</span>
      <span class="s1">msg = </span><span class="s4">&quot;scan body output must be a pair, got {}.&quot;</span>
      <span class="s3">raise </span><span class="s1">TypeError(msg.format(tree_unflatten(out_tree</span><span class="s3">, </span><span class="s1">jaxpr.out_avals)))</span>
    <span class="s1">carry_avals_out = jaxpr.out_avals[:out_tree_children[</span><span class="s5">0</span><span class="s1">].num_leaves]</span>
    <span class="s3">return </span><span class="s1">init_flat</span><span class="s3">, </span><span class="s1">carry_avals</span><span class="s3">, </span><span class="s1">carry_avals_out</span><span class="s3">, </span><span class="s1">init_tree</span><span class="s3">, </span><span class="s1">in_flat</span><span class="s3">, </span><span class="s1">jaxpr</span><span class="s3">, </span><span class="s1">consts</span><span class="s3">, </span><span class="s1">out_tree</span><span class="s3">, </span><span class="s1">out_tree_children</span>

  <span class="s0"># The carry input and output avals must match exactly. However, we want to account for</span>
  <span class="s0"># the case when init contains weakly-typed values (e.g. Python scalars), with avals that</span>
  <span class="s0"># may not match the output despite being compatible by virtue of their weak type.</span>
  <span class="s0"># To do this, we compute the jaxpr in two passes: first with the raw inputs, and if</span>
  <span class="s0"># necessary, a second time with modified init values.</span>
  <span class="s1">init_flat</span><span class="s3">, </span><span class="s1">carry_avals</span><span class="s3">, </span><span class="s1">carry_avals_out</span><span class="s3">, </span><span class="s1">init_tree</span><span class="s3">, </span><span class="s1">*rest = _create_jaxpr(init)</span>
  <span class="s1">new_init_flat</span><span class="s3">, </span><span class="s1">changed = _promote_weak_typed_inputs(init_flat</span><span class="s3">, </span><span class="s1">carry_avals</span><span class="s3">, </span><span class="s1">carry_avals_out)</span>
  <span class="s3">if </span><span class="s1">changed:</span>
    <span class="s1">init = tree_unflatten(init_tree</span><span class="s3">, </span><span class="s1">new_init_flat)</span>
    <span class="s1">init_flat</span><span class="s3">, </span><span class="s1">carry_avals</span><span class="s3">, </span><span class="s1">carry_avals_out</span><span class="s3">, </span><span class="s1">init_tree</span><span class="s3">, </span><span class="s1">*rest = _create_jaxpr(init)</span>
  <span class="s1">in_flat</span><span class="s3">, </span><span class="s1">jaxpr</span><span class="s3">, </span><span class="s1">consts</span><span class="s3">, </span><span class="s1">out_tree</span><span class="s3">, </span><span class="s1">out_tree_children = rest</span>

  <span class="s1">_check_scan_carry_type(f</span><span class="s3">, </span><span class="s1">init</span><span class="s3">, </span><span class="s1">out_tree_children[</span><span class="s5">0</span><span class="s1">]</span><span class="s3">, </span><span class="s1">carry_avals_out)</span>
  <span class="s1">disallowed_effects = allowed_effects.filter_not_in(jaxpr.effects)</span>
  <span class="s3">if </span><span class="s1">disallowed_effects:</span>
    <span class="s3">raise </span><span class="s1">NotImplementedError(</span>
        <span class="s4">f'Effects not supported in `scan`: </span><span class="s3">{</span><span class="s1">disallowed_effects</span><span class="s3">}</span><span class="s4">'</span><span class="s1">)</span>

  <span class="s1">out = scan_p.bind(*consts</span><span class="s3">, </span><span class="s1">*in_flat</span><span class="s3">,</span>
                    <span class="s1">reverse=reverse</span><span class="s3">, </span><span class="s1">length=length</span><span class="s3">, </span><span class="s1">jaxpr=jaxpr</span><span class="s3">,</span>
                    <span class="s1">num_consts=len(consts)</span><span class="s3">, </span><span class="s1">num_carry=len(init_flat)</span><span class="s3">,</span>
                    <span class="s1">linear=(</span><span class="s3">False,</span><span class="s1">) * (len(consts) + len(in_flat))</span><span class="s3">,</span>
                    <span class="s1">unroll=unroll)</span>
  <span class="s3">return </span><span class="s1">tree_unflatten(out_tree</span><span class="s3">, </span><span class="s1">out)</span>

<span class="s3">def </span><span class="s1">_check_scan_carry_type(body_fun</span><span class="s3">, </span><span class="s1">in_carry</span><span class="s3">, </span><span class="s1">out_carry_tree</span><span class="s3">, </span><span class="s1">out_avals):</span>
  <span class="s3">try</span><span class="s1">:</span>
    <span class="s1">sig = inspect.signature(body_fun)</span>
  <span class="s3">except </span><span class="s1">(ValueError</span><span class="s3">, </span><span class="s1">TypeError):</span>
    <span class="s1">sig = </span><span class="s3">None</span>
  <span class="s1">carry_name = sig </span><span class="s3">and </span><span class="s1">list(sig.parameters)[</span><span class="s5">0</span><span class="s1">]</span>
  <span class="s3">if </span><span class="s1">carry_name:</span>
    <span class="s1">component = </span><span class="s3">lambda </span><span class="s1">p: (</span><span class="s4">f'the input carry component </span><span class="s3">{</span><span class="s1">carry_name</span><span class="s3">}{</span><span class="s1">keystr(p)</span><span class="s3">}</span><span class="s4">'</span>
                           <span class="s3">if </span><span class="s1">p </span><span class="s3">else </span><span class="s4">f'the input carry </span><span class="s3">{</span><span class="s1">carry_name</span><span class="s3">}</span><span class="s4">'</span><span class="s1">)</span>
  <span class="s3">else</span><span class="s1">:</span>
    <span class="s1">component = </span><span class="s3">lambda </span><span class="s1">p: (</span><span class="s4">f'the input carry at path </span><span class="s3">{</span><span class="s1">keystr(p)</span><span class="s3">}</span><span class="s4">'</span>
                           <span class="s3">if </span><span class="s1">p </span><span class="s3">else </span><span class="s4">'the input carry'</span><span class="s1">)</span>
  <span class="s1">leaves_and_paths</span><span class="s3">, </span><span class="s1">in_carry_tree = tree_flatten_with_path(in_carry)</span>
  <span class="s1">paths</span><span class="s3">, </span><span class="s1">in_carry_flat = unzip2(leaves_and_paths)</span>
  <span class="s1">in_avals = _map(_abstractify</span><span class="s3">, </span><span class="s1">in_carry_flat)</span>
  <span class="s3">if </span><span class="s1">in_carry_tree != out_carry_tree:</span>
    <span class="s3">try</span><span class="s1">:</span>
      <span class="s1">out_carry = tree_unflatten(out_carry_tree</span><span class="s3">, </span><span class="s1">out_avals)</span>
    <span class="s3">except</span><span class="s1">:</span>
      <span class="s1">out_carry = </span><span class="s3">None</span>

    <span class="s3">if </span><span class="s1">out_carry </span><span class="s3">is None</span><span class="s1">:</span>
      <span class="s1">differences = [</span><span class="s4">f'the input tree structure is:</span><span class="s3">\n{</span><span class="s1">in_carry_tree</span><span class="s3">}\n</span><span class="s4">'</span><span class="s3">,</span>
                     <span class="s4">f'the output tree structure is:</span><span class="s3">\n{</span><span class="s1">out_carry_tree</span><span class="s3">}\n</span><span class="s4">'</span><span class="s1">]</span>
    <span class="s3">else</span><span class="s1">:</span>
      <span class="s1">differences = </span><span class="s4">'</span><span class="s3">\n</span><span class="s4">'</span><span class="s1">.join(</span>
          <span class="s4">f'  * </span><span class="s3">{</span><span class="s1">component(path)</span><span class="s3">} </span><span class="s4">is a </span><span class="s3">{</span><span class="s1">thing1</span><span class="s3">} </span><span class="s4">but the corresponding component '</span>
          <span class="s4">f'of the carry output is a </span><span class="s3">{</span><span class="s1">thing2</span><span class="s3">}</span><span class="s4">, so </span><span class="s3">{</span><span class="s1">explanation</span><span class="s3">}\n</span><span class="s4">'</span>
          <span class="s3">for </span><span class="s1">path</span><span class="s3">, </span><span class="s1">thing1</span><span class="s3">, </span><span class="s1">thing2</span><span class="s3">, </span><span class="s1">explanation</span>
          <span class="s3">in </span><span class="s1">equality_errors(in_carry</span><span class="s3">, </span><span class="s1">out_carry))</span>
    <span class="s3">raise </span><span class="s1">TypeError(</span>
        <span class="s4">&quot;Scanned function carry input and carry output must have the same &quot;</span>
        <span class="s4">&quot;pytree structure, but they differ:</span><span class="s3">\n</span><span class="s4">&quot;</span>
        <span class="s4">f&quot;</span><span class="s3">{</span><span class="s1">differences</span><span class="s3">}\n</span><span class="s4">&quot;</span>
        <span class="s4">&quot;Revise the scanned function so that its output is a pair where the &quot;</span>
        <span class="s4">&quot;first element has the same pytree structure as the first argument.&quot;</span>
    <span class="s1">)</span>
  <span class="s3">if not </span><span class="s1">all(_map(core.typematch</span><span class="s3">, </span><span class="s1">in_avals</span><span class="s3">, </span><span class="s1">out_avals)):</span>
    <span class="s1">differences = </span><span class="s4">'</span><span class="s3">\n</span><span class="s4">'</span><span class="s1">.join(</span>
        <span class="s4">f'  * </span><span class="s3">{</span><span class="s1">component(path)</span><span class="s3">} </span><span class="s4">has type </span><span class="s3">{</span><span class="s1">in_aval.str_short()</span><span class="s3">}</span><span class="s4">'</span>
        <span class="s4">' but the corresponding output carry component has type '</span>
        <span class="s4">f'</span><span class="s3">{</span><span class="s1">out_aval.str_short()</span><span class="s3">}{</span><span class="s1">_aval_mismatch_extra(in_aval</span><span class="s3">, </span><span class="s1">out_aval)</span><span class="s3">}\n</span><span class="s4">'</span>
        <span class="s3">for </span><span class="s1">path</span><span class="s3">, </span><span class="s1">in_aval</span><span class="s3">, </span><span class="s1">out_aval </span><span class="s3">in </span><span class="s1">zip(paths</span><span class="s3">, </span><span class="s1">in_avals</span><span class="s3">, </span><span class="s1">out_avals)</span>
        <span class="s3">if not </span><span class="s1">core.typematch(in_aval</span><span class="s3">, </span><span class="s1">out_aval))</span>
    <span class="s3">raise </span><span class="s1">TypeError(</span>
        <span class="s4">&quot;Scanned function carry input and carry output must have equal types &quot;</span>
        <span class="s4">&quot;(e.g. shapes and dtypes of arrays), &quot;</span>
        <span class="s4">&quot;but they differ:</span><span class="s3">\n</span><span class="s4">&quot;</span>
        <span class="s4">f&quot;</span><span class="s3">{</span><span class="s1">differences</span><span class="s3">}\n</span><span class="s4">&quot;</span>
        <span class="s4">&quot;Revise the scanned function so that all output types (e.g. shapes &quot;</span>
        <span class="s4">&quot;and dtypes) match the corresponding input types.&quot;</span>
    <span class="s1">)</span>

<span class="s3">def </span><span class="s1">_aval_mismatch_extra(a1: core.AbstractValue</span><span class="s3">, </span><span class="s1">a2: core.AbstractValue) -&gt; str:</span>
  <span class="s3">assert not </span><span class="s1">core.typematch(a1</span><span class="s3">, </span><span class="s1">a2)</span>
  <span class="s3">if </span><span class="s1">isinstance(a1</span><span class="s3">, </span><span class="s1">core.ShapedArray) </span><span class="s3">and </span><span class="s1">isinstance(a2</span><span class="s3">, </span><span class="s1">core.ShapedArray):</span>
    <span class="s1">dtype_mismatch = a1.dtype != a2.dtype</span>
    <span class="s1">shape_mismatch = a1.shape != a2.shape</span>
    <span class="s3">return </span><span class="s1">(</span><span class="s4">', so ' </span><span class="s1">* (dtype_mismatch </span><span class="s3">or </span><span class="s1">shape_mismatch) +</span>
            <span class="s4">'the dtypes do not match' </span><span class="s1">* dtype_mismatch +</span>
            <span class="s4">' and also ' </span><span class="s1">* (dtype_mismatch </span><span class="s3">and </span><span class="s1">shape_mismatch) +</span>
            <span class="s4">'the shapes do not match' </span><span class="s1">* shape_mismatch)</span>
  <span class="s3">return </span><span class="s4">''</span>


<span class="s3">def </span><span class="s1">_scan_impl_unrolled(*args</span><span class="s3">, </span><span class="s1">reverse</span><span class="s3">, </span><span class="s1">length</span><span class="s3">, </span><span class="s1">num_consts</span><span class="s3">, </span><span class="s1">num_carry</span><span class="s3">, </span><span class="s1">linear</span><span class="s3">,</span>
                        <span class="s1">f_impl</span><span class="s3">, </span><span class="s1">x_avals</span><span class="s3">, </span><span class="s1">y_avals):</span>
  <span class="s1">consts</span><span class="s3">, </span><span class="s1">init</span><span class="s3">, </span><span class="s1">xs = split_list(args</span><span class="s3">, </span><span class="s1">[num_consts</span><span class="s3">, </span><span class="s1">num_carry])</span>

  <span class="s1">carry = init</span>
  <span class="s1">ys = []</span>

  <span class="s3">for </span><span class="s1">i </span><span class="s3">in </span><span class="s1">range(length):</span>
    <span class="s1">i_ = length - i - </span><span class="s5">1 </span><span class="s3">if </span><span class="s1">reverse </span><span class="s3">else </span><span class="s1">i</span>
    <span class="s1">x = _map(partial(_index_array</span><span class="s3">, </span><span class="s1">i_)</span><span class="s3">, </span><span class="s1">x_avals</span><span class="s3">, </span><span class="s1">xs)</span>
    <span class="s1">out = f_impl(*consts</span><span class="s3">, </span><span class="s1">*carry</span><span class="s3">, </span><span class="s1">*x)</span>
    <span class="s1">carry</span><span class="s3">, </span><span class="s1">y = split_list(out</span><span class="s3">, </span><span class="s1">[num_carry])</span>
    <span class="s1">ys.append(y)</span>

  <span class="s1">ys = list(reversed(ys)) </span><span class="s3">if </span><span class="s1">reverse </span><span class="s3">else </span><span class="s1">ys</span>
  <span class="s1">ys = list(zip(*ys))</span>
  <span class="s1">ys = _map(_stack</span><span class="s3">, </span><span class="s1">y_avals</span><span class="s3">, </span><span class="s1">ys)</span>
  <span class="s3">return </span><span class="s1">(*carry</span><span class="s3">, </span><span class="s1">*ys)</span>

<span class="s3">def </span><span class="s1">_scan_impl_loop(*args</span><span class="s3">, </span><span class="s1">reverse</span><span class="s3">, </span><span class="s1">length</span><span class="s3">, </span><span class="s1">num_consts</span><span class="s3">, </span><span class="s1">num_carry</span><span class="s3">, </span><span class="s1">linear</span><span class="s3">,</span>
                    <span class="s1">f_impl</span><span class="s3">, </span><span class="s1">x_avals</span><span class="s3">, </span><span class="s1">y_avals):</span>
  <span class="s1">consts</span><span class="s3">, </span><span class="s1">init</span><span class="s3">, </span><span class="s1">xs = split_list(args</span><span class="s3">, </span><span class="s1">[num_consts</span><span class="s3">, </span><span class="s1">num_carry])</span>

  <span class="s3">def </span><span class="s1">cond_fun(vals):</span>
    <span class="s1">i</span><span class="s3">, </span><span class="s1">*_ = vals</span>
    <span class="s3">return </span><span class="s1">i &lt; length</span>

  <span class="s3">def </span><span class="s1">body_fun(vals):</span>
    <span class="s1">[i]</span><span class="s3">, </span><span class="s1">carry</span><span class="s3">, </span><span class="s1">ys = split_list(vals</span><span class="s3">, </span><span class="s1">[</span><span class="s5">1</span><span class="s3">, </span><span class="s1">num_carry])</span>
    <span class="s1">i_ = length - i - </span><span class="s5">1 </span><span class="s3">if </span><span class="s1">reverse </span><span class="s3">else </span><span class="s1">i</span>
    <span class="s1">x = _map(partial(_dynamic_index_array</span><span class="s3">, </span><span class="s1">i_)</span><span class="s3">, </span><span class="s1">x_avals</span><span class="s3">, </span><span class="s1">xs)</span>
    <span class="s1">out_flat = f_impl(*consts</span><span class="s3">, </span><span class="s1">*carry</span><span class="s3">, </span><span class="s1">*x)</span>
    <span class="s1">carry_out</span><span class="s3">, </span><span class="s1">y_updates = split_list(out_flat</span><span class="s3">, </span><span class="s1">[num_carry])</span>
    <span class="s1">ys_out = _map(partial(_update_array</span><span class="s3">, </span><span class="s1">i_)</span><span class="s3">, </span><span class="s1">y_avals</span><span class="s3">, </span><span class="s1">ys</span><span class="s3">, </span><span class="s1">y_updates)</span>
    <span class="s3">return </span><span class="s1">[i + </span><span class="s5">1</span><span class="s1">] + carry_out + ys_out</span>

  <span class="s1">ys_init = _map(partial(_empty_array</span><span class="s3">, </span><span class="s1">length)</span><span class="s3">, </span><span class="s1">y_avals)</span>
  <span class="s3">if </span><span class="s1">length == </span><span class="s5">0</span><span class="s1">:</span>
    <span class="s3">return </span><span class="s1">init + ys_init</span>
  <span class="s3">else</span><span class="s1">:</span>
    <span class="s1">init_val = [lax._const(length</span><span class="s3">, </span><span class="s5">0</span><span class="s1">)] + init + ys_init</span>
    <span class="s1">_</span><span class="s3">, </span><span class="s1">*outs = while_loop(cond_fun</span><span class="s3">, </span><span class="s1">body_fun</span><span class="s3">, </span><span class="s1">init_val)</span>
    <span class="s3">return </span><span class="s1">outs</span>

<span class="s3">def </span><span class="s1">_scan_impl_block_unrolled(*args</span><span class="s3">, </span><span class="s1">reverse</span><span class="s3">, </span><span class="s1">length</span><span class="s3">, </span><span class="s1">num_consts</span><span class="s3">, </span><span class="s1">num_carry</span><span class="s3">,</span>
                              <span class="s1">linear</span><span class="s3">, </span><span class="s1">block_length</span><span class="s3">, </span><span class="s1">f_impl</span><span class="s3">, </span><span class="s1">x_avals</span><span class="s3">, </span><span class="s1">y_avals):</span>
  <span class="s1">consts</span><span class="s3">, </span><span class="s1">init</span><span class="s3">, </span><span class="s1">xs = split_list(args</span><span class="s3">, </span><span class="s1">[num_consts</span><span class="s3">, </span><span class="s1">num_carry])</span>

  <span class="s1">num_blocks</span><span class="s3">, </span><span class="s1">rem = divmod(length</span><span class="s3">, </span><span class="s1">block_length)</span>
  <span class="s3">assert </span><span class="s1">rem == </span><span class="s5">0</span>

  <span class="s1">partition = partial(_partition_leading</span><span class="s3">, </span><span class="s1">num_blocks</span><span class="s3">, </span><span class="s1">block_length)</span>
  <span class="s1">xs_block = _map(partition</span><span class="s3">, </span><span class="s1">x_avals</span><span class="s3">, </span><span class="s1">xs)</span>

  <span class="s1">prepend_aval = partial(_prepend_dim_to_aval</span><span class="s3">, </span><span class="s1">block_length)</span>
  <span class="s1">x_block_avals = _map(prepend_aval</span><span class="s3">, </span><span class="s1">x_avals)</span>
  <span class="s1">y_block_avals = _map(prepend_aval</span><span class="s3">, </span><span class="s1">y_avals)</span>

  <span class="s1">f_impl_block = partial(</span>
      <span class="s1">_scan_impl_unrolled</span><span class="s3">, </span><span class="s1">reverse=reverse</span><span class="s3">, </span><span class="s1">length=block_length</span><span class="s3">,</span>
      <span class="s1">num_consts=num_consts</span><span class="s3">, </span><span class="s1">num_carry=num_carry</span><span class="s3">, </span><span class="s1">linear=linear</span><span class="s3">,</span>
      <span class="s1">f_impl=f_impl</span><span class="s3">, </span><span class="s1">x_avals=x_avals</span><span class="s3">, </span><span class="s1">y_avals=y_avals)</span>

  <span class="s1">outs = _scan_impl_loop(</span>
      <span class="s1">*consts</span><span class="s3">, </span><span class="s1">*init</span><span class="s3">, </span><span class="s1">*xs_block</span><span class="s3">, </span><span class="s1">reverse=reverse</span><span class="s3">, </span><span class="s1">length=num_blocks</span><span class="s3">,</span>
      <span class="s1">num_consts=num_consts</span><span class="s3">, </span><span class="s1">num_carry=num_carry</span><span class="s3">, </span><span class="s1">linear=linear</span><span class="s3">,</span>
      <span class="s1">f_impl=f_impl_block</span><span class="s3">, </span><span class="s1">x_avals=x_block_avals</span><span class="s3">, </span><span class="s1">y_avals=y_block_avals)</span>

  <span class="s1">carry</span><span class="s3">, </span><span class="s1">ys_blocks = split_list(outs</span><span class="s3">, </span><span class="s1">[num_carry])</span>
  <span class="s1">combine = partial(_combine_leading</span><span class="s3">, </span><span class="s1">num_blocks</span><span class="s3">, </span><span class="s1">block_length)</span>
  <span class="s1">ys = _map(combine</span><span class="s3">, </span><span class="s1">y_avals</span><span class="s3">, </span><span class="s1">ys_blocks)</span>
  <span class="s3">return </span><span class="s1">(*carry</span><span class="s3">, </span><span class="s1">*ys)</span>

<span class="s3">def </span><span class="s1">_scan_impl(*args</span><span class="s3">, </span><span class="s1">reverse</span><span class="s3">, </span><span class="s1">length</span><span class="s3">, </span><span class="s1">num_consts</span><span class="s3">, </span><span class="s1">num_carry</span><span class="s3">, </span><span class="s1">jaxpr</span><span class="s3">, </span><span class="s1">linear</span><span class="s3">,</span>
               <span class="s1">unroll):</span>
  <span class="s1">_</span><span class="s3">, </span><span class="s1">_</span><span class="s3">, </span><span class="s1">x_avals = split_list(jaxpr.in_avals</span><span class="s3">, </span><span class="s1">[num_consts</span><span class="s3">, </span><span class="s1">num_carry])</span>
  <span class="s1">_</span><span class="s3">, </span><span class="s1">y_avals = split_list(jaxpr.out_avals</span><span class="s3">, </span><span class="s1">[num_carry])</span>
  <span class="s1">f_impl = core.jaxpr_as_fun(jaxpr)</span>

  <span class="s3">if </span><span class="s1">unroll == </span><span class="s5">1</span><span class="s1">:</span>
    <span class="s3">return </span><span class="s1">_scan_impl_loop(</span>
        <span class="s1">*args</span><span class="s3">, </span><span class="s1">reverse=reverse</span><span class="s3">, </span><span class="s1">length=length</span><span class="s3">, </span><span class="s1">num_consts=num_consts</span><span class="s3">,</span>
        <span class="s1">num_carry=num_carry</span><span class="s3">, </span><span class="s1">linear=linear</span><span class="s3">, </span><span class="s1">f_impl=f_impl</span><span class="s3">, </span><span class="s1">x_avals=x_avals</span><span class="s3">,</span>
        <span class="s1">y_avals=y_avals)</span>

  <span class="s1">consts</span><span class="s3">, </span><span class="s1">init</span><span class="s3">, </span><span class="s1">xs = split_list(args</span><span class="s3">, </span><span class="s1">[num_consts</span><span class="s3">, </span><span class="s1">num_carry])</span>
  <span class="s1">num_blocks</span><span class="s3">, </span><span class="s1">rem = divmod(length</span><span class="s3">, </span><span class="s1">unroll)</span>
  <span class="s1">length_div = num_blocks * unroll</span>

  <span class="s3">if </span><span class="s1">rem &gt; </span><span class="s5">0</span><span class="s1">:</span>
    <span class="s3">if </span><span class="s1">reverse:</span>
      <span class="s1">split = partial(_split_leading_dim</span><span class="s3">, </span><span class="s1">rem)</span>
      <span class="s1">xs_rem</span><span class="s3">, </span><span class="s1">xs = unzip2(_map(split</span><span class="s3">, </span><span class="s1">x_avals</span><span class="s3">, </span><span class="s1">xs))</span>
    <span class="s3">else</span><span class="s1">:</span>
      <span class="s1">split = partial(_split_leading_dim</span><span class="s3">, </span><span class="s1">length_div)</span>
      <span class="s1">xs</span><span class="s3">, </span><span class="s1">xs_rem = unzip2(_map(split</span><span class="s3">, </span><span class="s1">x_avals</span><span class="s3">, </span><span class="s1">xs))</span>

  <span class="s1">outs = _scan_impl_block_unrolled(</span>
      <span class="s1">*consts</span><span class="s3">, </span><span class="s1">*init</span><span class="s3">, </span><span class="s1">*xs</span><span class="s3">, </span><span class="s1">reverse=reverse</span><span class="s3">, </span><span class="s1">length=length_div</span><span class="s3">,</span>
      <span class="s1">num_consts=num_consts</span><span class="s3">, </span><span class="s1">num_carry=num_carry</span><span class="s3">, </span><span class="s1">linear=linear</span><span class="s3">,</span>
      <span class="s1">block_length=unroll</span><span class="s3">, </span><span class="s1">f_impl=f_impl</span><span class="s3">, </span><span class="s1">x_avals=x_avals</span><span class="s3">, </span><span class="s1">y_avals=y_avals)</span>

  <span class="s1">carry</span><span class="s3">, </span><span class="s1">ys = split_list(outs</span><span class="s3">, </span><span class="s1">[num_carry])</span>

  <span class="s3">if </span><span class="s1">rem &gt; </span><span class="s5">0</span><span class="s1">:</span>
    <span class="s1">outs = _scan_impl_unrolled(</span>
        <span class="s1">*consts</span><span class="s3">, </span><span class="s1">*carry</span><span class="s3">, </span><span class="s1">*xs_rem</span><span class="s3">, </span><span class="s1">reverse=reverse</span><span class="s3">, </span><span class="s1">length=rem</span><span class="s3">,</span>
        <span class="s1">num_consts=num_consts</span><span class="s3">, </span><span class="s1">num_carry=num_carry</span><span class="s3">, </span><span class="s1">linear=linear</span><span class="s3">,</span>
        <span class="s1">f_impl=f_impl</span><span class="s3">, </span><span class="s1">x_avals=x_avals</span><span class="s3">, </span><span class="s1">y_avals=y_avals)</span>
    <span class="s1">carry</span><span class="s3">, </span><span class="s1">ys_rem = split_list(outs</span><span class="s3">, </span><span class="s1">[num_carry])</span>
    <span class="s3">if </span><span class="s1">reverse:</span>
      <span class="s1">ys = _map(_concatenate</span><span class="s3">, </span><span class="s1">y_avals</span><span class="s3">, </span><span class="s1">ys_rem</span><span class="s3">, </span><span class="s1">ys)</span>
    <span class="s3">else</span><span class="s1">:</span>
      <span class="s1">ys = _map(_concatenate</span><span class="s3">, </span><span class="s1">y_avals</span><span class="s3">, </span><span class="s1">ys</span><span class="s3">, </span><span class="s1">ys_rem)</span>

  <span class="s3">return </span><span class="s1">(*carry</span><span class="s3">, </span><span class="s1">*ys)</span>

<span class="s3">def </span><span class="s1">_stack(aval</span><span class="s3">, </span><span class="s1">vals):</span>
  <span class="s1">vals = [lax.expand_dims(x</span><span class="s3">, </span><span class="s1">(</span><span class="s5">0</span><span class="s3">,</span><span class="s1">)) </span><span class="s3">for </span><span class="s1">x </span><span class="s3">in </span><span class="s1">vals]</span>
  <span class="s3">return </span><span class="s1">lax.concatenate(vals</span><span class="s3">, </span><span class="s5">0</span><span class="s1">)</span>

<span class="s3">def </span><span class="s1">_concatenate(aval</span><span class="s3">, </span><span class="s1">x1</span><span class="s3">, </span><span class="s1">x2):</span>
  <span class="s3">return </span><span class="s1">lax.concatenate([x1</span><span class="s3">, </span><span class="s1">x2]</span><span class="s3">, </span><span class="s5">0</span><span class="s1">)</span>

<span class="s3">def </span><span class="s1">_split_leading_dim(i</span><span class="s3">, </span><span class="s1">aval</span><span class="s3">, </span><span class="s1">x):</span>
  <span class="s3">assert </span><span class="s1">x.ndim &gt;= </span><span class="s5">1</span>
  <span class="s3">return </span><span class="s1">(slicing.slice_in_dim(x</span><span class="s3">, </span><span class="s5">0</span><span class="s3">, </span><span class="s1">i)</span><span class="s3">,</span>
          <span class="s1">slicing.slice_in_dim(x</span><span class="s3">, </span><span class="s1">i</span><span class="s3">, </span><span class="s1">x.shape[</span><span class="s5">0</span><span class="s1">]))</span>

<span class="s3">def </span><span class="s1">_dynamic_index_array(i</span><span class="s3">, </span><span class="s1">aval</span><span class="s3">, </span><span class="s1">x):</span>
  <span class="s3">return </span><span class="s1">slicing.dynamic_index_in_dim(x</span><span class="s3">, </span><span class="s1">i</span><span class="s3">, </span><span class="s1">keepdims=</span><span class="s3">False</span><span class="s1">)</span>

<span class="s3">def </span><span class="s1">_index_array(i</span><span class="s3">, </span><span class="s1">aval</span><span class="s3">, </span><span class="s1">x):</span>
  <span class="s3">return </span><span class="s1">slicing.index_in_dim(x</span><span class="s3">, </span><span class="s1">i</span><span class="s3">, </span><span class="s1">keepdims=</span><span class="s3">False</span><span class="s1">)</span>

<span class="s3">def </span><span class="s1">_empty_array(sz</span><span class="s3">, </span><span class="s1">aval):</span>
  <span class="s3">return </span><span class="s1">lax.broadcast(lax.empty(aval.dtype)</span><span class="s3">, </span><span class="s1">(sz</span><span class="s3">, </span><span class="s1">*aval.shape))</span>

<span class="s3">def </span><span class="s1">_update_array(i</span><span class="s3">, </span><span class="s1">aval</span><span class="s3">, </span><span class="s1">xs</span><span class="s3">, </span><span class="s1">x):</span>
  <span class="s3">return </span><span class="s1">slicing.dynamic_update_index_in_dim(xs</span><span class="s3">, </span><span class="s1">x</span><span class="s3">, </span><span class="s1">i</span><span class="s3">, </span><span class="s5">0</span><span class="s1">)</span>

<span class="s3">def </span><span class="s1">_partition_leading(sz0</span><span class="s3">, </span><span class="s1">sz1</span><span class="s3">, </span><span class="s1">aval</span><span class="s3">, </span><span class="s1">x):</span>
  <span class="s3">assert </span><span class="s1">x.ndim &gt;= </span><span class="s5">1</span>
  <span class="s3">assert </span><span class="s1">x.shape[</span><span class="s5">0</span><span class="s1">] == sz0 * sz1</span>
  <span class="s3">return </span><span class="s1">lax.reshape(x</span><span class="s3">, </span><span class="s1">(sz0</span><span class="s3">, </span><span class="s1">sz1</span><span class="s3">, </span><span class="s1">*x.shape[</span><span class="s5">1</span><span class="s1">:]))</span>

<span class="s3">def </span><span class="s1">_combine_leading(sz0</span><span class="s3">, </span><span class="s1">sz1</span><span class="s3">, </span><span class="s1">aval</span><span class="s3">, </span><span class="s1">x):</span>
  <span class="s3">assert </span><span class="s1">x.ndim &gt;= </span><span class="s5">2</span>
  <span class="s3">assert </span><span class="s1">x.shape[</span><span class="s5">0</span><span class="s1">] == sz0</span>
  <span class="s3">assert </span><span class="s1">x.shape[</span><span class="s5">1</span><span class="s1">] == sz1</span>
  <span class="s3">return </span><span class="s1">lax.collapse(x</span><span class="s3">, </span><span class="s5">0</span><span class="s3">, </span><span class="s5">2</span><span class="s1">)</span>

<span class="s3">def </span><span class="s1">_prepend_dim_to_aval(sz</span><span class="s3">, </span><span class="s1">aval):</span>
  <span class="s3">return </span><span class="s1">core.unmapped_aval(sz</span><span class="s3">, </span><span class="s1">core.no_axis_name</span><span class="s3">, </span><span class="s5">0</span><span class="s3">, </span><span class="s1">aval)</span>

<span class="s3">def </span><span class="s1">_scan_abstract_eval(*args</span><span class="s3">, </span><span class="s1">reverse</span><span class="s3">, </span><span class="s1">length</span><span class="s3">, </span><span class="s1">num_consts</span><span class="s3">, </span><span class="s1">num_carry</span><span class="s3">, </span><span class="s1">jaxpr</span><span class="s3">,</span>
                        <span class="s1">linear</span><span class="s3">, </span><span class="s1">unroll):</span>
  <span class="s1">carry_avals</span><span class="s3">, </span><span class="s1">y_avals = split_list(jaxpr.out_avals</span><span class="s3">, </span><span class="s1">[num_carry])</span>
  <span class="s1">ys_avals = _map(partial(_prepend_dim_to_aval</span><span class="s3">, </span><span class="s1">length)</span><span class="s3">, </span><span class="s1">y_avals)</span>
  <span class="s3">return </span><span class="s1">carry_avals + ys_avals</span><span class="s3">, </span><span class="s1">jaxpr.effects</span>

<span class="s3">def </span><span class="s1">_scan_jvp(primals</span><span class="s3">, </span><span class="s1">tangents</span><span class="s3">, </span><span class="s1">reverse</span><span class="s3">, </span><span class="s1">length</span><span class="s3">, </span><span class="s1">jaxpr</span><span class="s3">, </span><span class="s1">num_consts</span><span class="s3">, </span><span class="s1">num_carry</span><span class="s3">,</span>
              <span class="s1">linear</span><span class="s3">, </span><span class="s1">unroll):</span>
  <span class="s1">num_xs = len(jaxpr.in_avals) - num_carry - num_consts</span>
  <span class="s1">num_ys = len(jaxpr.out_avals) - num_carry</span>
  <span class="s1">nonzeros = [type(t) </span><span class="s3">is not </span><span class="s1">ad_util.Zero </span><span class="s3">for </span><span class="s1">t </span><span class="s3">in </span><span class="s1">tangents]</span>
  <span class="s1">const_nz</span><span class="s3">, </span><span class="s1">init_nz</span><span class="s3">, </span><span class="s1">xs_nz = split_list(nonzeros</span><span class="s3">, </span><span class="s1">[num_consts</span><span class="s3">, </span><span class="s1">num_carry])</span>

  <span class="s0"># Fixpoint computation of which carry are not ad.zero: either</span>
  <span class="s0"># non-zero from init, or the carry out is non-zero. Each iteration promotes</span>
  <span class="s0"># at least one carry to non-zero. We need at most len(carry) iterations,</span>
  <span class="s0"># but we need one last iteration to prepare the jaxpr based on the final</span>
  <span class="s0"># carry_nz.</span>
  <span class="s1">carry_nz = init_nz</span>
  <span class="s3">for </span><span class="s1">_ </span><span class="s3">in </span><span class="s1">range(</span><span class="s5">1 </span><span class="s1">+ len(carry_nz)):</span>
    <span class="s1">nonzeros = const_nz + carry_nz + xs_nz</span>
    <span class="s1">jaxpr_jvp</span><span class="s3">, </span><span class="s1">nonzeros_out = ad.jvp_jaxpr(</span>
        <span class="s1">jaxpr</span><span class="s3">, </span><span class="s1">nonzeros</span><span class="s3">, </span><span class="s1">instantiate=carry_nz + [</span><span class="s3">False</span><span class="s1">] * num_ys)</span>
    <span class="s1">carry_nz_out</span><span class="s3">, </span><span class="s1">_ = nonzeros_out[:num_carry]</span><span class="s3">, </span><span class="s1">nonzeros_out[num_carry:]</span>
    <span class="s3">if </span><span class="s1">carry_nz_out == carry_nz:</span>
      <span class="s3">break</span>
    <span class="s3">else</span><span class="s1">:</span>
      <span class="s1">carry_nz = _map(operator.or_</span><span class="s3">, </span><span class="s1">carry_nz</span><span class="s3">, </span><span class="s1">carry_nz_out)</span>
  <span class="s3">else</span><span class="s1">:</span>
    <span class="s3">assert False, </span><span class="s4">&quot;Fixpoint not reached&quot;</span>

  <span class="s1">tangents = [ad.instantiate_zeros(t) </span><span class="s3">if </span><span class="s1">nz </span><span class="s3">else </span><span class="s1">t</span>
              <span class="s3">for </span><span class="s1">t</span><span class="s3">, </span><span class="s1">nz </span><span class="s3">in </span><span class="s1">zip(tangents</span><span class="s3">, </span><span class="s1">nonzeros)]</span>

  <span class="s1">consts</span><span class="s3">, </span><span class="s1">init</span><span class="s3">, </span><span class="s1">xs = split_list(primals</span><span class="s3">, </span><span class="s1">[num_consts</span><span class="s3">, </span><span class="s1">num_carry])</span>
  <span class="s1">all_tangents = split_list(tangents</span><span class="s3">, </span><span class="s1">[num_consts</span><span class="s3">, </span><span class="s1">num_carry])</span>
  <span class="s1">consts_dot</span><span class="s3">, </span><span class="s1">init_dot</span><span class="s3">, </span><span class="s1">xs_dot = _map(_prune_zeros</span><span class="s3">, </span><span class="s1">all_tangents)</span>

  <span class="s1">jaxpr_jvp_rearranged = ad.rearrange_binders(</span>
      <span class="s1">jaxpr_jvp</span><span class="s3">,</span>
      <span class="s1">[num_consts</span><span class="s3">, </span><span class="s1">num_carry</span><span class="s3">, </span><span class="s1">num_xs]</span><span class="s3">, </span><span class="s1">[len(consts_dot)</span><span class="s3">, </span><span class="s1">len(init_dot)</span><span class="s3">, </span><span class="s1">len(xs_dot)]</span><span class="s3">,</span>
      <span class="s1">[num_carry</span><span class="s3">, </span><span class="s1">num_ys]</span><span class="s3">, </span><span class="s1">[len(init_dot)</span><span class="s3">, </span><span class="s1">sum(nonzeros_out) - len(init_dot)])</span>

  <span class="s1">consts_linear</span><span class="s3">, </span><span class="s1">init_linear</span><span class="s3">, </span><span class="s1">xs_linear = split_list(linear</span><span class="s3">, </span><span class="s1">[num_consts</span><span class="s3">, </span><span class="s1">num_carry])</span>
  <span class="s1">jaxpr_jvp_linear = tuple(consts_linear + [</span><span class="s3">True</span><span class="s1">] * len(consts_dot)</span>
                           <span class="s1">+ init_linear + [</span><span class="s3">True</span><span class="s1">] * len(init_dot)</span>
                           <span class="s1">+ xs_linear + [</span><span class="s3">True</span><span class="s1">] * len(xs_dot))</span>

  <span class="s1">out_flat = scan_p.bind(</span>
      <span class="s1">*(consts + consts_dot + init + init_dot + xs + xs_dot)</span><span class="s3">,</span>
      <span class="s1">reverse=reverse</span><span class="s3">, </span><span class="s1">length=length</span><span class="s3">, </span><span class="s1">jaxpr=jaxpr_jvp_rearranged</span><span class="s3">,</span>
      <span class="s1">num_consts=num_consts + len(consts_dot)</span><span class="s3">,</span>
      <span class="s1">num_carry=num_carry + len(init_dot)</span><span class="s3">,</span>
      <span class="s1">linear=jaxpr_jvp_linear</span><span class="s3">, </span><span class="s1">unroll=unroll)</span>

  <span class="s1">carry</span><span class="s3">, </span><span class="s1">carry_dot</span><span class="s3">, </span><span class="s1">ys</span><span class="s3">, </span><span class="s1">ys_dot = split_list(out_flat</span><span class="s3">, </span><span class="s1">[num_carry</span><span class="s3">, </span><span class="s1">len(init_dot)</span><span class="s3">, </span><span class="s1">num_ys])</span>
  <span class="s1">primals_out = carry + ys</span>
  <span class="s1">tangents_out_iter = iter(carry_dot + ys_dot)</span>
  <span class="s1">tangents_out = [next(tangents_out_iter) </span><span class="s3">if </span><span class="s1">nz </span><span class="s3">else </span><span class="s1">ad_util.Zero.from_value(p)</span>
                  <span class="s3">for </span><span class="s1">p</span><span class="s3">, </span><span class="s1">nz </span><span class="s3">in </span><span class="s1">zip(primals_out</span><span class="s3">, </span><span class="s1">nonzeros_out)]</span>
  <span class="s3">return </span><span class="s1">primals_out</span><span class="s3">, </span><span class="s1">tangents_out</span>

<span class="s3">def </span><span class="s1">_scan_partial_eval(trace</span><span class="s3">, </span><span class="s1">*tracers</span><span class="s3">, </span><span class="s1">reverse</span><span class="s3">, </span><span class="s1">length</span><span class="s3">, </span><span class="s1">num_consts</span><span class="s3">, </span><span class="s1">num_carry</span><span class="s3">,</span>
                       <span class="s1">jaxpr</span><span class="s3">, </span><span class="s1">linear</span><span class="s3">, </span><span class="s1">unroll):</span>
  <span class="s1">num_ys = len(jaxpr.out_avals) - num_carry</span>
  <span class="s1">unknowns = [</span><span class="s3">not </span><span class="s1">t.pval.is_known() </span><span class="s3">for </span><span class="s1">t </span><span class="s3">in </span><span class="s1">tracers]</span>
  <span class="s1">const_uk</span><span class="s3">, </span><span class="s1">init_uk</span><span class="s3">, </span><span class="s1">xs_uk = split_list(unknowns</span><span class="s3">, </span><span class="s1">[num_consts</span><span class="s3">, </span><span class="s1">num_carry])</span>

  <span class="s0"># Fixpoint computation of which carry elements are unknown. Each iteration</span>
  <span class="s0"># promotes at least one carry to unknown. We need at most len(carry)</span>
  <span class="s0"># iterations, but we need one last iteration to prepare the jaxpr based on the</span>
  <span class="s0"># final carry_uk.</span>
  <span class="s1">carry_uk = init_uk</span>
  <span class="s3">for </span><span class="s1">_ </span><span class="s3">in </span><span class="s1">range(</span><span class="s5">1 </span><span class="s1">+ len(carry_uk)):</span>
    <span class="s1">unknowns = const_uk + carry_uk + xs_uk</span>
    <span class="s1">jaxpr_known</span><span class="s3">, </span><span class="s1">jaxpr_unknown</span><span class="s3">, </span><span class="s1">out_uk</span><span class="s3">, </span><span class="s1">res_avals = pe.partial_eval_jaxpr_nounits(</span>
        <span class="s1">jaxpr</span><span class="s3">, </span><span class="s1">unknowns</span><span class="s3">, </span><span class="s1">instantiate=carry_uk + [</span><span class="s3">False</span><span class="s1">] * num_ys)</span>
    <span class="s1">carry_uk_out</span><span class="s3">, </span><span class="s1">ys_uk = split_list(out_uk</span><span class="s3">, </span><span class="s1">[num_carry])</span>
    <span class="s3">if </span><span class="s1">carry_uk_out == carry_uk:</span>
      <span class="s3">break</span>
    <span class="s3">else</span><span class="s1">:</span>
      <span class="s1">carry_uk = _map(operator.or_</span><span class="s3">, </span><span class="s1">carry_uk</span><span class="s3">, </span><span class="s1">carry_uk_out)</span>
  <span class="s3">else</span><span class="s1">:</span>
    <span class="s3">assert False, </span><span class="s4">&quot;Fixpoint not reached&quot;</span>
  <span class="s1">num_res = len(res_avals)</span>
  <span class="s3">del </span><span class="s1">res_avals</span><span class="s3">, </span><span class="s1">carry_uk_out</span>

  <span class="s0"># Instantiate those inputs which must be treated as unknown from the fixpoint.</span>
  <span class="s1">tracers = [trace.instantiate_const(t) </span><span class="s3">if </span><span class="s1">uk </span><span class="s3">else </span><span class="s1">t</span>
             <span class="s3">for </span><span class="s1">t</span><span class="s3">, </span><span class="s1">uk </span><span class="s3">in </span><span class="s1">zip(tracers</span><span class="s3">, </span><span class="s1">unknowns)]</span>

  <span class="s0"># The residual inputs and outputs of the jaxprs produced haven't yet been</span>
  <span class="s0"># adapted to the scan calling convention; in particular, jaxpr_known has its</span>
  <span class="s0"># residual outputs all at the end, meaning they're extensive outputs (which is</span>
  <span class="s0"># fully general but may be wasteful for residuals which are loop-invariant)</span>
  <span class="s0"># while jaxpr_unknown has its corresponding residual inputs at the front (just</span>
  <span class="s0"># as a convention with partial_eval_jaxpr_nounits), making them constant</span>
  <span class="s0"># inputs. To make them consistent, we move the residual inputs on</span>
  <span class="s0"># jaxpr_unknown to the end, even though we may move some back in the sequel.</span>
  <span class="s1">jaxpr_unknown = pe.move_binders_to_back(</span>
      <span class="s1">jaxpr_unknown</span><span class="s3">, </span><span class="s1">[</span><span class="s3">True</span><span class="s1">] * num_res + [</span><span class="s3">False</span><span class="s1">] * sum(unknowns))</span>

  <span class="s0"># At this point, all residuals are treated as extensive outputs of jaxpr_known</span>
  <span class="s0"># (and extensive inputs to jaxpr_unknown). But residuals that are loop-</span>
  <span class="s0"># invariant can be hoisted out of the scan, rather than letting them get</span>
  <span class="s0"># broadcast (as in e.g. scanning multiplication by a constant matrix; we don't</span>
  <span class="s0"># want to broadcast the matrix!). So, outside the loop we perform a partial</span>
  <span class="s0"># evaluation with known 'const' inputs (but all other inputs unknown).</span>
  <span class="s1">const_pvals = [pe.PartialVal.known(t.pval.get_known())</span>
                 <span class="s3">for </span><span class="s1">t </span><span class="s3">in </span><span class="s1">tracers[:num_consts] </span><span class="s3">if </span><span class="s1">t.pval.is_known()]</span>
  <span class="s1">other_pvals = [pe.PartialVal.unknown(aval)</span>
                 <span class="s3">for </span><span class="s1">aval </span><span class="s3">in </span><span class="s1">jaxpr_known.in_avals[len(const_pvals):]]</span>
  <span class="s3">with </span><span class="s1">source_info_util.reset_name_stack():</span>
    <span class="s1">jaxpr_known_</span><span class="s3">, </span><span class="s1">invar_pvals_out</span><span class="s3">, </span><span class="s1">jaxpr_known_consts = pe.trace_to_jaxpr_nounits(</span>
        <span class="s1">lu.wrap_init(core.jaxpr_as_fun(jaxpr_known))</span><span class="s3">, </span><span class="s1">const_pvals + other_pvals</span><span class="s3">,</span>
        <span class="s1">instantiate=[</span><span class="s3">True</span><span class="s1">] * (len(out_uk) - sum(out_uk)) + [</span><span class="s3">False</span><span class="s1">] * num_res)</span>
  <span class="s1">jaxpr_known = pe.ClosedJaxpr(pe.convert_constvars_jaxpr(jaxpr_known_)</span><span class="s3">, </span><span class="s1">())</span>
  <span class="s0"># The above trace_to_jaxpr_nounits call computed loop-invariant residuals</span>
  <span class="s0"># (known values in invar_pvals_out) and also computed loop-invariant values</span>
  <span class="s0"># needed by the new jaxpr_known (in jaxpr_known_consts, which replace the</span>
  <span class="s0"># previous consts). We need to collect the computed inteisive residuals, and</span>
  <span class="s0"># move corresponding intensive residual binders in jaxpr_unknown to the front.</span>
  <span class="s1">res_pvals = invar_pvals_out[len(invar_pvals_out) - num_res:]</span>
  <span class="s1">intensive_res = [pval.get_known() </span><span class="s3">for </span><span class="s1">pval </span><span class="s3">in </span><span class="s1">res_pvals </span><span class="s3">if </span><span class="s1">pval.is_known()]</span>
  <span class="s1">jaxpr_unknown = pe.move_binders_to_front(</span>
      <span class="s1">jaxpr_unknown</span><span class="s3">,</span>
      <span class="s1">[</span><span class="s3">False</span><span class="s1">] * sum(unknowns) + [pval.is_known() </span><span class="s3">for </span><span class="s1">pval </span><span class="s3">in </span><span class="s1">res_pvals])</span>
  <span class="s3">del </span><span class="s1">const_pvals</span><span class="s3">, </span><span class="s1">other_pvals</span><span class="s3">, </span><span class="s1">invar_pvals_out</span><span class="s3">, </span><span class="s1">jaxpr_known_</span><span class="s3">, </span><span class="s1">res_pvals</span>
  <span class="s0"># We use `jaxpr_known_consts` when we call scan_p.bind with jaxpr_known, and</span>
  <span class="s0"># we use `intensive_res` when we build the jaxpr eqn with jaxpr_unknown.</span>

  <span class="s0"># As another optimization, for any extensive inputs that are just forwarded to</span>
  <span class="s0"># extensive outputs, to avoid a copy (which would be looping over</span>
  <span class="s0"># dynamic-update-slice) we'd rather forward the input tracer/value. That means</span>
  <span class="s0"># pruning some outputs from jaxpr_known here, and updating `out_flat` below.</span>
  <span class="s1">fwds_known = pe._jaxpr_forwarding(jaxpr_known.jaxpr)</span>
  <span class="s0"># Prune fwds_known to include only extensive input to extensive output.</span>
  <span class="s1">fwds_known = [in_idx </span><span class="s3">if </span><span class="s1">out_idx &gt;= num_carry - sum(carry_uk) </span><span class="s3">and</span>
                <span class="s1">in_idx </span><span class="s3">is not None and</span>
                <span class="s1">in_idx &gt;= len(jaxpr_known_consts) + num_carry - sum(carry_uk)</span>
                <span class="s3">else None for </span><span class="s1">out_idx</span><span class="s3">, </span><span class="s1">in_idx </span><span class="s3">in </span><span class="s1">enumerate(fwds_known)]</span>
  <span class="s0"># Drop any extensive output we can instead get by forwarding an input.</span>
  <span class="s0"># TODO(mattjj): use pe.dce_jaxpr here, though need a fixpoint</span>
  <span class="s1">jaxpr_known_</span><span class="s3">, </span><span class="s1">() = jaxpr_known.jaxpr</span><span class="s3">, </span><span class="s1">jaxpr_known.consts</span>
  <span class="s1">jaxpr_known_ = jaxpr_known_.replace(</span>
    <span class="s1">outvars=[x </span><span class="s3">for </span><span class="s1">x</span><span class="s3">, </span><span class="s1">i </span><span class="s3">in </span><span class="s1">zip(jaxpr_known_.outvars</span><span class="s3">, </span><span class="s1">fwds_known) </span><span class="s3">if </span><span class="s1">i </span><span class="s3">is None</span><span class="s1">])</span>
  <span class="s1">jaxpr_known = core.ClosedJaxpr(jaxpr_known_</span><span class="s3">, </span><span class="s1">())</span>
  <span class="s3">del </span><span class="s1">jaxpr_known_</span>
  <span class="s0"># We use `fwds_known` below when forming the output of scanning jaxpr_known.</span>

  <span class="s0"># Run the known part of the scan (if it has any outputs or effects).</span>
  <span class="s1">known_inputs = (list(jaxpr_known_consts) +</span>
                  <span class="s1">[t.pval.get_known() </span><span class="s3">for </span><span class="s1">t </span><span class="s3">in </span><span class="s1">tracers[num_consts:]</span>
                   <span class="s3">if </span><span class="s1">t.pval.is_known()])</span>
  <span class="s3">if not </span><span class="s1">jaxpr_known.out_avals </span><span class="s3">and not </span><span class="s1">jaxpr_known.effects:</span>
    <span class="s1">out_known = []</span>
  <span class="s3">else</span><span class="s1">:</span>
    <span class="s1">linear_known = [</span><span class="s3">False</span><span class="s1">] * len(known_inputs)  </span><span class="s0"># conservative!</span>
    <span class="s1">out_known = scan_p.bind(</span>
        <span class="s1">*known_inputs</span><span class="s3">, </span><span class="s1">reverse=reverse</span><span class="s3">, </span><span class="s1">length=length</span><span class="s3">, </span><span class="s1">jaxpr=jaxpr_known</span><span class="s3">,</span>
        <span class="s1">num_consts=len(jaxpr_known_consts)</span><span class="s3">, </span><span class="s1">num_carry=num_carry - sum(carry_uk)</span><span class="s3">,</span>
        <span class="s1">linear=tuple(linear_known)</span><span class="s3">, </span><span class="s1">unroll=unroll)</span>
    <span class="s3">del </span><span class="s1">linear_known</span>
  <span class="s0"># Complete the known output by filling in forwarded values using fwds_known.</span>
  <span class="s1">out_known_iter = iter(out_known)</span>
  <span class="s1">out_known = [next(out_known_iter) </span><span class="s3">if </span><span class="s1">f </span><span class="s3">is None</span>
               <span class="s3">else </span><span class="s1">_maybe_put(known_inputs[f]) </span><span class="s3">for </span><span class="s1">f </span><span class="s3">in </span><span class="s1">fwds_known]</span>
  <span class="s3">assert </span><span class="s1">next(out_known_iter</span><span class="s3">, None</span><span class="s1">) </span><span class="s3">is None</span>
  <span class="s3">del </span><span class="s1">known_inputs</span><span class="s3">, </span><span class="s1">out_known_iter</span>

  <span class="s0"># Split known outputs from residuals.</span>
  <span class="s1">out_known</span><span class="s3">, </span><span class="s1">extensive_res = split_list(out_known</span><span class="s3">, </span><span class="s1">[len(out_uk) - sum(out_uk)])</span>
  <span class="s3">assert </span><span class="s1">len(intensive_res) + len(extensive_res) == num_res</span>

  <span class="s0"># Create input tracers for jaxpr_unknown bind.</span>
  <span class="s1">unknown_inputs = [t </span><span class="s3">for </span><span class="s1">t </span><span class="s3">in </span><span class="s1">tracers </span><span class="s3">if not </span><span class="s1">t.pval.is_known()]</span>
  <span class="s1">intensive_res = _map(trace.new_instantiated_const</span><span class="s3">, </span><span class="s1">intensive_res)</span>
  <span class="s1">extensive_res = _map(trace.new_instantiated_const</span><span class="s3">, </span><span class="s1">extensive_res)</span>
  <span class="s0"># Create output tracers for jaxpr_unknown bind, adapting extensive shapes.</span>
  <span class="s1">carry_avals</span><span class="s3">, </span><span class="s1">y_avals = split_list(jaxpr_unknown.out_avals</span><span class="s3">, </span><span class="s1">[sum(carry_uk)])</span>
  <span class="s1">ys_avals = [core.unmapped_aval(length</span><span class="s3">, </span><span class="s1">core.no_axis_name</span><span class="s3">, </span><span class="s5">0</span><span class="s3">, </span><span class="s1">y_aval)</span>
              <span class="s3">for </span><span class="s1">y_aval </span><span class="s3">in </span><span class="s1">y_avals]</span>
  <span class="s1">out_tracers = [pe.JaxprTracer(trace</span><span class="s3">, </span><span class="s1">pe.PartialVal.unknown(a)</span><span class="s3">, None</span><span class="s1">)</span>
                 <span class="s3">for </span><span class="s1">a </span><span class="s3">in </span><span class="s1">itertools.chain(carry_avals</span><span class="s3">, </span><span class="s1">ys_avals)]</span>
  <span class="s3">del </span><span class="s1">carry_avals</span><span class="s3">, </span><span class="s1">y_avals</span>
  <span class="s0"># Create equation.</span>
  <span class="s1">linear_unknown = tuple([</span><span class="s3">False</span><span class="s1">] * len(intensive_res) +</span>
                         <span class="s1">[l </span><span class="s3">for </span><span class="s1">l</span><span class="s3">, </span><span class="s1">uk </span><span class="s3">in </span><span class="s1">zip(linear</span><span class="s3">, </span><span class="s1">unknowns) </span><span class="s3">if </span><span class="s1">uk] +</span>
                         <span class="s1">[</span><span class="s3">False</span><span class="s1">] * len(extensive_res))</span>
  <span class="s1">name_stack = source_info_util.current_name_stack()[len(trace.name_stack):]</span>
  <span class="s1">source = source_info_util.current().replace(name_stack=name_stack)</span>
  <span class="s3">assert </span><span class="s1">len(out_tracers) == len(jaxpr_unknown.out_avals)</span>
  <span class="s1">eqn = pe.new_eqn_recipe([*intensive_res</span><span class="s3">, </span><span class="s1">*unknown_inputs</span><span class="s3">, </span><span class="s1">*extensive_res]</span><span class="s3">,</span>
                          <span class="s1">out_tracers</span><span class="s3">, </span><span class="s1">scan_p</span><span class="s3">,</span>
                          <span class="s1">dict(reverse=reverse</span><span class="s3">, </span><span class="s1">length=length</span><span class="s3">, </span><span class="s1">unroll=unroll</span><span class="s3">,</span>
                               <span class="s1">jaxpr=jaxpr_unknown</span><span class="s3">, </span><span class="s1">linear=linear_unknown</span><span class="s3">,</span>
                               <span class="s1">num_consts=len(intensive_res) + sum(const_uk)</span><span class="s3">,</span>
                               <span class="s1">num_carry=sum(carry_uk))</span><span class="s3">,</span>
                          <span class="s1">jaxpr_unknown.effects</span><span class="s3">, </span><span class="s1">source)</span>
  <span class="s3">for </span><span class="s1">t </span><span class="s3">in </span><span class="s1">out_tracers: t.recipe = eqn</span>

  <span class="s0"># Merge known and unknown outputs into final result.</span>
  <span class="s3">return </span><span class="s1">util.merge_lists(out_uk</span><span class="s3">, </span><span class="s1">out_known</span><span class="s3">, </span><span class="s1">out_tracers)</span>

<span class="s3">def </span><span class="s1">_maybe_put(x):</span>
  <span class="s3">if </span><span class="s1">isinstance(x</span><span class="s3">, </span><span class="s1">np.ndarray):</span>
    <span class="s3">return </span><span class="s1">jax.device_put(x</span><span class="s3">, </span><span class="s1">jax.devices(</span><span class="s4">'cpu'</span><span class="s1">)[</span><span class="s5">0</span><span class="s1">])</span>
  <span class="s3">else</span><span class="s1">:</span>
    <span class="s3">return </span><span class="s1">x</span>

<span class="s3">def </span><span class="s1">_scan_transpose(reduce_axes</span><span class="s3">, </span><span class="s1">cts</span><span class="s3">, </span><span class="s1">*args</span><span class="s3">, </span><span class="s1">reverse</span><span class="s3">, </span><span class="s1">length</span><span class="s3">, </span><span class="s1">num_consts</span><span class="s3">,</span>
                    <span class="s1">num_carry</span><span class="s3">, </span><span class="s1">jaxpr</span><span class="s3">, </span><span class="s1">linear</span><span class="s3">, </span><span class="s1">unroll):</span>
  <span class="s0"># we've only implemented transposing scans with specific lin/nonlin patterns</span>
  <span class="s1">consts_lin</span><span class="s3">, </span><span class="s1">init_lin</span><span class="s3">, </span><span class="s1">xs_lin = split_list(linear</span><span class="s3">, </span><span class="s1">[num_consts</span><span class="s3">, </span><span class="s1">num_carry])</span>
  <span class="s1">num_ires = len(consts_lin) - sum(consts_lin)</span>
  <span class="s1">num_eres = len(xs_lin) - sum(xs_lin)</span>
  <span class="s3">if </span><span class="s1">consts_lin != [</span><span class="s3">False</span><span class="s1">] * num_ires + [</span><span class="s3">True</span><span class="s1">] * (len(consts_lin) - num_ires):</span>
    <span class="s3">raise </span><span class="s1">NotImplementedError</span>
  <span class="s3">if </span><span class="s1">xs_lin != [</span><span class="s3">True</span><span class="s1">] * (len(xs_lin) - num_eres) + [</span><span class="s3">False</span><span class="s1">] * num_eres:</span>
    <span class="s3">raise </span><span class="s1">NotImplementedError</span>
  <span class="s3">if not </span><span class="s1">all(init_lin):</span>
    <span class="s3">pass  </span><span class="s0"># TODO(mattjj): error check https://github.com/google/jax/issues/1963</span>

  <span class="s1">consts</span><span class="s3">, </span><span class="s1">_</span><span class="s3">, </span><span class="s1">xs = split_list(args</span><span class="s3">, </span><span class="s1">[num_consts</span><span class="s3">, </span><span class="s1">num_carry])</span>
  <span class="s1">ires</span><span class="s3">, </span><span class="s1">_ = split_list(consts</span><span class="s3">, </span><span class="s1">[num_ires])</span>
  <span class="s1">_</span><span class="s3">, </span><span class="s1">eres = split_list(xs</span><span class="s3">, </span><span class="s1">[sum(xs_lin)])</span>
  <span class="s3">assert not </span><span class="s1">any(ad.is_undefined_primal(r) </span><span class="s3">for </span><span class="s1">r </span><span class="s3">in </span><span class="s1">ires)</span>
  <span class="s3">assert not </span><span class="s1">any(ad.is_undefined_primal(r) </span><span class="s3">for </span><span class="s1">r </span><span class="s3">in </span><span class="s1">eres)</span>

  <span class="s1">carry_avals</span><span class="s3">, </span><span class="s1">y_avals = split_list(jaxpr.out_avals</span><span class="s3">, </span><span class="s1">[num_carry])</span>
  <span class="s1">ys_avals = _map(partial(_prepend_dim_to_aval</span><span class="s3">, </span><span class="s1">length)</span><span class="s3">, </span><span class="s1">y_avals)</span>
  <span class="s1">ct_carry</span><span class="s3">, </span><span class="s1">ct_ys = split_list(cts</span><span class="s3">, </span><span class="s1">[num_carry])</span>
  <span class="s1">ct_carry = _map(ad.instantiate_zeros_aval</span><span class="s3">, </span><span class="s1">carry_avals</span><span class="s3">, </span><span class="s1">ct_carry)</span>
  <span class="s1">ct_ys = _map(ad.instantiate_zeros_aval</span><span class="s3">, </span><span class="s1">ys_avals</span><span class="s3">, </span><span class="s1">ct_ys)</span>
  <span class="s1">ct_consts = _map(ad_util.zeros_like_aval</span><span class="s3">, </span><span class="s1">jaxpr.in_avals[num_ires:num_consts])</span>

  <span class="s0">#       jaxpr :: [ires, T d] -&gt; [T c] -&gt; [T a, eres] -&gt; ([T c], [T b])</span>
  <span class="s0"># jaxpr_trans :: [ires] -&gt; [CT d, CT c] -&gt; [CT b, eres] -&gt; ([CT d, CT c], [CT a])</span>
  <span class="s1">jaxpr_trans = _transpose_scan_jaxpr(</span>
      <span class="s1">num_ires</span><span class="s3">, </span><span class="s1">num_consts - num_ires</span><span class="s3">, </span><span class="s1">num_eres</span><span class="s3">, </span><span class="s1">jaxpr</span><span class="s3">, </span><span class="s1">reduce_axes)</span>
  <span class="s1">linear_trans = ([</span><span class="s3">False</span><span class="s1">] * num_ires +</span>
                  <span class="s1">[</span><span class="s3">True</span><span class="s1">] * (len(ct_consts) + len(ct_carry) + len(ct_ys)) +</span>
                  <span class="s1">[</span><span class="s3">False</span><span class="s1">] * num_eres)</span>

  <span class="s1">outs = scan_p.bind(</span>
      <span class="s1">*(ires + ct_consts + ct_carry + ct_ys + eres)</span><span class="s3">, </span><span class="s1">reverse=</span><span class="s3">not </span><span class="s1">reverse</span><span class="s3">,</span>
      <span class="s1">length=length</span><span class="s3">, </span><span class="s1">jaxpr=jaxpr_trans</span><span class="s3">, </span><span class="s1">num_consts=num_ires</span><span class="s3">,</span>
      <span class="s1">num_carry=num_consts-num_ires+num_carry</span><span class="s3">, </span><span class="s1">linear=tuple(linear_trans)</span><span class="s3">,</span>
      <span class="s1">unroll=unroll)</span>
  <span class="s1">ct_consts</span><span class="s3">, </span><span class="s1">ct_init</span><span class="s3">, </span><span class="s1">ct_xs = split_list(outs</span><span class="s3">, </span><span class="s1">[num_consts - num_ires</span><span class="s3">, </span><span class="s1">num_carry])</span>
  <span class="s3">return </span><span class="s1">[</span><span class="s3">None</span><span class="s1">] * num_ires + ct_consts + ct_init + ct_xs + [</span><span class="s3">None</span><span class="s1">] * num_eres</span>

<span class="s0"># transpose_scan_jaxpr :: ([res1, c, a, res2] -&gt; b)</span>
<span class="s0">#                         -&gt; ([res1, CT c, CT b, res2] -&gt; [CT c, CT a])</span>
<span class="s3">def </span><span class="s1">_transpose_scan_jaxpr(num_res1</span><span class="s3">, </span><span class="s1">num_c</span><span class="s3">, </span><span class="s1">num_res2</span><span class="s3">, </span><span class="s1">jaxpr</span><span class="s3">, </span><span class="s1">reduce_axes):</span>
  <span class="s1">num_a = len(jaxpr.in_avals) - num_res1 - num_c - num_res2</span>
  <span class="s0"># TODO: allow input cotangent avals to be batched relative to jaxpr.in_avals</span>
  <span class="s0"># if an axis isn't reduced</span>
  <span class="s1">res1_avals</span><span class="s3">, </span><span class="s1">c_avals</span><span class="s3">, </span><span class="s1">a_avals</span><span class="s3">, </span><span class="s1">res2_avals = split_list(</span>
      <span class="s1">jaxpr.in_avals</span><span class="s3">, </span><span class="s1">[num_res1</span><span class="s3">, </span><span class="s1">num_c</span><span class="s3">, </span><span class="s1">num_a])</span>
  <span class="s1">num_b = len(jaxpr.out_avals)</span>
  <span class="s1">b_avals = list(jaxpr.out_avals)</span>

  <span class="s1">@lu.wrap_init</span>
  <span class="s3">def </span><span class="s1">transposed(*res1_cbar_bbar_res2):</span>
    <span class="s1">res1</span><span class="s3">, </span><span class="s1">c_bar</span><span class="s3">, </span><span class="s1">b_bar</span><span class="s3">, </span><span class="s1">res2 = split_list(</span>
        <span class="s1">res1_cbar_bbar_res2</span><span class="s3">, </span><span class="s1">[num_res1</span><span class="s3">, </span><span class="s1">num_c</span><span class="s3">, </span><span class="s1">num_b])</span>
    <span class="s1">primals = (res1 + [ad.UndefinedPrimal(aval) </span><span class="s3">for </span><span class="s1">aval </span><span class="s3">in </span><span class="s1">c_avals] +</span>
               <span class="s1">[ad.UndefinedPrimal(aval) </span><span class="s3">for </span><span class="s1">aval </span><span class="s3">in </span><span class="s1">a_avals] + res2)</span>
    <span class="s1">cbar_abar = ad.backward_pass(jaxpr.jaxpr</span><span class="s3">, </span><span class="s1">reduce_axes</span><span class="s3">, False, </span><span class="s1">jaxpr.consts</span><span class="s3">,</span>
                                 <span class="s1">primals</span><span class="s3">, </span><span class="s1">b_bar)</span>
    <span class="s1">_</span><span class="s3">, </span><span class="s1">new_c_bar</span><span class="s3">, </span><span class="s1">a_bar</span><span class="s3">, </span><span class="s1">_ = split_list(cbar_abar</span><span class="s3">, </span><span class="s1">[num_res1</span><span class="s3">, </span><span class="s1">num_c</span><span class="s3">, </span><span class="s1">num_a])</span>
    <span class="s1">a_bar = _map(ad.instantiate_zeros_aval</span><span class="s3">, </span><span class="s1">a_avals</span><span class="s3">, </span><span class="s1">a_bar)</span>
    <span class="s1">c_bar = _map(ad.instantiate_zeros_aval</span><span class="s3">, </span><span class="s1">c_avals</span><span class="s3">,</span>
                <span class="s1">_map(ad.add_tangents</span><span class="s3">, </span><span class="s1">c_bar</span><span class="s3">, </span><span class="s1">new_c_bar))</span>
    <span class="s3">return </span><span class="s1">c_bar + a_bar</span>
  <span class="s3">return </span><span class="s1">_make_closed_jaxpr(transposed</span><span class="s3">, </span><span class="s1">res1_avals + c_avals + b_avals + res2_avals)</span>


<span class="s3">def </span><span class="s1">_scan_batching_rule(spmd_axis_name</span><span class="s3">, </span><span class="s1">axis_size</span><span class="s3">, </span><span class="s1">axis_name</span><span class="s3">, </span><span class="s1">main_type</span><span class="s3">, </span><span class="s1">args</span><span class="s3">,</span>
                        <span class="s1">dims</span><span class="s3">, </span><span class="s1">reverse</span><span class="s3">, </span><span class="s1">length</span><span class="s3">,</span>
                        <span class="s1">jaxpr</span><span class="s3">, </span><span class="s1">num_consts</span><span class="s3">, </span><span class="s1">num_carry</span><span class="s3">, </span><span class="s1">linear</span><span class="s3">, </span><span class="s1">unroll):</span>
  <span class="s1">num_ys = len(jaxpr.out_avals) - num_carry</span>
  <span class="s1">orig_batched = [d </span><span class="s3">is not </span><span class="s1">batching.not_mapped </span><span class="s3">for </span><span class="s1">d </span><span class="s3">in </span><span class="s1">dims]</span>
  <span class="s1">const_batched</span><span class="s3">, </span><span class="s1">init_batched</span><span class="s3">, </span><span class="s1">xs_batched = split_list(orig_batched</span><span class="s3">, </span><span class="s1">[num_consts</span><span class="s3">, </span><span class="s1">num_carry])</span>

  <span class="s0"># Fixpoint computation of which carry are batched: either</span>
  <span class="s0"># batched from init, or the carry out is batched. Each iteration promotes</span>
  <span class="s0"># at least one carry to batched. We need at most len(carry) iterations,</span>
  <span class="s0"># but we need one last iteration to prepare the jaxpr based on the final</span>
  <span class="s0"># carry_batched.</span>
  <span class="s1">carry_batched = init_batched</span>
  <span class="s3">for </span><span class="s1">_ </span><span class="s3">in </span><span class="s1">range(</span><span class="s5">1 </span><span class="s1">+ len(carry_batched)):</span>
    <span class="s1">batched = const_batched + carry_batched + xs_batched</span>
    <span class="s1">jaxpr_batched</span><span class="s3">, </span><span class="s1">batched_out = batching.batch_jaxpr(</span>
        <span class="s1">jaxpr</span><span class="s3">, </span><span class="s1">axis_size</span><span class="s3">, </span><span class="s1">batched</span><span class="s3">,</span>
        <span class="s1">instantiate=carry_batched + [</span><span class="s3">False</span><span class="s1">] * num_ys</span><span class="s3">,</span>
        <span class="s1">axis_name=axis_name</span><span class="s3">,</span>
        <span class="s1">spmd_axis_name=spmd_axis_name</span><span class="s3">,</span>
        <span class="s1">main_type=main_type)</span>
    <span class="s1">carry_batched_out</span><span class="s3">, </span><span class="s1">ys_batched = batched_out[:num_carry]</span><span class="s3">, </span><span class="s1">batched_out[num_carry:]</span>
    <span class="s3">if </span><span class="s1">carry_batched_out == carry_batched:</span>
      <span class="s3">break</span>
    <span class="s3">else</span><span class="s1">:</span>
      <span class="s1">carry_batched = _map(operator.or_</span><span class="s3">, </span><span class="s1">carry_batched</span><span class="s3">, </span><span class="s1">carry_batched_out)</span>
  <span class="s3">else</span><span class="s1">:</span>
    <span class="s3">assert False, </span><span class="s4">&quot;Fixpoint not reached&quot;</span>

  <span class="s1">consts</span><span class="s3">, </span><span class="s1">init</span><span class="s3">, </span><span class="s1">xs = split_list(args</span><span class="s3">, </span><span class="s1">[num_consts</span><span class="s3">, </span><span class="s1">num_carry])</span>
  <span class="s1">consts_bdims</span><span class="s3">, </span><span class="s1">init_bdims</span><span class="s3">, </span><span class="s1">xs_bdims = split_list(dims</span><span class="s3">, </span><span class="s1">[num_consts</span><span class="s3">, </span><span class="s1">num_carry])</span>
  <span class="s1">new_consts = [batching.moveaxis(x</span><span class="s3">, </span><span class="s1">d</span><span class="s3">, </span><span class="s5">0</span><span class="s1">) </span><span class="s3">if </span><span class="s1">d </span><span class="s3">is not </span><span class="s1">batching.not_mapped </span><span class="s3">and </span><span class="s1">d != </span><span class="s5">0</span>
                <span class="s3">else </span><span class="s1">x </span><span class="s3">for </span><span class="s1">x</span><span class="s3">, </span><span class="s1">d </span><span class="s3">in </span><span class="s1">zip(consts</span><span class="s3">, </span><span class="s1">consts_bdims)]</span>
  <span class="s1">new_init = [batching.broadcast(x</span><span class="s3">, </span><span class="s1">axis_size</span><span class="s3">, </span><span class="s5">0</span><span class="s1">) </span><span class="s3">if </span><span class="s1">now_batched </span><span class="s3">and not </span><span class="s1">was_batched</span>
              <span class="s3">else </span><span class="s1">batching.moveaxis(x</span><span class="s3">, </span><span class="s1">d</span><span class="s3">, </span><span class="s5">0</span><span class="s1">) </span><span class="s3">if </span><span class="s1">now_batched </span><span class="s3">else </span><span class="s1">x</span>
              <span class="s3">for </span><span class="s1">x</span><span class="s3">, </span><span class="s1">d</span><span class="s3">, </span><span class="s1">was_batched</span><span class="s3">, </span><span class="s1">now_batched </span><span class="s3">in</span>
              <span class="s1">zip(init</span><span class="s3">, </span><span class="s1">init_bdims</span><span class="s3">, </span><span class="s1">init_batched</span><span class="s3">, </span><span class="s1">carry_batched)]</span>
  <span class="s1">new_xs = [batching.moveaxis(x</span><span class="s3">, </span><span class="s1">d</span><span class="s3">, </span><span class="s5">1</span><span class="s1">) </span><span class="s3">if </span><span class="s1">d </span><span class="s3">is not </span><span class="s1">batching.not_mapped </span><span class="s3">and </span><span class="s1">d != </span><span class="s5">1</span>
            <span class="s3">else </span><span class="s1">x </span><span class="s3">for </span><span class="s1">x</span><span class="s3">, </span><span class="s1">d </span><span class="s3">in </span><span class="s1">zip(xs</span><span class="s3">, </span><span class="s1">xs_bdims)]</span>
  <span class="s1">new_args = new_consts + new_init + new_xs</span>

  <span class="s1">outs = scan_p.bind(</span>
      <span class="s1">*new_args</span><span class="s3">, </span><span class="s1">reverse=reverse</span><span class="s3">, </span><span class="s1">length=length</span><span class="s3">, </span><span class="s1">jaxpr=jaxpr_batched</span><span class="s3">,</span>
      <span class="s1">num_consts=num_consts</span><span class="s3">, </span><span class="s1">num_carry=num_carry</span><span class="s3">, </span><span class="s1">linear=linear</span><span class="s3">, </span><span class="s1">unroll=unroll)</span>
  <span class="s1">carry_bdims = [</span><span class="s5">0 </span><span class="s3">if </span><span class="s1">b </span><span class="s3">else </span><span class="s1">batching.not_mapped </span><span class="s3">for </span><span class="s1">b </span><span class="s3">in </span><span class="s1">carry_batched]</span>
  <span class="s1">ys_bdims = [</span><span class="s5">1 </span><span class="s3">if </span><span class="s1">b </span><span class="s3">else </span><span class="s1">batching.not_mapped </span><span class="s3">for </span><span class="s1">b </span><span class="s3">in </span><span class="s1">ys_batched]</span>
  <span class="s3">return </span><span class="s1">outs</span><span class="s3">, </span><span class="s1">carry_bdims + ys_bdims</span>

<span class="s3">def </span><span class="s1">_scan_padding_rule(in_avals</span><span class="s3">, </span><span class="s1">out_avals</span><span class="s3">, </span><span class="s1">*args</span><span class="s3">, </span><span class="s1">jaxpr</span><span class="s3">, </span><span class="s1">**params):</span>
  <span class="s1">padded_jaxpr = core.ClosedJaxpr(*pe.pad_jaxpr(jaxpr.jaxpr</span><span class="s3">, </span><span class="s1">jaxpr.consts))</span>
  <span class="s3">return </span><span class="s1">scan_p.bind(*args</span><span class="s3">, </span><span class="s1">jaxpr=padded_jaxpr</span><span class="s3">, </span><span class="s1">**params)</span>

<span class="s3">def </span><span class="s1">_scan_dce_rule(used_outputs: List[bool]</span><span class="s3">, </span><span class="s1">eqn: core.JaxprEqn</span>
                   <span class="s1">) -&gt; Tuple[List[bool]</span><span class="s3">, </span><span class="s1">core.JaxprEqn]:</span>
  <span class="s1">jaxpr = eqn.params[</span><span class="s4">'jaxpr'</span><span class="s1">]</span>
  <span class="s1">num_consts</span><span class="s3">, </span><span class="s1">num_carry = eqn.params[</span><span class="s4">'num_consts'</span><span class="s1">]</span><span class="s3">, </span><span class="s1">eqn.params[</span><span class="s4">'num_carry'</span><span class="s1">]</span>
  <span class="s1">num_xs = len(jaxpr.in_avals) - num_consts - num_carry</span>
  <span class="s1">used_carry_out</span><span class="s3">, </span><span class="s1">used_extensive_out = split_list(used_outputs</span><span class="s3">, </span><span class="s1">[num_carry])</span>
  <span class="s3">for </span><span class="s1">i </span><span class="s3">in </span><span class="s1">range(</span><span class="s5">1 </span><span class="s1">+ num_carry):</span>
    <span class="s1">used_outputs = used_carry_out + used_extensive_out</span>
    <span class="s1">jaxpr_dce</span><span class="s3">, </span><span class="s1">used_inputs = pe.dce_jaxpr(</span>
        <span class="s1">jaxpr.jaxpr</span><span class="s3">, </span><span class="s1">used_outputs</span><span class="s3">,</span>
        <span class="s1">instantiate=[</span><span class="s3">False</span><span class="s1">] * num_consts + used_carry_out + [</span><span class="s3">False</span><span class="s1">] * num_xs)</span>
    <span class="s1">used_consts</span><span class="s3">, </span><span class="s1">used_carry_in</span><span class="s3">, </span><span class="s1">used_extensive_in = \</span>
        <span class="s1">split_list(used_inputs</span><span class="s3">, </span><span class="s1">[num_consts</span><span class="s3">, </span><span class="s1">num_carry])</span>
    <span class="s3">if </span><span class="s1">list(used_carry_in) == list(used_carry_out):</span>
      <span class="s3">break</span>
    <span class="s3">else</span><span class="s1">:</span>
      <span class="s1">used_carry_out = _map(operator.or_</span><span class="s3">, </span><span class="s1">used_carry_out</span><span class="s3">, </span><span class="s1">used_carry_in)</span>
  <span class="s3">else</span><span class="s1">:</span>
    <span class="s3">assert False, </span><span class="s4">&quot;Fixpoint not reached&quot;</span>
  <span class="s3">if </span><span class="s1">config.jax_enable_checks: core.check_jaxpr(jaxpr.jaxpr)</span>

  <span class="s1">new_linear = [l </span><span class="s3">for </span><span class="s1">l</span><span class="s3">, </span><span class="s1">u </span><span class="s3">in </span><span class="s1">zip(eqn.params[</span><span class="s4">'linear'</span><span class="s1">]</span><span class="s3">, </span><span class="s1">used_inputs) </span><span class="s3">if </span><span class="s1">u]</span>
  <span class="s1">new_params = dict(eqn.params</span><span class="s3">, </span><span class="s1">num_consts=sum(used_consts)</span><span class="s3">,</span>
                    <span class="s1">num_carry=sum(used_carry_in)</span><span class="s3">, </span><span class="s1">linear=tuple(new_linear)</span><span class="s3">,</span>
                    <span class="s1">jaxpr=core.ClosedJaxpr(jaxpr_dce</span><span class="s3">, </span><span class="s1">jaxpr.consts))</span>
  <span class="s0"># TODO(mattjj,sharadmv): don't assume effects are never DCE'd?</span>
  <span class="s1">new_eqn = pe.new_jaxpr_eqn(</span>
      <span class="s1">[v </span><span class="s3">for </span><span class="s1">v</span><span class="s3">, </span><span class="s1">used </span><span class="s3">in </span><span class="s1">zip(eqn.invars</span><span class="s3">, </span><span class="s1">used_inputs) </span><span class="s3">if </span><span class="s1">used]</span><span class="s3">,</span>
      <span class="s1">[v </span><span class="s3">for </span><span class="s1">v</span><span class="s3">, </span><span class="s1">used </span><span class="s3">in </span><span class="s1">zip(eqn.outvars</span><span class="s3">, </span><span class="s1">used_outputs) </span><span class="s3">if </span><span class="s1">used]</span><span class="s3">,</span>
      <span class="s1">eqn.primitive</span><span class="s3">, </span><span class="s1">new_params</span><span class="s3">, </span><span class="s1">eqn.effects</span><span class="s3">, </span><span class="s1">eqn.source_info)</span>
  <span class="s3">assert </span><span class="s1">len(new_eqn.invars ) == len(new_params[</span><span class="s4">'jaxpr'</span><span class="s1">].in_avals )</span>
  <span class="s3">assert </span><span class="s1">len(new_eqn.outvars) == len(new_params[</span><span class="s4">'jaxpr'</span><span class="s1">].out_avals)</span>
  <span class="s3">return </span><span class="s1">used_inputs</span><span class="s3">, </span><span class="s1">new_eqn</span>

<span class="s0"># TODO(mattjj): de-duplicate code with _scan_partial_eval</span>
<span class="s3">def </span><span class="s1">_scan_partial_eval_custom(saveable</span><span class="s3">, </span><span class="s1">unks_in</span><span class="s3">, </span><span class="s1">inst_in</span><span class="s3">, </span><span class="s1">eqn):</span>
  <span class="s1">jaxpr = eqn.params[</span><span class="s4">'jaxpr'</span><span class="s1">]</span>
  <span class="s1">num_consts</span><span class="s3">, </span><span class="s1">num_carry = eqn.params[</span><span class="s4">'num_consts'</span><span class="s1">]</span><span class="s3">, </span><span class="s1">eqn.params[</span><span class="s4">'num_carry'</span><span class="s1">]</span>
  <span class="s1">num_ys = len(jaxpr.out_avals) - num_carry</span>

  <span class="s0"># Fixpoint (trivial on 'inst_in', since we might as well make all inputs</span>
  <span class="s0"># available as DCE can subsequently prune any unused ones)</span>
  <span class="s1">const_uk</span><span class="s3">, </span><span class="s1">carry_uk</span><span class="s3">, </span><span class="s1">xs_uk = split_list(unks_in</span><span class="s3">, </span><span class="s1">[num_consts</span><span class="s3">, </span><span class="s1">num_carry])</span>
  <span class="s3">for </span><span class="s1">_ </span><span class="s3">in </span><span class="s1">range(</span><span class="s5">1 </span><span class="s1">+ len(carry_uk)):</span>
    <span class="s1">unks_in = const_uk   + carry_uk   + xs_uk</span>
    <span class="s1">jaxpr_known_</span><span class="s3">, </span><span class="s1">jaxpr_staged_</span><span class="s3">, </span><span class="s1">unks_out</span><span class="s3">, </span><span class="s1">inst_out</span><span class="s3">, </span><span class="s1">num_res = \</span>
        <span class="s1">pe.partial_eval_jaxpr_custom(</span>
            <span class="s1">jaxpr.jaxpr</span><span class="s3">, </span><span class="s1">in_unknowns=unks_in</span><span class="s3">, </span><span class="s1">in_inst=</span><span class="s3">True,</span>
            <span class="s1">ensure_out_unknowns=carry_uk + [</span><span class="s3">False</span><span class="s1">] * num_ys</span><span class="s3">,</span>
            <span class="s1">ensure_out_inst=</span><span class="s3">True, </span><span class="s1">saveable=saveable)</span>
    <span class="s1">carry_uk_out</span><span class="s3">, </span><span class="s1">ys_uk = split_list(unks_out</span><span class="s3">, </span><span class="s1">[num_carry])</span>
    <span class="s3">if </span><span class="s1">carry_uk_out == carry_uk:</span>
      <span class="s3">break</span>
    <span class="s3">else</span><span class="s1">:</span>
      <span class="s1">carry_uk = _map(operator.or_</span><span class="s3">, </span><span class="s1">carry_uk</span><span class="s3">, </span><span class="s1">carry_uk_out)</span>
  <span class="s3">else</span><span class="s1">:</span>
    <span class="s3">assert False, </span><span class="s4">&quot;Fixpoint not reached&quot;</span>
  <span class="s1">jaxpr_known  = core.ClosedJaxpr(jaxpr_known_ </span><span class="s3">, </span><span class="s1">jaxpr.consts)</span>
  <span class="s1">jaxpr_staged = core.ClosedJaxpr(jaxpr_staged_</span><span class="s3">, </span><span class="s1">jaxpr.consts)</span>

  <span class="s0"># Move all residual binders to the back of jaxpr_staged so they're extensive.</span>
  <span class="s0"># TODO(mattjj): make jaxpr_staged only take instantiated inputs</span>
  <span class="s1">res_avals = jaxpr_staged.in_avals[:num_res]</span>
  <span class="s1">jaxpr_staged = pe.move_binders_to_back(</span>
      <span class="s1">jaxpr_staged</span><span class="s3">, </span><span class="s1">[</span><span class="s3">True</span><span class="s1">] * num_res + [</span><span class="s3">False</span><span class="s1">] * len(jaxpr.in_avals))</span>

  <span class="s0"># Instantiate all inputs (b/c jaxpr_staged takes all inputs, corresponding to</span>
  <span class="s0"># passing in_inst argument to partial_eval_jaxpr_custom above).</span>
  <span class="s1">new_inst = [x </span><span class="s3">for </span><span class="s1">x</span><span class="s3">, </span><span class="s1">inst </span><span class="s3">in </span><span class="s1">zip(eqn.invars</span><span class="s3">, </span><span class="s1">inst_in)</span>
              <span class="s3">if </span><span class="s1">type(x) </span><span class="s3">is </span><span class="s1">core.Var </span><span class="s3">and not </span><span class="s1">inst]</span>
  <span class="s1">inst_in = [</span><span class="s3">True</span><span class="s1">] * len(inst_in)</span>

  <span class="s0"># As an optimization, hoist loop-invariant residuals out of the loop rather</span>
  <span class="s0"># than using extensive outputs for them. See _scan_partial_eval for comments.</span>
  <span class="s1">num_const_known = len(const_uk) - sum(const_uk)</span>
  <span class="s1">num_carry_known = len(carry_uk) - sum(carry_uk)</span>
  <span class="s1">num_xs_known    = len(   xs_uk) - sum(   xs_uk)</span>
  <span class="s1">jaxpr_known_hoist</span><span class="s3">, </span><span class="s1">jaxpr_known_loop</span><span class="s3">, </span><span class="s1">loop_dep</span><span class="s3">, </span><span class="s1">consts_known_lp_avals = \</span>
      <span class="s1">pe.partial_eval_jaxpr_nounits(</span>
          <span class="s1">jaxpr_known</span><span class="s3">,</span>
          <span class="s1">[</span><span class="s3">False</span><span class="s1">] * num_const_known + [</span><span class="s3">True</span><span class="s1">] * (num_carry_known + num_xs_known)</span><span class="s3">,</span>
          <span class="s1">[</span><span class="s3">True</span><span class="s1">] * (len(unks_out) - sum(unks_out)) + [</span><span class="s3">False</span><span class="s1">] * num_res)</span>
  <span class="s0"># jaxpr_known_hoist produces intensive residuals followed by the constants for</span>
  <span class="s0"># jaxpr_known_loop. We adjust jaxpr_staged to accept intensive res as consts.</span>
  <span class="s1">_</span><span class="s3">, </span><span class="s1">loop_dep_res = split_list(loop_dep</span><span class="s3">, </span><span class="s1">[len(loop_dep) - num_res])</span>
  <span class="s1">jaxpr_staged = pe.move_binders_to_front(</span>
      <span class="s1">jaxpr_staged</span><span class="s3">, </span><span class="s1">[</span><span class="s3">False</span><span class="s1">] * sum(inst_in) + _map(operator.not_</span><span class="s3">, </span><span class="s1">loop_dep_res))</span>
  <span class="s1">num_intensive_res = len(loop_dep_res) - sum(loop_dep_res)</span>
  <span class="s3">del </span><span class="s1">loop_dep</span><span class="s3">, </span><span class="s1">num_carry_known</span><span class="s3">, </span><span class="s1">num_xs_known</span><span class="s3">, </span><span class="s1">const_uk</span>

  <span class="s0"># Create residual variables.</span>
  <span class="s1">intensive_avals</span><span class="s3">, </span><span class="s1">ext_avals_mapped = partition_list(loop_dep_res</span><span class="s3">, </span><span class="s1">res_avals)</span>
  <span class="s1">ext_avals = [core.unmapped_aval(eqn.params[</span><span class="s4">'length'</span><span class="s1">]</span><span class="s3">, </span><span class="s1">core.no_axis_name</span><span class="s3">, </span><span class="s5">0</span><span class="s3">, </span><span class="s1">a)</span>
               <span class="s3">for </span><span class="s1">a </span><span class="s3">in </span><span class="s1">ext_avals_mapped]</span>
  <span class="s1">newvar = core.gensym()</span>
  <span class="s1">intensive_res = _map(newvar</span><span class="s3">, </span><span class="s1">intensive_avals)</span>
  <span class="s1">extensive_res = _map(newvar</span><span class="s3">, </span><span class="s1">ext_avals)</span>

  <span class="s0"># Create known eqn, which is a call_p combining evaluation of</span>
  <span class="s0"># jaxpr_known_hoist and a scan of jaxpr_known_loop.</span>
  <span class="s1">ins_known</span><span class="s3">, </span><span class="s1">_ = partition_list(unks_in</span><span class="s3">, </span><span class="s1">eqn.invars)</span>
  <span class="s1">out_binders_known</span><span class="s3">, </span><span class="s1">_ = partition_list(unks_out</span><span class="s3">, </span><span class="s1">eqn.outvars)</span>
  <span class="s0"># jaxpr_known_loop takes as input constants output as res by jaxpr_known_hoist</span>
  <span class="s0"># (corresponding to consts_known_lp_avals) followed by known carry and xs.</span>
  <span class="s1">linear_known_ = [l </span><span class="s3">for </span><span class="s1">l</span><span class="s3">, </span><span class="s1">uk </span><span class="s3">in </span><span class="s1">zip(eqn.params[</span><span class="s4">'linear'</span><span class="s1">]</span><span class="s3">, </span><span class="s1">unks_in) </span><span class="s3">if not </span><span class="s1">uk]</span>
  <span class="s1">_</span><span class="s3">, </span><span class="s1">linear_known_ = split_list(linear_known_</span><span class="s3">, </span><span class="s1">[num_const_known])</span>
  <span class="s1">linear_known = [</span><span class="s3">False</span><span class="s1">] * len(consts_known_lp_avals) + linear_known_</span>
  <span class="s1">params_known = dict(eqn.params</span><span class="s3">, </span><span class="s1">jaxpr=jaxpr_known_loop</span><span class="s3">,</span>
                      <span class="s1">num_consts=len(consts_known_lp_avals)</span><span class="s3">,</span>
                      <span class="s1">num_carry=len(carry_uk)-sum(carry_uk)</span><span class="s3">,</span>
                      <span class="s1">linear=tuple(linear_known))</span>

  <span class="s1">@lu.wrap_init</span>
  <span class="s3">def </span><span class="s1">known(*ins_known):</span>
    <span class="s1">consts_known_hoist</span><span class="s3">, </span><span class="s1">ins_known_lp = split_list(ins_known</span><span class="s3">, </span><span class="s1">[num_const_known])</span>
    <span class="s1">out_hoist = core.jaxpr_as_fun(jaxpr_known_hoist)(*consts_known_hoist)</span>
    <span class="s1">intensive_res</span><span class="s3">, </span><span class="s1">consts_known_lp = split_list(out_hoist</span><span class="s3">, </span><span class="s1">[num_intensive_res])</span>
    <span class="s1">out_loop = scan_p.bind(*consts_known_lp</span><span class="s3">, </span><span class="s1">*ins_known_lp</span><span class="s3">, </span><span class="s1">**params_known)</span>
    <span class="s3">return </span><span class="s1">[*intensive_res</span><span class="s3">, </span><span class="s1">*out_loop]</span>
  <span class="s1">call_jaxpr_</span><span class="s3">, </span><span class="s1">_</span><span class="s3">, </span><span class="s1">call_jaxpr_consts = pe.trace_to_jaxpr_dynamic(</span>
      <span class="s1">known</span><span class="s3">, </span><span class="s1">[v.aval </span><span class="s3">for </span><span class="s1">v </span><span class="s3">in </span><span class="s1">ins_known])</span>
  <span class="s1">call_jaxpr = core.ClosedJaxpr(call_jaxpr_</span><span class="s3">, </span><span class="s1">call_jaxpr_consts)</span>
  <span class="s1">eqn_known = pe.new_jaxpr_eqn(</span>
      <span class="s1">ins_known</span><span class="s3">, </span><span class="s1">[*intensive_res</span><span class="s3">, </span><span class="s1">*out_binders_known</span><span class="s3">, </span><span class="s1">*extensive_res]</span><span class="s3">,</span>
      <span class="s1">core.closed_call_p</span><span class="s3">, </span><span class="s1">dict(call_jaxpr=call_jaxpr)</span><span class="s3">, </span><span class="s1">call_jaxpr.effects</span><span class="s3">,</span>
      <span class="s1">eqn.source_info)</span>

  <span class="s0"># Create the staged eqn.</span>
  <span class="s1">_</span><span class="s3">, </span><span class="s1">out_binders_staged = partition_list(inst_out</span><span class="s3">, </span><span class="s1">eqn.outvars)</span>
  <span class="s1">linear_staged = ([</span><span class="s3">False</span><span class="s1">] * len(intensive_res) + list(eqn.params[</span><span class="s4">'linear'</span><span class="s1">]) +</span>
                   <span class="s1">[</span><span class="s3">False</span><span class="s1">] * len(extensive_res))</span>
  <span class="s1">params_staged = dict(eqn.params</span><span class="s3">, </span><span class="s1">jaxpr=jaxpr_staged</span><span class="s3">,</span>
                       <span class="s1">num_consts=len(intensive_res) + eqn.params[</span><span class="s4">'num_consts'</span><span class="s1">]</span><span class="s3">,</span>
                       <span class="s1">linear=tuple(linear_staged))</span>
  <span class="s1">eqn_staged = pe.new_jaxpr_eqn([*intensive_res</span><span class="s3">, </span><span class="s1">*eqn.invars</span><span class="s3">, </span><span class="s1">*extensive_res]</span><span class="s3">,</span>
                                <span class="s1">out_binders_staged</span><span class="s3">, </span><span class="s1">eqn.primitive</span><span class="s3">,</span>
                                <span class="s1">params_staged</span><span class="s3">, </span><span class="s1">jaxpr_staged.effects</span><span class="s3">,</span>
                                <span class="s1">eqn.source_info)</span>

  <span class="s1">new_vars = [*new_inst</span><span class="s3">, </span><span class="s1">*intensive_res</span><span class="s3">, </span><span class="s1">*extensive_res]</span>
  <span class="s3">return </span><span class="s1">eqn_known</span><span class="s3">, </span><span class="s1">eqn_staged</span><span class="s3">, </span><span class="s1">unks_out</span><span class="s3">, </span><span class="s1">inst_out</span><span class="s3">, </span><span class="s1">new_vars</span>

<span class="s3">def </span><span class="s1">_scan_typecheck(bind_time</span><span class="s3">, </span><span class="s1">*in_atoms</span><span class="s3">, </span><span class="s1">reverse</span><span class="s3">, </span><span class="s1">length</span><span class="s3">, </span><span class="s1">num_consts</span><span class="s3">,</span>
                    <span class="s1">num_carry</span><span class="s3">, </span><span class="s1">jaxpr</span><span class="s3">, </span><span class="s1">linear</span><span class="s3">, </span><span class="s1">unroll):</span>
  <span class="s3">if not </span><span class="s1">bind_time:</span>
    <span class="s1">_</span><span class="s3">, </span><span class="s1">*in_atoms = in_atoms</span>
  <span class="s1">avals = [x.aval </span><span class="s3">for </span><span class="s1">x </span><span class="s3">in </span><span class="s1">in_atoms]</span>
  <span class="s1">tc = partial(_typecheck_param</span><span class="s3">, </span><span class="s4">'scan'</span><span class="s1">)</span>
  <span class="s1">tc(reverse</span><span class="s3">, </span><span class="s4">'reverse'</span><span class="s3">, </span><span class="s4">'bool'</span><span class="s3">, </span><span class="s1">type(reverse) </span><span class="s3">is </span><span class="s1">bool)</span>
  <span class="s1">tc(num_consts</span><span class="s3">, </span><span class="s4">'num_consts'</span><span class="s3">, </span><span class="s4">'non-negative int'</span><span class="s3">,</span>
     <span class="s1">type(num_consts) </span><span class="s3">is </span><span class="s1">int </span><span class="s3">and </span><span class="s1">num_consts &gt;= </span><span class="s5">0</span><span class="s1">)</span>
  <span class="s1">tc(num_carry</span><span class="s3">, </span><span class="s4">'num_carry'</span><span class="s3">, </span><span class="s4">'non-negative int'</span><span class="s3">,</span>
     <span class="s1">type(num_carry) </span><span class="s3">is </span><span class="s1">int </span><span class="s3">and </span><span class="s1">num_carry &gt;= </span><span class="s5">0</span><span class="s1">)</span>
  <span class="s1">tc(jaxpr</span><span class="s3">, </span><span class="s4">'jaxpr'</span><span class="s3">, </span><span class="s4">'ClosedJaxpr'</span><span class="s3">, </span><span class="s1">type(jaxpr) </span><span class="s3">is </span><span class="s1">core.ClosedJaxpr)</span>
  <span class="s1">tc(linear</span><span class="s3">, </span><span class="s4">'linear'</span><span class="s3">, </span><span class="s4">'tuple of bool'</span><span class="s3">,</span>
     <span class="s1">type(linear) </span><span class="s3">is </span><span class="s1">tuple </span><span class="s3">and </span><span class="s1">all(type(x) </span><span class="s3">is </span><span class="s1">bool </span><span class="s3">for </span><span class="s1">x </span><span class="s3">in </span><span class="s1">linear))</span>
  <span class="s1">tc(unroll</span><span class="s3">, </span><span class="s4">'unroll'</span><span class="s3">, </span><span class="s4">'positive int'</span><span class="s3">, </span><span class="s1">type(unroll) </span><span class="s3">is </span><span class="s1">int </span><span class="s3">and </span><span class="s1">unroll &gt; </span><span class="s5">0</span><span class="s1">)</span>

  <span class="s1">tc(length</span><span class="s3">, </span><span class="s4">'length'</span><span class="s3">, </span><span class="s4">'non-negative int'</span><span class="s3">, </span><span class="s1">core.greater_equal_dim(length</span><span class="s3">, </span><span class="s5">0</span><span class="s1">))</span>

  <span class="s3">if </span><span class="s1">len(linear) != len(avals):</span>
    <span class="s3">raise </span><span class="s1">core.JaxprTypeError(</span>
      <span class="s4">f'scan param linear has length </span><span class="s3">{</span><span class="s1">len(linear)</span><span class="s3">} </span><span class="s4">for </span><span class="s3">{</span><span class="s1">len(avals)</span><span class="s3">} </span><span class="s4">operands'</span><span class="s1">)</span>

  <span class="s1">const_avals</span><span class="s3">, </span><span class="s1">init_avals</span><span class="s3">, </span><span class="s1">x_avals = split_list(avals</span><span class="s3">, </span><span class="s1">[num_consts</span><span class="s3">, </span><span class="s1">num_carry])</span>
  <span class="s1">const_avals_jaxpr</span><span class="s3">, </span><span class="s1">init_avals_jaxpr</span><span class="s3">, </span><span class="s1">x_avals_jaxpr = split_list(</span>
      <span class="s1">jaxpr.in_avals</span><span class="s3">, </span><span class="s1">[num_consts</span><span class="s3">, </span><span class="s1">num_carry])</span>
  <span class="s1">carry_avals_jaxpr</span><span class="s3">, </span><span class="s1">y_avals_mapped = split_list(jaxpr.out_avals</span><span class="s3">, </span><span class="s1">[num_carry])</span>
  <span class="s1">x_avals_mapped = _map(partial(core.mapped_aval</span><span class="s3">, </span><span class="s1">length</span><span class="s3">, </span><span class="s5">0</span><span class="s1">)</span><span class="s3">, </span><span class="s1">x_avals)</span>
  <span class="s1">y_avals = [core.unmapped_aval(length</span><span class="s3">, </span><span class="s1">core.no_axis_name</span><span class="s3">, </span><span class="s5">0</span><span class="s3">, </span><span class="s1">a)</span>
             <span class="s3">for </span><span class="s1">a </span><span class="s3">in </span><span class="s1">y_avals_mapped]</span>

  <span class="s3">if not </span><span class="s1">all(_map(core.typematch</span><span class="s3">, </span><span class="s1">init_avals_jaxpr</span><span class="s3">, </span><span class="s1">carry_avals_jaxpr)):</span>
    <span class="s3">raise </span><span class="s1">core.JaxprTypeError(</span>
      <span class="s4">f'scan input carry input and output types mismatch: '</span>
      <span class="s4">f'</span><span class="s3">\n{</span><span class="s1">_avals_short(init_avals_jaxpr)</span><span class="s3">}\n</span><span class="s4">vs</span><span class="s3">\n{</span><span class="s1">_avals_short(carry_avals_jaxpr)</span><span class="s3">}</span><span class="s4">'</span><span class="s1">)</span>
  <span class="s3">if not </span><span class="s1">all(_map(core.typecompat</span><span class="s3">, </span><span class="s1">const_avals_jaxpr</span><span class="s3">, </span><span class="s1">const_avals)):</span>
    <span class="s3">raise </span><span class="s1">core.JaxprTypeError(</span>
      <span class="s4">f'scan jaxpr takes input const types</span><span class="s3">\n{</span><span class="s1">_avals_short(const_avals_jaxpr)</span><span class="s3">}</span><span class="s4">,</span><span class="s3">\n</span><span class="s4">'</span>
      <span class="s4">f'called with consts of type</span><span class="s3">\n{</span><span class="s1">_avals_short(const_avals)</span><span class="s3">}</span><span class="s4">'</span><span class="s1">)</span>
  <span class="s3">if not </span><span class="s1">all(_map(core.typecompat</span><span class="s3">, </span><span class="s1">init_avals_jaxpr</span><span class="s3">, </span><span class="s1">init_avals)):</span>
    <span class="s3">raise </span><span class="s1">core.JaxprTypeError(</span>
      <span class="s4">f'scan jaxpr takes input carry types</span><span class="s3">\n{</span><span class="s1">_avals_short(init_avals_jaxpr)</span><span class="s3">}</span><span class="s4">,</span><span class="s3">\n</span><span class="s4">'</span>
      <span class="s4">f'called with initial carry of type</span><span class="s3">\n{</span><span class="s1">_avals_short(init_avals)</span><span class="s3">}</span><span class="s4">'</span><span class="s1">)</span>
  <span class="s3">if not </span><span class="s1">all(_map(core.typecompat</span><span class="s3">, </span><span class="s1">x_avals_jaxpr</span><span class="s3">, </span><span class="s1">x_avals_mapped)):</span>
    <span class="s3">raise </span><span class="s1">core.JaxprTypeError(</span>
      <span class="s4">f'scan jaxpr takes input sequence types</span><span class="s3">\n{</span><span class="s1">_avals_short(x_avals_jaxpr)</span><span class="s3">}</span><span class="s4">,</span><span class="s3">\n</span><span class="s4">'</span>
      <span class="s4">f'called with sequence whose items have type</span><span class="s3">\n{</span><span class="s1">_avals_short(x_avals_mapped)</span><span class="s3">}</span><span class="s4">'</span><span class="s1">)</span>
  <span class="s3">return </span><span class="s1">[*init_avals</span><span class="s3">, </span><span class="s1">*y_avals]</span><span class="s3">, </span><span class="s1">jaxpr.effects</span>

<span class="s3">def </span><span class="s1">_scan_pp_rule(eqn</span><span class="s3">, </span><span class="s1">context</span><span class="s3">, </span><span class="s1">settings):</span>
  <span class="s1">printed_params = dict(eqn.params)</span>
  <span class="s3">del </span><span class="s1">printed_params[</span><span class="s4">'linear'</span><span class="s1">]</span>
  <span class="s3">if </span><span class="s1">eqn.params[</span><span class="s4">'num_consts'</span><span class="s1">] + eqn.params[</span><span class="s4">'num_carry'</span><span class="s1">] == len(eqn.invars):</span>
    <span class="s3">del </span><span class="s1">printed_params[</span><span class="s4">'length'</span><span class="s1">]</span>
  <span class="s3">if </span><span class="s1">printed_params[</span><span class="s4">'unroll'</span><span class="s1">] == </span><span class="s5">1</span><span class="s1">:</span>
    <span class="s3">del </span><span class="s1">printed_params[</span><span class="s4">'unroll'</span><span class="s1">]</span>
  <span class="s3">if </span><span class="s1">printed_params[</span><span class="s4">'num_carry'</span><span class="s1">] == </span><span class="s5">0</span><span class="s1">:</span>
    <span class="s3">del </span><span class="s1">printed_params[</span><span class="s4">'num_carry'</span><span class="s1">]</span>
  <span class="s3">if </span><span class="s1">printed_params[</span><span class="s4">'num_consts'</span><span class="s1">] == </span><span class="s5">0</span><span class="s1">:</span>
    <span class="s3">del </span><span class="s1">printed_params[</span><span class="s4">'num_consts'</span><span class="s1">]</span>
  <span class="s3">if not </span><span class="s1">printed_params[</span><span class="s4">'reverse'</span><span class="s1">]:</span>
    <span class="s3">del </span><span class="s1">printed_params[</span><span class="s4">'reverse'</span><span class="s1">]</span>
  <span class="s3">return </span><span class="s1">core._pp_eqn(eqn.replace(params=printed_params)</span><span class="s3">, </span><span class="s1">context</span><span class="s3">, </span><span class="s1">settings)</span>

<span class="s3">def </span><span class="s1">scan_bind(*args</span><span class="s3">, </span><span class="s1">**params):</span>
  <span class="s3">if </span><span class="s1">config.jax_enable_checks:</span>
    <span class="s1">avals = _map(core.get_aval</span><span class="s3">, </span><span class="s1">args)</span>
    <span class="s1">in_atoms = [core.Var(</span><span class="s5">0</span><span class="s3">, </span><span class="s4">''</span><span class="s3">, </span><span class="s1">a) </span><span class="s3">for </span><span class="s1">a </span><span class="s3">in </span><span class="s1">avals]  </span><span class="s0"># dummies</span>
    <span class="s1">_scan_typecheck(</span><span class="s3">True, </span><span class="s1">*in_atoms</span><span class="s3">, </span><span class="s1">**params)</span>
    <span class="s1">core.check_jaxpr(params[</span><span class="s4">'jaxpr'</span><span class="s1">].jaxpr)</span>
  <span class="s3">return </span><span class="s1">core.AxisPrimitive.bind(scan_p</span><span class="s3">, </span><span class="s1">*args</span><span class="s3">, </span><span class="s1">**params)</span>

<span class="s1">scan_p = core.AxisPrimitive(</span><span class="s4">&quot;scan&quot;</span><span class="s1">)</span>
<span class="s1">scan_p.multiple_results = </span><span class="s3">True</span>
<span class="s1">scan_p.def_custom_bind(scan_bind)</span>
<span class="s1">scan_p.def_impl(partial(dispatch.apply_primitive</span><span class="s3">, </span><span class="s1">scan_p))</span>
<span class="s1">scan_p.def_effectful_abstract_eval(_scan_abstract_eval)</span>
<span class="s1">ad.primitive_jvps[scan_p] = _scan_jvp</span>
<span class="s1">ad.reducing_transposes[scan_p] = _scan_transpose</span>
<span class="s1">pe.custom_partial_eval_rules[scan_p] = _scan_partial_eval</span>
<span class="s1">xla.register_initial_style_primitive(scan_p)</span>
<span class="s1">mlir.register_lowering(scan_p</span><span class="s3">,</span>
                       <span class="s1">mlir.lower_fun(_scan_impl</span><span class="s3">, </span><span class="s1">multiple_results=</span><span class="s3">True</span><span class="s1">))</span>
<span class="s1">batching.axis_primitive_batchers[scan_p] = partial(_scan_batching_rule</span><span class="s3">, None</span><span class="s1">)</span>
<span class="s1">batching.spmd_axis_primitive_batchers[scan_p] = _scan_batching_rule</span>
<span class="s1">core.custom_typechecks[scan_p] = partial(_scan_typecheck</span><span class="s3">, False</span><span class="s1">)</span>
<span class="s1">pe.partial_eval_jaxpr_custom_rules[scan_p] = _scan_partial_eval_custom</span>
<span class="s1">pe.padding_rules[scan_p] = _scan_padding_rule</span>
<span class="s1">pe.dce_rules[scan_p] = _scan_dce_rule</span>
<span class="s0"># TODO(mattjj,frostig): un-comment this pp rule</span>
<span class="s0"># core.pp_eqn_rules[scan_p] = _scan_pp_rule</span>

<span class="s0">### while_loop</span>

<span class="s1">@api_boundary</span>
<span class="s3">def </span><span class="s1">while_loop(cond_fun: Callable[[T]</span><span class="s3">, </span><span class="s1">BooleanNumeric]</span><span class="s3">,</span>
               <span class="s1">body_fun: Callable[[T]</span><span class="s3">, </span><span class="s1">T]</span><span class="s3">,</span>
               <span class="s1">init_val: T) -&gt; T:</span>
  <span class="s2">&quot;&quot;&quot;Call ``body_fun`` repeatedly in a loop while ``cond_fun`` is True. 
 
  The `Haskell-like type signature`_ in brief is 
 
  .. code-block:: haskell 
 
    while_loop :: (a -&gt; Bool) -&gt; (a -&gt; a) -&gt; a -&gt; a 
 
  The semantics of ``while_loop`` are given by this Python implementation:: 
 
    def while_loop(cond_fun, body_fun, init_val): 
      val = init_val 
      while cond_fun(val): 
        val = body_fun(val) 
      return val 
 
  Unlike that Python version, ``while_loop`` is a JAX primitive and is lowered 
  to a single WhileOp. That makes it useful for reducing compilation times 
  for jit-compiled functions, since native Python loop constructs in an ``@jit`` 
  function are unrolled, leading to large XLA computations. 
 
  Also unlike the Python analogue, the loop-carried value ``val`` must hold a 
  fixed shape and dtype across all iterations (and not just be consistent up to 
  NumPy rank/shape broadcasting and dtype promotion rules, for example). In 
  other words, the type ``a`` in the type signature above represents an array 
  with a fixed shape and dtype (or a nested tuple/list/dict container data 
  structure with a fixed structure and arrays with fixed shape and dtype at the 
  leaves). 
 
  Another difference from using Python-native loop constructs is that 
  ``while_loop`` is not reverse-mode differentiable because XLA computations 
  require static bounds on memory requirements. 
 
  .. note:: 
    :py:func:`while_loop` compiles ``cond_fun`` and ``body_fun``, so while it 
    can be combined with :py:func:`jit`, it's usually unnecessary. 
 
  Args: 
    cond_fun: function of type ``a -&gt; Bool``. 
    body_fun: function of type ``a -&gt; a``. 
    init_val: value of type ``a``, a type that can be a scalar, array, or any 
      pytree (nested Python tuple/list/dict) thereof, representing the initial 
      loop carry value. 
 
  Returns: 
    The output from the final iteration of body_fun, of type ``a``. 
 
  .. _Haskell-like type signature: https://wiki.haskell.org/Type_signature 
  &quot;&quot;&quot;</span>
  <span class="s3">if not </span><span class="s1">(callable(body_fun) </span><span class="s3">and </span><span class="s1">callable(cond_fun)):</span>
    <span class="s3">raise </span><span class="s1">TypeError(</span><span class="s4">&quot;lax.while_loop: body_fun and cond_fun arguments should be callable.&quot;</span><span class="s1">)</span>
  <span class="s3">if </span><span class="s1">config.jax_disable_jit:</span>
    <span class="s3">try</span><span class="s1">:</span>
      <span class="s1">val = init_val</span>
      <span class="s3">while </span><span class="s1">cond_fun(val):</span>
        <span class="s1">val = body_fun(val)</span>
      <span class="s3">return </span><span class="s1">val</span>
    <span class="s3">except </span><span class="s1">core.ConcretizationTypeError:</span>
      <span class="s0"># Can't run this while_loop in Python (e.g. because there's a vmap</span>
      <span class="s0"># transformation on it), so we fall back to the primitive version.</span>
      <span class="s3">pass</span>

  <span class="s3">def </span><span class="s1">_create_jaxpr(init_val):</span>
    <span class="s1">init_vals</span><span class="s3">, </span><span class="s1">in_tree = tree_flatten((init_val</span><span class="s3">,</span><span class="s1">))</span>
    <span class="s1">init_avals = tuple(_map(_abstractify</span><span class="s3">, </span><span class="s1">init_vals))</span>
    <span class="s1">cond_jaxpr</span><span class="s3">, </span><span class="s1">cond_consts</span><span class="s3">, </span><span class="s1">cond_tree = _initial_style_jaxpr(</span>
        <span class="s1">cond_fun</span><span class="s3">, </span><span class="s1">in_tree</span><span class="s3">, </span><span class="s1">init_avals</span><span class="s3">, </span><span class="s4">&quot;while_cond&quot;</span><span class="s1">)</span>
    <span class="s1">body_jaxpr</span><span class="s3">, </span><span class="s1">body_consts</span><span class="s3">, </span><span class="s1">body_tree = _initial_style_jaxpr(</span>
        <span class="s1">body_fun</span><span class="s3">, </span><span class="s1">in_tree</span><span class="s3">, </span><span class="s1">init_avals</span><span class="s3">, </span><span class="s4">&quot;while_loop&quot;</span><span class="s1">)</span>
    <span class="s3">if not </span><span class="s1">treedef_is_leaf(cond_tree) </span><span class="s3">or </span><span class="s1">len(cond_jaxpr.out_avals) != </span><span class="s5">1</span><span class="s1">:</span>
      <span class="s1">msg = </span><span class="s4">&quot;cond_fun must return a boolean scalar, but got pytree {}.&quot;</span>
      <span class="s3">raise </span><span class="s1">TypeError(msg.format(cond_tree))</span>
    <span class="s1">pred_aval = cond_jaxpr.out_avals[</span><span class="s5">0</span><span class="s1">]</span>
    <span class="s3">if </span><span class="s1">(</span><span class="s3">not </span><span class="s1">isinstance(pred_aval</span><span class="s3">, </span><span class="s1">ShapedArray)</span>
        <span class="s3">or </span><span class="s1">pred_aval.strip_weak_type().strip_named_shape() != ShapedArray(()</span><span class="s3">, </span><span class="s1">np.bool_)):</span>
      <span class="s1">msg = </span><span class="s4">&quot;cond_fun must return a boolean scalar, but got output type(s) {}.&quot;</span>
      <span class="s3">raise </span><span class="s1">TypeError(msg.format(cond_jaxpr.out_avals))</span>
    <span class="s3">return </span><span class="s1">init_vals</span><span class="s3">, </span><span class="s1">init_avals</span><span class="s3">, </span><span class="s1">body_jaxpr</span><span class="s3">, </span><span class="s1">in_tree</span><span class="s3">, </span><span class="s1">cond_jaxpr</span><span class="s3">, </span><span class="s1">cond_consts</span><span class="s3">, </span><span class="s1">body_consts</span><span class="s3">, </span><span class="s1">body_tree</span>

  <span class="s0"># The body input and output avals must match exactly. However, we want to account for</span>
  <span class="s0"># the case when init contains weakly-typed values (e.g. Python scalars), with avals that</span>
  <span class="s0"># may not match the output despite being compatible by virtue of their weak type.</span>
  <span class="s0"># To do this, we compute the jaxpr in two passes: first with the raw inputs, and if</span>
  <span class="s0"># necessary, a second time with modified init values.</span>
  <span class="s1">init_vals</span><span class="s3">, </span><span class="s1">init_avals</span><span class="s3">, </span><span class="s1">body_jaxpr</span><span class="s3">, </span><span class="s1">in_tree</span><span class="s3">, </span><span class="s1">*rest = _create_jaxpr(init_val)</span>
  <span class="s1">new_init_vals</span><span class="s3">, </span><span class="s1">changed = _promote_weak_typed_inputs(init_vals</span><span class="s3">, </span><span class="s1">init_avals</span><span class="s3">, </span><span class="s1">body_jaxpr.out_avals)</span>
  <span class="s3">if </span><span class="s1">changed:</span>
    <span class="s1">new_init_val</span><span class="s3">, </span><span class="s1">= tree_unflatten(in_tree</span><span class="s3">, </span><span class="s1">new_init_vals)</span>
    <span class="s1">init_vals</span><span class="s3">, </span><span class="s1">init_avals</span><span class="s3">, </span><span class="s1">body_jaxpr</span><span class="s3">, </span><span class="s1">in_tree</span><span class="s3">, </span><span class="s1">*rest = _create_jaxpr(new_init_val)</span>
  <span class="s1">cond_jaxpr</span><span class="s3">, </span><span class="s1">cond_consts</span><span class="s3">, </span><span class="s1">body_consts</span><span class="s3">, </span><span class="s1">body_tree = rest</span>

  <span class="s1">in_tree_children = in_tree.children()</span>
  <span class="s3">assert </span><span class="s1">len(in_tree_children) == </span><span class="s5">1</span>
  <span class="s1">_check_tree_and_avals(</span><span class="s4">&quot;body_fun output and input&quot;</span><span class="s3">,</span>
                        <span class="s1">body_tree</span><span class="s3">, </span><span class="s1">body_jaxpr.out_avals</span><span class="s3">,</span>
                        <span class="s1">in_tree_children[</span><span class="s5">0</span><span class="s1">]</span><span class="s3">, </span><span class="s1">init_avals)</span>
  <span class="s1">effects = core.join_effects(cond_jaxpr.effects</span><span class="s3">, </span><span class="s1">body_jaxpr.effects)</span>
  <span class="s1">disallowed_effects = allowed_effects.filter_not_in(effects)</span>
  <span class="s3">if </span><span class="s1">disallowed_effects:</span>
    <span class="s3">raise </span><span class="s1">NotImplementedError(</span>
        <span class="s4">f'Effects not supported in `while`: </span><span class="s3">{</span><span class="s1">disallowed_effects</span><span class="s3">}</span><span class="s4">'</span><span class="s1">)</span>
  <span class="s1">outs = while_p.bind(*cond_consts</span><span class="s3">, </span><span class="s1">*body_consts</span><span class="s3">, </span><span class="s1">*init_vals</span><span class="s3">,</span>
                      <span class="s1">cond_nconsts=len(cond_consts)</span><span class="s3">, </span><span class="s1">cond_jaxpr=cond_jaxpr</span><span class="s3">,</span>
                      <span class="s1">body_nconsts=len(body_consts)</span><span class="s3">, </span><span class="s1">body_jaxpr=body_jaxpr)</span>
  <span class="s3">return </span><span class="s1">tree_unflatten(body_tree</span><span class="s3">, </span><span class="s1">outs)</span>


<span class="s3">def </span><span class="s1">_join_while_effects(body_jaxpr</span><span class="s3">, </span><span class="s1">cond_jaxpr</span><span class="s3">, </span><span class="s1">body_nconsts</span><span class="s3">, </span><span class="s1">cond_nconsts</span>
                       <span class="s1">) -&gt; effects.Effects:</span>
  <span class="s1">joined_effects = set()</span>
  <span class="s3">for </span><span class="s1">eff </span><span class="s3">in </span><span class="s1">cond_jaxpr.effects:</span>
    <span class="s3">if </span><span class="s1">isinstance(eff</span><span class="s3">, </span><span class="s1">effects.JaxprInputEffect):</span>
      <span class="s1">index = eff.input_index</span>
      <span class="s3">if </span><span class="s1">index &gt;= cond_nconsts:</span>
        <span class="s1">index += body_nconsts</span>
      <span class="s1">eff = eff.replace(input_index=index)</span>
    <span class="s1">joined_effects.add(eff)</span>
  <span class="s3">for </span><span class="s1">eff </span><span class="s3">in </span><span class="s1">body_jaxpr.effects:</span>
    <span class="s3">if </span><span class="s1">isinstance(eff</span><span class="s3">, </span><span class="s1">effects.JaxprInputEffect):</span>
      <span class="s1">index = eff.input_index + cond_nconsts</span>
      <span class="s1">eff = eff.replace(input_index=index)</span>
    <span class="s1">joined_effects.add(eff)</span>
  <span class="s3">return </span><span class="s1">joined_effects</span>

<span class="s3">def </span><span class="s1">_while_loop_abstract_eval(*avals</span><span class="s3">, </span><span class="s1">cond_jaxpr</span><span class="s3">, </span><span class="s1">body_jaxpr</span><span class="s3">, </span><span class="s1">body_nconsts</span><span class="s3">,</span>
                              <span class="s1">cond_nconsts):</span>
  <span class="s3">del </span><span class="s1">avals</span>
  <span class="s1">joined_effects = _join_while_effects(body_jaxpr</span><span class="s3">, </span><span class="s1">cond_jaxpr</span><span class="s3">, </span><span class="s1">body_nconsts</span><span class="s3">,</span>
                                       <span class="s1">cond_nconsts)</span>
  <span class="s1">disallowed_effects = allowed_effects.filter_not_in(joined_effects)</span>
  <span class="s3">if </span><span class="s1">disallowed_effects:</span>
    <span class="s3">raise </span><span class="s1">NotImplementedError(</span>
        <span class="s4">f'Effects not supported in `while`: </span><span class="s3">{</span><span class="s1">disallowed_effects</span><span class="s3">}</span><span class="s4">'</span><span class="s1">)</span>
  <span class="s3">return </span><span class="s1">_map(raise_to_shaped</span><span class="s3">, </span><span class="s1">body_jaxpr.out_avals)</span><span class="s3">, </span><span class="s1">joined_effects</span>


<span class="s3">def </span><span class="s1">_while_loop_batching_rule(spmd_axis_name</span><span class="s3">, </span><span class="s1">axis_size</span><span class="s3">, </span><span class="s1">axis_name</span><span class="s3">, </span><span class="s1">main_type</span><span class="s3">,</span>
                              <span class="s1">args</span><span class="s3">, </span><span class="s1">dims</span><span class="s3">, </span><span class="s1">cond_nconsts</span><span class="s3">, </span><span class="s1">cond_jaxpr</span><span class="s3">,</span>
                              <span class="s1">body_nconsts</span><span class="s3">, </span><span class="s1">body_jaxpr):</span>
  <span class="s3">from </span><span class="s1">jax._src.callback </span><span class="s3">import </span><span class="s1">_IOEffect</span><span class="s3">, </span><span class="s1">_OrderedIOEffect</span>
  <span class="s3">if </span><span class="s1">any(eff </span><span class="s3">in </span><span class="s1">branch.effects </span><span class="s3">for </span><span class="s1">eff </span><span class="s3">in </span><span class="s1">[_IOEffect</span><span class="s3">, </span><span class="s1">_OrderedIOEffect]</span>
      <span class="s3">for </span><span class="s1">branch </span><span class="s3">in </span><span class="s1">[body_jaxpr</span><span class="s3">, </span><span class="s1">cond_jaxpr]):</span>
    <span class="s3">raise </span><span class="s1">NotImplementedError(</span>
        <span class="s4">&quot;IO effect not supported in vmap-of-while.&quot;</span><span class="s1">)</span>

  <span class="s1">orig_batched = [d </span><span class="s3">is not </span><span class="s1">batching.not_mapped </span><span class="s3">for </span><span class="s1">d </span><span class="s3">in </span><span class="s1">dims]</span>
  <span class="s1">cconst_bat</span><span class="s3">, </span><span class="s1">bconst_bat</span><span class="s3">, </span><span class="s1">init_bat = split_list(orig_batched</span><span class="s3">, </span><span class="s1">[cond_nconsts</span><span class="s3">, </span><span class="s1">body_nconsts])</span>
  <span class="s1">cconsts</span><span class="s3">, </span><span class="s1">bconsts</span><span class="s3">, </span><span class="s1">init = split_list(args</span><span class="s3">, </span><span class="s1">[cond_nconsts</span><span class="s3">, </span><span class="s1">body_nconsts])</span>
  <span class="s1">cconst_dims</span><span class="s3">, </span><span class="s1">bconst_dims</span><span class="s3">, </span><span class="s1">init_dims = split_list(dims</span><span class="s3">, </span><span class="s1">[cond_nconsts</span><span class="s3">, </span><span class="s1">body_nconsts])</span>

  <span class="s1">carry_bat = init_bat</span>
  <span class="s0"># Fixpoint computation of which carry are batched: either</span>
  <span class="s0"># batched from init, or the carry out is batched. Each iteration promotes</span>
  <span class="s0"># at least one carry to batched. We need at most len(carry) iterations to</span>
  <span class="s0"># reach a fixpoint.</span>
  <span class="s3">for </span><span class="s1">_ </span><span class="s3">in </span><span class="s1">range(</span><span class="s5">1 </span><span class="s1">+ len(carry_bat)):</span>
    <span class="s1">_</span><span class="s3">, </span><span class="s1">carry_bat_out = batching.batch_jaxpr(</span>
        <span class="s1">body_jaxpr</span><span class="s3">, </span><span class="s1">axis_size</span><span class="s3">, </span><span class="s1">bconst_bat + carry_bat</span><span class="s3">, </span><span class="s1">instantiate=carry_bat</span><span class="s3">,</span>
        <span class="s1">axis_name=axis_name</span><span class="s3">, </span><span class="s1">spmd_axis_name=spmd_axis_name</span><span class="s3">, </span><span class="s1">main_type=main_type)</span>
    <span class="s3">if </span><span class="s1">carry_bat == carry_bat_out:</span>
      <span class="s3">break</span>
    <span class="s1">carry_bat = safe_map(operator.or_</span><span class="s3">, </span><span class="s1">carry_bat</span><span class="s3">, </span><span class="s1">carry_bat_out)</span>
  <span class="s3">else</span><span class="s1">:</span>
    <span class="s3">assert False, </span><span class="s4">&quot;Fixpoint not reached&quot;</span>

  <span class="s0"># Knowing how the carry is batched now, we can determine if the predicate is</span>
  <span class="s0"># batched.</span>
  <span class="s1">_</span><span class="s3">, </span><span class="s1">(pred_bat</span><span class="s3">,</span><span class="s1">) = batching.batch_jaxpr(</span>
      <span class="s1">cond_jaxpr</span><span class="s3">, </span><span class="s1">axis_size</span><span class="s3">, </span><span class="s1">cconst_bat + carry_bat</span><span class="s3">, </span><span class="s1">instantiate=</span><span class="s3">False,</span>
      <span class="s1">axis_name=axis_name</span><span class="s3">, </span><span class="s1">spmd_axis_name=spmd_axis_name</span><span class="s3">, </span><span class="s1">main_type=main_type)</span>

  <span class="s3">if </span><span class="s1">pred_bat:</span>
    <span class="s0"># If the predicate is batched, we have to batch *all* of the carry</span>
    <span class="s0"># regardless of if the body needs it.</span>
    <span class="s1">carry_bat = [</span><span class="s3">True</span><span class="s1">] * len(carry_bat)</span>
    <span class="s1">carry_dims = [</span><span class="s5">0</span><span class="s1">] * len(carry_bat)</span>
    <span class="s1">body_jaxpr_batched</span><span class="s3">, </span><span class="s1">_ = batching.batch_jaxpr_axes(</span>
        <span class="s1">body_jaxpr</span><span class="s3">, </span><span class="s1">axis_size</span><span class="s3">, </span><span class="s1">bconst_dims + carry_dims</span><span class="s3">,</span>
        <span class="s1">carry_dims</span><span class="s3">, </span><span class="s1">axis_name=axis_name</span><span class="s3">, </span><span class="s1">spmd_axis_name=spmd_axis_name</span><span class="s3">,</span>
        <span class="s1">main_type=main_type)</span>
    <span class="s1">cond_jaxpr_batched</span><span class="s3">, </span><span class="s1">_ = batching.batch_jaxpr_axes(</span>
        <span class="s1">cond_jaxpr</span><span class="s3">, </span><span class="s1">axis_size</span><span class="s3">, </span><span class="s1">cconst_dims + carry_dims</span><span class="s3">, </span><span class="s1">[</span><span class="s5">0</span><span class="s1">]</span><span class="s3">,</span>
        <span class="s1">axis_name=axis_name</span><span class="s3">, </span><span class="s1">spmd_axis_name=spmd_axis_name</span><span class="s3">,</span>
        <span class="s1">main_type=main_type)</span>
  <span class="s3">else</span><span class="s1">:</span>
    <span class="s0"># If the predicate is not batched, we can look at the `cond_jaxpr`'s out</span>
    <span class="s0"># shape to determine the rank of the predicate. From this rank we pick the</span>
    <span class="s0"># dims of the carry to be batched to ensure that the predicate shape is a</span>
    <span class="s0"># prefix of the carry in and out shapes. We can then batch the `body_jaxpr`</span>
    <span class="s0"># according to these new batch dims.</span>
    <span class="s1">cond_rank = len(cond_jaxpr.out_avals[</span><span class="s5">0</span><span class="s1">].shape)</span>
    <span class="s1">carry_dims = [cond_rank </span><span class="s3">if </span><span class="s1">b </span><span class="s3">else None for </span><span class="s1">b </span><span class="s3">in </span><span class="s1">carry_bat]</span>
    <span class="s1">body_jaxpr_batched</span><span class="s3">, </span><span class="s1">_ = batching.batch_jaxpr_axes(</span>
        <span class="s1">body_jaxpr</span><span class="s3">, </span><span class="s1">axis_size</span><span class="s3">, </span><span class="s1">bconst_dims + carry_dims</span><span class="s3">, </span><span class="s1">carry_dims</span><span class="s3">,</span>
        <span class="s1">axis_name=axis_name</span><span class="s3">, </span><span class="s1">spmd_axis_name=spmd_axis_name</span><span class="s3">, </span><span class="s1">main_type=main_type)</span>
    <span class="s0"># Now we need to rebatch the `cond_jaxpr` according to the new dims of the</span>
    <span class="s0"># carry.</span>
    <span class="s1">cond_jaxpr_batched</span><span class="s3">, </span><span class="s1">_ = batching.batch_jaxpr_axes(</span>
        <span class="s1">cond_jaxpr</span><span class="s3">, </span><span class="s1">axis_size</span><span class="s3">, </span><span class="s1">cconst_dims + carry_dims</span><span class="s3">, </span><span class="s1">(</span><span class="s3">None,</span><span class="s1">)</span><span class="s3">,</span>
        <span class="s1">axis_name=axis_name</span><span class="s3">, </span><span class="s1">spmd_axis_name=spmd_axis_name</span><span class="s3">, </span><span class="s1">main_type=main_type)</span>

  <span class="s0"># To prepare the `init` to the `while_p`, we broadcast values if they are</span>
  <span class="s0"># unbatched and need to have an out axis. If their current batch axis does not</span>
  <span class="s0"># match the one it needs to be for the translation rule to work, we move it</span>
  <span class="s0"># into place.</span>
  <span class="s1">new_init = []</span>
  <span class="s3">for </span><span class="s1">x</span><span class="s3">, </span><span class="s1">old_axis</span><span class="s3">, </span><span class="s1">new_axis </span><span class="s3">in </span><span class="s1">zip(init</span><span class="s3">, </span><span class="s1">init_dims</span><span class="s3">, </span><span class="s1">carry_dims):</span>
    <span class="s3">if </span><span class="s1">old_axis </span><span class="s3">is </span><span class="s1">batching.not_mapped </span><span class="s3">and </span><span class="s1">new_axis </span><span class="s3">is not </span><span class="s1">batching.not_mapped:</span>
      <span class="s1">new_init.append(batching.broadcast(x</span><span class="s3">, </span><span class="s1">axis_size</span><span class="s3">, </span><span class="s1">new_axis))</span>
    <span class="s3">elif </span><span class="s1">old_axis </span><span class="s3">is </span><span class="s1">batching.not_mapped </span><span class="s3">and </span><span class="s1">new_axis </span><span class="s3">is </span><span class="s1">batching.not_mapped:</span>
      <span class="s1">new_init.append(x)</span>
    <span class="s3">else</span><span class="s1">:</span>
      <span class="s3">assert </span><span class="s1">new_axis </span><span class="s3">is not </span><span class="s1">batching.not_mapped</span>
      <span class="s1">new_init.append(batching.moveaxis(x</span><span class="s3">, </span><span class="s1">old_axis</span><span class="s3">, </span><span class="s1">new_axis))</span>

  <span class="s1">outs = while_p.bind(*(cconsts + bconsts + new_init)</span><span class="s3">,</span>
                      <span class="s1">cond_nconsts=cond_nconsts</span><span class="s3">, </span><span class="s1">cond_jaxpr=cond_jaxpr_batched</span><span class="s3">,</span>
                      <span class="s1">body_nconsts=body_nconsts</span><span class="s3">, </span><span class="s1">body_jaxpr=body_jaxpr_batched)</span>
  <span class="s3">return </span><span class="s1">outs</span><span class="s3">, </span><span class="s1">carry_dims</span>

<span class="s3">def </span><span class="s1">_while_loop_jvp(primals</span><span class="s3">, </span><span class="s1">tangents</span><span class="s3">, </span><span class="s1">cond_nconsts</span><span class="s3">, </span><span class="s1">cond_jaxpr</span><span class="s3">, </span><span class="s1">body_nconsts</span><span class="s3">,</span>
                    <span class="s1">body_jaxpr):</span>
  <span class="s1">nonzeros = [type(t) </span><span class="s3">is not </span><span class="s1">ad_util.Zero </span><span class="s3">for </span><span class="s1">t </span><span class="s3">in </span><span class="s1">tangents]</span>
  <span class="s1">cconst_nz</span><span class="s3">, </span><span class="s1">bconst_nz</span><span class="s3">, </span><span class="s1">init_nz = split_list(nonzeros</span><span class="s3">, </span><span class="s1">[cond_nconsts</span><span class="s3">, </span><span class="s1">body_nconsts])</span>

  <span class="s1">carry_nz = init_nz</span>
  <span class="s3">for </span><span class="s1">_ </span><span class="s3">in </span><span class="s1">range(</span><span class="s5">1 </span><span class="s1">+ len(carry_nz)):</span>
    <span class="s1">body_nonzeros = bconst_nz + carry_nz</span>
    <span class="s1">body_jvp</span><span class="s3">, </span><span class="s1">nonzeros_out = ad.jvp_jaxpr(</span>
        <span class="s1">body_jaxpr</span><span class="s3">, </span><span class="s1">body_nonzeros</span><span class="s3">, </span><span class="s1">instantiate=carry_nz)</span>
    <span class="s3">if </span><span class="s1">nonzeros_out == carry_nz:</span>
      <span class="s3">break</span>
    <span class="s1">carry_nz = _map(operator.or_</span><span class="s3">, </span><span class="s1">carry_nz</span><span class="s3">, </span><span class="s1">nonzeros_out)</span>
  <span class="s3">else</span><span class="s1">:</span>
    <span class="s3">assert False, </span><span class="s4">&quot;Fixpoint not reached&quot;</span>

  <span class="s1">nonzeros = cconst_nz + body_nonzeros</span>
  <span class="s1">tangents = [ad.instantiate_zeros(t) </span><span class="s3">if </span><span class="s1">nz </span><span class="s3">else </span><span class="s1">t</span>
              <span class="s3">for </span><span class="s1">t</span><span class="s3">, </span><span class="s1">nz </span><span class="s3">in </span><span class="s1">zip(tangents</span><span class="s3">, </span><span class="s1">nonzeros)]</span>

  <span class="s1">cconst</span><span class="s3">, </span><span class="s1">bconst</span><span class="s3">, </span><span class="s1">init = split_list(primals</span><span class="s3">, </span><span class="s1">[cond_nconsts</span><span class="s3">, </span><span class="s1">body_nconsts])</span>
  <span class="s1">_</span><span class="s3">, </span><span class="s1">bconst_dot</span><span class="s3">, </span><span class="s1">init_dot = split_list(tangents</span><span class="s3">, </span><span class="s1">[cond_nconsts</span><span class="s3">, </span><span class="s1">body_nconsts])</span>
  <span class="s1">bconst_dot = _prune_zeros(bconst_dot)</span>
  <span class="s1">init_dot = _prune_zeros(init_dot)</span>

  <span class="s1">num_carry = len(primals) - cond_nconsts - body_nconsts</span>

  <span class="s1">body_jvp_rearranged = ad.rearrange_binders(</span>
      <span class="s1">body_jvp</span><span class="s3">,</span>
      <span class="s1">[body_nconsts</span><span class="s3">, </span><span class="s1">num_carry]</span><span class="s3">, </span><span class="s1">[len(bconst_dot)</span><span class="s3">, </span><span class="s1">len(init_dot)]</span><span class="s3">,</span>
      <span class="s1">[num_carry]</span><span class="s3">, </span><span class="s1">[len(init_dot)])</span>

  <span class="s1">newvar = core.gensym([cond_jaxpr.jaxpr])</span>
  <span class="s1">invars_aug = (</span>
      <span class="s1">cond_jaxpr.jaxpr.invars + [newvar(core.get_aval(x)) </span><span class="s3">for </span><span class="s1">x </span><span class="s3">in </span><span class="s1">init_dot])</span>
  <span class="s1">cond_jaxpr_augmented = core.Jaxpr(cond_jaxpr.jaxpr.constvars</span><span class="s3">,</span>
                                    <span class="s1">invars_aug</span><span class="s3">,</span>
                                    <span class="s1">cond_jaxpr.jaxpr.outvars</span><span class="s3">,</span>
                                    <span class="s1">cond_jaxpr.jaxpr.eqns</span><span class="s3">,</span>
                                    <span class="s1">cond_jaxpr.jaxpr.effects)</span>
  <span class="s1">cond_jaxpr_augmented = core.ClosedJaxpr(cond_jaxpr_augmented</span><span class="s3">, </span><span class="s1">cond_jaxpr.consts)</span>

  <span class="s1">out = while_p.bind(</span>
      <span class="s1">*(cconst + bconst + bconst_dot + init + init_dot)</span><span class="s3">,</span>
      <span class="s1">cond_nconsts=cond_nconsts</span><span class="s3">,</span>
      <span class="s1">cond_jaxpr=cond_jaxpr_augmented</span><span class="s3">,</span>
      <span class="s1">body_nconsts=len(bconst) + len(bconst_dot)</span><span class="s3">,</span>
      <span class="s1">body_jaxpr=body_jvp_rearranged)</span>

  <span class="s1">out_carry</span><span class="s3">, </span><span class="s1">out_carry_dot = split_list(out</span><span class="s3">, </span><span class="s1">[num_carry])</span>
  <span class="s1">out_tangents_iter = iter(out_carry_dot)</span>
  <span class="s1">out_tangents = [next(out_tangents_iter) </span><span class="s3">if </span><span class="s1">nz </span><span class="s3">else </span><span class="s1">ad_util.Zero.from_value(p)</span>
                  <span class="s3">for </span><span class="s1">p</span><span class="s3">, </span><span class="s1">nz </span><span class="s3">in </span><span class="s1">zip(out_carry</span><span class="s3">, </span><span class="s1">nonzeros_out)]</span>
  <span class="s3">return </span><span class="s1">out_carry</span><span class="s3">, </span><span class="s1">out_tangents</span>

<span class="s3">def </span><span class="s1">_while_partial_eval(trace: pe.JaxprTrace</span><span class="s3">, </span><span class="s1">*tracers: pe.Tracer</span><span class="s3">, </span><span class="s1">cond_nconsts: int</span><span class="s3">,</span>
                        <span class="s1">cond_jaxpr: pe.ClosedJaxpr</span><span class="s3">, </span><span class="s1">body_nconsts: int</span><span class="s3">,</span>
                        <span class="s1">body_jaxpr: pe.ClosedJaxpr) -&gt; Sequence[pe.Tracer]:</span>
  <span class="s0"># As long as some carry (and hence output) are known and the output of</span>
  <span class="s0"># `cond_jaxpr` is known, we use a portion of the loop body to compute the</span>
  <span class="s0"># known outputs of the `while_loop`. For the unknown outputs we generate a</span>
  <span class="s0"># jaxpr to run the whole while, including recomputing the known parts,</span>
  <span class="s0"># basically like building in checkpointing/rematieralization. This means that</span>
  <span class="s0"># we don't actually save any computation by partial evaluation if there are</span>
  <span class="s0"># unknown outputs.</span>
  <span class="s0">#</span>
  <span class="s0"># What this achieves is twofold: jax.linearize works, and we can give a proper</span>
  <span class="s0"># error for reverse differentiation of `while`.</span>

  <span class="s1">unknowns = [</span><span class="s3">not </span><span class="s1">t.pval.is_known() </span><span class="s3">for </span><span class="s1">t </span><span class="s3">in </span><span class="s1">tracers]</span>
  <span class="s1">params = dict(cond_nconsts=cond_nconsts</span><span class="s3">, </span><span class="s1">cond_jaxpr=cond_jaxpr</span><span class="s3">,</span>
                <span class="s1">body_nconsts=body_nconsts</span><span class="s3">, </span><span class="s1">body_jaxpr=body_jaxpr)</span>

  <span class="s1">cond_consts_uk</span><span class="s3">, </span><span class="s1">body_consts_uk</span><span class="s3">, </span><span class="s1">carry_init_uk = \</span>
      <span class="s1">split_list(unknowns</span><span class="s3">, </span><span class="s1">[cond_nconsts</span><span class="s3">, </span><span class="s1">body_nconsts])</span>

  <span class="s0"># Fixpoint computation of unknown carry. Each iteration promotes at least one</span>
  <span class="s0"># carry to unknown. We need one last iteration to prepare the jaxpr.</span>
  <span class="s1">carry_uk = carry_init_uk</span>
  <span class="s3">for </span><span class="s1">_ </span><span class="s3">in </span><span class="s1">range(</span><span class="s5">1 </span><span class="s1">+ len(carry_uk)):</span>
    <span class="s1">body_jaxpr_known</span><span class="s3">, </span><span class="s1">_</span><span class="s3">, </span><span class="s1">carry_out_uk</span><span class="s3">, </span><span class="s1">body_res_avals = pe.partial_eval_jaxpr_nounits(  </span><span class="s0"># type: ignore</span>
        <span class="s1">body_jaxpr</span><span class="s3">, </span><span class="s1">body_consts_uk + carry_uk</span><span class="s3">, </span><span class="s1">instantiate=carry_uk)</span>
    <span class="s3">if </span><span class="s1">carry_out_uk == carry_uk:</span>
      <span class="s3">break</span>
    <span class="s3">else</span><span class="s1">:</span>
      <span class="s1">carry_uk = _map(operator.or_</span><span class="s3">, </span><span class="s1">carry_uk</span><span class="s3">, </span><span class="s1">carry_out_uk)</span>
  <span class="s3">else</span><span class="s1">:</span>
    <span class="s3">assert False, </span><span class="s4">&quot;Fixpoint not reached&quot;</span>

  <span class="s1">cond_jaxpr_known</span><span class="s3">, </span><span class="s1">_</span><span class="s3">, </span><span class="s1">cond_uk</span><span class="s3">, </span><span class="s1">_ = pe.partial_eval_jaxpr_nounits(  </span><span class="s0"># type: ignore</span>
      <span class="s1">cond_jaxpr</span><span class="s3">, </span><span class="s1">cond_consts_uk + carry_uk</span><span class="s3">, </span><span class="s1">instantiate=</span><span class="s3">False</span><span class="s1">)</span>

  <span class="s3">if </span><span class="s1">cond_uk[</span><span class="s5">0</span><span class="s1">] </span><span class="s3">or </span><span class="s1">all([</span><span class="s3">not </span><span class="s1">uk </span><span class="s3">for </span><span class="s1">uk </span><span class="s3">in </span><span class="s1">unknowns]) </span><span class="s3">or </span><span class="s1">all(unknowns):</span>
    <span class="s0"># If conditional is unknown, or all inputs are known, or all are unknown,</span>
    <span class="s0"># just do the default processing.</span>
    <span class="s3">return </span><span class="s1">trace.default_process_primitive(while_p</span><span class="s3">, </span><span class="s1">tracers</span><span class="s3">, </span><span class="s1">params)</span>

  <span class="s0"># Run the known part of the while.</span>
  <span class="s1">in_consts = [t.pval.get_known() </span><span class="s3">for </span><span class="s1">uk</span><span class="s3">, </span><span class="s1">t </span><span class="s3">in</span>
               <span class="s1">zip(cond_consts_uk + body_consts_uk + carry_uk</span><span class="s3">, </span><span class="s1">tracers)</span>
               <span class="s3">if not </span><span class="s1">uk]</span>
  <span class="s1">cond_nconsts_known = len(cond_consts_uk) - sum(cond_consts_uk)</span>
  <span class="s1">body_nconsts_known = len(body_consts_uk) - sum(body_consts_uk)</span>
  <span class="s1">num_known_outs = len(carry_uk) - sum(carry_uk)</span>
  <span class="s0"># TODO(mattjj): use pe.dce_jaxpr to drop res computations and not just outputs</span>
  <span class="s1">body_jaxpr_known = body_jaxpr_known.replace(</span>
    <span class="s1">jaxpr=body_jaxpr_known.jaxpr.replace(</span>
      <span class="s1">outvars=body_jaxpr_known.jaxpr.outvars[:num_known_outs]))</span>
  <span class="s1">out_known = while_p.bind(</span>
      <span class="s1">*in_consts</span><span class="s3">, </span><span class="s1">cond_nconsts=cond_nconsts_known</span><span class="s3">, </span><span class="s1">cond_jaxpr=cond_jaxpr_known</span><span class="s3">,</span>
      <span class="s1">body_nconsts=body_nconsts_known</span><span class="s3">, </span><span class="s1">body_jaxpr=body_jaxpr_known)</span>
  <span class="s3">del </span><span class="s1">body_jaxpr_known</span>

  <span class="s0"># Run the whole while_loop to get all the outputs, then merge with known ones</span>
  <span class="s1">out_tracers_ = trace.default_process_primitive(while_p</span><span class="s3">, </span><span class="s1">tracers</span><span class="s3">, </span><span class="s1">params)</span>
  <span class="s1">out_tracers = [t </span><span class="s3">for </span><span class="s1">t</span><span class="s3">, </span><span class="s1">uk </span><span class="s3">in </span><span class="s1">zip(out_tracers_</span><span class="s3">, </span><span class="s1">carry_uk) </span><span class="s3">if </span><span class="s1">uk]</span>
  <span class="s3">return </span><span class="s1">util.merge_lists(carry_uk</span><span class="s3">, </span><span class="s1">out_known</span><span class="s3">, </span><span class="s1">out_tracers)</span>

<span class="s0"># TODO(mattjj): de-duplicate code with _while_partial_eval</span>
<span class="s3">def </span><span class="s1">_while_partial_eval_custom(saveable</span><span class="s3">, </span><span class="s1">unks_in</span><span class="s3">, </span><span class="s1">inst_in</span><span class="s3">, </span><span class="s1">eqn):</span>
  <span class="s3">del </span><span class="s1">saveable  </span><span class="s0"># We can't save any residuals anyway (w/o dynamic shapes)!</span>
  <span class="s1">cond_jaxpr = eqn.params[</span><span class="s4">'cond_jaxpr'</span><span class="s1">]</span>
  <span class="s1">cond_nconsts = eqn.params[</span><span class="s4">'cond_nconsts'</span><span class="s1">]</span>
  <span class="s1">body_jaxpr = eqn.params[</span><span class="s4">'body_jaxpr'</span><span class="s1">]</span>
  <span class="s1">body_nconsts = eqn.params[</span><span class="s4">'body_nconsts'</span><span class="s1">]</span>

  <span class="s1">cond_consts_uk</span><span class="s3">, </span><span class="s1">body_consts_uk</span><span class="s3">, </span><span class="s1">carry_init_uk = \</span>
      <span class="s1">split_list(unks_in</span><span class="s3">, </span><span class="s1">[cond_nconsts</span><span class="s3">, </span><span class="s1">body_nconsts])</span>

  <span class="s0"># Fixpoint to compute known part of the body (trivial on 'inst_in', since we</span>
  <span class="s0"># make all inputs available as DCE can subsequently prune any unused ones)</span>
  <span class="s1">carry_uk = carry_init_uk</span>
  <span class="s3">for </span><span class="s1">_ </span><span class="s3">in </span><span class="s1">range(</span><span class="s5">1 </span><span class="s1">+ len(carry_uk)):</span>
    <span class="s1">body_unks_in = body_consts_uk + carry_uk</span>
    <span class="s1">jaxpr_known_</span><span class="s3">, </span><span class="s1">_</span><span class="s3">, </span><span class="s1">carry_uk_out</span><span class="s3">, </span><span class="s1">_</span><span class="s3">, </span><span class="s1">num_res = \</span>
        <span class="s1">pe.partial_eval_jaxpr_custom(</span>
            <span class="s1">body_jaxpr.jaxpr</span><span class="s3">, </span><span class="s1">in_unknowns=body_unks_in</span><span class="s3">, </span><span class="s1">in_inst=</span><span class="s3">True,</span>
            <span class="s1">ensure_out_unknowns=carry_uk</span><span class="s3">, </span><span class="s1">ensure_out_inst=</span><span class="s3">True,</span>
            <span class="s1">saveable=ad_checkpoint.nothing_saveable)</span>
    <span class="s3">if </span><span class="s1">carry_uk_out == carry_uk:</span>
      <span class="s3">break</span>
    <span class="s3">else</span><span class="s1">:</span>
      <span class="s1">carry_uk = _map(operator.or_</span><span class="s3">, </span><span class="s1">carry_uk</span><span class="s3">, </span><span class="s1">carry_uk_out)</span>
  <span class="s3">else</span><span class="s1">:</span>
    <span class="s3">assert False, </span><span class="s4">&quot;Fixpoint not reached&quot;</span>
  <span class="s3">assert not </span><span class="s1">num_res</span>
  <span class="s1">body_jaxpr_known = core.ClosedJaxpr(jaxpr_known_</span><span class="s3">, </span><span class="s1">body_jaxpr.consts)</span>
  <span class="s3">del </span><span class="s1">jaxpr_known_</span><span class="s3">, </span><span class="s1">carry_uk_out</span><span class="s3">, </span><span class="s1">num_res</span>

  <span class="s0"># Instantiate all inputs (b/c jaxpr_staged will take all inputs).</span>
  <span class="s1">new_inst = [x </span><span class="s3">for </span><span class="s1">x</span><span class="s3">, </span><span class="s1">inst </span><span class="s3">in </span><span class="s1">zip(eqn.invars</span><span class="s3">, </span><span class="s1">inst_in)</span>
              <span class="s3">if </span><span class="s1">type(x) </span><span class="s3">is </span><span class="s1">core.Var </span><span class="s3">and not </span><span class="s1">inst]</span>

  <span class="s0"># Compute the known part of cond_fun (basically pruning inputs on known side).</span>
  <span class="s1">cond_unks_in = cond_consts_uk + carry_uk</span>
  <span class="s1">cond_jaxpr_known_</span><span class="s3">, </span><span class="s1">_</span><span class="s3">, </span><span class="s1">[cond_uk]</span><span class="s3">, </span><span class="s1">_</span><span class="s3">, </span><span class="s1">_ = \</span>
      <span class="s1">pe.partial_eval_jaxpr_custom(</span>
          <span class="s1">cond_jaxpr.jaxpr</span><span class="s3">, </span><span class="s1">cond_unks_in</span><span class="s3">, </span><span class="s1">in_inst=</span><span class="s3">True,</span>
          <span class="s1">ensure_out_unknowns=</span><span class="s3">False, </span><span class="s1">ensure_out_inst=</span><span class="s3">True,</span>
          <span class="s1">saveable=ad_checkpoint.nothing_saveable)</span>
  <span class="s0"># NOTE(mattjj): I think it should be impossible for the condition to be</span>
  <span class="s0"># unknown, but asserting that caused a test failure in diffrax. So</span>
  <span class="s0"># we handle it: if it is unknown, stage out the whole cond function.</span>
  <span class="s3">if </span><span class="s1">cond_uk:</span>
    <span class="s3">return None, </span><span class="s1">eqn</span><span class="s3">, </span><span class="s1">[</span><span class="s3">True</span><span class="s1">] * len(carry_uk)</span><span class="s3">, </span><span class="s1">[</span><span class="s3">True</span><span class="s1">] * len(carry_uk)</span><span class="s3">, </span><span class="s1">new_inst</span>
  <span class="s1">cond_jaxpr_known = core.ClosedJaxpr(cond_jaxpr_known_</span><span class="s3">, </span><span class="s1">cond_jaxpr.consts)</span>
  <span class="s3">del </span><span class="s1">cond_uk</span>

  <span class="s0"># Build the known eqn.</span>
  <span class="s1">ins_known</span><span class="s3">, </span><span class="s1">_ = partition_list(unks_in</span><span class="s3">, </span><span class="s1">eqn.invars)</span>
  <span class="s1">out_binders_known</span><span class="s3">, </span><span class="s1">_ = partition_list(carry_uk</span><span class="s3">, </span><span class="s1">eqn.outvars)</span>
  <span class="s1">params_known = dict(cond_jaxpr=cond_jaxpr_known</span><span class="s3">, </span><span class="s1">body_jaxpr=body_jaxpr_known</span><span class="s3">,</span>
                      <span class="s1">cond_nconsts=len(cond_consts_uk) - sum(cond_consts_uk)</span><span class="s3">,</span>
                      <span class="s1">body_nconsts=len(body_consts_uk) - sum(body_consts_uk))</span>
  <span class="s1">effects_known = core.join_effects(cond_jaxpr_known.effects</span><span class="s3">,</span>
                                    <span class="s1">body_jaxpr_known.effects)</span>
  <span class="s1">eqn_known = pe.new_jaxpr_eqn(ins_known</span><span class="s3">, </span><span class="s1">out_binders_known</span><span class="s3">, </span><span class="s1">while_p</span><span class="s3">,</span>
                               <span class="s1">params_known</span><span class="s3">, </span><span class="s1">effects_known</span><span class="s3">, </span><span class="s1">eqn.source_info)</span>

  <span class="s0"># Staged eqn is same as input eqn.</span>
  <span class="s1">eqn_staged = eqn</span>

  <span class="s1">unks_out = carry_uk</span>
  <span class="s1">inst_out = [</span><span class="s3">True</span><span class="s1">] * len(unks_out)</span>
  <span class="s3">return </span><span class="s1">eqn_known</span><span class="s3">, </span><span class="s1">eqn_staged</span><span class="s3">, </span><span class="s1">unks_out</span><span class="s3">, </span><span class="s1">inst_out</span><span class="s3">, </span><span class="s1">new_inst</span>

<span class="s3">def </span><span class="s1">_while_transpose_error(*_</span><span class="s3">, </span><span class="s1">**kwargs):</span>
  <span class="s3">raise </span><span class="s1">ValueError(</span><span class="s4">&quot;Reverse-mode differentiation does not work for &quot;</span>
                   <span class="s4">&quot;lax.while_loop or lax.fori_loop. &quot;</span>
                   <span class="s4">&quot;Try using lax.scan instead.&quot;</span><span class="s1">)</span>

<span class="s0"># For a while loop with ordered effects in the cond, we need a special</span>
<span class="s0"># lowering. Fundamentally, we'd like to rewrite a while loop that looks like</span>
<span class="s0"># this:</span>
<span class="s0"># ```</span>
<span class="s0"># while cond(x):</span>
<span class="s0">#   x = body(x)</span>
<span class="s0"># ```</span>
<span class="s0"># into something that looks like this:</span>
<span class="s0"># ```</span>
<span class="s0"># while True:</span>
<span class="s0">#   token, pred = cond(token, x)</span>
<span class="s0">#   if not pred:</span>
<span class="s0">#     break</span>
<span class="s0">#   token, x = body(token, x)</span>
<span class="s0"># ```</span>
<span class="s0"># Unfortunately, with a WhileOp we can't (1) return multiple values</span>
<span class="s0"># from a `cond` and (2) can't break a while loop. We thus adopt the</span>
<span class="s0"># following rewrite strategy:</span>
<span class="s0"># ```</span>
<span class="s0"># def new_cond(pred, token, x):</span>
<span class="s0">#   return pred</span>
<span class="s0"># token, pred = cond(token, x)</span>
<span class="s0"># while new_cond(pred, token, x):</span>
<span class="s0">#   token, x = body(token, x)</span>
<span class="s0">#   token, pred = cond(token, x)</span>
<span class="s0"># ```</span>
<span class="s3">def </span><span class="s1">_while_lowering(ctx</span><span class="s3">, </span><span class="s1">*args</span><span class="s3">, </span><span class="s1">cond_jaxpr</span><span class="s3">, </span><span class="s1">body_jaxpr</span><span class="s3">, </span><span class="s1">cond_nconsts</span><span class="s3">,</span>
                    <span class="s1">body_nconsts):</span>
  <span class="s1">pred_aval = cond_jaxpr.out_avals[</span><span class="s5">0</span><span class="s1">]</span>
  <span class="s1">batched = bool(pred_aval.shape)</span>
  <span class="s1">cond_ordered_effects = effects.ordered_effects.filter_in(cond_jaxpr.effects)</span>
  <span class="s3">if </span><span class="s1">cond_ordered_effects:</span>
    <span class="s3">def </span><span class="s1">cond(args):</span>
      <span class="s0"># Pred can be batched</span>
      <span class="s1">pred = core.eval_jaxpr(cond_jaxpr.jaxpr</span><span class="s3">, </span><span class="s1">cond_jaxpr.consts</span><span class="s3">, </span><span class="s1">*args)[</span><span class="s5">0</span><span class="s1">]</span>
      <span class="s3">if </span><span class="s1">batched:</span>
        <span class="s1">pred = lax._reduce_or(pred</span><span class="s3">, </span><span class="s1">tuple(range(len(pred_aval.shape))))</span>
      <span class="s3">return </span><span class="s1">pred</span>
    <span class="s3">def </span><span class="s1">body(args):</span>
      <span class="s3">return </span><span class="s1">tuple(core.eval_jaxpr(body_jaxpr.jaxpr</span><span class="s3">, </span><span class="s1">body_jaxpr.consts</span><span class="s3">, </span><span class="s1">*args))</span>
    <span class="s3">def </span><span class="s1">new_cond(pred_args):</span>
      <span class="s1">pred</span><span class="s3">, </span><span class="s1">_ = pred_args</span>
      <span class="s3">return </span><span class="s1">pred</span>
    <span class="s3">def </span><span class="s1">new_body(pred_args):</span>
      <span class="s1">_</span><span class="s3">, </span><span class="s1">args  = pred_args</span>
      <span class="s1">args = body(args)</span>
      <span class="s1">pred = cond(args)</span>
      <span class="s3">return </span><span class="s1">pred</span><span class="s3">, </span><span class="s1">args</span>
    <span class="s3">def </span><span class="s1">fun(*args):</span>
      <span class="s1">pred = cond(args)</span>
      <span class="s1">_</span><span class="s3">, </span><span class="s1">out = while_loop(new_cond</span><span class="s3">, </span><span class="s1">new_body</span><span class="s3">, </span><span class="s1">(pred</span><span class="s3">, </span><span class="s1">args))</span>
      <span class="s3">return </span><span class="s1">out</span>
    <span class="s3">return </span><span class="s1">mlir.lower_fun(fun)(ctx</span><span class="s3">, </span><span class="s1">*args)</span>

  <span class="s1">loop_carry_types = _map(mlir.aval_to_ir_types</span><span class="s3">, </span><span class="s1">ctx.avals_in)</span>
  <span class="s1">body_effects = effects.ordered_effects.filter_in(body_jaxpr.effects)</span>
  <span class="s1">num_tokens = len(body_effects)</span>
  <span class="s1">tokens = [ctx.tokens_in.get(eff) </span><span class="s3">for </span><span class="s1">eff </span><span class="s3">in </span><span class="s1">body_effects]</span>
  <span class="s1">token_types = [mlir.token_type() </span><span class="s3">for </span><span class="s1">_ </span><span class="s3">in </span><span class="s1">tokens]</span>
  <span class="s1">loop_carry_types = [*token_types</span><span class="s3">, </span><span class="s1">*loop_carry_types]</span>
  <span class="s1">flat_loop_carry_types = util.flatten(loop_carry_types)</span>
  <span class="s1">args = [*tokens</span><span class="s3">, </span><span class="s1">*args]</span>

  <span class="s1">flat_args = mlir.flatten_lowering_ir_args(args)</span>
  <span class="s1">while_op = hlo.WhileOp(flat_loop_carry_types</span><span class="s3">, </span><span class="s1">flat_args)</span>

  <span class="s0"># Loop condition</span>
  <span class="s1">cond_block = while_op.regions[</span><span class="s5">0</span><span class="s1">].blocks.append(*flat_loop_carry_types)</span>
  <span class="s1">name_stack = ctx.module_context.name_stack.extend(</span><span class="s4">'while'</span><span class="s1">)</span>
  <span class="s3">with </span><span class="s1">ir.InsertionPoint(cond_block):</span>
    <span class="s1">flat_cond_args = [</span>
        <span class="s1">cond_block.arguments[i] </span><span class="s3">for </span><span class="s1">i </span><span class="s3">in </span><span class="s1">range(len(flat_loop_carry_types))</span>
    <span class="s1">]</span>
    <span class="s1">cond_args = util.unflatten(flat_cond_args</span><span class="s3">, </span><span class="s1">_map(len</span><span class="s3">, </span><span class="s1">loop_carry_types))</span>
    <span class="s0"># Remove tokens from cond args</span>
    <span class="s1">cond_args = cond_args[num_tokens:]</span>
    <span class="s1">x</span><span class="s3">, </span><span class="s1">_</span><span class="s3">, </span><span class="s1">z = util.split_list(cond_args</span><span class="s3">, </span><span class="s1">[cond_nconsts</span><span class="s3">, </span><span class="s1">body_nconsts])</span>
    <span class="s1">cond_ctx = ctx.module_context.replace(name_stack=name_stack.extend(</span><span class="s4">'cond'</span><span class="s1">))</span>
    <span class="s1">((pred</span><span class="s3">,</span><span class="s1">)</span><span class="s3">,</span><span class="s1">)</span><span class="s3">, </span><span class="s1">_ = mlir.jaxpr_subcomp(cond_ctx</span><span class="s3">, </span><span class="s1">cond_jaxpr.jaxpr</span><span class="s3">, </span><span class="s1">mlir.TokenSet()</span><span class="s3">,</span>
                                       <span class="s1">_map(mlir.ir_constants</span><span class="s3">, </span><span class="s1">cond_jaxpr.consts)</span><span class="s3">,</span>
                                       <span class="s1">*(x + z)</span><span class="s3">, </span><span class="s1">dim_var_values=ctx.dim_var_values)</span>
    <span class="s3">if </span><span class="s1">batched:</span>
      <span class="s1">pred_ctx = mlir.LoweringRuleContext(</span>
          <span class="s1">module_context=ctx.module_context</span><span class="s3">,</span>
          <span class="s1">primitive=</span><span class="s3">None,</span>
          <span class="s1">avals_in=[pred_aval]</span><span class="s3">,</span>
          <span class="s1">avals_out=[pred_aval.update(shape=())]</span><span class="s3">,</span>
          <span class="s1">tokens_in=mlir.TokenSet()</span><span class="s3">,</span>
          <span class="s1">tokens_out=</span><span class="s3">None</span><span class="s1">)</span>
      <span class="s1">pred</span><span class="s3">, </span><span class="s1">= lax._unary_reduce_lower(</span>
          <span class="s1">hlo.OrOp</span><span class="s3">,</span>
          <span class="s3">lambda </span><span class="s1">dtype: np.array(</span><span class="s3">False, </span><span class="s1">dtype)</span><span class="s3">,</span>
          <span class="s1">pred_ctx</span><span class="s3">,</span>
          <span class="s1">pred</span><span class="s3">,</span>
          <span class="s1">axes=tuple(range(len(pred_aval.shape))))</span>
    <span class="s1">hlo.ReturnOp([pred])</span>

  <span class="s0"># Loop body</span>
  <span class="s1">body_block = while_op.regions[</span><span class="s5">1</span><span class="s1">].blocks.append(*flat_loop_carry_types)</span>
  <span class="s3">with </span><span class="s1">ir.InsertionPoint(body_block):</span>
    <span class="s1">flat_body_args = [</span>
        <span class="s1">body_block.arguments[i] </span><span class="s3">for </span><span class="s1">i </span><span class="s3">in </span><span class="s1">range(len(flat_loop_carry_types))</span>
    <span class="s1">]</span>
    <span class="s1">body_args = util.unflatten(flat_body_args</span><span class="s3">, </span><span class="s1">_map(len</span><span class="s3">, </span><span class="s1">loop_carry_types))</span>
    <span class="s0"># Tokens are at the front of the args list to the while loop</span>
    <span class="s1">token_args</span><span class="s3">, </span><span class="s1">body_args = util.split_list(body_args</span><span class="s3">, </span><span class="s1">[num_tokens])</span>
    <span class="s1">tokens_in = mlir.TokenSet(zip(body_effects</span><span class="s3">, </span><span class="s1">token_args))</span>
    <span class="s1">x</span><span class="s3">, </span><span class="s1">y</span><span class="s3">, </span><span class="s1">z = util.split_list(body_args</span><span class="s3">, </span><span class="s1">[cond_nconsts</span><span class="s3">, </span><span class="s1">body_nconsts])</span>
    <span class="s1">body_ctx = ctx.module_context.replace(name_stack=name_stack.extend(</span><span class="s4">'body'</span><span class="s1">))</span>
    <span class="s1">new_z</span><span class="s3">, </span><span class="s1">tokens_out = mlir.jaxpr_subcomp(body_ctx</span><span class="s3">, </span><span class="s1">body_jaxpr.jaxpr</span><span class="s3">,</span>
        <span class="s1">tokens_in</span><span class="s3">, </span><span class="s1">_map(mlir.ir_constants</span><span class="s3">, </span><span class="s1">body_jaxpr.consts)</span><span class="s3">,</span>
        <span class="s1">*(y + z)</span><span class="s3">, </span><span class="s1">dim_var_values=ctx.dim_var_values)</span>
    <span class="s1">out_tokens = [tokens_out.get(eff) </span><span class="s3">for </span><span class="s1">eff </span><span class="s3">in </span><span class="s1">body_effects]</span>
    <span class="s3">if </span><span class="s1">batched:</span>
      <span class="s1">body_pred_ctx = ctx.module_context.replace(</span>
          <span class="s1">name_stack=name_stack.extend(</span><span class="s4">'body_pred'</span><span class="s1">))</span>
      <span class="s1">((body_pred</span><span class="s3">,</span><span class="s1">)</span><span class="s3">,</span><span class="s1">)</span><span class="s3">, </span><span class="s1">_ = mlir.jaxpr_subcomp(</span>
          <span class="s1">body_pred_ctx</span><span class="s3">, </span><span class="s1">cond_jaxpr.jaxpr</span><span class="s3">, </span><span class="s1">mlir.TokenSet()</span><span class="s3">,</span>
          <span class="s1">_map(mlir.ir_constants</span><span class="s3">, </span><span class="s1">cond_jaxpr.consts)</span><span class="s3">,</span>
          <span class="s1">*(x + z)</span><span class="s3">, </span><span class="s1">dim_var_values=ctx.dim_var_values)</span>
      <span class="s1">new_z = _map(</span>
          <span class="s1">partial(_pred_bcast_select_hlo</span><span class="s3">, </span><span class="s1">ctx</span><span class="s3">, </span><span class="s1">pred_aval</span><span class="s3">, </span><span class="s1">body_pred)</span><span class="s3">, </span><span class="s1">new_z</span><span class="s3">, </span><span class="s1">z</span><span class="s3">,</span>
          <span class="s1">body_jaxpr.out_avals)</span>

    <span class="s1">hlo.ReturnOp([*util.flatten(out_tokens)</span><span class="s3">, </span><span class="s1">*util.flatten(x)</span><span class="s3">,</span>
                  <span class="s1">*util.flatten(y)</span><span class="s3">, </span><span class="s1">*util.flatten(new_z)])</span>

  <span class="s1">outputs = util.unflatten(while_op.results</span><span class="s3">, </span><span class="s1">_map(len</span><span class="s3">, </span><span class="s1">loop_carry_types))</span>
  <span class="s1">tokens</span><span class="s3">, </span><span class="s1">_</span><span class="s3">, </span><span class="s1">_</span><span class="s3">, </span><span class="s1">z = util.split_list(outputs</span><span class="s3">, </span><span class="s1">[num_tokens</span><span class="s3">, </span><span class="s1">cond_nconsts</span><span class="s3">, </span><span class="s1">body_nconsts])</span>
  <span class="s3">if </span><span class="s1">tokens:</span>
    <span class="s1">ctx.set_tokens_out(mlir.TokenSet(zip(body_effects</span><span class="s3">, </span><span class="s1">tokens)))</span>
  <span class="s3">return </span><span class="s1">z</span>

<span class="s3">def </span><span class="s1">_while_typecheck(_</span><span class="s3">, </span><span class="s1">*in_atoms</span><span class="s3">, </span><span class="s1">cond_jaxpr</span><span class="s3">, </span><span class="s1">body_jaxpr</span><span class="s3">, </span><span class="s1">cond_nconsts</span><span class="s3">,</span>
                     <span class="s1">body_nconsts):</span>
  <span class="s0"># TODO(frostig,mattjj): check cond_jaxpr, body_jaxpr types</span>
  <span class="s1">joined_effects = _join_while_effects(body_jaxpr</span><span class="s3">, </span><span class="s1">cond_jaxpr</span><span class="s3">, </span><span class="s1">body_nconsts</span><span class="s3">,</span>
                                       <span class="s1">cond_nconsts)</span>
  <span class="s1">disallowed_effects = allowed_effects.filter_not_in(joined_effects)</span>
  <span class="s3">if </span><span class="s1">disallowed_effects:</span>
    <span class="s3">raise </span><span class="s1">NotImplementedError(</span>
        <span class="s4">f'Effects not supported in `while`: </span><span class="s3">{</span><span class="s1">disallowed_effects</span><span class="s3">}</span><span class="s4">'</span><span class="s1">)</span>
  <span class="s3">return </span><span class="s1">body_jaxpr.out_avals</span><span class="s3">, </span><span class="s1">joined_effects</span>

<span class="s1">while_p = core.AxisPrimitive(</span><span class="s4">'while'</span><span class="s1">)</span>
<span class="s1">while_p.multiple_results = </span><span class="s3">True</span>
<span class="s1">while_p.def_impl(partial(dispatch.apply_primitive</span><span class="s3">, </span><span class="s1">while_p))</span>
<span class="s1">while_p.def_effectful_abstract_eval(_while_loop_abstract_eval)</span>
<span class="s1">ad.primitive_jvps[while_p] = _while_loop_jvp</span>
<span class="s1">pe.custom_partial_eval_rules[while_p] = _while_partial_eval</span>
<span class="s1">xla.register_initial_style_primitive(while_p)</span>
<span class="s1">ad.primitive_transposes[while_p] = _while_transpose_error</span>
<span class="s1">batching.axis_primitive_batchers[while_p] = partial(_while_loop_batching_rule</span><span class="s3">, None</span><span class="s1">)</span>
<span class="s1">batching.spmd_axis_primitive_batchers[while_p] = _while_loop_batching_rule</span>
<span class="s1">pe.partial_eval_jaxpr_custom_rules[while_p] = _while_partial_eval_custom</span>
<span class="s1">mlir.register_lowering(while_p</span><span class="s3">, </span><span class="s1">_while_lowering)</span>
<span class="s1">core.custom_typechecks[while_p] = _while_typecheck</span>


<span class="s3">def </span><span class="s1">_pred_bcast_select_hlo(ctx</span><span class="s3">,</span>
    <span class="s1">pred_aval: core.ShapedArray</span><span class="s3">, </span><span class="s1">pred: ir.Value</span><span class="s3">, </span><span class="s1">xs: Sequence[ir.Value]</span><span class="s3">,</span>
    <span class="s1">ys: Sequence[ir.Value]</span><span class="s3">, </span><span class="s1">x_y_aval: core.AbstractValue) -&gt; Sequence[ir.Value]:</span>
  <span class="s3">if </span><span class="s1">x_y_aval </span><span class="s3">is </span><span class="s1">core.abstract_token:</span>
    <span class="s1">x</span><span class="s3">, </span><span class="s1">= xs</span>
    <span class="s1">y</span><span class="s3">, </span><span class="s1">= ys</span>
    <span class="s3">return </span><span class="s1">[hlo.AfterAllOp([x</span><span class="s3">, </span><span class="s1">y]).result]</span>
  <span class="s3">else</span><span class="s1">:</span>
    <span class="s3">assert </span><span class="s1">isinstance(x_y_aval</span><span class="s3">, </span><span class="s1">core.ShapedArray)</span><span class="s3">, </span><span class="s1">x_y_aval</span>
    <span class="s1">x</span><span class="s3">, </span><span class="s1">= xs</span>
    <span class="s1">y</span><span class="s3">, </span><span class="s1">= ys</span>
    <span class="s3">assert </span><span class="s1">x.type == y.type</span><span class="s3">, </span><span class="s1">(x.type</span><span class="s3">, </span><span class="s1">y.type)</span>
    <span class="s3">assert </span><span class="s1">(pred_aval.shape == x_y_aval.shape[:len(pred_aval.shape)])</span><span class="s3">, </span><span class="s1">(</span>
            <span class="s1">pred_aval.shape</span><span class="s3">, </span><span class="s1">x_y_aval)</span>
    <span class="s3">if </span><span class="s1">core.is_opaque_dtype(x_y_aval.dtype):</span>
      <span class="s1">x_y_shape = mlir.aval_to_ir_type(x_y_aval).shape</span>
    <span class="s3">else</span><span class="s1">:</span>
      <span class="s1">x_y_shape = x_y_aval.shape</span>
    <span class="s1">bcast_pred = mlir.broadcast_in_dim(ctx</span><span class="s3">, </span><span class="s1">pred</span><span class="s3">, </span><span class="s1">core.DShapedArray(x_y_shape</span><span class="s3">, </span><span class="s1">np.dtype(np.bool_))</span><span class="s3">,</span>
                                       <span class="s1">broadcast_dimensions=list(range(len(pred_aval.shape))))</span>
    <span class="s3">return </span><span class="s1">hlo.SelectOp(bcast_pred</span><span class="s3">, </span><span class="s1">x</span><span class="s3">, </span><span class="s1">y).results</span>

<span class="s0">### fori_loop</span>

<span class="s3">def </span><span class="s1">_fori_cond_fun(loop_carry):</span>
  <span class="s1">i</span><span class="s3">, </span><span class="s1">upper</span><span class="s3">, </span><span class="s1">_ = loop_carry</span>
  <span class="s3">return </span><span class="s1">lax.lt(i</span><span class="s3">, </span><span class="s1">upper)</span>

<span class="s1">@weakref_lru_cache</span>
<span class="s3">def </span><span class="s1">_fori_body_fun(body_fun):</span>
  <span class="s1">body_fun = weakref.ref(body_fun)</span>
  <span class="s3">def </span><span class="s1">while_body_fun(loop_carry):</span>
    <span class="s1">i</span><span class="s3">, </span><span class="s1">upper</span><span class="s3">, </span><span class="s1">x = loop_carry</span>
    <span class="s3">return </span><span class="s1">lax.add(i</span><span class="s3">, </span><span class="s1">lax._const(i</span><span class="s3">, </span><span class="s5">1</span><span class="s1">))</span><span class="s3">, </span><span class="s1">upper</span><span class="s3">, </span><span class="s1">body_fun()(i</span><span class="s3">, </span><span class="s1">x)</span>
  <span class="s3">return </span><span class="s1">while_body_fun</span>

<span class="s1">@weakref_lru_cache</span>
<span class="s3">def </span><span class="s1">_fori_scan_body_fun(body_fun):</span>
  <span class="s1">body_fun = weakref.ref(body_fun)</span>
  <span class="s3">def </span><span class="s1">scanned_fun(loop_carry</span><span class="s3">, </span><span class="s1">_):</span>
    <span class="s1">i</span><span class="s3">, </span><span class="s1">x = loop_carry</span>
    <span class="s3">return </span><span class="s1">(i + </span><span class="s5">1</span><span class="s3">, </span><span class="s1">body_fun()(i</span><span class="s3">, </span><span class="s1">x))</span><span class="s3">, None</span>
  <span class="s3">return </span><span class="s1">scanned_fun</span>

<span class="s1">@api_boundary</span>
<span class="s3">def </span><span class="s1">fori_loop(lower</span><span class="s3">, </span><span class="s1">upper</span><span class="s3">, </span><span class="s1">body_fun</span><span class="s3">, </span><span class="s1">init_val):</span>
  <span class="s2">&quot;&quot;&quot;Loop from ``lower`` to ``upper`` by reduction to :func:`jax.lax.while_loop`. 
 
  The `Haskell-like type signature`_ in brief is 
 
  .. code-block:: haskell 
 
    fori_loop :: Int -&gt; Int -&gt; ((Int, a) -&gt; a) -&gt; a -&gt; a 
 
  The semantics of ``fori_loop`` are given by this Python implementation:: 
 
    def fori_loop(lower, upper, body_fun, init_val): 
      val = init_val 
      for i in range(lower, upper): 
        val = body_fun(i, val) 
      return val 
 
  As the Python version suggests, setting ``upper &lt;= lower`` will produce no 
  iterations. Negative or custom increments are not supported. 
 
  Unlike that Python version, ``fori_loop`` is implemented in terms of either a 
  call to :func:`jax.lax.while_loop` or a call to :func:`jax.lax.scan`. If the 
  trip count is static (meaning known at tracing time, perhaps because ``lower`` 
  and ``upper`` are Python integer literals) then the ``fori_loop`` is 
  implemented in terms of :func:`~scan` and reverse-mode autodiff is supported; 
  otherwise, a ``while_loop`` is used and reverse-mode autodiff is not 
  supported.  See those functions' docstrings for more information. 
 
  Also unlike the Python analogue, the loop-carried value ``val`` must hold a 
  fixed shape and dtype across all iterations (and not just be consistent up to 
  NumPy rank/shape broadcasting and dtype promotion rules, for example). In 
  other words, the type ``a`` in the type signature above represents an array 
  with a fixed shape and dtype (or a nested tuple/list/dict container data 
  structure with a fixed structure and arrays with fixed shape and dtype at the 
  leaves). 
 
  .. note:: 
    :py:func:`fori_loop` compiles ``body_fun``, so while it can be combined with 
    :py:func:`jit`, it's usually unnecessary. 
 
  Args: 
    lower: an integer representing the loop index lower bound (inclusive) 
    upper: an integer representing the loop index upper bound (exclusive) 
    body_fun: function of type ``(int, a) -&gt; a``. 
    init_val: initial loop carry value of type ``a``. 
 
  Returns: 
    Loop value from the final iteration, of type ``a``. 
 
  .. _Haskell-like type signature: https://wiki.haskell.org/Type_signature 
  &quot;&quot;&quot;</span>
  <span class="s3">if not </span><span class="s1">callable(body_fun):</span>
    <span class="s3">raise </span><span class="s1">TypeError(</span><span class="s4">&quot;lax.fori_loop: body_fun argument should be callable.&quot;</span><span class="s1">)</span>
  <span class="s0"># TODO(phawkins): perhaps do more type checking here, better error messages.</span>
  <span class="s1">lower_dtype = dtypes.canonicalize_dtype(lax.dtype(lower))</span>
  <span class="s1">upper_dtype = dtypes.canonicalize_dtype(lax.dtype(upper))</span>
  <span class="s3">if </span><span class="s1">lower_dtype != upper_dtype:</span>
    <span class="s1">msg = (</span><span class="s4">&quot;lower and upper arguments to fori_loop must have equal types, &quot;</span>
           <span class="s4">&quot;got {} and {}&quot;</span><span class="s1">)</span>
    <span class="s3">raise </span><span class="s1">TypeError(msg.format(lower_dtype.name</span><span class="s3">, </span><span class="s1">upper_dtype.name))</span>

  <span class="s0"># If we can specialize on the trip count, call scan instead of a while_loop</span>
  <span class="s0"># to enable efficient reverse-mode differentiation.</span>
  <span class="s3">if </span><span class="s1">(isinstance(core.get_aval(lower)</span><span class="s3">, </span><span class="s1">ConcreteArray) </span><span class="s3">and</span>
      <span class="s1">isinstance(core.get_aval(upper)</span><span class="s3">, </span><span class="s1">ConcreteArray)):</span>
    <span class="s3">try</span><span class="s1">:</span>
      <span class="s1">lower_ = int(lower)</span>
      <span class="s1">upper_ = int(upper)</span>
    <span class="s3">except </span><span class="s1">TypeError:</span>
      <span class="s1">use_scan = </span><span class="s3">False</span>
    <span class="s3">else</span><span class="s1">:</span>
      <span class="s1">use_scan = </span><span class="s3">True</span>
  <span class="s3">else</span><span class="s1">:</span>
    <span class="s1">use_scan = </span><span class="s3">False</span>

  <span class="s3">if </span><span class="s1">use_scan:</span>
    <span class="s3">if </span><span class="s1">config.jax_disable_jit </span><span class="s3">and </span><span class="s1">upper_ == lower_:</span>
      <span class="s0"># non-jit implementation of scan does not support length=0</span>
      <span class="s3">return </span><span class="s1">init_val</span>

    <span class="s1">(_</span><span class="s3">, </span><span class="s1">result)</span><span class="s3">, </span><span class="s1">_ = scan(_fori_scan_body_fun(body_fun)</span><span class="s3">, </span><span class="s1">(lower_</span><span class="s3">, </span><span class="s1">init_val)</span><span class="s3">,</span>
                          <span class="s3">None, </span><span class="s1">length=upper_ - lower_)</span>
  <span class="s3">else</span><span class="s1">:</span>
    <span class="s1">_</span><span class="s3">, </span><span class="s1">_</span><span class="s3">, </span><span class="s1">result = while_loop(_fori_cond_fun</span><span class="s3">, </span><span class="s1">_fori_body_fun(body_fun)</span><span class="s3">,</span>
                              <span class="s1">(lower</span><span class="s3">, </span><span class="s1">upper</span><span class="s3">, </span><span class="s1">init_val))</span>
  <span class="s3">return </span><span class="s1">result</span>

<span class="s0">### map and miscellaneous rules</span>

<span class="s1">@api_boundary</span>
<span class="s3">def </span><span class="s1">map(f</span><span class="s3">, </span><span class="s1">xs):</span>
  <span class="s2">&quot;&quot;&quot;Map a function over leading array axes. 
 
  Like Python's builtin map, except inputs and outputs are in the form of 
  stacked arrays. Consider using the :func:`~jax.vmap` transform instead, unless you 
  need to apply a function element by element for reduced memory usage or 
  heterogeneous computation with other control flow primitives. 
 
  When ``xs`` is an array type, the semantics of :func:`~map` are given by this 
  Python implementation:: 
 
    def map(f, xs): 
      return np.stack([f(x) for x in xs]) 
 
  Like :func:`~scan`, :func:`~map` is implemented in terms of JAX primitives so 
  many of the same advantages over a Python loop apply: ``xs`` may be an 
  arbitrary nested pytree type, and the mapped computation is compiled only 
  once. 
 
  Args: 
    f: a Python function to apply element-wise over the first axis or axes of 
      ``xs``. 
    xs: values over which to map along the leading axis. 
 
  Returns: 
    Mapped values. 
  &quot;&quot;&quot;</span>
  <span class="s1">g = </span><span class="s3">lambda </span><span class="s1">_</span><span class="s3">, </span><span class="s1">x: (()</span><span class="s3">, </span><span class="s1">f(x))</span>
  <span class="s1">_</span><span class="s3">, </span><span class="s1">ys = scan(g</span><span class="s3">, </span><span class="s1">()</span><span class="s3">, </span><span class="s1">xs)</span>
  <span class="s3">return </span><span class="s1">ys</span>

<span class="s3">def </span><span class="s1">_rng_bit_generator_batching_rule(batched_args</span><span class="s3">, </span><span class="s1">batch_dims</span><span class="s3">, </span><span class="s1">*</span><span class="s3">, </span><span class="s1">shape</span><span class="s3">, </span><span class="s1">dtype</span><span class="s3">, </span><span class="s1">algorithm):</span>
  <span class="s2">&quot;&quot;&quot;Calls RBG in a loop and stacks the results.&quot;&quot;&quot;</span>
  <span class="s1">key</span><span class="s3">, </span><span class="s1">= batched_args</span>
  <span class="s1">bd</span><span class="s3">, </span><span class="s1">= batch_dims</span>
  <span class="s3">if </span><span class="s1">bd </span><span class="s3">is </span><span class="s1">batching.not_mapped:</span>
    <span class="s3">return </span><span class="s1">lax.rng_bit_generator_p.bind(key</span><span class="s3">, </span><span class="s1">shape=shape</span><span class="s3">, </span><span class="s1">dtype=dtype</span><span class="s3">,</span>
                                        <span class="s1">algorithm=algorithm)</span><span class="s3">, </span><span class="s1">(</span><span class="s3">None, None</span><span class="s1">)</span>
  <span class="s1">key = batching.moveaxis(key</span><span class="s3">, </span><span class="s1">bd</span><span class="s3">, </span><span class="s5">0</span><span class="s1">)</span>
  <span class="s1">map_body = </span><span class="s3">lambda </span><span class="s1">k: lax.rng_bit_generator_p.bind(k</span><span class="s3">, </span><span class="s1">shape=shape</span><span class="s3">, </span><span class="s1">dtype=dtype</span><span class="s3">, </span><span class="s1">algorithm=algorithm)</span>
  <span class="s1">stacked_keys</span><span class="s3">, </span><span class="s1">stacked_bits = map(map_body</span><span class="s3">, </span><span class="s1">key)</span>
  <span class="s3">return </span><span class="s1">(stacked_keys</span><span class="s3">, </span><span class="s1">stacked_bits)</span><span class="s3">, </span><span class="s1">(</span><span class="s5">0</span><span class="s3">, </span><span class="s5">0</span><span class="s1">)</span>

<span class="s1">batching.primitive_batchers[lax.rng_bit_generator_p] = _rng_bit_generator_batching_rule  </span><span class="s0"># type: ignore</span>

<span class="s0">### associative_scan</span>

<span class="s1">@api_boundary</span>
<span class="s3">def </span><span class="s1">associative_scan(fn: Callable</span><span class="s3">, </span><span class="s1">elems</span><span class="s3">, </span><span class="s1">reverse: bool = </span><span class="s3">False, </span><span class="s1">axis: int = </span><span class="s5">0</span><span class="s1">):</span>
  <span class="s2">&quot;&quot;&quot;Performs a scan with an associative binary operation, in parallel. 
 
  For an introduction to associative scans, see [BLE1990]_. 
 
  Args: 
    fn: A Python callable implementing an associative binary operation with 
      signature ``r = fn(a, b)``. Function `fn` must be associative, i.e., it 
      must satisfy the equation 
      ``fn(a, fn(b, c)) == fn(fn(a, b), c)``. 
 
      The inputs and result are (possibly nested Python tree structures of) 
      array(s) matching ``elems``. Each array has a dimension in place 
      of the ``axis`` dimension. `fn` should be applied elementwise over 
      the ``axis`` dimension (for example, by using :func:`jax.vmap` over the 
      elementwise function.) 
 
      The result ``r`` has the same shape (and structure) as the two inputs 
      ``a`` and ``b``. 
    elems: A (possibly nested Python tree structure of) array(s), each with 
      an ``axis`` dimension of size ``num_elems``. 
    reverse: A boolean stating if the scan should be reversed with respect to 
      the ``axis`` dimension. 
    axis: an integer identifying the axis over which the scan should occur. 
 
  Returns: 
    A (possibly nested Python tree structure of) array(s) of the same shape 
    and structure as ``elems``, in which the ``k``'th element of ``axis`` is the 
    result of recursively applying ``fn`` to combine the first ``k`` elements 
    of ``elems`` along ``axis``. For example, given ``elems = [a, b, c, ...]``, 
    the result would be ``[a, fn(a, b), fn(fn(a, b), c), ...]``. 
 
  Example 1: partial sums of an array of numbers: 
 
  &gt;&gt;&gt; lax.associative_scan(jnp.add, jnp.arange(0, 4)) 
  Array([0, 1, 3, 6], dtype=int32) 
 
  Example 2: partial products of an array of matrices 
 
  &gt;&gt;&gt; mats = jax.random.uniform(jax.random.PRNGKey(0), (4, 2, 2)) 
  &gt;&gt;&gt; partial_prods = lax.associative_scan(jnp.matmul, mats) 
  &gt;&gt;&gt; partial_prods.shape 
  (4, 2, 2) 
 
  Example 3: reversed partial sums of an array of numbers 
 
  &gt;&gt;&gt; lax.associative_scan(jnp.add, jnp.arange(0, 4), reverse=True) 
  Array([6, 6, 5, 3], dtype=int32) 
 
  .. [BLE1990] Blelloch, Guy E. 1990. &quot;Prefix Sums and Their Applications.&quot;, 
    Technical Report CMU-CS-90-190, School of Computer Science, Carnegie Mellon 
    University. 
  &quot;&quot;&quot;</span>
  <span class="s3">if not </span><span class="s1">callable(fn):</span>
    <span class="s3">raise </span><span class="s1">TypeError(</span><span class="s4">&quot;lax.associative_scan: fn argument should be callable.&quot;</span><span class="s1">)</span>
  <span class="s1">elems_flat</span><span class="s3">, </span><span class="s1">tree = tree_flatten(elems)</span>

  <span class="s3">if </span><span class="s1">reverse:</span>
    <span class="s1">elems_flat = [lax.rev(elem</span><span class="s3">, </span><span class="s1">[axis]) </span><span class="s3">for </span><span class="s1">elem </span><span class="s3">in </span><span class="s1">elems_flat]</span>

  <span class="s3">def </span><span class="s1">combine(a_flat</span><span class="s3">, </span><span class="s1">b_flat):</span>
    <span class="s0"># Lower `fn` to operate on flattened sequences of elems.</span>
    <span class="s1">a = tree_unflatten(tree</span><span class="s3">, </span><span class="s1">a_flat)</span>
    <span class="s1">b = tree_unflatten(tree</span><span class="s3">, </span><span class="s1">b_flat)</span>
    <span class="s1">c = fn(a</span><span class="s3">, </span><span class="s1">b)</span>
    <span class="s1">c_flat</span><span class="s3">, </span><span class="s1">_ = tree_flatten(c)</span>
    <span class="s3">return </span><span class="s1">c_flat</span>

  <span class="s0"># Check that all inputs have a consistent leading dimension `num_elems`.</span>
  <span class="s1">axis = util.canonicalize_axis(axis</span><span class="s3">, </span><span class="s1">elems_flat[</span><span class="s5">0</span><span class="s1">].ndim)</span>

  <span class="s3">if </span><span class="s1">core.is_special_dim_size(elems_flat[</span><span class="s5">0</span><span class="s1">].shape[axis]):</span>
    <span class="s3">raise </span><span class="s1">NotImplementedError(</span><span class="s4">&quot;associative scan over axis &quot;</span>
        <span class="s4">f&quot;of non-constant size: </span><span class="s3">{</span><span class="s1">elems_flat[</span><span class="s5">0</span><span class="s1">].shape[axis]</span><span class="s3">}</span><span class="s4">. You may be &quot;</span>
        <span class="s4">&quot;able to avoid this on TPU. See b/274176030.&quot;</span><span class="s1">)</span>
  <span class="s1">num_elems = int(elems_flat[</span><span class="s5">0</span><span class="s1">].shape[axis])</span>
  <span class="s3">if not </span><span class="s1">all(int(elem.shape[axis]) == num_elems </span><span class="s3">for </span><span class="s1">elem </span><span class="s3">in </span><span class="s1">elems_flat[</span><span class="s5">1</span><span class="s1">:]):</span>
    <span class="s3">raise </span><span class="s1">ValueError(</span><span class="s4">'Array inputs to associative_scan must have the same '</span>
                     <span class="s4">'first dimension. (saw: {})'</span>
                     <span class="s1">.format([elem.shape </span><span class="s3">for </span><span class="s1">elem </span><span class="s3">in </span><span class="s1">elems_flat]))</span>


  <span class="s0"># Summary of algorithm:</span>
  <span class="s0">#</span>
  <span class="s0"># Consider elements of `_scan(elems)` at odd indices. That's the same as first</span>
  <span class="s0"># summing successive pairs of elements of `elems` and performing a scan on</span>
  <span class="s0"># that half sized tensor. We perform the latter scan by recursion.</span>
  <span class="s0">#</span>
  <span class="s0"># Now consider the even elements of `_scan(elems)`. These can be computed</span>
  <span class="s0"># from the odd elements of `_scan(elems)` by adding each odd element of</span>
  <span class="s0"># `_scan(elems)` to the matching even element in the original `elems`.</span>
  <span class="s0">#</span>
  <span class="s0"># We return the odd and even elements interleaved.</span>
  <span class="s0">#</span>
  <span class="s0"># For the base case of the recursion we return the first element</span>
  <span class="s0"># of `elems` followed by the sum of the first two elements computed as</span>
  <span class="s0"># a (small two-down-to-one) reduction step.</span>
  <span class="s3">def </span><span class="s1">_scan(elems):</span>
    <span class="s2">&quot;&quot;&quot;Perform scan on `elems`.&quot;&quot;&quot;</span>

    <span class="s1">num_elems = elems[</span><span class="s5">0</span><span class="s1">].shape[axis]</span>

    <span class="s3">if </span><span class="s1">num_elems &lt; </span><span class="s5">2</span><span class="s1">:</span>
      <span class="s3">return </span><span class="s1">elems</span>

    <span class="s0"># Combine adjacent pairs of elements.</span>
    <span class="s1">reduced_elems = combine(</span>
      <span class="s1">[slicing.slice_in_dim(elem</span><span class="s3">, </span><span class="s5">0</span><span class="s3">, </span><span class="s1">-</span><span class="s5">1</span><span class="s3">, </span><span class="s1">stride=</span><span class="s5">2</span><span class="s3">, </span><span class="s1">axis=axis) </span><span class="s3">for </span><span class="s1">elem </span><span class="s3">in </span><span class="s1">elems]</span><span class="s3">,</span>
      <span class="s1">[slicing.slice_in_dim(elem</span><span class="s3">, </span><span class="s5">1</span><span class="s3">, None, </span><span class="s1">stride=</span><span class="s5">2</span><span class="s3">, </span><span class="s1">axis=axis)</span>
       <span class="s3">for </span><span class="s1">elem </span><span class="s3">in </span><span class="s1">elems])</span>

    <span class="s0"># Recursively compute scan for partially reduced tensors.</span>
    <span class="s1">odd_elems = _scan(reduced_elems)</span>

    <span class="s3">if </span><span class="s1">num_elems % </span><span class="s5">2 </span><span class="s1">== </span><span class="s5">0</span><span class="s1">:</span>
      <span class="s1">even_elems = combine(</span>
        <span class="s1">[slicing.slice_in_dim(e</span><span class="s3">, </span><span class="s5">0</span><span class="s3">, </span><span class="s1">-</span><span class="s5">1</span><span class="s3">, </span><span class="s1">axis=axis) </span><span class="s3">for </span><span class="s1">e </span><span class="s3">in </span><span class="s1">odd_elems]</span><span class="s3">,</span>
        <span class="s1">[slicing.slice_in_dim(e</span><span class="s3">, </span><span class="s5">2</span><span class="s3">, None, </span><span class="s1">stride=</span><span class="s5">2</span><span class="s3">, </span><span class="s1">axis=axis) </span><span class="s3">for </span><span class="s1">e </span><span class="s3">in </span><span class="s1">elems])</span>
    <span class="s3">else</span><span class="s1">:</span>
      <span class="s1">even_elems = combine(</span>
        <span class="s1">odd_elems</span><span class="s3">,</span>
        <span class="s1">[slicing.slice_in_dim(e</span><span class="s3">, </span><span class="s5">2</span><span class="s3">, None, </span><span class="s1">stride=</span><span class="s5">2</span><span class="s3">, </span><span class="s1">axis=axis) </span><span class="s3">for </span><span class="s1">e </span><span class="s3">in </span><span class="s1">elems])</span>

    <span class="s0"># The first element of a scan is the same as the first element</span>
    <span class="s0"># of the original `elems`.</span>
    <span class="s1">even_elems = [</span>
      <span class="s1">lax.concatenate([slicing.slice_in_dim(elem</span><span class="s3">, </span><span class="s5">0</span><span class="s3">, </span><span class="s5">1</span><span class="s3">, </span><span class="s1">axis=axis)</span><span class="s3">, </span><span class="s1">result]</span><span class="s3">,</span>
                      <span class="s1">dimension=axis)</span>
      <span class="s3">for </span><span class="s1">(elem</span><span class="s3">, </span><span class="s1">result) </span><span class="s3">in </span><span class="s1">zip(elems</span><span class="s3">, </span><span class="s1">even_elems)]</span>
    <span class="s3">return </span><span class="s1">list(_map(partial(_interleave</span><span class="s3">, </span><span class="s1">axis=axis)</span><span class="s3">, </span><span class="s1">even_elems</span><span class="s3">, </span><span class="s1">odd_elems))</span>

  <span class="s1">scans = _scan(elems_flat)</span>

  <span class="s3">if </span><span class="s1">reverse:</span>
    <span class="s1">scans = [lax.rev(scanned</span><span class="s3">, </span><span class="s1">[axis]) </span><span class="s3">for </span><span class="s1">scanned </span><span class="s3">in </span><span class="s1">scans]</span>

  <span class="s3">return </span><span class="s1">tree_unflatten(tree</span><span class="s3">, </span><span class="s1">scans)</span>

<span class="s3">def </span><span class="s1">_interleave(a</span><span class="s3">, </span><span class="s1">b</span><span class="s3">, </span><span class="s1">axis):</span>
  <span class="s2">&quot;&quot;&quot;Given two Tensors of static shape, interleave them along the first axis.&quot;&quot;&quot;</span>
  <span class="s3">assert </span><span class="s1">a.shape[axis] == b.shape[axis] </span><span class="s3">or </span><span class="s1">a.shape[axis] == b.shape[axis] + </span><span class="s5">1</span>
  <span class="s1">a_pad = [(</span><span class="s5">0</span><span class="s3">, </span><span class="s5">0</span><span class="s3">, </span><span class="s5">0</span><span class="s1">)] * a.ndim</span>
  <span class="s1">b_pad = [(</span><span class="s5">0</span><span class="s3">, </span><span class="s5">0</span><span class="s3">, </span><span class="s5">0</span><span class="s1">)] * b.ndim</span>
  <span class="s1">a_pad[axis] = (</span><span class="s5">0</span><span class="s3">, </span><span class="s5">1 </span><span class="s3">if </span><span class="s1">a.shape[axis] == b.shape[axis] </span><span class="s3">else </span><span class="s5">0</span><span class="s3">, </span><span class="s5">1</span><span class="s1">)</span>
  <span class="s1">b_pad[axis] = (</span><span class="s5">1</span><span class="s3">, </span><span class="s5">0 </span><span class="s3">if </span><span class="s1">a.shape[axis] == b.shape[axis] </span><span class="s3">else </span><span class="s5">1</span><span class="s3">, </span><span class="s5">1</span><span class="s1">)</span>
  <span class="s1">op = lax.bitwise_or </span><span class="s3">if </span><span class="s1">a.dtype == np.bool_ </span><span class="s3">else </span><span class="s1">lax.add</span>
  <span class="s3">return </span><span class="s1">op(lax.pad(a</span><span class="s3">, </span><span class="s1">lax._const(a</span><span class="s3">, </span><span class="s5">0</span><span class="s1">)</span><span class="s3">, </span><span class="s1">a_pad)</span><span class="s3">,</span>
            <span class="s1">lax.pad(b</span><span class="s3">, </span><span class="s1">lax._const(b</span><span class="s3">, </span><span class="s5">0</span><span class="s1">)</span><span class="s3">, </span><span class="s1">b_pad))</span>

<span class="s0">### Cumulative reductions.</span>

<span class="s3">def </span><span class="s1">cumsum(operand: Array</span><span class="s3">, </span><span class="s1">axis: int = </span><span class="s5">0</span><span class="s3">, </span><span class="s1">reverse: bool = </span><span class="s3">False</span><span class="s1">) -&gt; Array:</span>
  <span class="s2">&quot;&quot;&quot;Computes a cumulative sum along `axis`.&quot;&quot;&quot;</span>
  <span class="s3">return </span><span class="s1">cumsum_p.bind(operand</span><span class="s3">, </span><span class="s1">axis=int(axis)</span><span class="s3">, </span><span class="s1">reverse=bool(reverse))</span>

<span class="s3">def </span><span class="s1">cumprod(operand: Array</span><span class="s3">, </span><span class="s1">axis: int = </span><span class="s5">0</span><span class="s3">, </span><span class="s1">reverse: bool = </span><span class="s3">False</span><span class="s1">) -&gt; Array:</span>
  <span class="s2">&quot;&quot;&quot;Computes a cumulative product along `axis`.&quot;&quot;&quot;</span>
  <span class="s3">return </span><span class="s1">cumprod_p.bind(operand</span><span class="s3">, </span><span class="s1">axis=int(axis)</span><span class="s3">, </span><span class="s1">reverse=bool(reverse))</span>

<span class="s3">def </span><span class="s1">cummax(operand: Array</span><span class="s3">, </span><span class="s1">axis: int = </span><span class="s5">0</span><span class="s3">, </span><span class="s1">reverse: bool = </span><span class="s3">False</span><span class="s1">) -&gt; Array:</span>
  <span class="s2">&quot;&quot;&quot;Computes a cumulative maximum along `axis`.&quot;&quot;&quot;</span>
  <span class="s3">return </span><span class="s1">cummax_p.bind(operand</span><span class="s3">, </span><span class="s1">axis=int(axis)</span><span class="s3">, </span><span class="s1">reverse=bool(reverse))</span>

<span class="s3">def </span><span class="s1">cummin(operand: Array</span><span class="s3">, </span><span class="s1">axis: int = </span><span class="s5">0</span><span class="s3">, </span><span class="s1">reverse: bool = </span><span class="s3">False</span><span class="s1">) -&gt; Array:</span>
  <span class="s2">&quot;&quot;&quot;Computes a cumulative minimum along `axis`.&quot;&quot;&quot;</span>
  <span class="s3">return </span><span class="s1">cummin_p.bind(operand</span><span class="s3">, </span><span class="s1">axis=int(axis)</span><span class="s3">, </span><span class="s1">reverse=bool(reverse))</span>

<span class="s3">def </span><span class="s1">cumlogsumexp(operand: Array</span><span class="s3">, </span><span class="s1">axis: int = </span><span class="s5">0</span><span class="s3">, </span><span class="s1">reverse: bool = </span><span class="s3">False</span><span class="s1">) -&gt; Array:</span>
  <span class="s2">&quot;&quot;&quot;Computes a cumulative logsumexp along `axis`.&quot;&quot;&quot;</span>
  <span class="s3">return </span><span class="s1">cumlogsumexp_p.bind(operand</span><span class="s3">, </span><span class="s1">axis=int(axis)</span><span class="s3">, </span><span class="s1">reverse=bool(reverse))</span>

<span class="s3">def </span><span class="s1">_cumred_shape_rule(x</span><span class="s3">, </span><span class="s1">*</span><span class="s3">, </span><span class="s1">axis: int</span><span class="s3">, </span><span class="s1">reverse: bool):</span>
  <span class="s3">if </span><span class="s1">axis &lt; </span><span class="s5">0 </span><span class="s3">or </span><span class="s1">axis &gt;= x.ndim:</span>
    <span class="s3">raise </span><span class="s1">ValueError(</span>
        <span class="s4">f&quot;axis </span><span class="s3">{</span><span class="s1">axis</span><span class="s3">} </span><span class="s4">is out of bounds for array of shape </span><span class="s3">{</span><span class="s1">x.shape</span><span class="s3">}</span><span class="s4">&quot;</span><span class="s1">)</span>
  <span class="s3">return </span><span class="s1">x.shape</span>

<span class="s3">def </span><span class="s1">_cumsum_transpose_rule(t</span><span class="s3">, </span><span class="s1">operand</span><span class="s3">, </span><span class="s1">*</span><span class="s3">, </span><span class="s1">axis: int</span><span class="s3">, </span><span class="s1">reverse: bool):</span>
  <span class="s3">return </span><span class="s1">[cumsum(t</span><span class="s3">, </span><span class="s1">axis=axis</span><span class="s3">, </span><span class="s1">reverse=</span><span class="s3">not </span><span class="s1">reverse)]</span>



<span class="s3">def </span><span class="s1">cumred_reduce_window_impl(window_reduce: Callable</span><span class="s3">, </span><span class="s1">x</span><span class="s3">, </span><span class="s1">*</span><span class="s3">, </span><span class="s1">axis: int</span><span class="s3">,</span>
                              <span class="s1">reverse: bool):</span>
  <span class="s1">n = x.shape[axis]</span>
  <span class="s3">if </span><span class="s1">n == </span><span class="s5">0</span><span class="s1">:</span>
    <span class="s3">return </span><span class="s1">x</span>
  <span class="s1">padding = [(</span><span class="s5">0</span><span class="s3">, </span><span class="s5">0</span><span class="s1">)] * x.ndim</span>
  <span class="s1">padding[axis] = (</span><span class="s5">0</span><span class="s3">, </span><span class="s1">n - </span><span class="s5">1</span><span class="s1">) </span><span class="s3">if </span><span class="s1">reverse </span><span class="s3">else </span><span class="s1">(n - </span><span class="s5">1</span><span class="s3">, </span><span class="s5">0</span><span class="s1">)</span>
  <span class="s1">strides = [</span><span class="s5">1</span><span class="s1">] * x.ndim</span>
  <span class="s1">window_dims = [</span><span class="s5">1</span><span class="s1">] * x.ndim</span>
  <span class="s1">window_dims[axis] = n</span>
  <span class="s3">return </span><span class="s1">window_reduce(x</span><span class="s3">, </span><span class="s1">window_dims</span><span class="s3">, </span><span class="s1">strides</span><span class="s3">, </span><span class="s1">padding)</span>


<span class="s3">def </span><span class="s1">cumred_gpu_impl(window_reduce: Callable</span><span class="s3">, </span><span class="s1">reduce_fn: Callable</span><span class="s3">, </span><span class="s1">x</span><span class="s3">, </span><span class="s1">*</span><span class="s3">,</span>
                    <span class="s1">axis: int</span><span class="s3">, </span><span class="s1">reverse: bool):</span>
  <span class="s0"># On GPU, reduce_window is executed in a single fusion and associative_scan</span>
  <span class="s0"># is split into multiple to materialize intermediate calculations.</span>
  <span class="s0"># On small inputs reduce_window is faster being a single fusion,</span>
  <span class="s0"># but on larger ones is slower because of O(n^2) complexity.</span>
  <span class="s0"># This conservative value of the threshold was obtained via benchmarking.</span>
  <span class="s3">if </span><span class="s1">x.shape[axis] &gt; </span><span class="s5">32</span><span class="s1">:</span>
    <span class="s3">return </span><span class="s1">associative_scan(reduce_fn</span><span class="s3">, </span><span class="s1">x</span><span class="s3">, </span><span class="s1">reverse=reverse</span><span class="s3">, </span><span class="s1">axis=axis)</span>
  <span class="s3">return </span><span class="s1">cumred_reduce_window_impl(window_reduce</span><span class="s3">, </span><span class="s1">x</span><span class="s3">, </span><span class="s1">axis=axis</span><span class="s3">, </span><span class="s1">reverse=reverse)</span>


<span class="s3">def </span><span class="s1">_cumred_batch_rule(prim</span><span class="s3">, </span><span class="s1">batched_args</span><span class="s3">, </span><span class="s1">batch_dims</span><span class="s3">, </span><span class="s1">*</span><span class="s3">, </span><span class="s1">axis: int</span><span class="s3">,</span>
                       <span class="s1">reverse: bool):</span>
  <span class="s1">operand</span><span class="s3">, </span><span class="s1">= batched_args</span>
  <span class="s1">bdim</span><span class="s3">, </span><span class="s1">= batch_dims</span>
  <span class="s1">axis = axis </span><span class="s3">if </span><span class="s1">axis &lt; bdim </span><span class="s3">else </span><span class="s1">axis + </span><span class="s5">1</span>
  <span class="s3">return </span><span class="s1">prim.bind(operand</span><span class="s3">, </span><span class="s1">axis=axis</span><span class="s3">, </span><span class="s1">reverse=reverse)</span><span class="s3">, </span><span class="s1">bdim</span>

<span class="s3">def </span><span class="s1">_cumred_dtype_rule(name</span><span class="s3">, </span><span class="s1">operand</span><span class="s3">, </span><span class="s1">*args</span><span class="s3">, </span><span class="s1">**kw):</span>
  <span class="s3">if not </span><span class="s1">dtypes.issubdtype(operand.dtype</span><span class="s3">, </span><span class="s1">np.number):</span>
    <span class="s3">raise </span><span class="s1">TypeError(</span><span class="s4">&quot;{} does not accept dtype {}. Accepted dtypes are subtypes &quot;</span>
                    <span class="s4">&quot;of number.&quot;</span><span class="s1">.format(name</span><span class="s3">, </span><span class="s1">np.dtype(operand.dtype).name))</span>
  <span class="s3">return </span><span class="s1">dtypes.canonicalize_dtype(operand.dtype)</span>


<span class="s3">def </span><span class="s1">_cumulative_reduction_primitive(name</span><span class="s3">, </span><span class="s1">reduce_fn</span><span class="s3">, </span><span class="s1">reduce_window_fn):</span>
  <span class="s1">reducer_p = lax.standard_primitive(</span>
    <span class="s1">_cumred_shape_rule</span><span class="s3">, </span><span class="s1">partial(_cumred_dtype_rule</span><span class="s3">, </span><span class="s1">name)</span><span class="s3">,</span>
    <span class="s1">name)</span>
  <span class="s1">batching.primitive_batchers[reducer_p] = partial(_cumred_batch_rule</span><span class="s3">,</span>
                                                   <span class="s1">reducer_p)</span>

  <span class="s3">def </span><span class="s1">register_lowering(fn</span><span class="s3">, </span><span class="s1">platform=</span><span class="s3">None</span><span class="s1">):</span>
    <span class="s1">mlir.register_lowering(</span>
        <span class="s1">reducer_p</span><span class="s3">,</span>
        <span class="s1">mlir.cache_lowering(mlir.lower_fun(fn</span><span class="s3">, </span><span class="s1">multiple_results=</span><span class="s3">False</span><span class="s1">))</span><span class="s3">,</span>
        <span class="s1">platform=platform)</span>

  <span class="s0"># Default for platforms not treated specially below.</span>
  <span class="s1">register_lowering(partial(associative_scan</span><span class="s3">, </span><span class="s1">reduce_fn))</span>
  <span class="s0"># On GPU, we choose between window reduction and associative scan</span>
  <span class="s0"># based on the input size.</span>
  <span class="s3">for </span><span class="s1">platform </span><span class="s3">in </span><span class="s1">[</span><span class="s4">'cuda'</span><span class="s3">, </span><span class="s4">'rocm'</span><span class="s1">]:</span>
    <span class="s1">register_lowering(</span>
        <span class="s1">partial(cumred_gpu_impl</span><span class="s3">, </span><span class="s1">reduce_window_fn</span><span class="s3">, </span><span class="s1">reduce_fn)</span><span class="s3">, </span><span class="s1">platform)</span>
  <span class="s0"># On TPU, an implementation using reduce_window is handled specially by the</span>
  <span class="s0"># compiler and is efficient. On other backends, it is O(n^2).</span>
  <span class="s1">register_lowering(partial(cumred_reduce_window_impl</span><span class="s3">, </span><span class="s1">reduce_window_fn)</span><span class="s3">, </span><span class="s4">'tpu'</span><span class="s1">)</span>
  <span class="s3">return </span><span class="s1">reducer_p</span>

<span class="s1">cumsum_p = _cumulative_reduction_primitive(</span><span class="s4">&quot;cumsum&quot;</span><span class="s3">, </span><span class="s1">lax.add</span><span class="s3">, </span><span class="s1">windowed_reductions._reduce_window_sum)</span>
<span class="s1">ad.deflinear2(cumsum_p</span><span class="s3">, </span><span class="s1">_cumsum_transpose_rule)</span>

<span class="s1">cumlogsumexp_p = _cumulative_reduction_primitive(</span>
    <span class="s4">&quot;cumlogsumexp&quot;</span><span class="s3">, </span><span class="s1">logaddexp</span><span class="s3">, </span><span class="s1">windowed_reductions._reduce_window_logaddexp)</span>
<span class="s1">cumprod_p = _cumulative_reduction_primitive(</span><span class="s4">&quot;cumprod&quot;</span><span class="s3">, </span><span class="s1">lax.mul</span><span class="s3">, </span><span class="s1">windowed_reductions._reduce_window_prod)</span>
<span class="s1">cummax_p = _cumulative_reduction_primitive(</span><span class="s4">&quot;cummax&quot;</span><span class="s3">, </span><span class="s1">lax.max</span><span class="s3">, </span><span class="s1">windowed_reductions._reduce_window_max)</span>
<span class="s1">cummin_p = _cumulative_reduction_primitive(</span><span class="s4">&quot;cummin&quot;</span><span class="s3">, </span><span class="s1">lax.min</span><span class="s3">, </span><span class="s1">windowed_reductions._reduce_window_min)</span>


<span class="s3">def </span><span class="s1">_cumulative_jvp_rule(primals</span><span class="s3">, </span><span class="s1">tangents</span><span class="s3">, </span><span class="s1">*</span><span class="s3">, </span><span class="s1">axis: int</span><span class="s3">, </span><span class="s1">reverse: bool</span><span class="s3">,</span>
                         <span class="s1">combine_fn: Callable):</span>
  <span class="s0"># Irrespective of backend, we always use the parallel prefix scan</span>
  <span class="s0"># implementation when differentiating because reduce_window is not</span>
  <span class="s0"># arbitrarily differentiable.</span>
  <span class="s3">return </span><span class="s1">api.jvp(partial(associative_scan</span><span class="s3">, </span><span class="s1">combine_fn</span><span class="s3">, </span><span class="s1">axis=axis</span><span class="s3">,</span>
                         <span class="s1">reverse=reverse)</span><span class="s3">,</span>
                 <span class="s1">primals</span><span class="s3">, </span><span class="s1">tangents)</span>

<span class="s1">ad.primitive_jvps[cumlogsumexp_p] = partial(_cumulative_jvp_rule</span><span class="s3">, </span><span class="s1">combine_fn=logaddexp)</span>
<span class="s1">ad.primitive_jvps[cumprod_p] = partial(_cumulative_jvp_rule</span><span class="s3">, </span><span class="s1">combine_fn=lax.mul)</span>
<span class="s1">ad.primitive_jvps[cummin_p] = partial(_cumulative_jvp_rule</span><span class="s3">, </span><span class="s1">combine_fn=lax.min)</span>
<span class="s1">ad.primitive_jvps[cummax_p] = partial(_cumulative_jvp_rule</span><span class="s3">, </span><span class="s1">combine_fn=lax.max)</span>
</pre>
</body>
</html>