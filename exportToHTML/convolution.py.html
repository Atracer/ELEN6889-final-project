<html>
<head>
<title>convolution.py</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<style type="text/css">
.s0 { color: #808080;}
.s1 { color: #a9b7c6;}
.s2 { color: #cc7832;}
.s3 { color: #629755; font-style: italic;}
.s4 { color: #6897bb;}
.s5 { color: #6a8759;}
</style>
</head>
<body bgcolor="#2b2b2b">
<table CELLSPACING=0 CELLPADDING=5 COLS=1 WIDTH="100%" BGCOLOR="#606060" >
<tr><td><center>
<font face="Arial, Helvetica" color="#000000">
convolution.py</font>
</center></td></tr></table>
<pre><span class="s0"># Copyright 2018 The JAX Authors.</span>
<span class="s0">#</span>
<span class="s0"># Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);</span>
<span class="s0"># you may not use this file except in compliance with the License.</span>
<span class="s0"># You may obtain a copy of the License at</span>
<span class="s0">#</span>
<span class="s0">#     https://www.apache.org/licenses/LICENSE-2.0</span>
<span class="s0">#</span>
<span class="s0"># Unless required by applicable law or agreed to in writing, software</span>
<span class="s0"># distributed under the License is distributed on an &quot;AS IS&quot; BASIS,</span>
<span class="s0"># WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span>
<span class="s0"># See the License for the specific language governing permissions and</span>
<span class="s0"># limitations under the License.</span>

<span class="s2">import </span><span class="s1">builtins</span>
<span class="s2">from </span><span class="s1">functools </span><span class="s2">import </span><span class="s1">partial</span>
<span class="s2">import </span><span class="s1">operator</span>
<span class="s2">from </span><span class="s1">typing </span><span class="s2">import </span><span class="s1">Any</span><span class="s2">, </span><span class="s1">List</span><span class="s2">, </span><span class="s1">NamedTuple</span><span class="s2">, </span><span class="s1">Optional</span><span class="s2">, </span><span class="s1">Sequence</span><span class="s2">, </span><span class="s1">Tuple</span><span class="s2">, </span><span class="s1">Union</span>

<span class="s2">import </span><span class="s1">numpy </span><span class="s2">as </span><span class="s1">np</span>

<span class="s2">from </span><span class="s1">jax._src </span><span class="s2">import </span><span class="s1">core</span>
<span class="s2">from </span><span class="s1">jax._src </span><span class="s2">import </span><span class="s1">dtypes</span>
<span class="s2">from </span><span class="s1">jax._src </span><span class="s2">import </span><span class="s1">util</span>
<span class="s2">from </span><span class="s1">jax._src.interpreters </span><span class="s2">import </span><span class="s1">ad</span>
<span class="s2">from </span><span class="s1">jax._src.interpreters </span><span class="s2">import </span><span class="s1">batching</span>
<span class="s2">from </span><span class="s1">jax._src.lax </span><span class="s2">import </span><span class="s1">lax</span>
<span class="s2">from </span><span class="s1">jax._src.lib.mlir.dialects </span><span class="s2">import </span><span class="s1">hlo</span>

<span class="s2">from </span><span class="s1">jax.interpreters </span><span class="s2">import </span><span class="s1">mlir</span>
<span class="s2">from </span><span class="s1">jax._src.interpreters </span><span class="s2">import </span><span class="s1">mlir </span><span class="s2">as </span><span class="s1">internal_mlir</span>

<span class="s1">_max = builtins.max</span>

<span class="s1">Array = Any</span>
<span class="s1">DType = Any</span>
<span class="s1">Shape = core.Shape</span>

<span class="s2">class </span><span class="s1">ConvDimensionNumbers(NamedTuple):</span>
  <span class="s3">&quot;&quot;&quot;Describes batch, spatial, and feature dimensions of a convolution. 
 
  Args: 
    lhs_spec: a tuple of nonnegative integer dimension numbers containing 
      `(batch dimension, feature dimension, spatial dimensions...)`. 
    rhs_spec: a tuple of nonnegative integer dimension numbers containing 
      `(out feature dimension, in feature dimension, spatial dimensions...)`. 
    out_spec: a tuple of nonnegative integer dimension numbers containing 
      `(batch dimension, feature dimension, spatial dimensions...)`. 
  &quot;&quot;&quot;</span>
  <span class="s1">lhs_spec: Sequence[int]</span>
  <span class="s1">rhs_spec: Sequence[int]</span>
  <span class="s1">out_spec: Sequence[int]</span>

<span class="s1">ConvGeneralDilatedDimensionNumbers = Union[</span>
  <span class="s2">None, </span><span class="s1">ConvDimensionNumbers</span><span class="s2">, </span><span class="s1">Tuple[str</span><span class="s2">, </span><span class="s1">str</span><span class="s2">, </span><span class="s1">str]]</span>

<span class="s2">def </span><span class="s1">conv_general_dilated(</span>
  <span class="s1">lhs: Array</span><span class="s2">, </span><span class="s1">rhs: Array</span><span class="s2">, </span><span class="s1">window_strides: Sequence[int]</span><span class="s2">,</span>
  <span class="s1">padding: Union[str</span><span class="s2">, </span><span class="s1">Sequence[Tuple[int</span><span class="s2">, </span><span class="s1">int]]]</span><span class="s2">,</span>
  <span class="s1">lhs_dilation: Optional[Sequence[int]] = </span><span class="s2">None,</span>
  <span class="s1">rhs_dilation: Optional[Sequence[int]] = </span><span class="s2">None,</span>
  <span class="s1">dimension_numbers: ConvGeneralDilatedDimensionNumbers  = </span><span class="s2">None,</span>
  <span class="s1">feature_group_count: int = </span><span class="s4">1</span><span class="s2">, </span><span class="s1">batch_group_count: int = </span><span class="s4">1</span><span class="s2">,</span>
  <span class="s1">precision: lax.PrecisionLike = </span><span class="s2">None,</span>
  <span class="s1">preferred_element_type: Optional[DType] = </span><span class="s2">None</span><span class="s1">) -&gt; Array:</span>
  <span class="s3">&quot;&quot;&quot;General n-dimensional convolution operator, with optional dilation. 
 
  Wraps XLA's `Conv 
  &lt;https://www.tensorflow.org/xla/operation_semantics#conv_convolution&gt;`_ 
  operator. 
 
  Args: 
    lhs: a rank `n+2` dimensional input array. 
    rhs: a rank `n+2` dimensional array of kernel weights. 
    window_strides: a sequence of `n` integers, representing the inter-window 
      strides. 
    padding: either the strings `'SAME'`, `'SAME_LOWER'`, or `'VALID'`, or a 
      sequence of `n` `(low, high)` integer pairs that give the padding to apply 
      before and after each spatial dimension. `'SAME'` and `'SAME_LOWER'` add 
      padding to produce same output size as the input. The padding is split 
      between the two sides equally or almost equally. In case the padding is an 
      odd number, the extra padding is added at the end for `'SAME'` and at the 
      beginning for `'SAME_LOWER'`. 
    lhs_dilation: `None`, or a sequence of `n` integers, giving the dilation 
      factor to apply in each spatial dimension of `lhs`. LHS dilation is also 
      known as transposed convolution. 
    rhs_dilation: `None`, or a sequence of `n` integers, giving the dilation 
      factor to apply in each spatial dimension of `rhs`. RHS dilation is also 
      known as atrous convolution. 
    dimension_numbers: either `None`, a ``ConvDimensionNumbers`` object, or a 
      3-tuple ``(lhs_spec, rhs_spec, out_spec)``, where each element is a string 
      of length `n+2`. 
    feature_group_count: integer, default 1. See XLA HLO docs. 
    batch_group_count: integer, default 1. See XLA HLO docs. 
    precision: Optional. Either ``None``, which means the default precision for 
      the backend, a :class:`~jax.lax.Precision` enum value 
      (``Precision.DEFAULT``, ``Precision.HIGH`` or ``Precision.HIGHEST``), a 
      string (e.g. 'highest' or 'fastest', see the 
      ``jax.default_matmul_precision`` context manager), or a tuple of two 
      :class:`~jax.lax.Precision` enums or strings indicating precision of 
      ``lhs`` and ``rhs``. 
    preferred_element_type: Optional. Either ``None``, which means the default 
      accumulation type for the input types, or a datatype, indicating to 
      accumulate results to and return a result with that datatype. 
 
  Returns: 
    An array containing the convolution result. 
 
  In the string case of ``dimension_numbers``, each character identifies by 
  position: 
 
  - the batch dimensions in ``lhs``, ``rhs``, and the output with the character 
    'N', 
  - the feature dimensions in `lhs` and the output with the character 'C', 
  - the input and output feature dimensions in rhs with the characters 'I' 
    and 'O' respectively, and 
  - spatial dimension correspondences between lhs, rhs, and the output using 
    any distinct characters. 
 
  For example, to indicate dimension numbers consistent with the ``conv`` 
  function with two spatial dimensions, one could use ``('NCHW', 'OIHW', 
  'NCHW')``. As another example, to indicate dimension numbers consistent with 
  the TensorFlow Conv2D operation, one could use ``('NHWC', 'HWIO', 'NHWC')``. 
  When using the latter form of convolution dimension specification, window 
  strides are associated with spatial dimension character labels according to 
  the order in which the labels appear in the ``rhs_spec`` string, so that 
  ``window_strides[0]`` is matched with the dimension corresponding to the first 
  character appearing in rhs_spec that is not ``'I'`` or ``'O'``. 
 
  If ``dimension_numbers`` is ``None``, the default is ``('NCHW', 'OIHW', 
  'NCHW')`` (for a 2D convolution). 
  &quot;&quot;&quot;</span>
  <span class="s1">dnums = conv_dimension_numbers(lhs.shape</span><span class="s2">, </span><span class="s1">rhs.shape</span><span class="s2">, </span><span class="s1">dimension_numbers)</span>
  <span class="s2">if </span><span class="s1">lhs_dilation </span><span class="s2">is None</span><span class="s1">:</span>
    <span class="s1">lhs_dilation = (</span><span class="s4">1</span><span class="s2">,</span><span class="s1">) * (lhs.ndim - </span><span class="s4">2</span><span class="s1">)</span>
  <span class="s2">elif </span><span class="s1">isinstance(padding</span><span class="s2">, </span><span class="s1">str) </span><span class="s2">and not </span><span class="s1">len(lhs_dilation) == lhs_dilation.count(</span><span class="s4">1</span><span class="s1">):</span>
    <span class="s2">raise </span><span class="s1">ValueError(</span>
        <span class="s5">&quot;String padding is not implemented for transposed convolution &quot;</span>
        <span class="s5">&quot;using this op. Please either exactly specify the required padding or &quot;</span>
        <span class="s5">&quot;use conv_transpose.&quot;</span><span class="s1">)</span>
  <span class="s2">if </span><span class="s1">rhs_dilation </span><span class="s2">is None</span><span class="s1">:</span>
    <span class="s1">rhs_dilation = (</span><span class="s4">1</span><span class="s2">,</span><span class="s1">) * (rhs.ndim - </span><span class="s4">2</span><span class="s1">)</span>
  <span class="s2">if </span><span class="s1">isinstance(padding</span><span class="s2">, </span><span class="s1">str):</span>
    <span class="s1">lhs_perm</span><span class="s2">, </span><span class="s1">rhs_perm</span><span class="s2">, </span><span class="s1">_ = dnums</span>
    <span class="s1">rhs_shape = np.take(rhs.shape</span><span class="s2">, </span><span class="s1">rhs_perm)[</span><span class="s4">2</span><span class="s1">:]  </span><span class="s0"># type: ignore[index]</span>
    <span class="s1">effective_rhs_shape = [(k-</span><span class="s4">1</span><span class="s1">) * r + </span><span class="s4">1 </span><span class="s2">for </span><span class="s1">k</span><span class="s2">, </span><span class="s1">r </span><span class="s2">in </span><span class="s1">zip(rhs_shape</span><span class="s2">, </span><span class="s1">rhs_dilation)]</span>
    <span class="s1">padding = lax.padtype_to_pads(</span>
        <span class="s1">np.take(lhs.shape</span><span class="s2">, </span><span class="s1">lhs_perm)[</span><span class="s4">2</span><span class="s1">:]</span><span class="s2">, </span><span class="s1">effective_rhs_shape</span><span class="s2">,  </span><span class="s0"># type: ignore[index]</span>
        <span class="s1">window_strides</span><span class="s2">, </span><span class="s1">padding)</span>
  <span class="s2">else</span><span class="s1">:</span>
    <span class="s2">try</span><span class="s1">:</span>
      <span class="s1">padding = tuple((operator.index(lo)</span><span class="s2">, </span><span class="s1">operator.index(hi))</span>
                      <span class="s2">for </span><span class="s1">lo</span><span class="s2">, </span><span class="s1">hi </span><span class="s2">in </span><span class="s1">padding)</span>
    <span class="s2">except </span><span class="s1">(ValueError</span><span class="s2">, </span><span class="s1">TypeError) </span><span class="s2">as </span><span class="s1">e:</span>
      <span class="s2">raise </span><span class="s1">ValueError(</span>
        <span class="s5">&quot;padding argument to conv_general_dilated should be a string or a &quot;</span>
        <span class="s5">f&quot;sequence of (low, high) pairs, got </span><span class="s2">{</span><span class="s1">padding</span><span class="s2">}</span><span class="s5">&quot;</span><span class="s1">) </span><span class="s2">from </span><span class="s1">e</span>

  <span class="s1">preferred_element_type = (</span>
      <span class="s2">None if </span><span class="s1">preferred_element_type </span><span class="s2">is None else</span>
      <span class="s1">dtypes.canonicalize_dtype(np.dtype(preferred_element_type)))</span>
  <span class="s2">return </span><span class="s1">conv_general_dilated_p.bind(</span>
      <span class="s1">lhs</span><span class="s2">, </span><span class="s1">rhs</span><span class="s2">, </span><span class="s1">window_strides=tuple(window_strides)</span><span class="s2">, </span><span class="s1">padding=tuple(padding)</span><span class="s2">,</span>
      <span class="s1">lhs_dilation=tuple(lhs_dilation)</span><span class="s2">, </span><span class="s1">rhs_dilation=tuple(rhs_dilation)</span><span class="s2">,</span>
      <span class="s1">dimension_numbers=dnums</span><span class="s2">,</span>
      <span class="s1">feature_group_count=feature_group_count</span><span class="s2">,</span>
      <span class="s1">batch_group_count=batch_group_count</span><span class="s2">,</span>
      <span class="s1">precision=lax.canonicalize_precision(precision)</span><span class="s2">,</span>
      <span class="s1">preferred_element_type=preferred_element_type)</span>


<span class="s0">### convenience wrappers around traceables</span>


<span class="s2">def </span><span class="s1">conv(lhs: Array</span><span class="s2">, </span><span class="s1">rhs: Array</span><span class="s2">, </span><span class="s1">window_strides: Sequence[int]</span><span class="s2">,</span>
         <span class="s1">padding: str</span><span class="s2">, </span><span class="s1">precision: lax.PrecisionLike = </span><span class="s2">None,</span>
         <span class="s1">preferred_element_type: Optional[DType] = </span><span class="s2">None</span><span class="s1">) -&gt; Array:</span>
  <span class="s3">&quot;&quot;&quot;Convenience wrapper around `conv_general_dilated`. 
 
  Args: 
    lhs: a rank `n+2` dimensional input array. 
    rhs: a rank `n+2` dimensional array of kernel weights. 
    window_strides: a sequence of `n` integers, representing the inter-window 
      strides. 
    padding: either the string `'SAME'`, the string `'VALID'`. 
    precision: Optional. Either ``None``, which means the default precision for 
      the backend, a :class:`~jax.lax.Precision` enum value (``Precision.DEFAULT``, 
      ``Precision.HIGH`` or ``Precision.HIGHEST``) or a tuple of two 
      :class:`~jax.lax.Precision` enums indicating precision of ``lhs``` and ``rhs``. 
    preferred_element_type: Optional. Either ``None``, which means the default 
      accumulation type for the input types, or a datatype, indicating to 
      accumulate results to and return a result with that datatype. 
 
  Returns: 
    An array containing the convolution result. 
  &quot;&quot;&quot;</span>
  <span class="s2">return </span><span class="s1">conv_general_dilated(lhs</span><span class="s2">, </span><span class="s1">rhs</span><span class="s2">, </span><span class="s1">window_strides</span><span class="s2">, </span><span class="s1">padding</span><span class="s2">,</span>
                              <span class="s1">precision=precision</span><span class="s2">,</span>
                              <span class="s1">preferred_element_type=preferred_element_type)</span>

<span class="s2">def </span><span class="s1">conv_with_general_padding(lhs: Array</span><span class="s2">, </span><span class="s1">rhs: Array</span><span class="s2">,</span>
                              <span class="s1">window_strides: Sequence[int]</span><span class="s2">,</span>
                              <span class="s1">padding: Union[str</span><span class="s2">, </span><span class="s1">Sequence[Tuple[int</span><span class="s2">, </span><span class="s1">int]]]</span><span class="s2">,</span>
                              <span class="s1">lhs_dilation: Optional[Sequence[int]]</span><span class="s2">,</span>
                              <span class="s1">rhs_dilation: Optional[Sequence[int]]</span><span class="s2">,</span>
                              <span class="s1">precision: lax.PrecisionLike = </span><span class="s2">None,</span>
                              <span class="s1">preferred_element_type: Optional[DType] = </span><span class="s2">None</span><span class="s1">) -&gt; Array:</span>
  <span class="s3">&quot;&quot;&quot;Convenience wrapper around `conv_general_dilated`. 
 
  Args: 
    lhs: a rank `n+2` dimensional input array. 
    rhs: a rank `n+2` dimensional array of kernel weights. 
    window_strides: a sequence of `n` integers, representing the inter-window 
      strides. 
    padding: either the string `'SAME'`, the string `'VALID'`, or a sequence of 
      `n` `(low, high)` integer pairs that give the padding to apply before and 
      after each spatial dimension. 
    lhs_dilation: `None`, or a sequence of `n` integers, giving the 
      dilation factor to apply in each spatial dimension of `lhs`. LHS dilation 
      is also known as transposed convolution. 
    rhs_dilation: `None`, or a sequence of `n` integers, giving the 
      dilation factor to apply in each spatial dimension of `rhs`. RHS dilation 
      is also known as atrous convolution. 
    precision: Optional. Either ``None``, which means the default precision for 
      the backend, a :class:`~jax.lax.Precision` enum value (``Precision.DEFAULT``, 
      ``Precision.HIGH`` or ``Precision.HIGHEST``) or a tuple of two 
      :class:`~jax.lax.Precision` enums indicating precision of ``lhs``` and ``rhs``. 
    preferred_element_type: Optional. Either ``None``, which means the default 
      accumulation type for the input types, or a datatype, indicating to 
      accumulate results to and return a result with that datatype. 
 
  Returns: 
    An array containing the convolution result. 
  &quot;&quot;&quot;</span>
  <span class="s2">return </span><span class="s1">conv_general_dilated(</span>
      <span class="s1">lhs</span><span class="s2">, </span><span class="s1">rhs</span><span class="s2">, </span><span class="s1">window_strides</span><span class="s2">, </span><span class="s1">padding</span><span class="s2">, </span><span class="s1">lhs_dilation=lhs_dilation</span><span class="s2">,</span>
      <span class="s1">rhs_dilation=rhs_dilation</span><span class="s2">, </span><span class="s1">precision=precision</span><span class="s2">,</span>
      <span class="s1">preferred_element_type=preferred_element_type)</span>


<span class="s2">def </span><span class="s1">_conv_transpose_padding(k</span><span class="s2">, </span><span class="s1">s</span><span class="s2">, </span><span class="s1">padding):</span>
  <span class="s3">&quot;&quot;&quot;Calculate before and after padding for a dim of transposed convolution. 
 
  Args: 
    k: int: kernel dimension. 
    s: int: dimension stride value. 
    padding: 'same' or 'valid' padding mode for original forward conv. 
 
  Returns: 
    2-tuple: ints: before and after padding for transposed convolution. 
  &quot;&quot;&quot;</span>
  <span class="s2">if </span><span class="s1">padding == </span><span class="s5">'SAME'</span><span class="s1">:</span>
    <span class="s1">pad_len = k + s - </span><span class="s4">2</span>
    <span class="s2">if </span><span class="s1">s &gt; k - </span><span class="s4">1</span><span class="s1">:</span>
      <span class="s1">pad_a = k - </span><span class="s4">1</span>
    <span class="s2">else</span><span class="s1">:</span>
      <span class="s1">pad_a = int(np.ceil(pad_len / </span><span class="s4">2</span><span class="s1">))</span>
  <span class="s2">elif </span><span class="s1">padding == </span><span class="s5">'VALID'</span><span class="s1">:</span>
    <span class="s1">pad_len = k + s - </span><span class="s4">2 </span><span class="s1">+ _max(k - s</span><span class="s2">, </span><span class="s4">0</span><span class="s1">)</span>
    <span class="s1">pad_a = k - </span><span class="s4">1</span>
  <span class="s2">else</span><span class="s1">:</span>
    <span class="s2">raise </span><span class="s1">ValueError(</span><span class="s5">'Padding mode must be `SAME` or `VALID`.'</span><span class="s1">)</span>
  <span class="s1">pad_b = pad_len - pad_a</span>
  <span class="s2">return </span><span class="s1">pad_a</span><span class="s2">, </span><span class="s1">pad_b</span>


<span class="s2">def </span><span class="s1">_flip_axes(x</span><span class="s2">, </span><span class="s1">axes):</span>
  <span class="s3">&quot;&quot;&quot;Flip ndarray 'x' along each axis specified in axes tuple.&quot;&quot;&quot;</span>
  <span class="s2">for </span><span class="s1">axis </span><span class="s2">in </span><span class="s1">axes:</span>
    <span class="s1">x = np.flip(x</span><span class="s2">, </span><span class="s1">axis)</span>
  <span class="s2">return </span><span class="s1">x</span>


<span class="s2">def </span><span class="s1">conv_transpose(lhs: Array</span><span class="s2">, </span><span class="s1">rhs: Array</span><span class="s2">, </span><span class="s1">strides: Sequence[int]</span><span class="s2">,</span>
                   <span class="s1">padding: Union[str</span><span class="s2">, </span><span class="s1">Sequence[Tuple[int</span><span class="s2">, </span><span class="s1">int]]]</span><span class="s2">,</span>
                   <span class="s1">rhs_dilation: Optional[Sequence[int]] = </span><span class="s2">None,</span>
                   <span class="s1">dimension_numbers: ConvGeneralDilatedDimensionNumbers = </span><span class="s2">None,</span>
                   <span class="s1">transpose_kernel: bool = </span><span class="s2">False,</span>
                   <span class="s1">precision: lax.PrecisionLike = </span><span class="s2">None,</span>
                   <span class="s1">preferred_element_type: Optional[DType] = </span><span class="s2">None</span><span class="s1">) -&gt; Array:</span>
  <span class="s3">&quot;&quot;&quot;Convenience wrapper for calculating the N-d convolution &quot;transpose&quot;. 
 
  This function directly calculates a fractionally strided conv rather than 
  indirectly calculating the gradient (transpose) of a forward convolution. 
 
  Args: 
    lhs: a rank `n+2` dimensional input array. 
    rhs: a rank `n+2` dimensional array of kernel weights. 
    strides: sequence of `n` integers, sets fractional stride. 
    padding: 'SAME', 'VALID' will set as transpose of corresponding forward 
      conv, or a sequence of `n` integer 2-tuples describing before-and-after 
      padding for each `n` spatial dimension. 
    rhs_dilation: `None`, or a sequence of `n` integers, giving the 
      dilation factor to apply in each spatial dimension of `rhs`. RHS dilation 
      is also known as atrous convolution. 
    dimension_numbers: tuple of dimension descriptors as in 
      lax.conv_general_dilated. Defaults to tensorflow convention. 
    transpose_kernel: if True flips spatial axes and swaps the input/output 
      channel axes of the kernel. This makes the output of this function identical 
      to the gradient-derived functions like keras.layers.Conv2DTranspose 
      applied to the same kernel. For typical use in neural nets this is completely 
      pointless and just makes input/output channel specification confusing. 
    precision: Optional. Either ``None``, which means the default precision for 
      the backend, a :class:`~jax.lax.Precision` enum value (``Precision.DEFAULT``, 
      ``Precision.HIGH`` or ``Precision.HIGHEST``) or a tuple of two 
      :class:`~jax.lax.Precision` enums indicating precision of ``lhs``` and ``rhs``. 
    preferred_element_type: Optional. Either ``None``, which means the default 
      accumulation type for the input types, or a datatype, indicating to 
      accumulate results to and return a result with that datatype. 
 
  Returns: 
    Transposed N-d convolution, with output padding following the conventions of 
    keras.layers.Conv2DTranspose. 
  &quot;&quot;&quot;</span>
  <span class="s2">assert </span><span class="s1">len(lhs.shape) == len(rhs.shape) </span><span class="s2">and </span><span class="s1">len(lhs.shape) &gt;= </span><span class="s4">2</span>
  <span class="s1">ndims = len(lhs.shape)</span>
  <span class="s1">one = (</span><span class="s4">1</span><span class="s2">,</span><span class="s1">) * (ndims - </span><span class="s4">2</span><span class="s1">)</span>
  <span class="s0"># Set dimensional layout defaults if not specified.</span>
  <span class="s2">if </span><span class="s1">dimension_numbers </span><span class="s2">is None</span><span class="s1">:</span>
    <span class="s2">if </span><span class="s1">ndims == </span><span class="s4">2</span><span class="s1">:</span>
      <span class="s1">dimension_numbers = (</span><span class="s5">'NC'</span><span class="s2">, </span><span class="s5">'IO'</span><span class="s2">, </span><span class="s5">'NC'</span><span class="s1">)</span>
    <span class="s2">elif </span><span class="s1">ndims == </span><span class="s4">3</span><span class="s1">:</span>
      <span class="s1">dimension_numbers = (</span><span class="s5">'NHC'</span><span class="s2">, </span><span class="s5">'HIO'</span><span class="s2">, </span><span class="s5">'NHC'</span><span class="s1">)</span>
    <span class="s2">elif </span><span class="s1">ndims == </span><span class="s4">4</span><span class="s1">:</span>
      <span class="s1">dimension_numbers = (</span><span class="s5">'NHWC'</span><span class="s2">, </span><span class="s5">'HWIO'</span><span class="s2">, </span><span class="s5">'NHWC'</span><span class="s1">)</span>
    <span class="s2">elif </span><span class="s1">ndims == </span><span class="s4">5</span><span class="s1">:</span>
      <span class="s1">dimension_numbers = (</span><span class="s5">'NHWDC'</span><span class="s2">, </span><span class="s5">'HWDIO'</span><span class="s2">, </span><span class="s5">'NHWDC'</span><span class="s1">)</span>
    <span class="s2">else</span><span class="s1">:</span>
      <span class="s2">raise </span><span class="s1">ValueError(</span><span class="s5">'No 4+ dimensional dimension_number defaults.'</span><span class="s1">)</span>
  <span class="s1">dn = conv_dimension_numbers(lhs.shape</span><span class="s2">, </span><span class="s1">rhs.shape</span><span class="s2">, </span><span class="s1">dimension_numbers)</span>
  <span class="s1">k_shape = np.take(rhs.shape</span><span class="s2">, </span><span class="s1">dn.rhs_spec)</span>
  <span class="s1">k_sdims = k_shape[</span><span class="s4">2</span><span class="s1">:]  </span><span class="s0"># type: ignore[index]</span>
  <span class="s0"># Calculate correct output shape given padding and strides.</span>
  <span class="s1">pads: Union[str</span><span class="s2">, </span><span class="s1">Sequence[Tuple[int</span><span class="s2">, </span><span class="s1">int]]]</span>
  <span class="s2">if </span><span class="s1">isinstance(padding</span><span class="s2">, </span><span class="s1">str) </span><span class="s2">and </span><span class="s1">padding </span><span class="s2">in </span><span class="s1">{</span><span class="s5">'SAME'</span><span class="s2">, </span><span class="s5">'VALID'</span><span class="s1">}:</span>
    <span class="s2">if </span><span class="s1">rhs_dilation </span><span class="s2">is None</span><span class="s1">:</span>
      <span class="s1">rhs_dilation = (</span><span class="s4">1</span><span class="s2">,</span><span class="s1">) * (rhs.ndim - </span><span class="s4">2</span><span class="s1">)</span>
    <span class="s1">effective_k_size = map(</span><span class="s2">lambda </span><span class="s1">k</span><span class="s2">, </span><span class="s1">r: (k-</span><span class="s4">1</span><span class="s1">) * r + </span><span class="s4">1</span><span class="s2">, </span><span class="s1">k_sdims</span><span class="s2">, </span><span class="s1">rhs_dilation)</span>
    <span class="s1">pads = [_conv_transpose_padding(k</span><span class="s2">, </span><span class="s1">s</span><span class="s2">, </span><span class="s1">padding)</span>
            <span class="s2">for </span><span class="s1">k</span><span class="s2">,</span><span class="s1">s </span><span class="s2">in </span><span class="s1">zip(effective_k_size</span><span class="s2">, </span><span class="s1">strides)]</span>
  <span class="s2">else</span><span class="s1">:</span>
    <span class="s1">pads = padding</span>
  <span class="s2">if </span><span class="s1">transpose_kernel:</span>
    <span class="s0"># flip spatial dims and swap input / output channel axes</span>
    <span class="s1">rhs = _flip_axes(rhs</span><span class="s2">, </span><span class="s1">np.array(dn.rhs_spec)[</span><span class="s4">2</span><span class="s1">:])</span>
    <span class="s1">rhs = np.swapaxes(rhs</span><span class="s2">, </span><span class="s1">dn.rhs_spec[</span><span class="s4">0</span><span class="s1">]</span><span class="s2">, </span><span class="s1">dn.rhs_spec[</span><span class="s4">1</span><span class="s1">])</span>
  <span class="s2">return </span><span class="s1">conv_general_dilated(lhs</span><span class="s2">, </span><span class="s1">rhs</span><span class="s2">, </span><span class="s1">one</span><span class="s2">, </span><span class="s1">pads</span><span class="s2">, </span><span class="s1">strides</span><span class="s2">, </span><span class="s1">rhs_dilation</span><span class="s2">, </span><span class="s1">dn</span><span class="s2">,</span>
                              <span class="s1">precision=precision</span><span class="s2">,</span>
                              <span class="s1">preferred_element_type=preferred_element_type)</span>


<span class="s2">def </span><span class="s1">_conv_general_dilated_shape_rule(</span>
    <span class="s1">lhs: core.ShapedArray</span><span class="s2">, </span><span class="s1">rhs: core.ShapedArray</span><span class="s2">, </span><span class="s1">*</span><span class="s2">, </span><span class="s1">window_strides</span><span class="s2">, </span><span class="s1">padding</span><span class="s2">,</span>
    <span class="s1">lhs_dilation</span><span class="s2">, </span><span class="s1">rhs_dilation</span><span class="s2">, </span><span class="s1">dimension_numbers</span><span class="s2">, </span><span class="s1">feature_group_count</span><span class="s2">,</span>
    <span class="s1">batch_group_count</span><span class="s2">, </span><span class="s1">**unused_kwargs) -&gt; Tuple[int</span><span class="s2">, </span><span class="s1">...]:</span>
  <span class="s2">assert </span><span class="s1">type(dimension_numbers) </span><span class="s2">is </span><span class="s1">ConvDimensionNumbers</span>
  <span class="s2">if </span><span class="s1">len(lhs.shape) != len(rhs.shape):</span>
    <span class="s1">msg = (</span><span class="s5">&quot;conv_general_dilated lhs and rhs must have the same number of &quot;</span>
           <span class="s5">&quot;dimensions, but got {} and {}.&quot;</span><span class="s1">)</span>
    <span class="s2">raise </span><span class="s1">ValueError(msg.format(lhs.shape</span><span class="s2">, </span><span class="s1">rhs.shape))</span>
  <span class="s2">if not </span><span class="s1">feature_group_count &gt; </span><span class="s4">0</span><span class="s1">:</span>
    <span class="s1">msg = (</span><span class="s5">&quot;conv_general_dilated feature_group_count &quot;</span>
           <span class="s5">&quot;must be a positive integer, got {}.&quot;</span><span class="s1">)</span>
    <span class="s2">raise </span><span class="s1">ValueError(msg.format(feature_group_count))</span>
  <span class="s1">lhs_feature_count = lhs.shape[dimension_numbers.lhs_spec[</span><span class="s4">1</span><span class="s1">]]</span>
  <span class="s1">quot</span><span class="s2">, </span><span class="s1">rem = divmod(lhs_feature_count</span><span class="s2">, </span><span class="s1">feature_group_count)</span>
  <span class="s2">if </span><span class="s1">rem:</span>
    <span class="s1">msg = (</span><span class="s5">&quot;conv_general_dilated feature_group_count must divide lhs feature &quot;</span>
           <span class="s5">&quot;dimension size, but {} does not divide {}.&quot;</span><span class="s1">)</span>
    <span class="s2">raise </span><span class="s1">ValueError(msg.format(feature_group_count</span><span class="s2">, </span><span class="s1">lhs_feature_count))</span>
  <span class="s2">if not </span><span class="s1">core.symbolic_equal_dim(quot</span><span class="s2">, </span><span class="s1">rhs.shape[dimension_numbers.rhs_spec[</span><span class="s4">1</span><span class="s1">]]):</span>
    <span class="s1">msg = (</span><span class="s5">&quot;conv_general_dilated lhs feature dimension size divided by &quot;</span>
           <span class="s5">&quot;feature_group_count must equal the rhs input feature dimension &quot;</span>
           <span class="s5">&quot;size, but {} // {} != {}.&quot;</span><span class="s1">)</span>
    <span class="s2">raise </span><span class="s1">ValueError(msg.format(lhs_feature_count</span><span class="s2">, </span><span class="s1">feature_group_count</span><span class="s2">,</span>
                                <span class="s1">rhs.shape[dimension_numbers.rhs_spec[</span><span class="s4">1</span><span class="s1">]]))</span>
  <span class="s2">if </span><span class="s1">rhs.shape[dimension_numbers.rhs_spec[</span><span class="s4">0</span><span class="s1">]] % feature_group_count:</span>
    <span class="s1">msg = (</span><span class="s5">&quot;conv_general_dilated rhs output feature dimension size must be a &quot;</span>
           <span class="s5">&quot;multiple of feature_group_count, but {} is not a multiple of {}.&quot;</span><span class="s1">)</span>
    <span class="s2">raise </span><span class="s1">ValueError(msg.format(rhs.shape[dimension_numbers.rhs_spec[</span><span class="s4">0</span><span class="s1">]]</span><span class="s2">,</span>
                                <span class="s1">feature_group_count))</span>

  <span class="s2">if not </span><span class="s1">batch_group_count &gt; </span><span class="s4">0</span><span class="s1">:</span>
    <span class="s1">msg = (</span><span class="s5">&quot;conv_general_dilated batch_group_count &quot;</span>
           <span class="s5">&quot;must be a positive integer, got {}.&quot;</span><span class="s1">)</span>
    <span class="s2">raise </span><span class="s1">ValueError(msg.format(batch_group_count))</span>
  <span class="s1">lhs_batch_count = lhs.shape[dimension_numbers.lhs_spec[</span><span class="s4">0</span><span class="s1">]]</span>
  <span class="s2">if </span><span class="s1">batch_group_count &gt; </span><span class="s4">1 </span><span class="s2">and </span><span class="s1">lhs_batch_count % batch_group_count != </span><span class="s4">0</span><span class="s1">:</span>
    <span class="s1">msg = (</span><span class="s5">&quot;conv_general_dilated batch_group_count must divide lhs batch &quot;</span>
           <span class="s5">&quot;dimension size, but {} does not divide {}.&quot;</span><span class="s1">)</span>
    <span class="s2">raise </span><span class="s1">ValueError(msg.format(batch_group_count</span><span class="s2">, </span><span class="s1">lhs_batch_count))</span>

  <span class="s2">if </span><span class="s1">rhs.shape[dimension_numbers.rhs_spec[</span><span class="s4">0</span><span class="s1">]] % batch_group_count:</span>
    <span class="s1">msg = (</span><span class="s5">&quot;conv_general_dilated rhs output feature dimension size must be a &quot;</span>
           <span class="s5">&quot;multiple of batch_group_count, but {} is not a multiple of {}.&quot;</span><span class="s1">)</span>
    <span class="s2">raise </span><span class="s1">ValueError(msg.format(rhs.shape[dimension_numbers.rhs_spec[</span><span class="s4">0</span><span class="s1">]]</span><span class="s2">,</span>
                                <span class="s1">batch_group_count))</span>

  <span class="s2">if </span><span class="s1">batch_group_count &gt; </span><span class="s4">1 </span><span class="s2">and </span><span class="s1">feature_group_count &gt; </span><span class="s4">1</span><span class="s1">:</span>
    <span class="s1">msg = (</span><span class="s5">&quot;At most one of batch_group_count and feature_group_count may be &gt; &quot;</span>
           <span class="s5">&quot;1, got batch_group_count={} and feature_group_count={}&quot;</span><span class="s1">)</span>
    <span class="s2">raise </span><span class="s1">ValueError(msg.format(batch_group_count</span><span class="s2">, </span><span class="s1">feature_group_count))</span>

  <span class="s2">if </span><span class="s1">len(_conv_sdims(dimension_numbers.rhs_spec)) != len(window_strides):</span>
    <span class="s1">msg = (</span><span class="s5">&quot;conv_general_dilated window and window_strides must have &quot;</span>
           <span class="s5">&quot;the same number of dimensions, but got {} and {}&quot;</span><span class="s1">)</span>
    <span class="s2">raise </span><span class="s1">ValueError(</span>
        <span class="s1">msg.format(len(_conv_sdims(dimension_numbers.rhs_spec))</span><span class="s2">, </span><span class="s1">len(window_strides)))</span>

  <span class="s1">lhs_perm</span><span class="s2">, </span><span class="s1">rhs_perm</span><span class="s2">, </span><span class="s1">out_perm = dimension_numbers</span>
  <span class="s1">lhs_trans = lax._dilate_shape(np.take(lhs.shape</span><span class="s2">, </span><span class="s1">lhs_perm)</span><span class="s2">, </span><span class="s1">lhs_dilation)</span>
  <span class="s1">rhs_trans = lax._dilate_shape(np.take(rhs.shape</span><span class="s2">, </span><span class="s1">rhs_perm)</span><span class="s2">, </span><span class="s1">rhs_dilation)</span>
  <span class="s1">out_trans = conv_shape_tuple(lhs_trans</span><span class="s2">, </span><span class="s1">rhs_trans</span><span class="s2">, </span><span class="s1">window_strides</span><span class="s2">, </span><span class="s1">padding</span><span class="s2">,</span>
                               <span class="s1">batch_group_count)</span>
  <span class="s2">return </span><span class="s1">tuple(np.take(out_trans</span><span class="s2">, </span><span class="s1">np.argsort(out_perm)))  </span><span class="s0"># type: ignore[arg-type]</span>


<span class="s2">def </span><span class="s1">_conv_general_dilated_dtype_rule(</span>
    <span class="s1">lhs</span><span class="s2">, </span><span class="s1">rhs</span><span class="s2">, </span><span class="s1">*</span><span class="s2">, </span><span class="s1">window_strides</span><span class="s2">, </span><span class="s1">padding</span><span class="s2">, </span><span class="s1">lhs_dilation</span><span class="s2">, </span><span class="s1">rhs_dilation</span><span class="s2">,</span>
    <span class="s1">dimension_numbers</span><span class="s2">, </span><span class="s1">preferred_element_type</span><span class="s2">, </span><span class="s1">**unused_kwargs):</span>
  <span class="s1">input_dtype = lax.naryop_dtype_rule(lax._input_dtype</span><span class="s2">, </span><span class="s1">[lax._any</span><span class="s2">, </span><span class="s1">lax._any]</span><span class="s2">,</span>
                                      <span class="s5">'conv_general_dilated'</span><span class="s2">, </span><span class="s1">lhs</span><span class="s2">, </span><span class="s1">rhs)</span>
  <span class="s2">if </span><span class="s1">preferred_element_type </span><span class="s2">is None</span><span class="s1">:</span>
    <span class="s2">return </span><span class="s1">input_dtype</span>
  <span class="s1">lax._validate_preferred_element_type(input_dtype</span><span class="s2">, </span><span class="s1">preferred_element_type)</span>
  <span class="s2">return </span><span class="s1">preferred_element_type</span>

<span class="s1">_conv_spec_transpose = </span><span class="s2">lambda </span><span class="s1">spec: (spec[</span><span class="s4">1</span><span class="s1">]</span><span class="s2">, </span><span class="s1">spec[</span><span class="s4">0</span><span class="s1">]) + spec[</span><span class="s4">2</span><span class="s1">:]</span>
<span class="s1">_conv_sdims = </span><span class="s2">lambda </span><span class="s1">spec: spec[</span><span class="s4">2</span><span class="s1">:]</span>

<span class="s0"># Understanding the convolution transpose rules:</span>
<span class="s0"># Ignoring the spatial dimensions, let m = batch, j = input feature,</span>
<span class="s0"># k = output feature.</span>
<span class="s0">#</span>
<span class="s0"># Convolution computes the following contraction:</span>
<span class="s0"># Forward: [m, j] [j, k] -&gt; [m, k]</span>
<span class="s0">#</span>
<span class="s0"># The transposes are similar to the rules for transposing a matmul:</span>
<span class="s0"># LHS transpose: [m, k] [k, j] -&gt; [m, j]</span>
<span class="s0"># RHS transpose: [j, m] [m, k] -&gt; [j, k]</span>
<span class="s0">#</span>
<span class="s0"># With feature grouping, we have the following signatures:</span>
<span class="s0"># Forward: [m, gj] [j, gk] -&gt; [m, gk]</span>
<span class="s0"># LHS transpose: [m, gk] [k, gj] -&gt; [m, gj]</span>
<span class="s0"># --&gt; implemented as feature grouping after transposing the group from the</span>
<span class="s0">#     kernel input features to the kernel output features.</span>
<span class="s0"># RHS transpose: [gj, m] [m, gk] -&gt; [j, gk]</span>
<span class="s0"># --&gt; which is batch grouping.</span>
<span class="s0">#</span>
<span class="s0"># With batch grouping, we have the following signatures:</span>
<span class="s0"># Forward: [gm,j] [j,gk]-&gt;[m,gk]</span>
<span class="s0"># LHS transpose: [m, gk][gk, j] -&gt; [gm, j]</span>
<span class="s0"># --&gt; implemented as feature grouping with transposing the group on the kernel</span>
<span class="s0">#     and the output.</span>
<span class="s0"># RHS transpose: [j, gm][m, gk] -&gt; [j, gk]</span>
<span class="s0"># --&gt; which is feature grouping.</span>

<span class="s2">def </span><span class="s1">_conv_general_dilated_transpose_lhs(</span>
    <span class="s1">g</span><span class="s2">, </span><span class="s1">lhs</span><span class="s2">, </span><span class="s1">rhs</span><span class="s2">, </span><span class="s1">*</span><span class="s2">, </span><span class="s1">window_strides</span><span class="s2">, </span><span class="s1">padding</span><span class="s2">, </span><span class="s1">lhs_dilation</span><span class="s2">, </span><span class="s1">rhs_dilation</span><span class="s2">,</span>
    <span class="s1">dimension_numbers</span><span class="s2">, </span><span class="s1">feature_group_count</span><span class="s2">, </span><span class="s1">batch_group_count</span><span class="s2">,</span>
    <span class="s1">precision</span><span class="s2">, </span><span class="s1">preferred_element_type):</span>
  <span class="s2">assert </span><span class="s1">type(dimension_numbers) </span><span class="s2">is </span><span class="s1">ConvDimensionNumbers</span>
  <span class="s2">assert </span><span class="s1">batch_group_count == </span><span class="s4">1 </span><span class="s2">or </span><span class="s1">feature_group_count == </span><span class="s4">1</span>
  <span class="s1">rhs_shape = rhs.shape</span>
  <span class="s1">lhs_shape = lhs.aval.shape</span>
  <span class="s1">lhs_sdims</span><span class="s2">, </span><span class="s1">rhs_sdims</span><span class="s2">, </span><span class="s1">out_sdims = map(_conv_sdims</span><span class="s2">, </span><span class="s1">dimension_numbers)</span>
  <span class="s1">lhs_spec</span><span class="s2">, </span><span class="s1">rhs_spec</span><span class="s2">, </span><span class="s1">out_spec = dimension_numbers</span>
  <span class="s1">t_rhs_spec = _conv_spec_transpose(rhs_spec)</span>
  <span class="s2">if </span><span class="s1">feature_group_count &gt; </span><span class="s4">1</span><span class="s1">:</span>
    <span class="s0"># in addition to switching the dims in the spec, need to move the feature</span>
    <span class="s0"># group axis into the transposed rhs's output feature dim</span>
    <span class="s1">rhs = _reshape_axis_out_of(rhs_spec[</span><span class="s4">0</span><span class="s1">]</span><span class="s2">, </span><span class="s1">feature_group_count</span><span class="s2">, </span><span class="s1">rhs)</span>
    <span class="s1">rhs = _reshape_axis_into(rhs_spec[</span><span class="s4">0</span><span class="s1">]</span><span class="s2">, </span><span class="s1">rhs_spec[</span><span class="s4">1</span><span class="s1">]</span><span class="s2">, </span><span class="s1">rhs)</span>
  <span class="s2">elif </span><span class="s1">batch_group_count &gt; </span><span class="s4">1</span><span class="s1">:</span>
    <span class="s1">rhs = _reshape_axis_out_of(rhs_spec[</span><span class="s4">0</span><span class="s1">]</span><span class="s2">, </span><span class="s1">batch_group_count</span><span class="s2">, </span><span class="s1">rhs)</span>
    <span class="s1">rhs = _reshape_axis_into(rhs_spec[</span><span class="s4">0</span><span class="s1">]</span><span class="s2">, </span><span class="s1">rhs_spec[</span><span class="s4">1</span><span class="s1">]</span><span class="s2">, </span><span class="s1">rhs)</span>
    <span class="s1">feature_group_count = batch_group_count</span>
  <span class="s1">trans_dimension_numbers = ConvDimensionNumbers(out_spec</span><span class="s2">, </span><span class="s1">t_rhs_spec</span><span class="s2">, </span><span class="s1">lhs_spec)</span>
  <span class="s1">padding = _conv_general_vjp_lhs_padding(</span>
      <span class="s1">np.take(lhs_shape</span><span class="s2">, </span><span class="s1">lhs_sdims)</span><span class="s2">, </span><span class="s1">np.take(rhs_shape</span><span class="s2">, </span><span class="s1">rhs_sdims)</span><span class="s2">,</span>
      <span class="s1">window_strides</span><span class="s2">, </span><span class="s1">np.take(g.shape</span><span class="s2">, </span><span class="s1">out_sdims)</span><span class="s2">, </span><span class="s1">padding</span><span class="s2">, </span><span class="s1">lhs_dilation</span><span class="s2">,</span>
      <span class="s1">rhs_dilation)</span>
  <span class="s1">revd_weights = lax.rev(rhs</span><span class="s2">, </span><span class="s1">rhs_sdims)</span>
  <span class="s1">out = conv_general_dilated(</span>
      <span class="s1">g</span><span class="s2">, </span><span class="s1">revd_weights</span><span class="s2">, </span><span class="s1">window_strides=lhs_dilation</span><span class="s2">, </span><span class="s1">padding=padding</span><span class="s2">,</span>
      <span class="s1">lhs_dilation=window_strides</span><span class="s2">, </span><span class="s1">rhs_dilation=rhs_dilation</span><span class="s2">,</span>
      <span class="s1">dimension_numbers=trans_dimension_numbers</span><span class="s2">,</span>
      <span class="s1">feature_group_count=feature_group_count</span><span class="s2">,</span>
      <span class="s1">batch_group_count=</span><span class="s4">1</span><span class="s2">, </span><span class="s1">precision=precision</span><span class="s2">,</span>
      <span class="s1">preferred_element_type=preferred_element_type)</span>
  <span class="s2">if </span><span class="s1">batch_group_count &gt; </span><span class="s4">1</span><span class="s1">:</span>
    <span class="s1">out = _reshape_axis_out_of(lhs_spec[</span><span class="s4">1</span><span class="s1">]</span><span class="s2">, </span><span class="s1">batch_group_count</span><span class="s2">, </span><span class="s1">out)</span>
    <span class="s1">out = _reshape_axis_into(lhs_spec[</span><span class="s4">1</span><span class="s1">]</span><span class="s2">, </span><span class="s1">lhs_spec[</span><span class="s4">0</span><span class="s1">]</span><span class="s2">, </span><span class="s1">out)</span>
  <span class="s2">return </span><span class="s1">out</span>

<span class="s2">def </span><span class="s1">_conv_general_dilated_transpose_rhs(</span>
    <span class="s1">g</span><span class="s2">, </span><span class="s1">lhs</span><span class="s2">, </span><span class="s1">rhs</span><span class="s2">, </span><span class="s1">*</span><span class="s2">, </span><span class="s1">window_strides</span><span class="s2">, </span><span class="s1">padding</span><span class="s2">, </span><span class="s1">lhs_dilation</span><span class="s2">, </span><span class="s1">rhs_dilation</span><span class="s2">,</span>
    <span class="s1">dimension_numbers: ConvDimensionNumbers</span><span class="s2">, </span><span class="s1">feature_group_count: int</span><span class="s2">,</span>
    <span class="s1">batch_group_count: int</span><span class="s2">, </span><span class="s1">precision</span><span class="s2">, </span><span class="s1">preferred_element_type):</span>
  <span class="s2">assert </span><span class="s1">type(dimension_numbers) </span><span class="s2">is </span><span class="s1">ConvDimensionNumbers</span>
  <span class="s2">if </span><span class="s1">np.size(g) == </span><span class="s4">0</span><span class="s1">:</span>
    <span class="s0"># Avoids forming degenerate convolutions where the RHS has spatial size 0.</span>
    <span class="s2">return </span><span class="s1">ad.Zero(rhs.aval)</span>
  <span class="s1">lhs_shape = lhs.shape</span>
  <span class="s1">rhs_shape = rhs.aval.shape</span>
  <span class="s1">lhs_sdims</span><span class="s2">, </span><span class="s1">rhs_sdims</span><span class="s2">, </span><span class="s1">out_sdims = map(_conv_sdims</span><span class="s2">, </span><span class="s1">dimension_numbers)</span>
  <span class="s1">lhs_trans</span><span class="s2">, </span><span class="s1">rhs_trans</span><span class="s2">, </span><span class="s1">out_trans = map(_conv_spec_transpose</span><span class="s2">, </span><span class="s1">dimension_numbers)</span>
  <span class="s2">assert </span><span class="s1">batch_group_count == </span><span class="s4">1 </span><span class="s2">or </span><span class="s1">feature_group_count == </span><span class="s4">1</span>
  <span class="s2">if </span><span class="s1">batch_group_count &gt; </span><span class="s4">1</span><span class="s1">:</span>
    <span class="s1">feature_group_count = batch_group_count</span>
    <span class="s1">batch_group_count = </span><span class="s4">1</span>
  <span class="s2">elif </span><span class="s1">feature_group_count &gt; </span><span class="s4">1</span><span class="s1">:</span>
    <span class="s1">batch_group_count = feature_group_count</span>
    <span class="s1">feature_group_count = </span><span class="s4">1</span>
  <span class="s1">trans_dimension_numbers = ConvDimensionNumbers(lhs_trans</span><span class="s2">, </span><span class="s1">out_trans</span><span class="s2">, </span><span class="s1">rhs_trans)</span>
  <span class="s1">padding = _conv_general_vjp_rhs_padding(</span>
      <span class="s1">np.take(lhs_shape</span><span class="s2">, </span><span class="s1">lhs_sdims)</span><span class="s2">, </span><span class="s1">np.take(rhs_shape</span><span class="s2">, </span><span class="s1">rhs_sdims)</span><span class="s2">,</span>
      <span class="s1">window_strides</span><span class="s2">, </span><span class="s1">np.take(g.shape</span><span class="s2">, </span><span class="s1">out_sdims)</span><span class="s2">, </span><span class="s1">padding</span><span class="s2">, </span><span class="s1">lhs_dilation</span><span class="s2">,</span>
      <span class="s1">rhs_dilation)</span>
  <span class="s2">return </span><span class="s1">conv_general_dilated(</span>
      <span class="s1">lhs</span><span class="s2">, </span><span class="s1">g</span><span class="s2">, </span><span class="s1">window_strides=rhs_dilation</span><span class="s2">, </span><span class="s1">padding=padding</span><span class="s2">,</span>
      <span class="s1">lhs_dilation=lhs_dilation</span><span class="s2">, </span><span class="s1">rhs_dilation=window_strides</span><span class="s2">,</span>
      <span class="s1">dimension_numbers=trans_dimension_numbers</span><span class="s2">,</span>
      <span class="s1">feature_group_count=feature_group_count</span><span class="s2">,</span>
      <span class="s1">batch_group_count=batch_group_count</span><span class="s2">, </span><span class="s1">precision=precision</span><span class="s2">,</span>
      <span class="s1">preferred_element_type=preferred_element_type)</span>

<span class="s2">def </span><span class="s1">_conv_general_dilated_batch_rule(</span>
    <span class="s1">batched_args</span><span class="s2">, </span><span class="s1">batch_dims</span><span class="s2">, </span><span class="s1">*</span><span class="s2">, </span><span class="s1">window_strides</span><span class="s2">, </span><span class="s1">padding</span><span class="s2">,</span>
    <span class="s1">lhs_dilation</span><span class="s2">, </span><span class="s1">rhs_dilation</span><span class="s2">, </span><span class="s1">dimension_numbers</span><span class="s2">,</span>
    <span class="s1">feature_group_count</span><span class="s2">, </span><span class="s1">batch_group_count</span><span class="s2">, </span><span class="s1">precision</span><span class="s2">,</span>
    <span class="s1">preferred_element_type</span><span class="s2">, </span><span class="s1">**unused_kwargs):</span>
  <span class="s2">assert </span><span class="s1">batch_group_count == </span><span class="s4">1 </span><span class="s2">or </span><span class="s1">feature_group_count == </span><span class="s4">1</span>
  <span class="s1">lhs</span><span class="s2">, </span><span class="s1">rhs = batched_args</span>
  <span class="s1">lhs_bdim</span><span class="s2">, </span><span class="s1">rhs_bdim = batch_dims</span>
  <span class="s1">lhs_spec</span><span class="s2">, </span><span class="s1">rhs_spec</span><span class="s2">, </span><span class="s1">out_spec = dimension_numbers</span>

  <span class="s0"># Some of the cases that reshape into batch or feature dimensions do not work</span>
  <span class="s0"># with size 0 batch dimensions. The best fix would be to extend HLO to support</span>
  <span class="s0"># multiple batch dimensions.</span>
  <span class="s2">if </span><span class="s1">((lhs_bdim </span><span class="s2">is not None and </span><span class="s1">lhs.shape[lhs_bdim] == </span><span class="s4">0</span><span class="s1">) </span><span class="s2">or</span>
      <span class="s1">(rhs_bdim </span><span class="s2">is not None and </span><span class="s1">rhs.shape[rhs_bdim] == </span><span class="s4">0</span><span class="s1">)):</span>
    <span class="s1">lhs_shape_unbatched</span><span class="s2">, </span><span class="s1">rhs_shape_unbatched = list(lhs.shape)</span><span class="s2">, </span><span class="s1">list(rhs.shape)</span>
    <span class="s2">if </span><span class="s1">lhs_bdim </span><span class="s2">is not None</span><span class="s1">:</span>
      <span class="s1">lhs_shape_unbatched.pop(lhs_bdim)</span>
    <span class="s2">if </span><span class="s1">rhs_bdim </span><span class="s2">is not None</span><span class="s1">:</span>
      <span class="s1">rhs_shape_unbatched.pop(rhs_bdim)</span>
    <span class="s1">shape = _conv_general_dilated_shape_rule(</span>
      <span class="s1">core.ShapedArray(lhs_shape_unbatched</span><span class="s2">, </span><span class="s1">lhs.dtype)</span><span class="s2">,</span>
      <span class="s1">core.ShapedArray(rhs_shape_unbatched</span><span class="s2">, </span><span class="s1">rhs.dtype)</span><span class="s2">,</span>
      <span class="s1">window_strides=window_strides</span><span class="s2">, </span><span class="s1">padding=padding</span><span class="s2">, </span><span class="s1">lhs_dilation=lhs_dilation</span><span class="s2">,</span>
      <span class="s1">rhs_dilation=rhs_dilation</span><span class="s2">, </span><span class="s1">dimension_numbers=dimension_numbers</span><span class="s2">,</span>
      <span class="s1">feature_group_count=feature_group_count</span><span class="s2">,</span>
      <span class="s1">batch_group_count=batch_group_count)</span>
    <span class="s2">return </span><span class="s1">lax.full(</span>
      <span class="s1">(</span><span class="s4">0</span><span class="s2">,</span><span class="s1">) + shape</span><span class="s2">, </span><span class="s4">0</span><span class="s2">,</span>
      <span class="s1">dtype=lhs.dtype </span><span class="s2">if </span><span class="s1">preferred_element_type </span><span class="s2">is None</span>
            <span class="s2">else </span><span class="s1">preferred_element_type)</span><span class="s2">, </span><span class="s4">0</span>


  <span class="s2">if </span><span class="s1">lhs_bdim </span><span class="s2">is not None and </span><span class="s1">rhs_bdim </span><span class="s2">is not None</span><span class="s1">:</span>
    <span class="s2">assert </span><span class="s1">lhs.shape[lhs_bdim] == rhs.shape[rhs_bdim]</span>
    <span class="s2">if </span><span class="s1">batch_group_count &gt; </span><span class="s4">1</span><span class="s1">:</span>
      <span class="s1">new_lhs = _reshape_axis_into(lhs_bdim</span><span class="s2">, </span><span class="s1">lhs_spec[</span><span class="s4">0</span><span class="s1">]</span><span class="s2">, </span><span class="s1">lhs)</span>
      <span class="s1">batch_group_count *= lhs.shape[lhs_bdim]</span>
    <span class="s2">else</span><span class="s1">:</span>
      <span class="s1">new_lhs = _reshape_axis_into(lhs_bdim</span><span class="s2">, </span><span class="s1">lhs_spec[</span><span class="s4">1</span><span class="s1">]</span><span class="s2">, </span><span class="s1">lhs)</span>
      <span class="s1">feature_group_count *= lhs.shape[lhs_bdim]</span>
    <span class="s1">new_rhs = _reshape_axis_into(rhs_bdim</span><span class="s2">, </span><span class="s1">rhs_spec[</span><span class="s4">0</span><span class="s1">]</span><span class="s2">, </span><span class="s1">rhs)</span>
    <span class="s1">out = conv_general_dilated(</span>
      <span class="s1">new_lhs</span><span class="s2">, </span><span class="s1">new_rhs</span><span class="s2">, </span><span class="s1">window_strides</span><span class="s2">, </span><span class="s1">padding</span><span class="s2">, </span><span class="s1">lhs_dilation</span><span class="s2">, </span><span class="s1">rhs_dilation</span><span class="s2">,</span>
      <span class="s1">dimension_numbers</span><span class="s2">, </span><span class="s1">feature_group_count=feature_group_count</span><span class="s2">,</span>
      <span class="s1">batch_group_count=batch_group_count</span><span class="s2">, </span><span class="s1">precision=precision</span><span class="s2">,</span>
      <span class="s1">preferred_element_type=preferred_element_type)</span>
    <span class="s1">out = _reshape_axis_out_of(out_spec[</span><span class="s4">1</span><span class="s1">]</span><span class="s2">, </span><span class="s1">lhs.shape[lhs_bdim]</span><span class="s2">, </span><span class="s1">out)</span>
    <span class="s2">return </span><span class="s1">out</span><span class="s2">, </span><span class="s1">out_spec[</span><span class="s4">1</span><span class="s1">]</span>

  <span class="s2">elif </span><span class="s1">lhs_bdim </span><span class="s2">is not None</span><span class="s1">:</span>
    <span class="s2">if </span><span class="s1">batch_group_count == </span><span class="s4">1</span><span class="s1">:</span>
      <span class="s1">new_lhs = _reshape_axis_into(lhs_bdim</span><span class="s2">, </span><span class="s1">lhs_spec[</span><span class="s4">0</span><span class="s1">]</span><span class="s2">, </span><span class="s1">lhs)</span>
      <span class="s1">out = conv_general_dilated(new_lhs</span><span class="s2">, </span><span class="s1">rhs</span><span class="s2">, </span><span class="s1">window_strides</span><span class="s2">, </span><span class="s1">padding</span><span class="s2">,</span>
                                 <span class="s1">lhs_dilation</span><span class="s2">, </span><span class="s1">rhs_dilation</span><span class="s2">, </span><span class="s1">dimension_numbers</span><span class="s2">,</span>
                                 <span class="s1">feature_group_count</span><span class="s2">, </span><span class="s1">precision=precision</span><span class="s2">,</span>
                                 <span class="s1">preferred_element_type=preferred_element_type)</span>
      <span class="s1">out = _reshape_axis_out_of(out_spec[</span><span class="s4">0</span><span class="s1">]</span><span class="s2">, </span><span class="s1">lhs.shape[lhs_bdim]</span><span class="s2">, </span><span class="s1">out)</span>
      <span class="s2">return </span><span class="s1">out</span><span class="s2">, </span><span class="s1">out_spec[</span><span class="s4">0</span><span class="s1">]</span>
    <span class="s2">else</span><span class="s1">:</span>
      <span class="s1">new_lhs = _reshape_axis_out_of(lhs_spec[</span><span class="s4">0</span><span class="s1">] + int(lhs_bdim &lt;= lhs_spec[</span><span class="s4">0</span><span class="s1">])</span><span class="s2">,</span>
                                     <span class="s1">batch_group_count</span><span class="s2">, </span><span class="s1">lhs)</span>
      <span class="s1">new_lhs = _reshape_axis_into(lhs_bdim + int(lhs_spec[</span><span class="s4">0</span><span class="s1">] &lt; lhs_bdim)</span><span class="s2">,</span>
                                   <span class="s1">lhs_spec[</span><span class="s4">0</span><span class="s1">] + </span><span class="s4">1</span><span class="s2">,</span>
                                   <span class="s1">new_lhs)</span>
      <span class="s1">new_lhs = _reshape_axis_into(lhs_spec[</span><span class="s4">0</span><span class="s1">]</span><span class="s2">, </span><span class="s1">lhs_spec[</span><span class="s4">0</span><span class="s1">]</span><span class="s2">, </span><span class="s1">new_lhs)</span>
      <span class="s1">out = conv_general_dilated(new_lhs</span><span class="s2">, </span><span class="s1">rhs</span><span class="s2">, </span><span class="s1">window_strides</span><span class="s2">, </span><span class="s1">padding</span><span class="s2">,</span>
                                 <span class="s1">lhs_dilation</span><span class="s2">, </span><span class="s1">rhs_dilation</span><span class="s2">, </span><span class="s1">dimension_numbers</span><span class="s2">,</span>
                                 <span class="s1">feature_group_count</span><span class="s2">, </span><span class="s1">batch_group_count</span><span class="s2">,</span>
                                 <span class="s1">precision=precision</span><span class="s2">,</span>
                                 <span class="s1">preferred_element_type=preferred_element_type)</span>
      <span class="s1">out = _reshape_axis_out_of(out_spec[</span><span class="s4">0</span><span class="s1">]</span><span class="s2">, </span><span class="s1">lhs.shape[lhs_bdim]</span><span class="s2">, </span><span class="s1">out)</span>
      <span class="s2">return </span><span class="s1">out</span><span class="s2">, </span><span class="s1">out_spec[</span><span class="s4">0</span><span class="s1">]</span>

  <span class="s2">elif </span><span class="s1">rhs_bdim </span><span class="s2">is not None</span><span class="s1">:</span>
    <span class="s2">if </span><span class="s1">feature_group_count == </span><span class="s4">1 </span><span class="s2">and </span><span class="s1">batch_group_count == </span><span class="s4">1</span><span class="s1">:</span>
      <span class="s1">new_rhs = _reshape_axis_into(rhs_bdim</span><span class="s2">, </span><span class="s1">rhs_spec[</span><span class="s4">0</span><span class="s1">]</span><span class="s2">, </span><span class="s1">rhs)</span>
      <span class="s1">out = conv_general_dilated(lhs</span><span class="s2">, </span><span class="s1">new_rhs</span><span class="s2">, </span><span class="s1">window_strides</span><span class="s2">, </span><span class="s1">padding</span><span class="s2">,</span>
                                 <span class="s1">lhs_dilation</span><span class="s2">, </span><span class="s1">rhs_dilation</span><span class="s2">, </span><span class="s1">dimension_numbers</span><span class="s2">,</span>
                                 <span class="s1">feature_group_count</span><span class="s2">, </span><span class="s1">batch_group_count</span><span class="s2">,</span>
                                 <span class="s1">precision=precision</span><span class="s2">,</span>
                                 <span class="s1">preferred_element_type=preferred_element_type)</span>
      <span class="s1">out = _reshape_axis_out_of(out_spec[</span><span class="s4">1</span><span class="s1">]</span><span class="s2">, </span><span class="s1">rhs.shape[rhs_bdim]</span><span class="s2">, </span><span class="s1">out)</span>
      <span class="s2">return </span><span class="s1">out</span><span class="s2">, </span><span class="s1">out_spec[</span><span class="s4">1</span><span class="s1">]</span>
    <span class="s2">else</span><span class="s1">:</span>
      <span class="s0"># groups need to be outermost, so we need to factor them out of the</span>
      <span class="s0"># rhs output feature dim, then factor the batch dim into the remaining rhs</span>
      <span class="s0"># output feature dim, then put groups back in. We do something</span>
      <span class="s0"># similar on the output. An alternative which would require more FLOPs but</span>
      <span class="s0"># fewer reshapes would be to broadcast lhs.</span>
      <span class="s1">group_count = (feature_group_count </span><span class="s2">if </span><span class="s1">feature_group_count &gt; </span><span class="s4">1</span>
                     <span class="s2">else </span><span class="s1">batch_group_count)</span>
      <span class="s1">new_rhs = _reshape_axis_out_of(rhs_spec[</span><span class="s4">0</span><span class="s1">] + int(rhs_bdim &lt;= rhs_spec[</span><span class="s4">0</span><span class="s1">])</span><span class="s2">,</span>
                                     <span class="s1">group_count</span><span class="s2">, </span><span class="s1">rhs)</span>
      <span class="s1">new_rhs = _reshape_axis_into(rhs_bdim + int(rhs_spec[</span><span class="s4">0</span><span class="s1">] &lt; rhs_bdim)</span><span class="s2">,</span>
                                   <span class="s1">rhs_spec[</span><span class="s4">0</span><span class="s1">] + </span><span class="s4">1</span><span class="s2">, </span><span class="s1">new_rhs)</span>
      <span class="s1">new_rhs = _reshape_axis_into(rhs_spec[</span><span class="s4">0</span><span class="s1">]</span><span class="s2">, </span><span class="s1">rhs_spec[</span><span class="s4">0</span><span class="s1">]</span><span class="s2">, </span><span class="s1">new_rhs)</span>
      <span class="s1">out = conv_general_dilated(lhs</span><span class="s2">, </span><span class="s1">new_rhs</span><span class="s2">, </span><span class="s1">window_strides</span><span class="s2">, </span><span class="s1">padding</span><span class="s2">,</span>
                                 <span class="s1">lhs_dilation</span><span class="s2">, </span><span class="s1">rhs_dilation</span><span class="s2">, </span><span class="s1">dimension_numbers</span><span class="s2">,</span>
                                 <span class="s1">feature_group_count</span><span class="s2">, </span><span class="s1">batch_group_count</span><span class="s2">,</span>
                                 <span class="s1">precision=precision</span><span class="s2">,</span>
                                 <span class="s1">preferred_element_type=preferred_element_type)</span>
      <span class="s1">out = _reshape_axis_out_of(out_spec[</span><span class="s4">1</span><span class="s1">]</span><span class="s2">, </span><span class="s1">group_count</span><span class="s2">, </span><span class="s1">out)</span>
      <span class="s1">out = _reshape_axis_out_of(out_spec[</span><span class="s4">1</span><span class="s1">] + </span><span class="s4">1</span><span class="s2">, </span><span class="s1">rhs.shape[rhs_bdim]</span><span class="s2">, </span><span class="s1">out)</span>
      <span class="s1">out = _reshape_axis_into(out_spec[</span><span class="s4">1</span><span class="s1">]</span><span class="s2">, </span><span class="s1">out_spec[</span><span class="s4">1</span><span class="s1">] + </span><span class="s4">1</span><span class="s2">, </span><span class="s1">out)</span>
      <span class="s2">return </span><span class="s1">out</span><span class="s2">, </span><span class="s1">out_spec[</span><span class="s4">1</span><span class="s1">]</span>

<span class="s1">conv_general_dilated_p = lax.standard_primitive(</span>
    <span class="s1">_conv_general_dilated_shape_rule</span><span class="s2">, </span><span class="s1">_conv_general_dilated_dtype_rule</span><span class="s2">,</span>
    <span class="s5">'conv_general_dilated'</span><span class="s1">)</span>

<span class="s1">ad.defbilinear(conv_general_dilated_p</span><span class="s2">,</span>
               <span class="s1">_conv_general_dilated_transpose_lhs</span><span class="s2">,</span>
               <span class="s1">_conv_general_dilated_transpose_rhs)</span>
<span class="s1">batching.primitive_batchers[conv_general_dilated_p] = \</span>
    <span class="s1">_conv_general_dilated_batch_rule</span>

<span class="s2">def </span><span class="s1">_complex_mul(mul</span><span class="s2">, </span><span class="s1">x</span><span class="s2">, </span><span class="s1">y):</span>
  <span class="s0"># We use a trick for complex multiplication sometimes attributed to Gauss</span>
  <span class="s0"># which uses three multiplications and five additions; instead of the naive</span>
  <span class="s0"># method of four multiplications and two additions.</span>
  <span class="s0"># https://en.wikipedia.org/wiki/Multiplication_algorithm#Complex_multiplication_algorithm</span>
  <span class="s0">#</span>
  <span class="s0"># This performance win comes with a trade-off in accuracy; especially in</span>
  <span class="s0"># cases when the real and imaginary differ hugely in magnitude. The relative</span>
  <span class="s0"># error bound (e.g. 1p-24 in case of float32) would be relative to the</span>
  <span class="s0"># maximum of real and imaginary parts of the result instead of being</span>
  <span class="s0"># satisfied by the real and imaginary parts independently of each other.</span>
  <span class="s1">x_re</span><span class="s2">, </span><span class="s1">x_im = lax.real(x)</span><span class="s2">, </span><span class="s1">lax.imag(x)</span>
  <span class="s1">y_re</span><span class="s2">, </span><span class="s1">y_im = lax.real(y)</span><span class="s2">, </span><span class="s1">lax.imag(y)</span>
  <span class="s1">k1 = mul(lax.add(x_re</span><span class="s2">, </span><span class="s1">x_im)</span><span class="s2">, </span><span class="s1">y_re)</span>
  <span class="s1">k2 = mul(x_re</span><span class="s2">, </span><span class="s1">lax.sub(y_im</span><span class="s2">, </span><span class="s1">y_re))</span>
  <span class="s1">k3 = mul(x_im</span><span class="s2">, </span><span class="s1">lax.add(y_re</span><span class="s2">, </span><span class="s1">y_im))</span>
  <span class="s2">return </span><span class="s1">lax.complex(lax.sub(k1</span><span class="s2">, </span><span class="s1">k3)</span><span class="s2">, </span><span class="s1">lax.add(k1</span><span class="s2">, </span><span class="s1">k2))</span>


<span class="s1">_real_dtype = </span><span class="s2">lambda </span><span class="s1">dtype: np.finfo(dtype).dtype</span>

<span class="s2">def </span><span class="s1">_conv_general_dilated_lower(</span>
    <span class="s1">ctx</span><span class="s2">, </span><span class="s1">lhs</span><span class="s2">, </span><span class="s1">rhs</span><span class="s2">, </span><span class="s1">*</span><span class="s2">, </span><span class="s1">window_strides</span><span class="s2">, </span><span class="s1">padding</span><span class="s2">,</span>
    <span class="s1">lhs_dilation</span><span class="s2">, </span><span class="s1">rhs_dilation</span><span class="s2">, </span><span class="s1">dimension_numbers</span><span class="s2">, </span><span class="s1">feature_group_count</span><span class="s2">,</span>
    <span class="s1">batch_group_count</span><span class="s2">, </span><span class="s1">precision</span><span class="s2">, </span><span class="s1">preferred_element_type</span><span class="s2">,</span>
    <span class="s1">expand_complex_convolutions=</span><span class="s2">False, </span><span class="s1">**unused_kwargs):</span>
  <span class="s1">lhs_aval</span><span class="s2">, </span><span class="s1">rhs_aval = ctx.avals_in</span>
  <span class="s1">aval_out</span><span class="s2">, </span><span class="s1">= ctx.avals_out</span>
  <span class="s2">assert </span><span class="s1">isinstance(dimension_numbers</span><span class="s2">, </span><span class="s1">ConvDimensionNumbers)</span>
  <span class="s1">dtype = lhs_aval.dtype</span>
  <span class="s2">if </span><span class="s1">expand_complex_convolutions </span><span class="s2">and </span><span class="s1">np.issubdtype(dtype</span><span class="s2">, </span><span class="s1">np.complexfloating):</span>
    <span class="s2">if </span><span class="s1">preferred_element_type </span><span class="s2">is not None</span><span class="s1">:</span>
      <span class="s0"># Convert complex dtype to types used for real and imaginary parts</span>
      <span class="s2">assert </span><span class="s1">np.issubdtype(preferred_element_type</span><span class="s2">, </span><span class="s1">np.complexfloating)</span>
      <span class="s1">preferred_element_type = _real_dtype(preferred_element_type)</span>
    <span class="s1">complex_conv = mlir.lower_fun(</span>
      <span class="s1">partial(</span>
        <span class="s1">_complex_mul</span><span class="s2">,</span>
        <span class="s1">partial(conv_general_dilated</span><span class="s2">, </span><span class="s1">window_strides=window_strides</span><span class="s2">,</span>
                <span class="s1">padding=padding</span><span class="s2">, </span><span class="s1">lhs_dilation=lhs_dilation</span><span class="s2">,</span>
                <span class="s1">rhs_dilation=rhs_dilation</span><span class="s2">, </span><span class="s1">dimension_numbers=dimension_numbers</span><span class="s2">,</span>
                <span class="s1">feature_group_count=feature_group_count</span><span class="s2">,</span>
                <span class="s1">batch_group_count=batch_group_count</span><span class="s2">, </span><span class="s1">precision=precision</span><span class="s2">,</span>
                <span class="s1">preferred_element_type=preferred_element_type))</span><span class="s2">,</span>
      <span class="s1">multiple_results=</span><span class="s2">False</span><span class="s1">)</span>
    <span class="s2">return </span><span class="s1">complex_conv(ctx</span><span class="s2">, </span><span class="s1">lhs</span><span class="s2">, </span><span class="s1">rhs)</span>

  <span class="s1">lhs_spec</span><span class="s2">, </span><span class="s1">rhs_spec</span><span class="s2">, </span><span class="s1">out_spec = dimension_numbers</span>
  <span class="s1">dnums = hlo.ConvDimensionNumbers.get(</span>
    <span class="s1">input_batch_dimension=lhs_spec[</span><span class="s4">0</span><span class="s1">]</span><span class="s2">,</span>
    <span class="s1">input_feature_dimension=lhs_spec[</span><span class="s4">1</span><span class="s1">]</span><span class="s2">,</span>
    <span class="s1">input_spatial_dimensions=list(lhs_spec[</span><span class="s4">2</span><span class="s1">:])</span><span class="s2">,</span>
    <span class="s1">kernel_output_feature_dimension=rhs_spec[</span><span class="s4">0</span><span class="s1">]</span><span class="s2">,</span>
    <span class="s1">kernel_input_feature_dimension=rhs_spec[</span><span class="s4">1</span><span class="s1">]</span><span class="s2">,</span>
    <span class="s1">kernel_spatial_dimensions=list(rhs_spec[</span><span class="s4">2</span><span class="s1">:])</span><span class="s2">,</span>
    <span class="s1">output_batch_dimension=out_spec[</span><span class="s4">0</span><span class="s1">]</span><span class="s2">,</span>
    <span class="s1">output_feature_dimension=out_spec[</span><span class="s4">1</span><span class="s1">]</span><span class="s2">,</span>
    <span class="s1">output_spatial_dimensions=list(out_spec[</span><span class="s4">2</span><span class="s1">:]))</span>
  <span class="s1">num_spatial_dims = len(rhs_spec) - </span><span class="s4">2</span>
  <span class="s2">if </span><span class="s1">len(padding) == </span><span class="s4">0</span><span class="s1">:</span>
    <span class="s1">padding = np.zeros((</span><span class="s4">0</span><span class="s2">, </span><span class="s4">2</span><span class="s1">)</span><span class="s2">, </span><span class="s1">dtype=np.int64)</span>
  <span class="s1">window_reversal = mlir.dense_bool_elements([</span><span class="s2">False</span><span class="s1">] * num_spatial_dims)</span>
  <span class="s2">if </span><span class="s1">(</span><span class="s2">not </span><span class="s1">core.is_constant_shape(window_strides) </span><span class="s2">or</span>
      <span class="s2">not </span><span class="s1">core.is_constant_shape(lhs_dilation) </span><span class="s2">or</span>
      <span class="s2">not </span><span class="s1">core.is_constant_shape(rhs_dilation)):</span>
    <span class="s2">raise </span><span class="s1">NotImplementedError(</span><span class="s5">&quot;Convolutions with non-static strides or dilation&quot;</span><span class="s1">)</span>
  <span class="s2">if </span><span class="s1">all(core.is_constant_shape(p) </span><span class="s2">for </span><span class="s1">p </span><span class="s2">in </span><span class="s1">padding):</span>
    <span class="s2">return </span><span class="s1">[</span>
        <span class="s1">hlo.ConvolutionOp(</span>
          <span class="s1">mlir.aval_to_ir_type(aval_out)</span><span class="s2">,</span>
          <span class="s1">lhs</span><span class="s2">,</span>
          <span class="s1">rhs</span><span class="s2">,</span>
          <span class="s1">dimension_numbers=dnums</span><span class="s2">,</span>
          <span class="s1">feature_group_count=mlir.i64_attr(feature_group_count)</span><span class="s2">,</span>
          <span class="s1">batch_group_count=mlir.i64_attr(batch_group_count)</span><span class="s2">,</span>
          <span class="s1">window_strides=mlir.dense_int_elements(window_strides)</span><span class="s2">,</span>
          <span class="s1">padding=mlir.dense_int_elements(padding)</span><span class="s2">,</span>
          <span class="s1">lhs_dilation=mlir.dense_int_elements(lhs_dilation)</span><span class="s2">,</span>
          <span class="s1">rhs_dilation=mlir.dense_int_elements(rhs_dilation)</span><span class="s2">,</span>
          <span class="s1">window_reversal=window_reversal</span><span class="s2">,</span>
          <span class="s1">precision_config=lax.precision_attr(precision)).result</span>
    <span class="s1">]</span>
  <span class="s2">else</span><span class="s1">:</span>
    <span class="s0"># d_padding will be an array i32[N, 2] with pad_lo and pad_hi for each</span>
    <span class="s0"># spatial dimension.</span>
    <span class="s1">int2d = mlir.aval_to_ir_type(core.ShapedArray((</span><span class="s4">1</span><span class="s2">, </span><span class="s4">2</span><span class="s1">)</span><span class="s2">, </span><span class="s1">np.int32))</span>
    <span class="s2">def </span><span class="s1">prep_one_pad(pad_lo_hi: Tuple[core.DimSize</span><span class="s2">, </span><span class="s1">core.DimSize]):</span>
      <span class="s1">pad1 = mlir.shape_tensor(internal_mlir.eval_dynamic_shape(ctx</span><span class="s2">, </span><span class="s1">pad_lo_hi))  </span><span class="s0"># i32[2]</span>
      <span class="s2">return </span><span class="s1">hlo.ReshapeOp(int2d</span><span class="s2">, </span><span class="s1">pad1)</span>
    <span class="s1">d_padding = hlo.ConcatenateOp(list(map(prep_one_pad</span><span class="s2">, </span><span class="s1">padding))</span><span class="s2">,</span>
                                  <span class="s1">mlir.i64_attr(</span><span class="s4">0</span><span class="s1">))</span>
    <span class="s2">return </span><span class="s1">[</span>
        <span class="s1">hlo.DynamicConvOp(</span>
          <span class="s1">mlir.aval_to_ir_type(aval_out)</span><span class="s2">,</span>
          <span class="s1">lhs</span><span class="s2">,</span>
          <span class="s1">rhs</span><span class="s2">,</span>
          <span class="s1">d_padding</span><span class="s2">,</span>
          <span class="s1">dimension_numbers=dnums</span><span class="s2">,</span>
          <span class="s1">feature_group_count=mlir.i64_attr(feature_group_count)</span><span class="s2">,</span>
          <span class="s1">batch_group_count=mlir.i64_attr(batch_group_count)</span><span class="s2">,</span>
          <span class="s1">window_strides=mlir.dense_int_elements(window_strides)</span><span class="s2">,</span>
          <span class="s1">lhs_dilation=mlir.dense_int_elements(lhs_dilation)</span><span class="s2">,</span>
          <span class="s1">rhs_dilation=mlir.dense_int_elements(rhs_dilation)</span><span class="s2">,</span>
          <span class="s1">window_reversal=window_reversal</span><span class="s2">,</span>
          <span class="s1">precision_config=lax.precision_attr(precision)).result</span>
    <span class="s1">]</span>

<span class="s1">mlir.register_lowering(conv_general_dilated_p</span><span class="s2">, </span><span class="s1">_conv_general_dilated_lower)</span>
<span class="s0"># TODO(b/161124619, b/161126248): XLA does not support complex convolution on</span>
<span class="s0"># GPU, and on CPU it uses a slow loop-based implementation;</span>
<span class="s0"># on these backends, lower complex convolutions away.</span>
<span class="s1">mlir.register_lowering(</span>
    <span class="s1">conv_general_dilated_p</span><span class="s2">,</span>
    <span class="s1">partial(_conv_general_dilated_lower</span><span class="s2">, </span><span class="s1">expand_complex_convolutions=</span><span class="s2">True</span><span class="s1">)</span><span class="s2">,</span>
    <span class="s1">platform=</span><span class="s5">'cpu'</span><span class="s1">)</span>
<span class="s1">mlir.register_lowering(</span>
    <span class="s1">conv_general_dilated_p</span><span class="s2">,</span>
    <span class="s1">partial(_conv_general_dilated_lower</span><span class="s2">, </span><span class="s1">expand_complex_convolutions=</span><span class="s2">True</span><span class="s1">)</span><span class="s2">,</span>
    <span class="s1">platform=</span><span class="s5">'gpu'</span><span class="s1">)</span>


<span class="s2">def </span><span class="s1">_reshape_axis_into(src</span><span class="s2">, </span><span class="s1">dst</span><span class="s2">, </span><span class="s1">x):</span>
  <span class="s0"># NB: `dst` is the number of the dimension that we should reshape into</span>
  <span class="s0"># *after* `src` is removed from `x`'s list of dimensions. For example, if</span>
  <span class="s0"># `src` is an added batch dimension, `dst` might name a target dimension in</span>
  <span class="s0"># the unbatched list of dimensions.</span>
  <span class="s1">perm = [i </span><span class="s2">for </span><span class="s1">i </span><span class="s2">in </span><span class="s1">range(x.ndim) </span><span class="s2">if </span><span class="s1">i != src]</span>
  <span class="s1">perm.insert(dst</span><span class="s2">, </span><span class="s1">src)</span>
  <span class="s1">new_shape = list(np.delete(x.shape</span><span class="s2">, </span><span class="s1">src))</span>
  <span class="s1">new_shape[dst] *= x.shape[src]</span>
  <span class="s2">return </span><span class="s1">lax.reshape(x</span><span class="s2">, </span><span class="s1">new_shape</span><span class="s2">, </span><span class="s1">perm)</span>

<span class="s2">def </span><span class="s1">_reshape_axis_out_of(src</span><span class="s2">, </span><span class="s1">size1</span><span class="s2">, </span><span class="s1">x):</span>
  <span class="s1">shape = list(x.shape)</span>
  <span class="s1">size2</span><span class="s2">, </span><span class="s1">ragged = divmod(shape[src]</span><span class="s2">, </span><span class="s1">size1)</span>
  <span class="s2">assert not </span><span class="s1">ragged</span>
  <span class="s1">shape[src:src+</span><span class="s4">1</span><span class="s1">] = [size1</span><span class="s2">, </span><span class="s1">size2]</span>
  <span class="s2">return </span><span class="s1">lax.reshape(x</span><span class="s2">, </span><span class="s1">shape)</span>


<span class="s2">def </span><span class="s1">conv_shape_tuple(lhs_shape</span><span class="s2">, </span><span class="s1">rhs_shape</span><span class="s2">, </span><span class="s1">strides</span><span class="s2">, </span><span class="s1">pads</span><span class="s2">, </span><span class="s1">batch_group_count=</span><span class="s4">1</span><span class="s1">):</span>
  <span class="s3">&quot;&quot;&quot;Compute the shape tuple of a conv given input shapes in canonical order.&quot;&quot;&quot;</span>
  <span class="s2">if </span><span class="s1">isinstance(pads</span><span class="s2">, </span><span class="s1">str):</span>
    <span class="s1">pads = lax.padtype_to_pads(lhs_shape[</span><span class="s4">2</span><span class="s1">:]</span><span class="s2">, </span><span class="s1">rhs_shape[</span><span class="s4">2</span><span class="s1">:]</span><span class="s2">, </span><span class="s1">strides</span><span class="s2">, </span><span class="s1">pads)</span>
  <span class="s2">if </span><span class="s1">len(pads) != len(lhs_shape) - </span><span class="s4">2</span><span class="s1">:</span>
    <span class="s1">msg = </span><span class="s5">&quot;Wrong number of explicit pads for convolution: expected {}, got {}.&quot;</span>
    <span class="s2">raise </span><span class="s1">TypeError(msg.format(len(lhs_shape) - </span><span class="s4">2</span><span class="s2">, </span><span class="s1">len(pads)))</span>

  <span class="s1">lhs_padded = np.add(lhs_shape[</span><span class="s4">2</span><span class="s1">:]</span><span class="s2">, </span><span class="s1">np.sum(np.array(pads).reshape(-</span><span class="s4">1</span><span class="s2">, </span><span class="s4">2</span><span class="s1">)</span><span class="s2">,</span>
                                              <span class="s1">axis=</span><span class="s4">1</span><span class="s1">))</span>
  <span class="s2">if </span><span class="s1">np.any(lhs_padded &lt; </span><span class="s4">0</span><span class="s1">):</span>
    <span class="s2">raise </span><span class="s1">ValueError(</span><span class="s5">&quot;Negative padding is larger than the size of the corresponding dimension: &quot;</span>
                     <span class="s5">f&quot;got padding=</span><span class="s2">{</span><span class="s1">pads</span><span class="s2">} </span><span class="s5">for lhs_shape[2:]=</span><span class="s2">{</span><span class="s1">lhs_shape[</span><span class="s4">2</span><span class="s1">:]</span><span class="s2">}</span><span class="s5">&quot;</span><span class="s1">)</span>
  <span class="s1">out_space = core.stride_shape(lhs_padded</span><span class="s2">, </span><span class="s1">rhs_shape[</span><span class="s4">2</span><span class="s1">:]</span><span class="s2">, </span><span class="s1">strides)</span>
  <span class="s1">out_space = [d </span><span class="s2">if </span><span class="s1">core.greater_equal_dim(d</span><span class="s2">, </span><span class="s4">0</span><span class="s1">) </span><span class="s2">else </span><span class="s4">0</span>
               <span class="s2">for </span><span class="s1">d </span><span class="s2">in </span><span class="s1">out_space]</span>
  <span class="s2">if </span><span class="s1">batch_group_count &gt; </span><span class="s4">1</span><span class="s1">:</span>
    <span class="s2">assert </span><span class="s1">lhs_shape[</span><span class="s4">0</span><span class="s1">] % batch_group_count == </span><span class="s4">0</span>
    <span class="s1">out_shape_0 = lhs_shape[</span><span class="s4">0</span><span class="s1">] // batch_group_count</span>
  <span class="s2">else</span><span class="s1">:</span>
    <span class="s1">out_shape_0 = lhs_shape[</span><span class="s4">0</span><span class="s1">]</span>
  <span class="s1">out_shape = (out_shape_0</span><span class="s2">, </span><span class="s1">rhs_shape[</span><span class="s4">0</span><span class="s1">])</span>
  <span class="s2">return </span><span class="s1">tuple(out_shape + tuple(out_space))</span>


<span class="s2">def </span><span class="s1">conv_general_shape_tuple(lhs_shape</span><span class="s2">, </span><span class="s1">rhs_shape</span><span class="s2">, </span><span class="s1">window_strides</span><span class="s2">, </span><span class="s1">padding</span><span class="s2">,</span>
                             <span class="s1">dimension_numbers):</span>
  <span class="s1">lhs_perm</span><span class="s2">, </span><span class="s1">rhs_perm</span><span class="s2">, </span><span class="s1">out_perm = conv_general_permutations(dimension_numbers)</span>
  <span class="s1">lhs_trans = np.take(lhs_shape</span><span class="s2">, </span><span class="s1">lhs_perm)</span>
  <span class="s1">rhs_trans = np.take(rhs_shape</span><span class="s2">, </span><span class="s1">rhs_perm)</span>
  <span class="s1">out_trans = conv_shape_tuple(lhs_trans</span><span class="s2">, </span><span class="s1">rhs_trans</span><span class="s2">, </span><span class="s1">window_strides</span><span class="s2">, </span><span class="s1">padding)</span>
  <span class="s2">return </span><span class="s1">tuple(np.take(out_trans</span><span class="s2">, </span><span class="s1">np.argsort(out_perm)))</span>


<span class="s2">def </span><span class="s1">conv_transpose_shape_tuple(lhs_shape</span><span class="s2">, </span><span class="s1">rhs_shape</span><span class="s2">, </span><span class="s1">window_strides</span><span class="s2">, </span><span class="s1">padding</span><span class="s2">,</span>
                               <span class="s1">dimension_numbers):</span>
  <span class="s1">lhs_perm</span><span class="s2">, </span><span class="s1">rhs_perm</span><span class="s2">, </span><span class="s1">out_perm = conv_general_permutations(dimension_numbers)</span>
  <span class="s1">lhs_trans = np.take(lhs_shape</span><span class="s2">, </span><span class="s1">lhs_perm)</span>
  <span class="s1">rhs_trans = np.take(rhs_shape</span><span class="s2">, </span><span class="s1">rhs_perm)</span>
  <span class="s2">if </span><span class="s1">isinstance(padding</span><span class="s2">, </span><span class="s1">str):</span>
    <span class="s1">padding = [_conv_transpose_padding(k</span><span class="s2">, </span><span class="s1">s</span><span class="s2">, </span><span class="s1">padding)</span>
               <span class="s2">for </span><span class="s1">k</span><span class="s2">,</span><span class="s1">s </span><span class="s2">in </span><span class="s1">zip(rhs_trans[</span><span class="s4">2</span><span class="s1">:]</span><span class="s2">, </span><span class="s1">window_strides)]</span>
  <span class="s1">padding = list(map(np.sum</span><span class="s2">, </span><span class="s1">padding))</span>
  <span class="s1">unpad_out_space = [(i-</span><span class="s4">1</span><span class="s1">) * s - k + </span><span class="s4">2</span>
                     <span class="s2">for </span><span class="s1">i</span><span class="s2">, </span><span class="s1">k</span><span class="s2">, </span><span class="s1">s </span><span class="s2">in </span><span class="s1">zip(lhs_trans[</span><span class="s4">2</span><span class="s1">:]</span><span class="s2">,</span>
                                        <span class="s1">rhs_trans[</span><span class="s4">2</span><span class="s1">:]</span><span class="s2">,</span>
                                        <span class="s1">window_strides)]</span>
  <span class="s1">out_space = np.sum([unpad_out_space</span><span class="s2">, </span><span class="s1">padding]</span><span class="s2">, </span><span class="s1">axis=</span><span class="s4">0</span><span class="s1">).tolist()</span>
  <span class="s1">out_trans = tuple((lhs_trans[</span><span class="s4">0</span><span class="s1">]</span><span class="s2">, </span><span class="s1">rhs_trans[</span><span class="s4">0</span><span class="s1">]) + tuple(out_space))</span>
  <span class="s2">return </span><span class="s1">tuple(np.take(out_trans</span><span class="s2">, </span><span class="s1">np.argsort(out_perm)))</span>

<span class="s2">def </span><span class="s1">conv_dimension_numbers(lhs_shape</span><span class="s2">, </span><span class="s1">rhs_shape</span><span class="s2">, </span><span class="s1">dimension_numbers</span>
                           <span class="s1">) -&gt; ConvDimensionNumbers:</span>
  <span class="s3">&quot;&quot;&quot;Converts convolution `dimension_numbers` to a `ConvDimensionNumbers`. 
 
  Args: 
    lhs_shape: tuple of nonnegative integers, shape of the convolution input. 
    rhs_shape: tuple of nonnegative integers, shape of the convolution kernel. 
    dimension_numbers: None or a tuple/list of strings or a ConvDimensionNumbers 
      object following the convolution dimension number specification format in 
      xla_client.py. 
 
  Returns: 
    A `ConvDimensionNumbers` object that represents `dimension_numbers` in the 
    canonical form used by lax functions. 
  &quot;&quot;&quot;</span>
  <span class="s2">if </span><span class="s1">isinstance(dimension_numbers</span><span class="s2">, </span><span class="s1">ConvDimensionNumbers):</span>
    <span class="s2">return </span><span class="s1">dimension_numbers</span>
  <span class="s2">if </span><span class="s1">len(lhs_shape) != len(rhs_shape):</span>
    <span class="s1">msg = </span><span class="s5">&quot;convolution requires lhs and rhs ndim to be equal, got {} and {}.&quot;</span>
    <span class="s2">raise </span><span class="s1">TypeError(msg.format(len(lhs_shape)</span><span class="s2">, </span><span class="s1">len(rhs_shape)))</span>

  <span class="s2">if </span><span class="s1">dimension_numbers </span><span class="s2">is None</span><span class="s1">:</span>
    <span class="s1">iota = tuple(range(len(lhs_shape)))</span>
    <span class="s2">return </span><span class="s1">ConvDimensionNumbers(iota</span><span class="s2">, </span><span class="s1">iota</span><span class="s2">, </span><span class="s1">iota)</span>
  <span class="s2">elif </span><span class="s1">isinstance(dimension_numbers</span><span class="s2">, </span><span class="s1">(list</span><span class="s2">, </span><span class="s1">tuple)):</span>
    <span class="s2">if </span><span class="s1">len(dimension_numbers) != </span><span class="s4">3</span><span class="s1">:</span>
      <span class="s1">msg = </span><span class="s5">&quot;convolution dimension_numbers list/tuple must be length 3, got {}.&quot;</span>
      <span class="s2">raise </span><span class="s1">TypeError(msg.format(len(dimension_numbers)))</span>
    <span class="s2">if not </span><span class="s1">all(isinstance(elt</span><span class="s2">, </span><span class="s1">str) </span><span class="s2">for </span><span class="s1">elt </span><span class="s2">in </span><span class="s1">dimension_numbers):</span>
      <span class="s1">msg = </span><span class="s5">&quot;convolution dimension_numbers elements must be strings, got {}.&quot;</span>
      <span class="s2">raise </span><span class="s1">TypeError(msg.format(tuple(map(type</span><span class="s2">, </span><span class="s1">dimension_numbers))))</span>
    <span class="s1">msg = (</span><span class="s5">&quot;convolution dimension_numbers[{}] must have len equal to the ndim &quot;</span>
           <span class="s5">&quot;of lhs and rhs, got {} for lhs and rhs shapes {} and {}.&quot;</span><span class="s1">)</span>
    <span class="s2">for </span><span class="s1">i</span><span class="s2">, </span><span class="s1">elt </span><span class="s2">in </span><span class="s1">enumerate(dimension_numbers):</span>
      <span class="s2">if </span><span class="s1">len(elt) != len(lhs_shape):</span>
        <span class="s2">raise </span><span class="s1">TypeError(msg.format(i</span><span class="s2">, </span><span class="s1">len(elt)</span><span class="s2">, </span><span class="s1">lhs_shape</span><span class="s2">, </span><span class="s1">rhs_shape))</span>

    <span class="s1">lhs_spec</span><span class="s2">, </span><span class="s1">rhs_spec</span><span class="s2">, </span><span class="s1">out_spec = conv_general_permutations(dimension_numbers)</span>
    <span class="s2">return </span><span class="s1">ConvDimensionNumbers(lhs_spec</span><span class="s2">, </span><span class="s1">rhs_spec</span><span class="s2">, </span><span class="s1">out_spec)</span>
  <span class="s2">else</span><span class="s1">:</span>
    <span class="s1">msg = </span><span class="s5">&quot;convolution dimension_numbers must be tuple/list or None, got {}.&quot;</span>
    <span class="s2">raise </span><span class="s1">TypeError(msg.format(type(dimension_numbers)))</span>


<span class="s2">def </span><span class="s1">conv_general_permutations(dimension_numbers):</span>
  <span class="s3">&quot;&quot;&quot;Utility for convolution dimension permutations relative to Conv HLO.&quot;&quot;&quot;</span>
  <span class="s1">lhs_spec</span><span class="s2">, </span><span class="s1">rhs_spec</span><span class="s2">, </span><span class="s1">out_spec = dimension_numbers</span>
  <span class="s1">lhs_char</span><span class="s2">, </span><span class="s1">rhs_char</span><span class="s2">, </span><span class="s1">out_char = charpairs = (</span><span class="s5">&quot;N&quot;</span><span class="s2">, </span><span class="s5">&quot;C&quot;</span><span class="s1">)</span><span class="s2">, </span><span class="s1">(</span><span class="s5">&quot;O&quot;</span><span class="s2">, </span><span class="s5">&quot;I&quot;</span><span class="s1">)</span><span class="s2">, </span><span class="s1">(</span><span class="s5">&quot;N&quot;</span><span class="s2">, </span><span class="s5">&quot;C&quot;</span><span class="s1">)</span>
  <span class="s2">for </span><span class="s1">i</span><span class="s2">, </span><span class="s1">(a</span><span class="s2">, </span><span class="s1">b) </span><span class="s2">in </span><span class="s1">enumerate(charpairs):</span>
    <span class="s2">if not </span><span class="s1">dimension_numbers[i].count(a) == dimension_numbers[i].count(b) == </span><span class="s4">1</span><span class="s1">:</span>
      <span class="s1">msg = (</span><span class="s5">&quot;convolution dimension_numbers[{}] must contain the characters &quot;</span>
             <span class="s5">&quot;'{}' and '{}' exactly once, got {}.&quot;</span><span class="s1">)</span>
      <span class="s2">raise </span><span class="s1">TypeError(msg.format(i</span><span class="s2">, </span><span class="s1">a</span><span class="s2">, </span><span class="s1">b</span><span class="s2">, </span><span class="s1">dimension_numbers[i]))</span>
    <span class="s2">if </span><span class="s1">len(dimension_numbers[i]) != len(set(dimension_numbers[i])):</span>
      <span class="s1">msg = (</span><span class="s5">&quot;convolution dimension_numbers[{}] cannot have duplicate &quot;</span>
             <span class="s5">&quot;characters, got {}.&quot;</span><span class="s1">)</span>
      <span class="s2">raise </span><span class="s1">TypeError(msg.format(i</span><span class="s2">, </span><span class="s1">dimension_numbers[i]))</span>
  <span class="s2">if not </span><span class="s1">(set(lhs_spec) - set(lhs_char) == set(rhs_spec) - set(rhs_char) ==</span>
          <span class="s1">set(out_spec) - set(out_char)):</span>
    <span class="s1">msg = (</span><span class="s5">&quot;convolution dimension_numbers elements must each have the same &quot;</span>
           <span class="s5">&quot;set of spatial characters, got {}.&quot;</span><span class="s1">)</span>
    <span class="s2">raise </span><span class="s1">TypeError(msg.format(dimension_numbers))</span>

  <span class="s2">def </span><span class="s1">getperm(spec</span><span class="s2">, </span><span class="s1">charpair):</span>
    <span class="s1">spatial = (i </span><span class="s2">for </span><span class="s1">i</span><span class="s2">, </span><span class="s1">c </span><span class="s2">in </span><span class="s1">enumerate(spec) </span><span class="s2">if </span><span class="s1">c </span><span class="s2">not in </span><span class="s1">charpair)</span>
    <span class="s2">if </span><span class="s1">spec </span><span class="s2">is not </span><span class="s1">rhs_spec:</span>
      <span class="s1">spatial = sorted(spatial</span><span class="s2">, </span><span class="s1">key=</span><span class="s2">lambda </span><span class="s1">i: rhs_spec.index(spec[i]))</span>
    <span class="s2">return </span><span class="s1">(spec.index(charpair[</span><span class="s4">0</span><span class="s1">])</span><span class="s2">, </span><span class="s1">spec.index(charpair[</span><span class="s4">1</span><span class="s1">])) + tuple(spatial)</span>

  <span class="s1">lhs_perm</span><span class="s2">, </span><span class="s1">rhs_perm</span><span class="s2">, </span><span class="s1">out_perm = map(getperm</span><span class="s2">, </span><span class="s1">dimension_numbers</span><span class="s2">, </span><span class="s1">charpairs)</span>
  <span class="s2">return </span><span class="s1">lhs_perm</span><span class="s2">, </span><span class="s1">rhs_perm</span><span class="s2">, </span><span class="s1">out_perm</span>


<span class="s2">def </span><span class="s1">_conv_general_vjp_lhs_padding(</span>
    <span class="s1">in_shape</span><span class="s2">, </span><span class="s1">window_dimensions</span><span class="s2">, </span><span class="s1">window_strides</span><span class="s2">, </span><span class="s1">out_shape</span><span class="s2">, </span><span class="s1">padding</span><span class="s2">,</span>
    <span class="s1">lhs_dilation</span><span class="s2">, </span><span class="s1">rhs_dilation) -&gt; List[Tuple[int</span><span class="s2">, </span><span class="s1">int]]:</span>
  <span class="s1">lhs_dilated_shape = lax._dilate_shape(in_shape</span><span class="s2">, </span><span class="s1">lhs_dilation)</span>
  <span class="s1">rhs_dilated_shape = lax._dilate_shape(window_dimensions</span><span class="s2">, </span><span class="s1">rhs_dilation)</span>
  <span class="s1">out_dilated_shape = lax._dilate_shape(out_shape</span><span class="s2">, </span><span class="s1">window_strides)</span>
  <span class="s1">pad_before = np.subtract(rhs_dilated_shape</span><span class="s2">, </span><span class="s1">[lo </span><span class="s2">for </span><span class="s1">lo</span><span class="s2">, </span><span class="s1">_ </span><span class="s2">in </span><span class="s1">padding]) - </span><span class="s4">1</span>
  <span class="s1">pad_after = (np.add(lhs_dilated_shape</span><span class="s2">, </span><span class="s1">rhs_dilated_shape) - </span><span class="s4">1</span>
               <span class="s1">- out_dilated_shape - pad_before)</span>
  <span class="s2">return </span><span class="s1">util.safe_zip(pad_before</span><span class="s2">, </span><span class="s1">pad_after)</span>


<span class="s2">def </span><span class="s1">_conv_general_vjp_rhs_padding(</span>
    <span class="s1">in_shape</span><span class="s2">, </span><span class="s1">window_dimensions</span><span class="s2">, </span><span class="s1">window_strides</span><span class="s2">, </span><span class="s1">out_shape</span><span class="s2">, </span><span class="s1">padding</span><span class="s2">,</span>
    <span class="s1">lhs_dilation</span><span class="s2">, </span><span class="s1">rhs_dilation):</span>

  <span class="s2">if </span><span class="s1">len(in_shape) == </span><span class="s4">0</span><span class="s1">:  </span><span class="s0"># 0D conv</span>
    <span class="s2">return </span><span class="s1">[]</span>
  <span class="s1">lhs_dilated_shape = lax._dilate_shape(in_shape</span><span class="s2">, </span><span class="s1">lhs_dilation)</span>
  <span class="s1">rhs_dilated_shape = lax._dilate_shape(window_dimensions</span><span class="s2">, </span><span class="s1">rhs_dilation)</span>
  <span class="s1">out_dilated_shape = lax._dilate_shape(out_shape</span><span class="s2">, </span><span class="s1">window_strides)</span>
  <span class="s1">pads_lo</span><span class="s2">, </span><span class="s1">_ = util.unzip2(padding)</span>
  <span class="s1">pads_from_lhs = core.diff_shape(out_dilated_shape</span><span class="s2">, </span><span class="s1">lhs_dilated_shape)</span>
  <span class="s1">pads_from_rhs = core.diff_shape(core.diff_shape(rhs_dilated_shape</span><span class="s2">, </span><span class="s1">pads_lo)</span><span class="s2">,</span>
                                  <span class="s1">(</span><span class="s4">1</span><span class="s2">,</span><span class="s1">) * len(pads_lo))</span>
  <span class="s1">pads_hi = core.sum_shapes(pads_from_lhs</span><span class="s2">, </span><span class="s1">pads_from_rhs)</span>
  <span class="s2">return </span><span class="s1">list(zip(pads_lo</span><span class="s2">, </span><span class="s1">pads_hi))</span>
</pre>
</body>
</html>