<html>
<head>
<title>minimize.py</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<style type="text/css">
.s0 { color: #808080;}
.s1 { color: #a9b7c6;}
.s2 { color: #cc7832;}
.s3 { color: #629755; font-style: italic;}
.s4 { color: #6a8759;}
</style>
</head>
<body bgcolor="#2b2b2b">
<table CELLSPACING=0 CELLPADDING=5 COLS=1 WIDTH="100%" BGCOLOR="#606060" >
<tr><td><center>
<font face="Arial, Helvetica" color="#000000">
minimize.py</font>
</center></td></tr></table>
<pre><span class="s0"># Copyright 2020 The JAX Authors.</span>
<span class="s0">#</span>
<span class="s0"># Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);</span>
<span class="s0"># you may not use this file except in compliance with the License.</span>
<span class="s0"># You may obtain a copy of the License at</span>
<span class="s0">#</span>
<span class="s0">#     https://www.apache.org/licenses/LICENSE-2.0</span>
<span class="s0">#</span>
<span class="s0"># Unless required by applicable law or agreed to in writing, software</span>
<span class="s0"># distributed under the License is distributed on an &quot;AS IS&quot; BASIS,</span>
<span class="s0"># WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span>
<span class="s0"># See the License for the specific language governing permissions and</span>
<span class="s0"># limitations under the License.</span>
<span class="s2">from </span><span class="s1">typing </span><span class="s2">import </span><span class="s1">Any</span><span class="s2">, </span><span class="s1">Callable</span><span class="s2">, </span><span class="s1">Mapping</span><span class="s2">, </span><span class="s1">Optional</span><span class="s2">, </span><span class="s1">Tuple</span><span class="s2">, </span><span class="s1">Union</span>

<span class="s2">import </span><span class="s1">jax</span>
<span class="s2">from </span><span class="s1">jax._src.scipy.optimize.bfgs </span><span class="s2">import </span><span class="s1">minimize_bfgs</span>
<span class="s2">from </span><span class="s1">jax._src.scipy.optimize._lbfgs </span><span class="s2">import </span><span class="s1">_minimize_lbfgs</span>
<span class="s2">from </span><span class="s1">typing </span><span class="s2">import </span><span class="s1">NamedTuple</span>
<span class="s2">import </span><span class="s1">jax.numpy </span><span class="s2">as </span><span class="s1">jnp</span>


<span class="s2">class </span><span class="s1">OptimizeResults(NamedTuple):</span>
  <span class="s3">&quot;&quot;&quot;Object holding optimization results. 
 
  Parameters: 
    x: final solution. 
    success: ``True`` if optimization succeeded. 
    status: integer solver specific return code. 0 means converged (nominal), 
      1=max BFGS iters reached, 3=zoom failed, 4=saddle point reached, 
      5=max line search iters reached, -1=undefined 
    fun: final function value. 
    jac: final jacobian array. 
    hess_inv: final inverse Hessian estimate. 
    nfev: integer number of function calls used. 
    njev: integer number of gradient evaluations. 
    nit: integer number of iterations of the optimization algorithm. 
  &quot;&quot;&quot;</span>
  <span class="s1">x: jax.Array</span>
  <span class="s1">success: Union[bool</span><span class="s2">, </span><span class="s1">jax.Array]</span>
  <span class="s1">status: Union[int</span><span class="s2">, </span><span class="s1">jax.Array]</span>
  <span class="s1">fun: jax.Array</span>
  <span class="s1">jac: jax.Array</span>
  <span class="s1">hess_inv: Optional[jax.Array]</span>
  <span class="s1">nfev: Union[int</span><span class="s2">, </span><span class="s1">jax.Array]</span>
  <span class="s1">njev: Union[int</span><span class="s2">, </span><span class="s1">jax.Array]</span>
  <span class="s1">nit: Union[int</span><span class="s2">, </span><span class="s1">jax.Array]</span>


<span class="s2">def </span><span class="s1">minimize(</span>
    <span class="s1">fun: Callable</span><span class="s2">,</span>
    <span class="s1">x0: jax.Array</span><span class="s2">,</span>
    <span class="s1">args: Tuple = ()</span><span class="s2">,</span>
    <span class="s1">*</span><span class="s2">,</span>
    <span class="s1">method: str</span><span class="s2">,</span>
    <span class="s1">tol: Optional[float] = </span><span class="s2">None,</span>
    <span class="s1">options: Optional[Mapping[str</span><span class="s2">, </span><span class="s1">Any]] = </span><span class="s2">None,</span>
<span class="s1">) -&gt; OptimizeResults:</span>
  <span class="s3">&quot;&quot;&quot;Minimization of scalar function of one or more variables. 
 
  This API for this function matches SciPy with some minor deviations: 
 
  - Gradients of ``fun`` are calculated automatically using JAX's autodiff 
    support when required. 
  - The ``method`` argument is required. You must specify a solver. 
  - Various optional arguments in the SciPy interface have not yet been 
    implemented. 
  - Optimization results may differ from SciPy due to differences in the line 
    search implementation. 
 
  ``minimize`` supports :func:`~jax.jit` compilation. It does not yet support 
  differentiation or arguments in the form of multi-dimensional arrays, but 
  support for both is planned. 
 
  Args: 
    fun: the objective function to be minimized, ``fun(x, *args) -&gt; float``, 
      where ``x`` is a 1-D array with shape ``(n,)`` and ``args`` is a tuple 
      of the fixed parameters needed to completely specify the function. 
      ``fun`` must support differentiation. 
    x0: initial guess. Array of real elements of size ``(n,)``, where ``n`` is 
      the number of independent variables. 
    args: extra arguments passed to the objective function. 
    method: solver type. Currently only ``&quot;BFGS&quot;`` is supported. 
    tol: tolerance for termination. For detailed control, use solver-specific 
      options. 
    options: a dictionary of solver options. All methods accept the following 
      generic options: 
 
      - maxiter (int): Maximum number of iterations to perform. Depending on the 
        method each iteration may use several function evaluations. 
 
  Returns: 
    An :class:`OptimizeResults` object. 
  &quot;&quot;&quot;</span>
  <span class="s2">if </span><span class="s1">options </span><span class="s2">is None</span><span class="s1">:</span>
    <span class="s1">options = {}</span>

  <span class="s2">if not </span><span class="s1">isinstance(args</span><span class="s2">, </span><span class="s1">tuple):</span>
    <span class="s1">msg = </span><span class="s4">&quot;args argument to jax.scipy.optimize.minimize must be a tuple, got {}&quot;</span>
    <span class="s2">raise </span><span class="s1">TypeError(msg.format(args))</span>

  <span class="s1">fun_with_args = </span><span class="s2">lambda </span><span class="s1">x: fun(x</span><span class="s2">, </span><span class="s1">*args)</span>

  <span class="s2">if </span><span class="s1">method.lower() == </span><span class="s4">'bfgs'</span><span class="s1">:</span>
    <span class="s1">results = minimize_bfgs(fun_with_args</span><span class="s2">, </span><span class="s1">x0</span><span class="s2">, </span><span class="s1">**options)</span>
    <span class="s1">success = results.converged &amp; jnp.logical_not(results.failed)</span>
    <span class="s2">return </span><span class="s1">OptimizeResults(x=results.x_k</span><span class="s2">,</span>
                           <span class="s1">success=success</span><span class="s2">,</span>
                           <span class="s1">status=results.status</span><span class="s2">,</span>
                           <span class="s1">fun=results.f_k</span><span class="s2">,</span>
                           <span class="s1">jac=results.g_k</span><span class="s2">,</span>
                           <span class="s1">hess_inv=results.H_k</span><span class="s2">,</span>
                           <span class="s1">nfev=results.nfev</span><span class="s2">,</span>
                           <span class="s1">njev=results.ngev</span><span class="s2">,</span>
                           <span class="s1">nit=results.k)</span>

  <span class="s2">if </span><span class="s1">method.lower() == </span><span class="s4">'l-bfgs-experimental-do-not-rely-on-this'</span><span class="s1">:</span>
    <span class="s1">results = _minimize_lbfgs(fun_with_args</span><span class="s2">, </span><span class="s1">x0</span><span class="s2">, </span><span class="s1">**options)</span>
    <span class="s1">success = results.converged &amp; jnp.logical_not(results.failed)</span>
    <span class="s2">return </span><span class="s1">OptimizeResults(x=results.x_k</span><span class="s2">,</span>
                           <span class="s1">success=success</span><span class="s2">,</span>
                           <span class="s1">status=results.status</span><span class="s2">,</span>
                           <span class="s1">fun=results.f_k</span><span class="s2">,</span>
                           <span class="s1">jac=results.g_k</span><span class="s2">,</span>
                           <span class="s1">hess_inv=</span><span class="s2">None,</span>
                           <span class="s1">nfev=results.nfev</span><span class="s2">,</span>
                           <span class="s1">njev=results.ngev</span><span class="s2">,</span>
                           <span class="s1">nit=results.k)</span>

  <span class="s2">raise </span><span class="s1">ValueError(</span><span class="s4">f&quot;Method </span><span class="s2">{</span><span class="s1">method</span><span class="s2">} </span><span class="s4">not recognized&quot;</span><span class="s1">)</span>
</pre>
</body>
</html>