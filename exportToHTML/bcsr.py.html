<html>
<head>
<title>bcsr.py</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<style type="text/css">
.s0 { color: #808080;}
.s1 { color: #a9b7c6;}
.s2 { color: #629755; font-style: italic;}
.s3 { color: #cc7832;}
.s4 { color: #6897bb;}
.s5 { color: #6a8759;}
</style>
</head>
<body bgcolor="#2b2b2b">
<table CELLSPACING=0 CELLPADDING=5 COLS=1 WIDTH="100%" BGCOLOR="#606060" >
<tr><td><center>
<font face="Arial, Helvetica" color="#000000">
bcsr.py</font>
</center></td></tr></table>
<pre><span class="s0"># Copyright 2022 The JAX Authors.</span>
<span class="s0">#</span>
<span class="s0"># Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);</span>
<span class="s0"># you may not use this file except in compliance with the License.</span>
<span class="s0"># You may obtain a copy of the License at</span>
<span class="s0">#</span>
<span class="s0">#     https://www.apache.org/licenses/LICENSE-2.0</span>
<span class="s0">#</span>
<span class="s0"># Unless required by applicable law or agreed to in writing, software</span>
<span class="s0"># distributed under the License is distributed on an &quot;AS IS&quot; BASIS,</span>
<span class="s0"># WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span>
<span class="s0"># See the License for the specific language governing permissions and</span>
<span class="s0"># limitations under the License.</span>

<span class="s2">&quot;&quot;&quot;BCSR (Bached compressed row) matrix object and associated primitives.&quot;&quot;&quot;</span>
<span class="s3">from </span><span class="s1">__future__ </span><span class="s3">import </span><span class="s1">annotations</span>

<span class="s3">from </span><span class="s1">functools </span><span class="s3">import </span><span class="s1">partial</span>
<span class="s3">import </span><span class="s1">operator</span>
<span class="s3">import </span><span class="s1">warnings</span>

<span class="s3">from </span><span class="s1">typing </span><span class="s3">import </span><span class="s1">NamedTuple</span><span class="s3">, </span><span class="s1">Optional</span><span class="s3">, </span><span class="s1">Sequence</span><span class="s3">, </span><span class="s1">Tuple</span><span class="s3">, </span><span class="s1">Union</span>

<span class="s3">import </span><span class="s1">numpy </span><span class="s3">as </span><span class="s1">np</span>

<span class="s3">import </span><span class="s1">jax</span>
<span class="s3">import </span><span class="s1">jax.numpy </span><span class="s3">as </span><span class="s1">jnp</span>
<span class="s3">from </span><span class="s1">jax </span><span class="s3">import </span><span class="s1">config</span>
<span class="s3">from </span><span class="s1">jax </span><span class="s3">import </span><span class="s1">lax</span>
<span class="s3">from </span><span class="s1">jax </span><span class="s3">import </span><span class="s1">tree_util</span>
<span class="s3">from </span><span class="s1">jax.experimental.sparse._base </span><span class="s3">import </span><span class="s1">JAXSparse</span>
<span class="s3">from </span><span class="s1">jax.experimental.sparse </span><span class="s3">import </span><span class="s1">bcoo</span>
<span class="s3">from </span><span class="s1">jax.experimental.sparse.util </span><span class="s3">import </span><span class="s1">(</span>
    <span class="s1">nfold_vmap</span><span class="s3">, </span><span class="s1">_count_stored_elements</span><span class="s3">,</span>
    <span class="s1">_csr_to_coo</span><span class="s3">, </span><span class="s1">_dot_general_validated_shape</span><span class="s3">,</span>
    <span class="s1">CuSparseEfficiencyWarning</span><span class="s3">, </span><span class="s1">SparseInfo</span><span class="s3">, </span><span class="s1">Shape)</span>
<span class="s3">from </span><span class="s1">jax.util </span><span class="s3">import </span><span class="s1">split_list</span><span class="s3">, </span><span class="s1">safe_zip</span>

<span class="s3">from </span><span class="s1">jax._src </span><span class="s3">import </span><span class="s1">api_util</span>
<span class="s3">from </span><span class="s1">jax._src </span><span class="s3">import </span><span class="s1">core</span>
<span class="s3">from </span><span class="s1">jax._src </span><span class="s3">import </span><span class="s1">dispatch</span>
<span class="s3">from </span><span class="s1">jax._src.lax.lax </span><span class="s3">import </span><span class="s1">DotDimensionNumbers</span><span class="s3">, </span><span class="s1">_dot_general_batch_dim_nums</span>
<span class="s3">from </span><span class="s1">jax._src.lib </span><span class="s3">import </span><span class="s1">gpu_sparse</span>
<span class="s3">from </span><span class="s1">jax._src.lib.mlir.dialects </span><span class="s3">import </span><span class="s1">hlo</span>
<span class="s3">from </span><span class="s1">jax._src.interpreters </span><span class="s3">import </span><span class="s1">ad</span>
<span class="s3">from </span><span class="s1">jax._src.interpreters </span><span class="s3">import </span><span class="s1">batching</span>
<span class="s3">from </span><span class="s1">jax._src.interpreters </span><span class="s3">import </span><span class="s1">mlir</span>
<span class="s3">from </span><span class="s1">jax._src.typing </span><span class="s3">import </span><span class="s1">Array</span><span class="s3">, </span><span class="s1">ArrayLike</span><span class="s3">, </span><span class="s1">DTypeLike</span>


<span class="s3">def </span><span class="s1">bcsr_eliminate_zeros(mat: BCSR</span><span class="s3">, </span><span class="s1">nse: Optional[int] = </span><span class="s3">None</span><span class="s1">) -&gt; BCSR:</span>
  <span class="s2">&quot;&quot;&quot;Eliminate zeros in BCSR representation.&quot;&quot;&quot;</span>
  <span class="s3">return </span><span class="s1">BCSR.from_bcoo(bcoo.bcoo_eliminate_zeros(mat.to_bcoo()</span><span class="s3">, </span><span class="s1">nse=nse))</span>


<span class="s3">def </span><span class="s1">bcsr_sum_duplicates(mat: BCSR</span><span class="s3">, </span><span class="s1">nse: Optional[int] = </span><span class="s3">None</span><span class="s1">) -&gt; BCSR:</span>
  <span class="s2">&quot;&quot;&quot;Sums duplicate indices within a BCSR array, returning an array with sorted indices. 
 
  Args: 
    mat : BCSR array 
    nse : integer (optional). The number of specified elements in the output matrix. This must 
      be specified for bcoo_sum_duplicates to be compatible with JIT and other JAX transformations. 
      If not specified, the optimal nse will be computed based on the contents of the data and 
      index arrays. If specified nse is larger than necessary, data and index arrays will be padded 
      with standard fill values. If smaller than necessary, data elements will be dropped from the 
      output matrix. 
 
  Returns: 
    mat_out : BCSR array with sorted indices and no duplicate indices. 
  &quot;&quot;&quot;</span>
  <span class="s3">return </span><span class="s1">BCSR.from_bcoo(bcoo.bcoo_sum_duplicates(mat.to_bcoo()</span><span class="s3">, </span><span class="s1">nse=nse))</span>


<span class="s3">def </span><span class="s1">_bcsr_batch_dims_to_front(batched_args</span><span class="s3">, </span><span class="s1">batch_dims</span><span class="s3">, </span><span class="s1">spinfo</span><span class="s3">, </span><span class="s1">batch_size=</span><span class="s3">None</span><span class="s1">):</span>
  <span class="s1">data</span><span class="s3">, </span><span class="s1">indices</span><span class="s3">, </span><span class="s1">indptr = batched_args</span>
  <span class="s1">data_bdim</span><span class="s3">, </span><span class="s1">indices_bdim</span><span class="s3">, </span><span class="s1">indptr_bdim = batch_dims</span>
  <span class="s1">n_batch = indices.ndim - </span><span class="s4">1 </span><span class="s1">+ int(indices_bdim </span><span class="s3">is None</span><span class="s1">)</span>
  <span class="s3">if not </span><span class="s1">all(b </span><span class="s3">is None or </span><span class="s4">0 </span><span class="s1">&lt;= b &lt; n_batch </span><span class="s3">for </span><span class="s1">b </span><span class="s3">in </span><span class="s1">batch_dims):</span>
    <span class="s3">raise </span><span class="s1">NotImplementedError(</span><span class="s5">&quot;batch_dims must be None or satisfy 0 &lt; dim &lt; n_batch. &quot;</span>
                              <span class="s5">f&quot;Got </span><span class="s3">{</span><span class="s1">batch_dims=</span><span class="s3">} </span><span class="s5">for </span><span class="s3">{</span><span class="s1">n_batch=</span><span class="s3">}</span><span class="s5">.&quot;</span><span class="s1">)</span>
  <span class="s1">batched_data</span><span class="s3">, </span><span class="s1">batched_indices</span><span class="s3">, </span><span class="s1">batched_indptr = [</span>
      <span class="s1">lax.expand_dims(arg</span><span class="s3">, </span><span class="s1">[</span><span class="s4">0</span><span class="s1">]) </span><span class="s3">if </span><span class="s1">bdim </span><span class="s3">is None else </span><span class="s1">jnp.moveaxis(arg</span><span class="s3">, </span><span class="s1">bdim</span><span class="s3">, </span><span class="s4">0</span><span class="s1">)</span>
      <span class="s3">for </span><span class="s1">arg</span><span class="s3">, </span><span class="s1">bdim </span><span class="s3">in </span><span class="s1">[(data</span><span class="s3">, </span><span class="s1">data_bdim)</span><span class="s3">, </span><span class="s1">(indices</span><span class="s3">, </span><span class="s1">indices_bdim)</span><span class="s3">, </span><span class="s1">(indptr</span><span class="s3">, </span><span class="s1">indptr_bdim)]]</span>
  <span class="s3">if </span><span class="s1">batch_size </span><span class="s3">is None</span><span class="s1">:</span>
    <span class="s1">batch_size = max(arg.shape[dim] </span><span class="s3">for </span><span class="s1">arg</span><span class="s3">, </span><span class="s1">dim </span><span class="s3">in </span><span class="s1">zip(batched_args</span><span class="s3">, </span><span class="s1">batch_dims) </span><span class="s3">if </span><span class="s1">dim </span><span class="s3">is not None</span><span class="s1">)</span>
  <span class="s1">batched_spinfo = SparseInfo((batch_size</span><span class="s3">, </span><span class="s1">*spinfo.shape)</span><span class="s3">,</span>
                              <span class="s1">indices_sorted=spinfo.indices_sorted</span><span class="s3">,</span>
                              <span class="s1">unique_indices=spinfo.unique_indices)</span>
  <span class="s3">return </span><span class="s1">batched_data</span><span class="s3">, </span><span class="s1">batched_indices</span><span class="s3">, </span><span class="s1">batched_indptr</span><span class="s3">, </span><span class="s1">batched_spinfo</span>


<span class="s3">class </span><span class="s1">BCSRProperties(NamedTuple):</span>
  <span class="s1">n_batch: int</span>
  <span class="s1">n_dense: int</span>
  <span class="s1">nse: int</span>


<span class="s3">def </span><span class="s1">_compatible(shape1: Sequence[int]</span><span class="s3">, </span><span class="s1">shape2: Sequence[int]) -&gt; bool:</span>
  <span class="s3">return </span><span class="s1">all(s1 </span><span class="s3">in </span><span class="s1">(</span><span class="s4">1</span><span class="s3">, </span><span class="s1">s2) </span><span class="s3">for </span><span class="s1">s1</span><span class="s3">, </span><span class="s1">s2 </span><span class="s3">in </span><span class="s1">safe_zip(shape1</span><span class="s3">, </span><span class="s1">shape2))</span>


<span class="s3">def </span><span class="s1">_validate_bcsr_indices(indices: jax.Array</span><span class="s3">, </span><span class="s1">indptr: jax.Array</span><span class="s3">,</span>
                           <span class="s1">shape: Sequence[int]) -&gt; BCSRProperties:</span>
  <span class="s3">assert </span><span class="s1">jnp.issubdtype(indices.dtype</span><span class="s3">, </span><span class="s1">jnp.integer)</span>
  <span class="s3">assert </span><span class="s1">jnp.issubdtype(indptr.dtype</span><span class="s3">, </span><span class="s1">jnp.integer)</span>
  <span class="s1">shape = tuple(shape)</span>

  <span class="s1">nse = indices.shape[-</span><span class="s4">1</span><span class="s1">]</span>
  <span class="s1">n_batch = indices.ndim - </span><span class="s4">1</span>
  <span class="s1">n_dense = len(shape) - n_batch - </span><span class="s4">2</span>
  <span class="s3">assert </span><span class="s1">n_dense &gt;= </span><span class="s4">0</span>

  <span class="s3">if not </span><span class="s1">_compatible(indices.shape[:n_batch]</span><span class="s3">, </span><span class="s1">shape[:n_batch]):</span>
    <span class="s3">raise </span><span class="s1">ValueError(</span><span class="s5">f&quot;indices batch dimensions not compatible for </span><span class="s3">{</span><span class="s1">indices.shape=</span><span class="s3">}</span><span class="s5">, </span><span class="s3">{</span><span class="s1">shape=</span><span class="s3">}</span><span class="s5">&quot;</span><span class="s1">)</span>
  <span class="s3">if not </span><span class="s1">_compatible(indptr.shape[:n_batch]</span><span class="s3">, </span><span class="s1">shape[:n_batch]):</span>
    <span class="s3">raise </span><span class="s1">ValueError(</span><span class="s5">f&quot;indptr batch dimensions not compatible for </span><span class="s3">{</span><span class="s1">indptr.shape=</span><span class="s3">}</span><span class="s5">, </span><span class="s3">{</span><span class="s1">shape=</span><span class="s3">}</span><span class="s5">&quot;</span><span class="s1">)</span>
  <span class="s3">if </span><span class="s1">indptr.shape[n_batch:] != (shape[n_batch] + </span><span class="s4">1</span><span class="s3">,</span><span class="s1">):</span>
    <span class="s3">raise </span><span class="s1">ValueError(</span><span class="s5">&quot;indptr shape must match the matrix shape plus 1.&quot;</span><span class="s1">)</span>

  <span class="s3">return </span><span class="s1">BCSRProperties(n_batch=n_batch</span><span class="s3">, </span><span class="s1">n_dense=n_dense</span><span class="s3">, </span><span class="s1">nse=nse)</span>


<span class="s3">def </span><span class="s1">_validate_bcsr(data: jax.Array</span><span class="s3">, </span><span class="s1">indices: jax.Array</span><span class="s3">,</span>
                   <span class="s1">indptr: jax.Array</span><span class="s3">, </span><span class="s1">shape: Sequence[int]) -&gt; BCSRProperties:</span>
  <span class="s1">props = _validate_bcsr_indices(indices</span><span class="s3">, </span><span class="s1">indptr</span><span class="s3">, </span><span class="s1">shape)</span>
  <span class="s1">shape = tuple(shape)</span>
  <span class="s1">n_batch</span><span class="s3">, </span><span class="s1">n_dense</span><span class="s3">, </span><span class="s1">nse = props.n_batch</span><span class="s3">, </span><span class="s1">props.n_dense</span><span class="s3">, </span><span class="s1">props.nse</span>
  <span class="s1">n_sparse = len(shape) - n_batch - n_dense</span>
  <span class="s3">if </span><span class="s1">n_sparse != </span><span class="s4">2</span><span class="s1">:</span>
    <span class="s3">raise </span><span class="s1">ValueError(</span><span class="s5">&quot;BCSR array must have 2 sparse dimensions; &quot;</span>
                     <span class="s5">f&quot;</span><span class="s3">{</span><span class="s1">n_sparse</span><span class="s3">} </span><span class="s5">is given.&quot;</span><span class="s1">)</span>
  <span class="s3">if not </span><span class="s1">_compatible(data.shape[:n_batch]</span><span class="s3">, </span><span class="s1">shape[:n_batch]):</span>
    <span class="s3">raise </span><span class="s1">ValueError(</span><span class="s5">f&quot;data batch dimensions not compatible for </span><span class="s3">{</span><span class="s1">data.shape=</span><span class="s3">}</span><span class="s5">, </span><span class="s3">{</span><span class="s1">shape=</span><span class="s3">}</span><span class="s5">&quot;</span><span class="s1">)</span>
  <span class="s3">if </span><span class="s1">data.shape[-(n_dense + </span><span class="s4">1</span><span class="s1">):] != (nse</span><span class="s3">,</span><span class="s1">) + shape[n_batch + </span><span class="s4">2</span><span class="s1">:]:</span>
    <span class="s3">raise </span><span class="s1">ValueError(</span><span class="s5">f&quot;Invalid </span><span class="s3">{</span><span class="s1">data.shape=</span><span class="s3">} </span><span class="s5">for </span><span class="s3">{</span><span class="s1">nse=</span><span class="s3">}</span><span class="s5">, </span><span class="s3">{</span><span class="s1">n_batch=</span><span class="s3">}</span><span class="s5">, </span><span class="s3">{</span><span class="s1">n_dense=</span><span class="s3">}</span><span class="s5">&quot;</span><span class="s1">)</span>
  <span class="s3">return </span><span class="s1">props</span>


<span class="s3">def </span><span class="s1">_bcsr_to_bcoo(indices: jax.Array</span><span class="s3">, </span><span class="s1">indptr: jax.Array</span><span class="s3">, </span><span class="s1">*</span><span class="s3">,</span>
                  <span class="s1">shape: Sequence[int]) -&gt; jax.Array:</span>
  <span class="s2">&quot;&quot;&quot;Given BCSR (indices, indptr), return BCOO (indices).&quot;&quot;&quot;</span>
  <span class="s1">n_batch</span><span class="s3">, </span><span class="s1">_</span><span class="s3">, </span><span class="s1">_ = _validate_bcsr_indices(indices</span><span class="s3">, </span><span class="s1">indptr</span><span class="s3">, </span><span class="s1">shape)</span>
  <span class="s1">csr_to_coo = nfold_vmap(_csr_to_coo</span><span class="s3">, </span><span class="s1">n_batch)</span>
  <span class="s3">return </span><span class="s1">jnp.stack(csr_to_coo(indices</span><span class="s3">, </span><span class="s1">indptr)</span><span class="s3">, </span><span class="s1">axis=indices.ndim)</span>


<span class="s3">def </span><span class="s1">_bcoo_to_bcsr(indices: Array</span><span class="s3">, </span><span class="s1">*</span><span class="s3">, </span><span class="s1">shape: Sequence[int]</span><span class="s3">,</span>
                  <span class="s1">index_dtype: DTypeLike = jnp.int32) -&gt; Tuple[Array</span><span class="s3">, </span><span class="s1">Array]:</span>
  <span class="s2">&quot;&quot;&quot;Given BCOO (indices), return BCSR (indices, indptr). 
 
  Note: this assumes that ``indices`` are lexicographically sorted within each batch. 
  &quot;&quot;&quot;</span>
  <span class="s1">n_batch</span><span class="s3">, </span><span class="s1">n_sparse</span><span class="s3">, </span><span class="s1">_</span><span class="s3">, </span><span class="s1">_ = bcoo._validate_bcoo_indices(indices</span><span class="s3">, </span><span class="s1">shape)</span>

  <span class="s3">if </span><span class="s1">n_sparse != </span><span class="s4">2</span><span class="s1">:</span>
    <span class="s3">raise </span><span class="s1">ValueError(</span><span class="s5">&quot;Must have 2 sparse dimensions to be converted to BCSR.&quot;</span><span class="s1">)</span>

  <span class="s1">n_rows = shape[n_batch]</span>

  <span class="s1">@partial(nfold_vmap</span><span class="s3">, </span><span class="s1">N=n_batch</span><span class="s3">, </span><span class="s1">broadcasted=</span><span class="s3">False</span><span class="s1">)</span>
  <span class="s3">def </span><span class="s1">get_ptr(i):</span>
    <span class="s1">indptr = jnp.zeros(n_rows + </span><span class="s4">1</span><span class="s3">, </span><span class="s1">index_dtype)</span>
    <span class="s3">return </span><span class="s1">indptr.at[</span><span class="s4">1</span><span class="s1">:].set(jnp.cumsum(</span>
        <span class="s1">jnp.bincount(i</span><span class="s3">, </span><span class="s1">length=n_rows).astype(index_dtype)))</span>

  <span class="s3">return </span><span class="s1">indices[...</span><span class="s3">, </span><span class="s4">1</span><span class="s1">]</span><span class="s3">, </span><span class="s1">get_ptr(indices[...</span><span class="s3">, </span><span class="s4">0</span><span class="s1">])</span>


<span class="s0">#--------------------------------------------------------------------</span>
<span class="s0"># bcsr_fromdense</span>
<span class="s1">bcsr_fromdense_p = core.Primitive(</span><span class="s5">'bcsr_fromdense'</span><span class="s1">)</span>
<span class="s1">bcsr_fromdense_p.multiple_results = </span><span class="s3">True</span>


<span class="s1">_TRACED_NSE_ERROR = </span><span class="s5">&quot;&quot;&quot; 
The error arose for the nse argument of bcsr_fromdense. In order for 
BCSR.fromdense() to be used in traced/compiled code, you must pass a concrete 
value to the nse (number of stored elements) argument. 
&quot;&quot;&quot;</span>


<span class="s3">def </span><span class="s1">bcsr_fromdense(mat: ArrayLike</span><span class="s3">, </span><span class="s1">*</span><span class="s3">, </span><span class="s1">nse: Optional[int] = </span><span class="s3">None, </span><span class="s1">n_batch: int = </span><span class="s4">0</span><span class="s3">,</span>
                   <span class="s1">n_dense:int = </span><span class="s4">0</span><span class="s3">, </span><span class="s1">index_dtype: DTypeLike = jnp.int32) -&gt; BCSR:</span>
  <span class="s2">&quot;&quot;&quot;Create BCSR-format sparse matrix from a dense matrix. 
 
  Args: 
    mat : array to be converted to BCOO. 
    nse : number of stored elements in each batch 
    n_batch : number of batch dimensions (default: 0) 
    n_dense : number of dense dimensions (default: 0) 
    index_dtype : dtype of sparse indices (default: int32) 
 
  Returns: 
    mat_bcsr: BCSR representation of the matrix. 
  &quot;&quot;&quot;</span>
  <span class="s1">mat = jnp.asarray(mat)</span>
  <span class="s3">if </span><span class="s1">nse </span><span class="s3">is None</span><span class="s1">:</span>
    <span class="s1">nse = _count_stored_elements(mat</span><span class="s3">, </span><span class="s1">n_batch</span><span class="s3">, </span><span class="s1">n_dense)</span>
  <span class="s1">nse_int: int = core.concrete_or_error(operator.index</span><span class="s3">, </span><span class="s1">nse</span><span class="s3">, </span><span class="s1">_TRACED_NSE_ERROR)</span>
  <span class="s3">return </span><span class="s1">BCSR(_bcsr_fromdense(mat</span><span class="s3">, </span><span class="s1">nse=nse_int</span><span class="s3">, </span><span class="s1">n_batch=n_batch</span><span class="s3">, </span><span class="s1">n_dense=n_dense</span><span class="s3">,</span>
                              <span class="s1">index_dtype=index_dtype)</span><span class="s3">,</span>
              <span class="s1">shape=mat.shape)</span>


<span class="s3">def </span><span class="s1">_bcsr_fromdense(mat: ArrayLike</span><span class="s3">, </span><span class="s1">*</span><span class="s3">, </span><span class="s1">nse: int</span><span class="s3">, </span><span class="s1">n_batch: int = </span><span class="s4">0</span><span class="s3">, </span><span class="s1">n_dense: int = </span><span class="s4">0</span><span class="s3">,</span>
                    <span class="s1">index_dtype: DTypeLike = jnp.int32) -&gt; Tuple[Array</span><span class="s3">, </span><span class="s1">Array</span><span class="s3">, </span><span class="s1">Array]:</span>
  <span class="s2">&quot;&quot;&quot;Create BCSR-format sparse matrix from a dense matrix. 
 
  Args: 
    mat : array to be converted to BCSR, with 
      ``ndim = n_batch + n_sparse + n_dense``. 
    nse : number of stored elements in each batch. 
    n_batch : number of batch dimensions (default: 0) 
    n_dense : number of dense dimensions (default: 0) 
    index_dtype : dtype of sparse indices (default: int32) 
 
  Returns: 
    data : array of shape 
    ``mat.shape[:n_batch] + (nse,) + mat.shape[mat.ndim - n_dense:]`` 
      and dtype ``mat.dtype`` 
    indices : array of shape ``mat.shape[:n_batch] + (nse,)`` and dtype of 
      ``index_type``. 
    indptr: array of shape ``mat.shape[:n_batch] + (mat.shape[n_batch] + 1,)`` 
      and dtype of ``index_type``. 
  &quot;&quot;&quot;</span>
  <span class="s1">mat = jnp.asarray(mat)</span>
  <span class="s1">nse = core.concrete_or_error(operator.index</span><span class="s3">, </span><span class="s1">nse</span><span class="s3">, </span><span class="s1">_TRACED_NSE_ERROR)</span>
  <span class="s3">return </span><span class="s1">bcsr_fromdense_p.bind(mat</span><span class="s3">, </span><span class="s1">nse=nse</span><span class="s3">, </span><span class="s1">n_batch=n_batch</span><span class="s3">, </span><span class="s1">n_dense=n_dense</span><span class="s3">,</span>
                               <span class="s1">index_dtype=index_dtype)</span>


<span class="s1">@bcsr_fromdense_p.def_impl</span>
<span class="s3">def </span><span class="s1">_bcsr_fromdense_impl(mat</span><span class="s3">, </span><span class="s1">*</span><span class="s3">, </span><span class="s1">nse</span><span class="s3">, </span><span class="s1">n_batch</span><span class="s3">, </span><span class="s1">n_dense</span><span class="s3">, </span><span class="s1">index_dtype):</span>
  <span class="s1">mat = jnp.asarray(mat)</span>
  <span class="s1">n_sparse = mat.ndim - n_dense - n_batch</span>
  <span class="s3">if </span><span class="s1">n_sparse != </span><span class="s4">2</span><span class="s1">:</span>
    <span class="s3">raise </span><span class="s1">ValueError(</span><span class="s5">&quot;bcsr_fromdense: must have 2 sparse dimensions.&quot;</span><span class="s1">)</span>
  <span class="s1">bcoo_mat = bcoo.bcoo_fromdense(mat</span><span class="s3">, </span><span class="s1">nse=nse</span><span class="s3">, </span><span class="s1">index_dtype=index_dtype</span><span class="s3">,</span>
                                 <span class="s1">n_dense=n_dense</span><span class="s3">, </span><span class="s1">n_batch=n_batch)</span>
  <span class="s1">indices</span><span class="s3">, </span><span class="s1">indptr = _bcoo_to_bcsr(bcoo_mat.indices</span><span class="s3">, </span><span class="s1">shape=mat.shape)</span>
  <span class="s3">return </span><span class="s1">bcoo_mat.data</span><span class="s3">, </span><span class="s1">indices</span><span class="s3">, </span><span class="s1">indptr</span>


<span class="s1">@bcsr_fromdense_p.def_abstract_eval</span>
<span class="s3">def </span><span class="s1">_bcsr_fromdense_abstract_eval(mat</span><span class="s3">, </span><span class="s1">*</span><span class="s3">, </span><span class="s1">nse</span><span class="s3">, </span><span class="s1">n_batch</span><span class="s3">, </span><span class="s1">n_dense</span><span class="s3">, </span><span class="s1">index_dtype):</span>
  <span class="s1">n_sparse = mat.ndim - n_batch - n_dense</span>
  <span class="s3">if </span><span class="s1">n_sparse != </span><span class="s4">2</span><span class="s1">:</span>
    <span class="s3">raise </span><span class="s1">ValueError(</span><span class="s5">&quot;bcsr_fromdense: must have 2 sparse dimensions.&quot;</span><span class="s1">)</span>
  <span class="s1">data_shape = mat.shape[:n_batch] + (nse</span><span class="s3">,</span><span class="s1">) + mat.shape[n_batch + n_sparse:]</span>
  <span class="s1">index_shape = mat.shape[:n_batch] + (nse</span><span class="s3">,</span><span class="s1">)</span>
  <span class="s1">indptr_shape = mat.shape[:n_batch] + (mat.shape[n_batch] + </span><span class="s4">1</span><span class="s3">,</span><span class="s1">)</span>
  <span class="s3">return </span><span class="s1">(core.ShapedArray(data_shape</span><span class="s3">, </span><span class="s1">mat.dtype)</span><span class="s3">,</span>
          <span class="s1">core.ShapedArray(index_shape</span><span class="s3">, </span><span class="s1">index_dtype)</span><span class="s3">,</span>
          <span class="s1">core.ShapedArray(indptr_shape</span><span class="s3">, </span><span class="s1">index_dtype))</span>


<span class="s3">def </span><span class="s1">_bcsr_fromdense_batching_rule(batched_args</span><span class="s3">, </span><span class="s1">batch_dims</span><span class="s3">, </span><span class="s1">*</span><span class="s3">, </span><span class="s1">nse</span><span class="s3">, </span><span class="s1">n_batch</span><span class="s3">,</span>
                                  <span class="s1">n_dense</span><span class="s3">, </span><span class="s1">index_dtype):</span>
  <span class="s1">M</span><span class="s3">, </span><span class="s1">= batched_args</span>
  <span class="s1">bdim</span><span class="s3">, </span><span class="s1">= batch_dims</span>
  <span class="s3">if not </span><span class="s1">(</span><span class="s4">0 </span><span class="s1">&lt;= bdim &lt;= n_batch):</span>
    <span class="s3">raise </span><span class="s1">ValueError(</span><span class="s5">f&quot;Expected 0 &lt; bdim &lt;= n_batch; got </span><span class="s3">{</span><span class="s1">bdim=</span><span class="s3">}</span><span class="s5">, </span><span class="s3">{</span><span class="s1">n_batch=</span><span class="s3">}</span><span class="s5">&quot;</span><span class="s1">)</span>
  <span class="s3">return </span><span class="s1">_bcsr_fromdense(M</span><span class="s3">, </span><span class="s1">nse=nse</span><span class="s3">, </span><span class="s1">n_batch=n_batch + </span><span class="s4">1</span><span class="s3">, </span><span class="s1">n_dense=n_dense</span><span class="s3">, </span><span class="s1">index_dtype=index_dtype)</span><span class="s3">, </span><span class="s1">(bdim</span><span class="s3">, </span><span class="s1">bdim</span><span class="s3">, </span><span class="s1">bdim)</span>


<span class="s3">def </span><span class="s1">_bcsr_fromdense_jvp(primals</span><span class="s3">, </span><span class="s1">tangents</span><span class="s3">, </span><span class="s1">*</span><span class="s3">, </span><span class="s1">nse</span><span class="s3">, </span><span class="s1">n_batch</span><span class="s3">, </span><span class="s1">n_dense</span><span class="s3">, </span><span class="s1">index_dtype):</span>
  <span class="s1">M</span><span class="s3">, </span><span class="s1">= primals</span>
  <span class="s1">Mdot</span><span class="s3">, </span><span class="s1">= tangents</span>

  <span class="s1">primals_out = _bcsr_fromdense(M</span><span class="s3">, </span><span class="s1">nse=nse</span><span class="s3">, </span><span class="s1">n_batch=n_batch</span><span class="s3">, </span><span class="s1">n_dense=n_dense</span><span class="s3">, </span><span class="s1">index_dtype=index_dtype)</span>
  <span class="s1">data</span><span class="s3">, </span><span class="s1">indices</span><span class="s3">, </span><span class="s1">indptr = primals_out</span>

  <span class="s3">if </span><span class="s1">type(Mdot) </span><span class="s3">is </span><span class="s1">ad.Zero:</span>
    <span class="s1">data_dot = ad.Zero.from_value(data)</span>
  <span class="s3">else</span><span class="s1">:</span>
    <span class="s1">data_dot = bcsr_extract(indices</span><span class="s3">, </span><span class="s1">indptr</span><span class="s3">, </span><span class="s1">Mdot)</span>

  <span class="s1">tangents_out = (data_dot</span><span class="s3">, </span><span class="s1">ad.Zero.from_value(indices)</span><span class="s3">, </span><span class="s1">ad.Zero.from_value(indptr))</span>

  <span class="s3">return </span><span class="s1">primals_out</span><span class="s3">, </span><span class="s1">tangents_out</span>


<span class="s3">def </span><span class="s1">_bcsr_fromdense_transpose(ct</span><span class="s3">, </span><span class="s1">M</span><span class="s3">, </span><span class="s1">*</span><span class="s3">, </span><span class="s1">nse</span><span class="s3">, </span><span class="s1">n_batch</span><span class="s3">, </span><span class="s1">n_dense</span><span class="s3">, </span><span class="s1">index_dtype):</span>
  <span class="s1">data</span><span class="s3">, </span><span class="s1">indices</span><span class="s3">, </span><span class="s1">indptr = ct</span>
  <span class="s1">n_sparse = M.ndim - n_batch - n_dense</span>
  <span class="s3">assert </span><span class="s1">data.shape == M.shape[:n_batch] + (nse</span><span class="s3">,</span><span class="s1">) + M.shape[n_batch + n_sparse:]</span>
  <span class="s3">assert </span><span class="s1">indices.shape == M.shape[:n_batch] + (n_sparse</span><span class="s3">, </span><span class="s1">nse)</span>
  <span class="s3">assert </span><span class="s1">indptr.shape == M.shape[:n_batch] + (M.shape[n_batch] + </span><span class="s4">1</span><span class="s3">,</span><span class="s1">)</span>
  <span class="s3">assert </span><span class="s1">indices.dtype == index_dtype</span>
  <span class="s3">assert </span><span class="s1">indptr.dtype == index_dtype</span>
  <span class="s3">if </span><span class="s1">isinstance(indices</span><span class="s3">, </span><span class="s1">ad.Zero) </span><span class="s3">or </span><span class="s1">isinstance(indptr</span><span class="s3">, </span><span class="s1">ad.Zero):</span>
    <span class="s3">raise </span><span class="s1">ValueError(</span><span class="s5">&quot;Cannot transpose with respect to sparse indices&quot;</span><span class="s1">)</span>
  <span class="s3">assert </span><span class="s1">ad.is_undefined_primal(M)</span>
  <span class="s3">return </span><span class="s1">_bcsr_todense(data</span><span class="s3">, </span><span class="s1">indices</span><span class="s3">, </span><span class="s1">indptr</span><span class="s3">, </span><span class="s1">spinfo=SparseInfo(M.aval.shape))</span>


<span class="s1">ad.primitive_jvps[bcsr_fromdense_p] = _bcsr_fromdense_jvp</span>
<span class="s1">ad.primitive_transposes[bcsr_fromdense_p] = _bcsr_fromdense_transpose</span>
<span class="s1">batching.primitive_batchers[bcsr_fromdense_p] = _bcsr_fromdense_batching_rule</span>
<span class="s1">mlir.register_lowering(bcsr_fromdense_p</span><span class="s3">, </span><span class="s1">mlir.lower_fun(</span>
    <span class="s1">_bcsr_fromdense_impl</span><span class="s3">, </span><span class="s1">multiple_results=</span><span class="s3">True</span><span class="s1">))</span>


<span class="s0">#----------------------------------------------------------------------</span>
<span class="s0"># bcsr_todense</span>
<span class="s1">bcsr_todense_p = core.Primitive(</span><span class="s5">'bcsr_todense'</span><span class="s1">)</span>


<span class="s3">def </span><span class="s1">bcsr_todense(mat: BCSR) -&gt; Array:</span>
  <span class="s2">&quot;&quot;&quot;Convert batched sparse matrix to a dense matrix. 
 
  Args: 
    mat: BCSR matrix. 
 
  Returns: 
    The dense version of ``mat``. 
  &quot;&quot;&quot;</span>
  <span class="s3">return </span><span class="s1">_bcsr_todense(mat.data</span><span class="s3">, </span><span class="s1">mat.indices</span><span class="s3">, </span><span class="s1">mat.indptr</span><span class="s3">, </span><span class="s1">spinfo=mat._info)</span>


<span class="s3">def </span><span class="s1">_bcsr_todense(data: ArrayLike</span><span class="s3">, </span><span class="s1">indices: ArrayLike</span><span class="s3">, </span><span class="s1">indptr: ArrayLike</span><span class="s3">, </span><span class="s1">*</span><span class="s3">,</span>
                  <span class="s1">spinfo: SparseInfo) -&gt; Array:</span>
  <span class="s2">&quot;&quot;&quot;Convert batched sparse matrix to a dense matrix. 
 
  Args: 
    data : array of shape ``batch_dims + (nse,) + dense_dims``. 
    indices : array of shape ``batch_dims + (nse,)``. 
    indptr : array of shape ``batch_dims + (shape[len(batch_dims)] + 1,). 
    spinfo : SparseInfo. In particular, this includes the shape 
      of the matrix, which is equal to 
      ``batch_dims + 2(sparse_dims) + block_dims`` where 
      ``len(sparse_dims) == 2``. 
  Returns: 
    mat : array with specified shape and dtype matching ``data`` 
  &quot;&quot;&quot;</span>
  <span class="s3">return </span><span class="s1">bcsr_todense_p.bind(jnp.asarray(data)</span><span class="s3">, </span><span class="s1">jnp.asarray(indices)</span><span class="s3">,</span>
                             <span class="s1">jnp.asarray(indptr)</span><span class="s3">, </span><span class="s1">spinfo=spinfo)</span>


<span class="s1">@bcsr_todense_p.def_impl</span>
<span class="s3">def </span><span class="s1">_bcsr_todense_impl(data</span><span class="s3">, </span><span class="s1">indices</span><span class="s3">, </span><span class="s1">indptr</span><span class="s3">, </span><span class="s1">*</span><span class="s3">, </span><span class="s1">spinfo):</span>
  <span class="s1">shape = spinfo.shape</span>
  <span class="s1">bcoo_indices = _bcsr_to_bcoo(indices</span><span class="s3">, </span><span class="s1">indptr</span><span class="s3">, </span><span class="s1">shape=shape)</span>
  <span class="s3">return </span><span class="s1">(bcoo.BCOO((data</span><span class="s3">, </span><span class="s1">bcoo_indices)</span><span class="s3">, </span><span class="s1">shape=shape)).todense()</span>


<span class="s1">@bcsr_todense_p.def_abstract_eval</span>
<span class="s3">def </span><span class="s1">_bcsr_todense_abstract_eval(data</span><span class="s3">, </span><span class="s1">indices</span><span class="s3">, </span><span class="s1">indptr</span><span class="s3">, </span><span class="s1">*</span><span class="s3">, </span><span class="s1">spinfo):</span>
  <span class="s1">shape = spinfo.shape</span>
  <span class="s1">_validate_bcsr(data</span><span class="s3">, </span><span class="s1">indices</span><span class="s3">, </span><span class="s1">indptr</span><span class="s3">, </span><span class="s1">shape)</span>
  <span class="s3">return </span><span class="s1">core.ShapedArray(shape</span><span class="s3">, </span><span class="s1">data.dtype)</span>


<span class="s3">def </span><span class="s1">_bcsr_todense_batching_rule(batched_args</span><span class="s3">, </span><span class="s1">batch_dims</span><span class="s3">, </span><span class="s1">*</span><span class="s3">, </span><span class="s1">spinfo):</span>
  <span class="s1">data</span><span class="s3">, </span><span class="s1">indices</span><span class="s3">, </span><span class="s1">indptr</span><span class="s3">, </span><span class="s1">spinfo = _bcsr_batch_dims_to_front(batched_args</span><span class="s3">, </span><span class="s1">batch_dims</span><span class="s3">, </span><span class="s1">spinfo)</span>
  <span class="s3">return </span><span class="s1">_bcsr_todense(data</span><span class="s3">, </span><span class="s1">indices</span><span class="s3">, </span><span class="s1">indptr</span><span class="s3">, </span><span class="s1">spinfo=spinfo)</span><span class="s3">, </span><span class="s4">0</span>


<span class="s3">def </span><span class="s1">_bcsr_todense_jvp(data_dot</span><span class="s3">, </span><span class="s1">data</span><span class="s3">, </span><span class="s1">indices</span><span class="s3">, </span><span class="s1">indptr</span><span class="s3">, </span><span class="s1">*</span><span class="s3">, </span><span class="s1">spinfo):</span>
  <span class="s3">del </span><span class="s1">data</span>
  <span class="s3">return </span><span class="s1">_bcsr_todense(data_dot</span><span class="s3">, </span><span class="s1">indices</span><span class="s3">, </span><span class="s1">indptr</span><span class="s3">, </span><span class="s1">spinfo=spinfo)</span>


<span class="s3">def </span><span class="s1">_bcsr_todense_transpose(ct</span><span class="s3">, </span><span class="s1">data</span><span class="s3">, </span><span class="s1">indices</span><span class="s3">, </span><span class="s1">indptr</span><span class="s3">, </span><span class="s1">*</span><span class="s3">, </span><span class="s1">spinfo):</span>
  <span class="s1">shape = spinfo.shape</span>
  <span class="s3">assert </span><span class="s1">ad.is_undefined_primal(data)</span>
  <span class="s3">if </span><span class="s1">ad.is_undefined_primal(indices) </span><span class="s3">or </span><span class="s1">ad.is_undefined_primal(indptr):</span>
    <span class="s3">raise </span><span class="s1">ValueError(</span><span class="s5">&quot;Cannot transpose with respect to sparse indices&quot;</span><span class="s1">)</span>
  <span class="s3">assert </span><span class="s1">ct.shape == shape</span>
  <span class="s3">assert </span><span class="s1">ct.dtype == data.aval.dtype</span>
  <span class="s3">return </span><span class="s1">bcsr_extract(indices</span><span class="s3">, </span><span class="s1">indptr</span><span class="s3">, </span><span class="s1">ct)</span><span class="s3">, </span><span class="s1">indices</span><span class="s3">, </span><span class="s1">indptr</span>


<span class="s1">ad.defjvp(bcsr_todense_p</span><span class="s3">, </span><span class="s1">_bcsr_todense_jvp</span><span class="s3">, None, None</span><span class="s1">)</span>
<span class="s1">ad.primitive_transposes[bcsr_todense_p] = _bcsr_todense_transpose</span>
<span class="s1">batching.primitive_batchers[bcsr_todense_p] = _bcsr_todense_batching_rule</span>
<span class="s1">mlir.register_lowering(bcsr_todense_p</span><span class="s3">, </span><span class="s1">mlir.lower_fun(</span>
    <span class="s1">_bcsr_todense_impl</span><span class="s3">, </span><span class="s1">multiple_results=</span><span class="s3">False</span><span class="s1">))</span>


<span class="s0">#--------------------------------------------------------------------</span>
<span class="s0"># bcsr_extract</span>
<span class="s1">bcsr_extract_p = core.Primitive(</span><span class="s5">'bcsr_extract'</span><span class="s1">)</span>


<span class="s3">def </span><span class="s1">bcsr_extract(indices: ArrayLike</span><span class="s3">, </span><span class="s1">indptr: ArrayLike</span><span class="s3">, </span><span class="s1">mat: ArrayLike) -&gt; Array:</span>
  <span class="s2">&quot;&quot;&quot;Extract values from a dense matrix at given BCSR (indices, indptr). 
 
  Args: 
    indices: An ndarray; see BCSR indices. 
    indptr: An ndarray; see BCSR indptr. 
    mat: A dense matrix. 
 
  Returns: 
    An ndarray; see BCSR data. 
  &quot;&quot;&quot;</span>
  <span class="s3">return </span><span class="s1">bcsr_extract_p.bind(indices</span><span class="s3">, </span><span class="s1">indptr</span><span class="s3">, </span><span class="s1">mat)</span>


<span class="s1">@bcsr_extract_p.def_impl</span>
<span class="s3">def </span><span class="s1">_bcsr_extract_impl(indices</span><span class="s3">, </span><span class="s1">indptr</span><span class="s3">, </span><span class="s1">mat):</span>
  <span class="s1">mat = jnp.asarray(mat)</span>
  <span class="s1">bcoo_indices = _bcsr_to_bcoo(indices</span><span class="s3">, </span><span class="s1">indptr</span><span class="s3">, </span><span class="s1">shape=mat.shape)</span>
  <span class="s3">return </span><span class="s1">bcoo._bcoo_extract(bcoo_indices</span><span class="s3">, </span><span class="s1">mat)</span>


<span class="s1">@bcsr_extract_p.def_abstract_eval</span>
<span class="s3">def </span><span class="s1">_bcsr_extract_abstract_eval(indices</span><span class="s3">, </span><span class="s1">indptr</span><span class="s3">, </span><span class="s1">mat):</span>
  <span class="s1">n_batch</span><span class="s3">, </span><span class="s1">n_dense</span><span class="s3">, </span><span class="s1">nse = _validate_bcsr_indices(indices</span><span class="s3">, </span><span class="s1">indptr</span><span class="s3">, </span><span class="s1">mat.shape)</span>
  <span class="s1">out_shape = mat.shape[:n_batch] + (nse</span><span class="s3">,</span><span class="s1">) + mat.shape[mat.ndim - n_dense:]</span>
  <span class="s3">return </span><span class="s1">core.ShapedArray(out_shape</span><span class="s3">, </span><span class="s1">mat.dtype)</span>


<span class="s3">def </span><span class="s1">_bcsr_extract_jvp(arr_dot</span><span class="s3">, </span><span class="s1">indices</span><span class="s3">, </span><span class="s1">indptr</span><span class="s3">, </span><span class="s1">arr):</span>
  <span class="s3">assert </span><span class="s1">arr_dot.shape == arr.shape</span>
  <span class="s3">return </span><span class="s1">bcsr_extract(indices</span><span class="s3">, </span><span class="s1">indptr</span><span class="s3">, </span><span class="s1">arr_dot)</span>


<span class="s3">def </span><span class="s1">_bcsr_extract_transpose(ct</span><span class="s3">, </span><span class="s1">indices</span><span class="s3">, </span><span class="s1">indptr</span><span class="s3">, </span><span class="s1">arr):</span>
  <span class="s3">assert </span><span class="s1">ad.is_undefined_primal(arr)</span>
  <span class="s3">if </span><span class="s1">ad.is_undefined_primal(indices) </span><span class="s3">or </span><span class="s1">ad.is_undefined_primal(indptr):</span>
    <span class="s3">raise </span><span class="s1">ValueError(</span><span class="s5">&quot;Cannot transpose with respect to sparse indices&quot;</span><span class="s1">)</span>
  <span class="s3">assert </span><span class="s1">ct.dtype == arr.aval.dtype</span>
  <span class="s3">return </span><span class="s1">indices</span><span class="s3">, </span><span class="s1">indptr</span><span class="s3">, </span><span class="s1">_bcsr_todense(ct</span><span class="s3">, </span><span class="s1">indices</span><span class="s3">, </span><span class="s1">indptr</span><span class="s3">, </span><span class="s1">spinfo=SparseInfo(arr.aval.shape))</span>


<span class="s3">def </span><span class="s1">_bcsr_extract_batching_rule(batched_args</span><span class="s3">, </span><span class="s1">batch_dims):</span>
  <span class="s1">indices</span><span class="s3">, </span><span class="s1">indptr</span><span class="s3">, </span><span class="s1">arr = batched_args</span>
  <span class="s1">bdim_set = {b </span><span class="s3">for </span><span class="s1">b </span><span class="s3">in </span><span class="s1">batch_dims </span><span class="s3">if </span><span class="s1">b </span><span class="s3">is not None</span><span class="s1">}</span>
  <span class="s3">if </span><span class="s1">len(bdim_set) != </span><span class="s4">1</span><span class="s1">:</span>
    <span class="s0"># TODO(jakevdp): handle this by moving bdim to front?</span>
    <span class="s3">raise </span><span class="s1">NotImplementedError(</span><span class="s5">&quot;bcoo_extract with unequal batch dimensions.&quot;</span><span class="s1">)</span>
  <span class="s1">bdim = next(iter(bdim_set))</span>
  <span class="s3">if </span><span class="s1">batch_dims[</span><span class="s4">0</span><span class="s1">] </span><span class="s3">is None</span><span class="s1">:</span>
    <span class="s1">indices = lax.expand_dims(indices</span><span class="s3">, </span><span class="s1">(bdim</span><span class="s3">,</span><span class="s1">))</span>
  <span class="s3">if </span><span class="s1">batch_dims[</span><span class="s4">1</span><span class="s1">] </span><span class="s3">is None</span><span class="s1">:</span>
    <span class="s1">indptr = lax.expand_dims(indptr</span><span class="s3">, </span><span class="s1">(bdim</span><span class="s3">,</span><span class="s1">))</span>
  <span class="s3">if </span><span class="s1">batch_dims[</span><span class="s4">2</span><span class="s1">] </span><span class="s3">is None</span><span class="s1">:</span>
    <span class="s0"># TODO(jakevdp) can we handle this case without explicit broadcasting?</span>
    <span class="s1">result_shape = list(arr.shape)</span>
    <span class="s1">result_shape.insert(bdim</span><span class="s3">, </span><span class="s1">indices.shape[bdim])</span>
    <span class="s1">arr = lax.broadcast_in_dim(arr</span><span class="s3">, </span><span class="s1">result_shape</span><span class="s3">, </span><span class="s1">(bdim</span><span class="s3">,</span><span class="s1">))</span>
  <span class="s1">n_batch = indices.ndim - </span><span class="s4">1</span>
  <span class="s3">if </span><span class="s1">bdim &gt;= n_batch:</span>
    <span class="s3">raise </span><span class="s1">ValueError(</span><span class="s5">f&quot;</span><span class="s3">{</span><span class="s1">batch_dims=</span><span class="s3">} </span><span class="s5">out of range for indices with </span><span class="s3">{</span><span class="s1">n_batch=</span><span class="s3">}</span><span class="s5">&quot;</span><span class="s1">)</span>
  <span class="s3">return </span><span class="s1">bcsr_extract(indices</span><span class="s3">, </span><span class="s1">indptr</span><span class="s3">, </span><span class="s1">arr)</span><span class="s3">, </span><span class="s1">bdim</span>

<span class="s1">ad.defjvp(bcsr_extract_p</span><span class="s3">, None, None, </span><span class="s1">_bcsr_extract_jvp)</span>
<span class="s1">ad.primitive_transposes[bcsr_extract_p] = _bcsr_extract_transpose</span>
<span class="s1">batching.primitive_batchers[bcsr_extract_p] = _bcsr_extract_batching_rule</span>
<span class="s1">mlir.register_lowering(bcsr_extract_p</span><span class="s3">, </span><span class="s1">mlir.lower_fun(</span>
    <span class="s1">_bcsr_extract_impl</span><span class="s3">, </span><span class="s1">multiple_results=</span><span class="s3">False</span><span class="s1">))</span>


<span class="s0">#----------------------------------------------------------------------</span>
<span class="s0"># bcsr_dot_general</span>


<span class="s1">bcsr_dot_general_p = core.Primitive(</span><span class="s5">'bcsr_dot_general'</span><span class="s1">)</span>


<span class="s3">def </span><span class="s1">bcsr_dot_general(lhs: Union[BCSR</span><span class="s3">, </span><span class="s1">Array]</span><span class="s3">, </span><span class="s1">rhs: Array</span><span class="s3">, </span><span class="s1">*</span><span class="s3">,</span>
                     <span class="s1">dimension_numbers: DotDimensionNumbers</span><span class="s3">,</span>
                     <span class="s1">precision: </span><span class="s3">None </span><span class="s1">= </span><span class="s3">None,</span>
                     <span class="s1">preferred_element_type: </span><span class="s3">None </span><span class="s1">= </span><span class="s3">None</span><span class="s1">) -&gt; Array:</span>
  <span class="s2">&quot;&quot;&quot;A general contraction operation. 
 
  Args: 
    lhs: An ndarray or BCSR-format sparse array. 
    rhs: An ndarray or BCSR-format sparse array.. 
    dimension_numbers: a tuple of tuples of the form 
      `((lhs_contracting_dims, rhs_contracting_dims), 
      (lhs_batch_dims, rhs_batch_dims))`. 
    precision: unused 
    preferred_element_type: unused 
 
  Returns: 
    An ndarray or BCSR-format sparse array containing the result. If both inputs 
    are sparse, the result will be sparse, of type BCSR. If either input is 
    dense, the result will be dense, of type ndarray. 
  &quot;&quot;&quot;</span>
  <span class="s3">del </span><span class="s1">precision</span><span class="s3">, </span><span class="s1">preferred_element_type  </span><span class="s0"># unused</span>
  <span class="s3">if </span><span class="s1">isinstance(rhs</span><span class="s3">, </span><span class="s1">(np.ndarray</span><span class="s3">, </span><span class="s1">jax.Array)):</span>
    <span class="s3">if </span><span class="s1">isinstance(lhs</span><span class="s3">, </span><span class="s1">(np.ndarray</span><span class="s3">, </span><span class="s1">jax.Array)):</span>
      <span class="s3">return </span><span class="s1">lax.dot_general(lhs</span><span class="s3">, </span><span class="s1">rhs</span><span class="s3">, </span><span class="s1">dimension_numbers=dimension_numbers)</span>

    <span class="s3">if </span><span class="s1">isinstance(lhs</span><span class="s3">, </span><span class="s1">BCSR):</span>
      <span class="s1">lhs_data</span><span class="s3">, </span><span class="s1">lhs_indices</span><span class="s3">, </span><span class="s1">lhs_indptr = lhs._bufs</span>
      <span class="s3">return </span><span class="s1">_bcsr_dot_general(lhs_data</span><span class="s3">, </span><span class="s1">lhs_indices</span><span class="s3">, </span><span class="s1">lhs_indptr</span><span class="s3">, </span><span class="s1">rhs</span><span class="s3">,</span>
                               <span class="s1">dimension_numbers=dimension_numbers</span><span class="s3">,</span>
                               <span class="s1">lhs_spinfo=lhs._info)</span>

  <span class="s3">raise </span><span class="s1">NotImplementedError(</span><span class="s5">&quot;bcsr_dot_general currently implemented for BCSR &quot;</span>
                            <span class="s5">&quot;lhs and ndarray rhs.&quot;</span><span class="s1">)</span>


<span class="s3">def </span><span class="s1">_bcsr_dot_general(lhs_data: jax.Array</span><span class="s3">, </span><span class="s1">lhs_indices: jax.Array</span><span class="s3">,</span>
                      <span class="s1">lhs_indptr: jax.Array</span><span class="s3">, </span><span class="s1">rhs: Array</span><span class="s3">, </span><span class="s1">*</span><span class="s3">,</span>
                      <span class="s1">dimension_numbers: DotDimensionNumbers</span><span class="s3">,</span>
                      <span class="s1">lhs_spinfo: SparseInfo) -&gt; Array:</span>
  <span class="s1">(lhs_contract</span><span class="s3">, </span><span class="s1">rhs_contract)</span><span class="s3">, </span><span class="s1">(lhs_batch</span><span class="s3">, </span><span class="s1">rhs_batch) = dimension_numbers</span>
  <span class="s1">cdims = (api_util._ensure_index_tuple(lhs_contract)</span><span class="s3">,</span>
           <span class="s1">api_util._ensure_index_tuple(rhs_contract))</span>
  <span class="s1">bdims = (api_util._ensure_index_tuple(lhs_batch)</span><span class="s3">,</span>
           <span class="s1">api_util._ensure_index_tuple(rhs_batch))</span>
  <span class="s3">return </span><span class="s1">bcsr_dot_general_p.bind(jnp.asarray(lhs_data)</span><span class="s3">,</span>
                                 <span class="s1">jnp.asarray(lhs_indices)</span><span class="s3">,</span>
                                 <span class="s1">jnp.asarray(lhs_indptr)</span><span class="s3">, </span><span class="s1">jnp.asarray(rhs)</span><span class="s3">,</span>
                                 <span class="s1">dimension_numbers=(cdims</span><span class="s3">, </span><span class="s1">bdims)</span><span class="s3">,</span>
                                 <span class="s1">lhs_spinfo=lhs_spinfo)</span>


<span class="s3">def </span><span class="s1">_bcsr_dot_general_impl(lhs_data</span><span class="s3">, </span><span class="s1">lhs_indices</span><span class="s3">, </span><span class="s1">lhs_indptr</span><span class="s3">, </span><span class="s1">rhs</span><span class="s3">, </span><span class="s1">*</span><span class="s3">,</span>
                           <span class="s1">dimension_numbers</span><span class="s3">, </span><span class="s1">lhs_spinfo):</span>
  <span class="s1">lhs_data = jnp.asarray(lhs_data)</span>
  <span class="s1">lhs_bcsr_indices = jnp.asarray(lhs_indices)</span>
  <span class="s1">lhs_bcsr_indptr = jnp.asarray(lhs_indptr)</span>
  <span class="s1">rhs = jnp.asarray(rhs)</span>
  <span class="s1">lhs_bcoo_indices = _bcsr_to_bcoo(lhs_bcsr_indices</span><span class="s3">, </span><span class="s1">lhs_bcsr_indptr</span><span class="s3">,</span>
                                   <span class="s1">shape=lhs_spinfo.shape)</span>
  <span class="s3">return </span><span class="s1">bcoo._bcoo_dot_general_impl(lhs_data</span><span class="s3">, </span><span class="s1">lhs_bcoo_indices</span><span class="s3">, </span><span class="s1">rhs</span><span class="s3">,</span>
                                     <span class="s1">dimension_numbers=dimension_numbers</span><span class="s3">,</span>
                                     <span class="s1">lhs_spinfo=lhs_spinfo)</span>


<span class="s1">@bcsr_dot_general_p.def_abstract_eval</span>
<span class="s3">def </span><span class="s1">_bcsr_dot_general_abstract_eval(lhs_data</span><span class="s3">, </span><span class="s1">lhs_indices</span><span class="s3">, </span><span class="s1">lhs_indptr</span><span class="s3">, </span><span class="s1">rhs</span><span class="s3">, </span><span class="s1">*</span><span class="s3">,</span>
                                    <span class="s1">dimension_numbers</span><span class="s3">, </span><span class="s1">lhs_spinfo):</span>
  <span class="s3">if </span><span class="s1">lhs_data.dtype != rhs.dtype:</span>
    <span class="s3">raise </span><span class="s1">ValueError(</span><span class="s5">&quot;bcsr_dot_general requires arguments to have matching &quot;</span>
                     <span class="s5">f&quot;dtypes; got lhs.dtype=</span><span class="s3">{</span><span class="s1">lhs_data.dtype</span><span class="s3">}</span><span class="s5">, &quot;</span>
                     <span class="s5">f&quot;rhs.dtype=</span><span class="s3">{</span><span class="s1">rhs.dtype</span><span class="s3">}</span><span class="s5">&quot;</span><span class="s1">)</span>

  <span class="s1">(lhs_contracting</span><span class="s3">, </span><span class="s1">_)</span><span class="s3">, </span><span class="s1">(lhs_batch</span><span class="s3">, </span><span class="s1">_) = dimension_numbers</span>
  <span class="s1">props = _validate_bcsr_indices(lhs_indices</span><span class="s3">, </span><span class="s1">lhs_indptr</span><span class="s3">, </span><span class="s1">lhs_spinfo.shape)</span>
  <span class="s1">out_shape = _dot_general_validated_shape(lhs_spinfo.shape</span><span class="s3">, </span><span class="s1">rhs.shape</span><span class="s3">,</span>
                                           <span class="s1">dimension_numbers)</span>

  <span class="s3">if </span><span class="s1">lhs_batch </span><span class="s3">and </span><span class="s1">max(lhs_batch) &gt;= props.n_batch:</span>
    <span class="s3">raise </span><span class="s1">NotImplementedError(</span>
      <span class="s5">&quot;bcsr_dot_general batch dimensions must be among the batch dimensions in the sparse representtaion.</span><span class="s3">\n</span><span class="s5">&quot;</span>
      <span class="s5">f&quot;got </span><span class="s3">{</span><span class="s1">lhs_batch=</span><span class="s3">}</span><span class="s5">, </span><span class="s3">{</span><span class="s1">props.n_batch=</span><span class="s3">}</span><span class="s5">&quot;</span><span class="s1">)</span>

  <span class="s0"># TODO: support contraction of dense dimensions?</span>
  <span class="s3">if </span><span class="s1">any(d &gt;= props.n_batch + </span><span class="s4">2 </span><span class="s3">for </span><span class="s1">d </span><span class="s3">in </span><span class="s1">lhs_contracting):</span>
    <span class="s3">raise </span><span class="s1">NotImplementedError(</span><span class="s5">&quot;bcsr_dot_general: contracting over dense dimensions.&quot;</span><span class="s1">)</span>

  <span class="s3">return </span><span class="s1">core.ShapedArray(out_shape</span><span class="s3">, </span><span class="s1">lhs_data.dtype)</span>


<span class="s3">def </span><span class="s1">_bcsr_dot_general_jvp_lhs(lhs_data_dot</span><span class="s3">, </span><span class="s1">lhs_data</span><span class="s3">, </span><span class="s1">lhs_indices</span><span class="s3">, </span><span class="s1">lhs_indptr</span><span class="s3">, </span><span class="s1">rhs</span><span class="s3">, </span><span class="s1">*</span><span class="s3">,</span>
                              <span class="s1">dimension_numbers</span><span class="s3">, </span><span class="s1">lhs_spinfo):</span>
  <span class="s3">del </span><span class="s1">lhs_data</span>
  <span class="s3">return </span><span class="s1">_bcsr_dot_general(lhs_data_dot</span><span class="s3">, </span><span class="s1">lhs_indices</span><span class="s3">, </span><span class="s1">lhs_indptr</span><span class="s3">, </span><span class="s1">rhs</span><span class="s3">,</span>
                           <span class="s1">dimension_numbers=dimension_numbers</span><span class="s3">,</span>
                           <span class="s1">lhs_spinfo=lhs_spinfo)</span>


<span class="s3">def </span><span class="s1">_bcsr_dot_general_jvp_rhs(rhs_dot</span><span class="s3">, </span><span class="s1">lhs_data</span><span class="s3">, </span><span class="s1">lhs_indices</span><span class="s3">, </span><span class="s1">lhs_indptr</span><span class="s3">, </span><span class="s1">rhs</span><span class="s3">, </span><span class="s1">*</span><span class="s3">,</span>
                              <span class="s1">dimension_numbers</span><span class="s3">, </span><span class="s1">lhs_spinfo):</span>
  <span class="s3">del </span><span class="s1">rhs</span>
  <span class="s3">return </span><span class="s1">_bcsr_dot_general(lhs_data</span><span class="s3">, </span><span class="s1">lhs_indices</span><span class="s3">, </span><span class="s1">lhs_indptr</span><span class="s3">, </span><span class="s1">rhs_dot</span><span class="s3">,</span>
                           <span class="s1">dimension_numbers=dimension_numbers</span><span class="s3">,</span>
                           <span class="s1">lhs_spinfo=lhs_spinfo)</span>


<span class="s3">def </span><span class="s1">_bcsr_dot_general_transpose(ct</span><span class="s3">, </span><span class="s1">lhs_data</span><span class="s3">, </span><span class="s1">lhs_indices</span><span class="s3">, </span><span class="s1">lhs_indptr</span><span class="s3">, </span><span class="s1">rhs</span><span class="s3">, </span><span class="s1">*</span><span class="s3">,</span>
                                 <span class="s1">dimension_numbers</span><span class="s3">, </span><span class="s1">lhs_spinfo):</span>
  <span class="s0"># TODO(jakevdp): implement this in terms of bcsr_dot_general</span>
  <span class="s1">lhs_bcoo_indices = _bcsr_to_bcoo(</span>
    <span class="s1">lhs_indices</span><span class="s3">, </span><span class="s1">lhs_indptr</span><span class="s3">, </span><span class="s1">shape=lhs_spinfo.shape)</span>
  <span class="s1">data_out</span><span class="s3">, </span><span class="s1">_</span><span class="s3">, </span><span class="s1">rhs_out = bcoo._bcoo_dot_general_transpose(</span>
      <span class="s1">ct</span><span class="s3">, </span><span class="s1">lhs_data</span><span class="s3">, </span><span class="s1">lhs_bcoo_indices</span><span class="s3">, </span><span class="s1">rhs</span><span class="s3">, </span><span class="s1">dimension_numbers=dimension_numbers</span><span class="s3">,</span>
      <span class="s1">lhs_spinfo=lhs_spinfo)</span>
  <span class="s3">return </span><span class="s1">data_out</span><span class="s3">, </span><span class="s1">lhs_indices</span><span class="s3">, </span><span class="s1">lhs_indptr</span><span class="s3">, </span><span class="s1">rhs_out</span>


<span class="s3">def </span><span class="s1">_bcsr_dot_general_batch_rule(batched_args</span><span class="s3">, </span><span class="s1">batch_dims</span><span class="s3">, </span><span class="s1">*</span><span class="s3">,</span>
                                 <span class="s1">dimension_numbers</span><span class="s3">, </span><span class="s1">lhs_spinfo):</span>
  <span class="s1">*lhs_args</span><span class="s3">, </span><span class="s1">rhs = batched_args</span>
  <span class="s1">*lhs_dims</span><span class="s3">, </span><span class="s1">rhs_bdim = batch_dims</span>
  <span class="s1">*new_lhs_args</span><span class="s3">, </span><span class="s1">new_lhs_spinfo = _bcsr_batch_dims_to_front(</span>
    <span class="s1">lhs_args</span><span class="s3">, </span><span class="s1">lhs_dims</span><span class="s3">, </span><span class="s1">lhs_spinfo</span><span class="s3">,</span>
    <span class="s1">batch_size=</span><span class="s3">None if </span><span class="s1">rhs_bdim </span><span class="s3">is None else </span><span class="s1">rhs.shape[rhs_bdim])</span>
  <span class="s1">new_dimension_numbers</span><span class="s3">, </span><span class="s1">result_batch_dim = _dot_general_batch_dim_nums(</span>
      <span class="s1">(len(lhs_spinfo.shape)</span><span class="s3">, </span><span class="s1">rhs.ndim)</span><span class="s3">, </span><span class="s1">(</span><span class="s4">0</span><span class="s3">, </span><span class="s1">rhs_bdim)</span><span class="s3">, </span><span class="s1">dimension_numbers)</span>
  <span class="s1">batched_out = _bcsr_dot_general(*new_lhs_args</span><span class="s3">, </span><span class="s1">rhs</span><span class="s3">, </span><span class="s1">lhs_spinfo=new_lhs_spinfo</span><span class="s3">,</span>
                                  <span class="s1">dimension_numbers=new_dimension_numbers)</span>
  <span class="s3">return </span><span class="s1">batched_out</span><span class="s3">, </span><span class="s1">result_batch_dim</span>


<span class="s1">ad.defjvp(bcsr_dot_general_p</span><span class="s3">, </span><span class="s1">_bcsr_dot_general_jvp_lhs</span><span class="s3">, None, None,</span>
          <span class="s1">_bcsr_dot_general_jvp_rhs)</span>
<span class="s1">ad.primitive_transposes[bcsr_dot_general_p] = _bcsr_dot_general_transpose</span>
<span class="s1">batching.primitive_batchers[bcsr_dot_general_p] = _bcsr_dot_general_batch_rule</span>


<span class="s3">def </span><span class="s1">_bcsr_correct_out_of_bound_indices(data</span><span class="s3">, </span><span class="s1">indices</span><span class="s3">, </span><span class="s1">indptr</span><span class="s3">, </span><span class="s1">rhs</span><span class="s3">, </span><span class="s1">*</span><span class="s3">, </span><span class="s1">shape):</span>
  <span class="s1">props = _validate_bcsr(data</span><span class="s3">, </span><span class="s1">indices</span><span class="s3">, </span><span class="s1">indptr</span><span class="s3">, </span><span class="s1">shape)</span>
  <span class="s3">if </span><span class="s1">props.n_batch:</span>
    <span class="s1">f = partial(_bcsr_correct_out_of_bound_indices</span><span class="s3">, </span><span class="s1">rhs=rhs</span><span class="s3">, </span><span class="s1">shape=shape[props.n_batch:])</span>
    <span class="s3">return </span><span class="s1">nfold_vmap(f</span><span class="s3">, </span><span class="s1">props.n_batch)(data</span><span class="s3">, </span><span class="s1">indices</span><span class="s3">, </span><span class="s1">indptr)</span>
  <span class="s1">extent = indptr[-</span><span class="s4">1</span><span class="s1">]</span>
  <span class="s1">i_data = lax.broadcasted_iota(indptr.dtype</span><span class="s3">, </span><span class="s1">data.shape</span><span class="s3">, </span><span class="s4">0</span><span class="s1">)</span>
  <span class="s1">data = jnp.where(i_data &lt; extent</span><span class="s3">, </span><span class="s1">data</span><span class="s3">, </span><span class="s4">0</span><span class="s1">)</span>
  <span class="s1">i_indices = lax.broadcasted_iota(indptr.dtype</span><span class="s3">, </span><span class="s1">indices.shape</span><span class="s3">, </span><span class="s4">0</span><span class="s1">)</span>
  <span class="s1">indices = jnp.where(i_indices &lt; extent</span><span class="s3">, </span><span class="s1">indices</span><span class="s3">, </span><span class="s4">0</span><span class="s1">)</span>
  <span class="s3">return </span><span class="s1">[data</span><span class="s3">, </span><span class="s1">indices]</span>

<span class="s1">_bcsr_correct_out_of_bound_indices_lowered = mlir.lower_fun(</span>
    <span class="s1">_bcsr_correct_out_of_bound_indices</span><span class="s3">, </span><span class="s1">multiple_results=</span><span class="s3">True</span><span class="s1">)</span>

<span class="s3">def </span><span class="s1">_bcsr_dot_general_gpu_lowering(</span>
    <span class="s1">csr_matvec_lowering</span><span class="s3">, </span><span class="s1">csr_matmat_lowering</span><span class="s3">,</span>
    <span class="s1">ctx</span><span class="s3">, </span><span class="s1">lhs_data</span><span class="s3">, </span><span class="s1">lhs_indices</span><span class="s3">, </span><span class="s1">lhs_indptr</span><span class="s3">, </span><span class="s1">rhs</span><span class="s3">, </span><span class="s1">*</span><span class="s3">, </span><span class="s1">dimension_numbers</span><span class="s3">,</span>
    <span class="s1">lhs_spinfo: SparseInfo):</span>

  <span class="s3">if not </span><span class="s1">config.jax_bcoo_cusparse_lowering:</span>
    <span class="s3">return </span><span class="s1">_bcsr_dot_general_default_lowering(</span>
      <span class="s1">ctx</span><span class="s3">, </span><span class="s1">lhs_data</span><span class="s3">, </span><span class="s1">lhs_indices</span><span class="s3">, </span><span class="s1">lhs_indptr</span><span class="s3">, </span><span class="s1">rhs</span><span class="s3">,</span>
      <span class="s1">dimension_numbers=dimension_numbers</span><span class="s3">, </span><span class="s1">lhs_spinfo=lhs_spinfo)</span>

  <span class="s1">(lhs_contract</span><span class="s3">, </span><span class="s1">rhs_contract)</span><span class="s3">, </span><span class="s1">(lhs_batch</span><span class="s3">, </span><span class="s1">rhs_batch) = dimension_numbers</span>
  <span class="s1">lhs_data_aval</span><span class="s3">, </span><span class="s1">lhs_indices_aval</span><span class="s3">, </span><span class="s1">lhs_indptr_aval</span><span class="s3">, </span><span class="s1">rhs_aval = ctx.avals_in</span>
  <span class="s1">props = _validate_bcsr(</span>
      <span class="s1">lhs_data_aval</span><span class="s3">, </span><span class="s1">lhs_indices_aval</span><span class="s3">, </span><span class="s1">lhs_indptr_aval</span><span class="s3">, </span><span class="s1">lhs_spinfo.shape)</span>

  <span class="s1">use_default_lowering = </span><span class="s3">False</span>
  <span class="s1">dtype = lhs_data_aval.dtype</span>
  <span class="s0"># TODO(vanderplas, tianjianlu): lower batched matmuls to GPU</span>
  <span class="s3">if </span><span class="s1">lhs_batch </span><span class="s3">or </span><span class="s1">rhs_batch:</span>
    <span class="s0"># batch dimensions in dot_general are not supported</span>
    <span class="s1">use_default_lowering = </span><span class="s3">True</span>
  <span class="s3">elif </span><span class="s1">len(lhs_spinfo.shape) != </span><span class="s4">2 </span><span class="s3">or </span><span class="s1">rhs_aval.ndim </span><span class="s3">not in </span><span class="s1">[</span><span class="s4">1</span><span class="s3">, </span><span class="s4">2</span><span class="s1">]:</span>
    <span class="s0"># only matmat / matvec supported</span>
    <span class="s1">use_default_lowering = </span><span class="s3">True</span>
  <span class="s3">elif </span><span class="s1">props.n_batch </span><span class="s3">or </span><span class="s1">props.n_dense:</span>
    <span class="s0"># batch and dense dimensions in BCSR not supported</span>
    <span class="s1">use_default_lowering = </span><span class="s3">True</span>
  <span class="s3">elif </span><span class="s1">list(lhs_contract) != [</span><span class="s4">1</span><span class="s1">]:</span>
    <span class="s0"># cusparse cannot contract over more than one dimension</span>
    <span class="s1">use_default_lowering = </span><span class="s3">True</span>
  <span class="s3">elif </span><span class="s1">dtype </span><span class="s3">not in </span><span class="s1">[np.float32</span><span class="s3">, </span><span class="s1">np.float64</span><span class="s3">, </span><span class="s1">np.complex64</span><span class="s3">, </span><span class="s1">np.complex128]:</span>
    <span class="s0"># This would be supported if not for the dtype.</span>
    <span class="s1">warnings.warn(</span><span class="s5">f'bcsr_dot_general cusparse/hipsparse lowering not available '</span>
                  <span class="s5">f'for </span><span class="s3">{</span><span class="s1">dtype=</span><span class="s3">}</span><span class="s5">. Falling back to default implementation.'</span><span class="s3">,</span>
                  <span class="s1">CuSparseEfficiencyWarning)</span>
    <span class="s1">use_default_lowering = </span><span class="s3">True</span>

  <span class="s3">if </span><span class="s1">use_default_lowering:</span>
    <span class="s3">return </span><span class="s1">_bcsr_dot_general_default_lowering(</span>
      <span class="s1">ctx</span><span class="s3">, </span><span class="s1">lhs_data</span><span class="s3">, </span><span class="s1">lhs_indices</span><span class="s3">, </span><span class="s1">lhs_indptr</span><span class="s3">, </span><span class="s1">rhs</span><span class="s3">,</span>
      <span class="s1">dimension_numbers=dimension_numbers</span><span class="s3">, </span><span class="s1">lhs_spinfo=lhs_spinfo)</span>

  <span class="s0"># Account for a bug in cusparse: it references indices and data beyond</span>
  <span class="s0"># the extent of indptr.</span>
  <span class="s1">(lhs_data</span><span class="s3">,</span><span class="s1">)</span><span class="s3">, </span><span class="s1">(lhs_indices</span><span class="s3">,</span><span class="s1">) = _bcsr_correct_out_of_bound_indices_lowered(</span>
      <span class="s1">ctx</span><span class="s3">, </span><span class="s1">lhs_data</span><span class="s3">, </span><span class="s1">lhs_indices</span><span class="s3">, </span><span class="s1">lhs_indptr</span><span class="s3">, </span><span class="s1">rhs</span><span class="s3">, </span><span class="s1">shape=lhs_spinfo.shape)</span>

  <span class="s3">if </span><span class="s1">rhs_aval.ndim == </span><span class="s4">1</span><span class="s1">:</span>
    <span class="s1">dot_general_fn = csr_matvec_lowering</span>
    <span class="s1">x_dtype = </span><span class="s5">'x_dtype'</span>
  <span class="s3">elif </span><span class="s1">rhs_aval.ndim == </span><span class="s4">2</span><span class="s1">:</span>
    <span class="s1">dot_general_fn = csr_matmat_lowering</span>
    <span class="s1">x_dtype = </span><span class="s5">'B_dtype'</span>
    <span class="s3">if </span><span class="s1">rhs_contract[</span><span class="s4">0</span><span class="s1">] == </span><span class="s4">1</span><span class="s1">:</span>
      <span class="s1">rhs = hlo.TransposeOp(</span>
          <span class="s1">rhs</span><span class="s3">, </span><span class="s1">permutation=mlir.dense_int_elements([</span><span class="s4">1</span><span class="s3">, </span><span class="s4">0</span><span class="s1">])).result</span>
  <span class="s3">else</span><span class="s1">:</span>
    <span class="s3">raise </span><span class="s1">ValueError(</span><span class="s5">f&quot;rhs has to be 1d or 2d; get </span><span class="s3">{</span><span class="s1">rhs_aval.ndim</span><span class="s3">}</span><span class="s5">d.&quot;</span><span class="s1">)</span>

  <span class="s3">return </span><span class="s1">[dot_general_fn(lhs_data</span><span class="s3">, </span><span class="s1">lhs_indices</span><span class="s3">, </span><span class="s1">lhs_indptr</span><span class="s3">, </span><span class="s1">rhs</span><span class="s3">,</span>
                         <span class="s1">shape=lhs_spinfo.shape</span><span class="s3">, </span><span class="s1">transpose=</span><span class="s3">False,</span>
                         <span class="s1">data_dtype=lhs_data_aval.dtype</span><span class="s3">,</span>
                         <span class="s1">index_dtype=lhs_indices_aval.dtype</span><span class="s3">,</span>
                         <span class="s1">**{x_dtype: rhs_aval.dtype})]</span>

<span class="s1">_bcsr_dot_general_default_lowering = mlir.lower_fun(</span>
    <span class="s1">_bcsr_dot_general_impl</span><span class="s3">, </span><span class="s1">multiple_results=</span><span class="s3">False</span><span class="s1">)</span>
<span class="s1">mlir.register_lowering(</span>
    <span class="s1">bcsr_dot_general_p</span><span class="s3">, </span><span class="s1">_bcsr_dot_general_default_lowering)</span>
<span class="s1">dispatch.simple_impl(bcsr_dot_general_p)</span>

<span class="s3">if </span><span class="s1">gpu_sparse.cuda_is_supported:</span>
  <span class="s1">mlir.register_lowering(bcsr_dot_general_p</span><span class="s3">,</span>
                          <span class="s1">partial(_bcsr_dot_general_gpu_lowering</span><span class="s3">,</span>
                                  <span class="s1">gpu_sparse.cuda_csr_matvec</span><span class="s3">,</span>
                                  <span class="s1">gpu_sparse.cuda_csr_matmat)</span><span class="s3">,</span>
                          <span class="s1">platform=</span><span class="s5">'cuda'</span><span class="s1">)</span>
<span class="s3">if </span><span class="s1">gpu_sparse.rocm_is_supported:</span>
  <span class="s1">mlir.register_lowering(bcsr_dot_general_p</span><span class="s3">,</span>
                          <span class="s1">partial(_bcsr_dot_general_gpu_lowering</span><span class="s3">,</span>
                                  <span class="s1">gpu_sparse.rocm_csr_matvec</span><span class="s3">,</span>
                                  <span class="s1">gpu_sparse.rocm_csr_matmat)</span><span class="s3">,</span>
                          <span class="s1">platform=</span><span class="s5">'rocm'</span><span class="s1">)</span>


<span class="s0">#----------------------------------------------------------------------</span>
<span class="s0"># BCOO functions that maybe should be primitives?</span>

<span class="s3">def </span><span class="s1">bcsr_broadcast_in_dim(mat: BCSR</span><span class="s3">, </span><span class="s1">*</span><span class="s3">, </span><span class="s1">shape: Shape</span><span class="s3">, </span><span class="s1">broadcast_dimensions: Sequence[int]) -&gt; BCSR:</span>
  <span class="s1">result_bcoo = bcoo.bcoo_broadcast_in_dim(</span>
    <span class="s1">mat.to_bcoo()</span><span class="s3">, </span><span class="s1">shape=shape</span><span class="s3">, </span><span class="s1">broadcast_dimensions=broadcast_dimensions)</span>
  <span class="s3">return </span><span class="s1">BCSR.from_bcoo(result_bcoo)</span>

<span class="s3">def </span><span class="s1">bcsr_concatenate(operands: Sequence[BCSR]</span><span class="s3">, </span><span class="s1">*</span><span class="s3">, </span><span class="s1">dimension: int) -&gt; BCSR:</span>
  <span class="s2">&quot;&quot;&quot;Sparse implementation of :func:`jax.lax.concatenate` 
 
  Args: 
    operands : Sequence of BCSR arrays to concatenate. The arrays must have equal 
      shapes, except in the `dimension` axis. Additionally, the arrays must have 
      have equivalent batch, sparse, and dense dimensions. 
    dimension : Positive integer specifying the dimension along which to concatenate 
      the arrays. The dimension must be among batch or sparse dimensions of the input; 
      concatenation along dense dimensions is not supported. 
 
  Returns: 
    A BCSR array containing the concatenation of the inputs. 
  &quot;&quot;&quot;</span>
  <span class="s3">return </span><span class="s1">BCSR.from_bcoo(</span>
    <span class="s1">bcoo.bcoo_concatenate([mat.to_bcoo() </span><span class="s3">for </span><span class="s1">mat </span><span class="s3">in </span><span class="s1">operands]</span><span class="s3">, </span><span class="s1">dimension=dimension))</span>

<span class="s1">@tree_util.register_pytree_node_class</span>
<span class="s3">class </span><span class="s1">BCSR(JAXSparse):</span>
  <span class="s2">&quot;&quot;&quot;Experimental batched CSR matrix implemented in JAX.&quot;&quot;&quot;</span>

  <span class="s1">data: jax.Array</span>
  <span class="s1">indices: jax.Array</span>
  <span class="s1">indptr: jax.Array</span>
  <span class="s1">shape: Shape</span>
  <span class="s1">nse = property(</span><span class="s3">lambda </span><span class="s1">self: self.indices.shape[-</span><span class="s4">1</span><span class="s1">])</span>
  <span class="s1">dtype = property(</span><span class="s3">lambda </span><span class="s1">self: self.data.dtype)</span>
  <span class="s1">n_batch = property(</span><span class="s3">lambda </span><span class="s1">self: self.indices.ndim - </span><span class="s4">1</span><span class="s1">)</span>
  <span class="s1">n_sparse = property(</span><span class="s3">lambda </span><span class="s1">_: </span><span class="s4">2</span><span class="s1">)</span>
  <span class="s1">n_dense = property(</span><span class="s3">lambda </span><span class="s1">self: self.data.ndim - self.indices.ndim)</span>
  <span class="s1">indices_sorted: bool</span>
  <span class="s1">unique_indices: bool</span>
  <span class="s1">_bufs = property(</span><span class="s3">lambda </span><span class="s1">self: (self.data</span><span class="s3">, </span><span class="s1">self.indices</span><span class="s3">, </span><span class="s1">self.indptr))</span>
  <span class="s1">_info = property(</span><span class="s3">lambda </span><span class="s1">self: SparseInfo(self.shape</span><span class="s3">, </span><span class="s1">self.indices_sorted</span><span class="s3">,</span>
                                           <span class="s1">self.unique_indices))</span>

  <span class="s1">@property</span>
  <span class="s3">def </span><span class="s1">_sparse_shape(self):</span>
    <span class="s3">return </span><span class="s1">tuple(self.shape[self.n_batch:self.n_batch + </span><span class="s4">2</span><span class="s1">])</span>

  <span class="s3">def </span><span class="s1">__init__(self</span><span class="s3">, </span><span class="s1">args: Tuple[Array</span><span class="s3">, </span><span class="s1">Array</span><span class="s3">, </span><span class="s1">Array]</span><span class="s3">, </span><span class="s1">*</span><span class="s3">, </span><span class="s1">shape: Sequence[int]</span><span class="s3">,</span>
               <span class="s1">indices_sorted: bool = </span><span class="s3">False, </span><span class="s1">unique_indices: bool = </span><span class="s3">False</span><span class="s1">):</span>
    <span class="s1">self.data</span><span class="s3">, </span><span class="s1">self.indices</span><span class="s3">, </span><span class="s1">self.indptr = map(jnp.asarray</span><span class="s3">, </span><span class="s1">args)</span>
    <span class="s1">self.indices_sorted = indices_sorted</span>
    <span class="s1">self.unique_indices = unique_indices</span>
    <span class="s1">super().__init__(args</span><span class="s3">, </span><span class="s1">shape=shape)</span>
    <span class="s1">_validate_bcsr(self.data</span><span class="s3">, </span><span class="s1">self.indices</span><span class="s3">, </span><span class="s1">self.indptr</span><span class="s3">, </span><span class="s1">self.shape)</span>

  <span class="s3">def </span><span class="s1">__repr__(self):</span>
    <span class="s1">name = self.__class__.__name__</span>
    <span class="s3">try</span><span class="s1">:</span>
      <span class="s1">nse = self.nse</span>
      <span class="s1">n_batch = self.n_batch</span>
      <span class="s1">n_dense = self.n_dense</span>
      <span class="s1">dtype = self.dtype</span>
      <span class="s1">shape = list(self.shape)</span>
    <span class="s3">except </span><span class="s1">Exception:  </span><span class="s0"># pylint: disable=broad-except</span>
      <span class="s1">repr_ = </span><span class="s5">f&quot;</span><span class="s3">{</span><span class="s1">name</span><span class="s3">}</span><span class="s5">(&lt;invalid&gt;)&quot;</span>
    <span class="s3">else</span><span class="s1">:</span>
      <span class="s1">extra = </span><span class="s5">f&quot;, </span><span class="s3">{</span><span class="s1">nse=</span><span class="s3">}</span><span class="s5">&quot;</span>
      <span class="s3">if </span><span class="s1">n_batch: extra += </span><span class="s5">f&quot;, </span><span class="s3">{</span><span class="s1">n_batch=</span><span class="s3">}</span><span class="s5">&quot;</span>
      <span class="s3">if </span><span class="s1">n_dense: extra += </span><span class="s5">f&quot;, </span><span class="s3">{</span><span class="s1">n_dense=</span><span class="s3">}</span><span class="s5">&quot;</span>
      <span class="s1">repr_ = </span><span class="s5">f&quot;</span><span class="s3">{</span><span class="s1">name</span><span class="s3">}</span><span class="s5">(</span><span class="s3">{</span><span class="s1">dtype</span><span class="s3">}{</span><span class="s1">shape</span><span class="s3">}{</span><span class="s1">extra</span><span class="s3">}</span><span class="s5">)&quot;</span>
    <span class="s3">if </span><span class="s1">isinstance(self.data</span><span class="s3">, </span><span class="s1">core.Tracer):</span>
      <span class="s1">repr_ = </span><span class="s5">f&quot;</span><span class="s3">{</span><span class="s1">type(self.data).__name__</span><span class="s3">}</span><span class="s5">[</span><span class="s3">{</span><span class="s1">repr_</span><span class="s3">}</span><span class="s5">]&quot;</span>
    <span class="s3">return </span><span class="s1">repr_</span>

  <span class="s3">def </span><span class="s1">transpose(self</span><span class="s3">, </span><span class="s1">*args</span><span class="s3">, </span><span class="s1">**kwargs):</span>
    <span class="s3">raise </span><span class="s1">NotImplementedError(</span><span class="s5">&quot;Tranpose is not implemented.&quot;</span><span class="s1">)</span>

  <span class="s3">def </span><span class="s1">tree_flatten(self):</span>
    <span class="s3">return </span><span class="s1">(self.data</span><span class="s3">, </span><span class="s1">self.indices</span><span class="s3">, </span><span class="s1">self.indptr)</span><span class="s3">, </span><span class="s1">self._info._asdict()</span>

  <span class="s1">@classmethod</span>
  <span class="s3">def </span><span class="s1">tree_unflatten(cls</span><span class="s3">, </span><span class="s1">aux_data</span><span class="s3">, </span><span class="s1">children):</span>
    <span class="s1">obj = object.__new__(cls)</span>
    <span class="s1">obj.data</span><span class="s3">, </span><span class="s1">obj.indices</span><span class="s3">, </span><span class="s1">obj.indptr = children</span>
    <span class="s3">if </span><span class="s1">aux_data.keys() != {</span><span class="s5">'shape'</span><span class="s3">, </span><span class="s5">'indices_sorted'</span><span class="s3">, </span><span class="s5">'unique_indices'</span><span class="s1">}:</span>
      <span class="s3">raise </span><span class="s1">ValueError(</span><span class="s5">f&quot;BCSR.tree_unflatten: invalid </span><span class="s3">{</span><span class="s1">aux_data=</span><span class="s3">}</span><span class="s5">&quot;</span><span class="s1">)</span>
    <span class="s1">obj.__dict__.update(**aux_data)</span>
    <span class="s3">return </span><span class="s1">obj</span>

  <span class="s1">@classmethod</span>
  <span class="s3">def </span><span class="s1">_empty(cls</span><span class="s3">, </span><span class="s1">shape</span><span class="s3">, </span><span class="s1">*</span><span class="s3">, </span><span class="s1">dtype=</span><span class="s3">None, </span><span class="s1">index_dtype=</span><span class="s5">'int32'</span><span class="s3">, </span><span class="s1">n_dense=</span><span class="s4">0</span><span class="s3">,</span>
             <span class="s1">n_batch=</span><span class="s4">0</span><span class="s3">, </span><span class="s1">nse=</span><span class="s4">0</span><span class="s1">):</span>
    <span class="s2">&quot;&quot;&quot;Create an empty BCSR instance. Public method is sparse.empty().&quot;&quot;&quot;</span>
    <span class="s1">shape = tuple(shape)</span>
    <span class="s3">if </span><span class="s1">n_dense &lt; </span><span class="s4">0 </span><span class="s3">or </span><span class="s1">n_batch &lt; </span><span class="s4">0 </span><span class="s3">or </span><span class="s1">nse &lt; </span><span class="s4">0</span><span class="s1">:</span>
      <span class="s3">raise </span><span class="s1">ValueError(</span><span class="s5">f&quot;Invalid inputs: </span><span class="s3">{</span><span class="s1">shape=</span><span class="s3">}</span><span class="s5">, </span><span class="s3">{</span><span class="s1">n_dense=</span><span class="s3">}</span><span class="s5">, </span><span class="s3">{</span><span class="s1">n_batch=</span><span class="s3">}</span><span class="s5">, </span><span class="s3">{</span><span class="s1">nse=</span><span class="s3">}</span><span class="s5">&quot;</span><span class="s1">)</span>
    <span class="s1">n_sparse = len(shape) - n_dense - n_batch</span>
    <span class="s3">if </span><span class="s1">n_sparse != </span><span class="s4">2</span><span class="s1">:</span>
      <span class="s3">raise </span><span class="s1">ValueError(</span><span class="s5">&quot;BCSR sparse.empty: must have 2 sparse dimensions.&quot;</span><span class="s1">)</span>
    <span class="s1">batch_shape</span><span class="s3">, </span><span class="s1">sparse_shape</span><span class="s3">, </span><span class="s1">dense_shape = split_list(shape</span><span class="s3">,</span>
                                                        <span class="s1">[n_batch</span><span class="s3">, </span><span class="s1">n_sparse])</span>
    <span class="s1">data = jnp.zeros((*batch_shape</span><span class="s3">, </span><span class="s1">nse</span><span class="s3">, </span><span class="s1">*dense_shape)</span><span class="s3">, </span><span class="s1">dtype)</span>
    <span class="s1">indices = jnp.full((*batch_shape</span><span class="s3">, </span><span class="s1">nse)</span><span class="s3">, </span><span class="s1">jnp.array(sparse_shape[</span><span class="s4">1</span><span class="s1">])</span><span class="s3">,</span>
                       <span class="s1">index_dtype)</span>
    <span class="s1">indptr = jnp.zeros((*batch_shape</span><span class="s3">, </span><span class="s1">sparse_shape[</span><span class="s4">0</span><span class="s1">] + </span><span class="s4">1</span><span class="s1">)</span><span class="s3">, </span><span class="s1">index_dtype)</span>
    <span class="s3">return </span><span class="s1">cls((data</span><span class="s3">, </span><span class="s1">indices</span><span class="s3">, </span><span class="s1">indptr)</span><span class="s3">, </span><span class="s1">shape=shape)</span>

  <span class="s3">def </span><span class="s1">sum_duplicates(self</span><span class="s3">, </span><span class="s1">nse: Optional[int] = </span><span class="s3">None, </span><span class="s1">remove_zeros: bool = </span><span class="s3">True</span><span class="s1">) -&gt; BCSR:</span>
    <span class="s2">&quot;&quot;&quot;Return a copy of the array with duplicate indices summed. 
 
    Additionally, this operation will result in explicit zero entries removed, and 
    indices being sorted in lexicographic order. 
 
    Because the size of the resulting representation depends on the values in the 
    arrays, this operation is not compatible with JIT or other transforms. To use 
    ``sum_duplicates`` in such cases, you may pass a value to `nse` to specify the 
    desired size of the output representation. 
 
    Args: 
      nse : integer (optional), if specified, gives the number of specified elements in 
        the output sparse representation; if it is larger than the number required, data 
        will be padded with zeros and indices will be padded with out-of-bounds values. 
        If it is smaller than the number required, data will be silently discarded. 
      remove_zeros : bool (default=True). If True, remove explicit zeros from the data 
        as part of summing duplicates. If False, then explicit zeros at unique indices 
        will remain among the specified elements. Note: remove_zeros=True is incompatible 
        with autodiff. 
    &quot;&quot;&quot;</span>
    <span class="s3">if </span><span class="s1">remove_zeros:</span>
      <span class="s3">return </span><span class="s1">bcsr_eliminate_zeros(self</span><span class="s3">, </span><span class="s1">nse=nse)</span>
    <span class="s3">else</span><span class="s1">:</span>
      <span class="s3">return </span><span class="s1">bcsr_sum_duplicates(self</span><span class="s3">, </span><span class="s1">nse=nse)</span>

  <span class="s1">@classmethod</span>
  <span class="s3">def </span><span class="s1">fromdense(cls</span><span class="s3">, </span><span class="s1">mat</span><span class="s3">, </span><span class="s1">*</span><span class="s3">, </span><span class="s1">nse=</span><span class="s3">None, </span><span class="s1">index_dtype=np.int32</span><span class="s3">, </span><span class="s1">n_dense=</span><span class="s4">0</span><span class="s3">,</span>
                <span class="s1">n_batch=</span><span class="s4">0</span><span class="s1">):</span>
    <span class="s2">&quot;&quot;&quot;Create a BCSR array from a (dense) :class:`DeviceArray`.&quot;&quot;&quot;</span>
    <span class="s3">return </span><span class="s1">bcsr_fromdense(mat</span><span class="s3">, </span><span class="s1">nse=nse</span><span class="s3">, </span><span class="s1">index_dtype=index_dtype</span><span class="s3">,</span>
                          <span class="s1">n_dense=n_dense</span><span class="s3">, </span><span class="s1">n_batch=n_batch)</span>

  <span class="s3">def </span><span class="s1">todense(self):</span>
    <span class="s2">&quot;&quot;&quot;Create a dense version of the array.&quot;&quot;&quot;</span>
    <span class="s3">return </span><span class="s1">bcsr_todense(self)</span>

  <span class="s3">def </span><span class="s1">to_bcoo(self) -&gt; bcoo.BCOO:</span>
    <span class="s1">coo_indices = _bcsr_to_bcoo(self.indices</span><span class="s3">, </span><span class="s1">self.indptr</span><span class="s3">, </span><span class="s1">shape=self.shape)</span>
    <span class="s3">return </span><span class="s1">bcoo.BCOO((self.data</span><span class="s3">, </span><span class="s1">coo_indices)</span><span class="s3">, </span><span class="s1">shape=self.shape)</span>

  <span class="s1">@classmethod</span>
  <span class="s3">def </span><span class="s1">from_bcoo(cls</span><span class="s3">, </span><span class="s1">arr: bcoo.BCOO) -&gt; BCSR:</span>
    <span class="s3">if </span><span class="s1">arr.n_sparse != </span><span class="s4">2</span><span class="s1">:</span>
      <span class="s3">raise </span><span class="s1">NotImplementedError(</span><span class="s5">f&quot;BSCR.from_bcoo requires n_sparse=2; got </span><span class="s3">{</span><span class="s1">arr.n_sparse=</span><span class="s3">}</span><span class="s5">&quot;</span><span class="s1">)</span>
    <span class="s3">if not </span><span class="s1">arr.indices_sorted:</span>
      <span class="s1">arr = arr.sort_indices()</span>
    <span class="s1">indices</span><span class="s3">, </span><span class="s1">indptr = _bcoo_to_bcsr(arr.indices</span><span class="s3">, </span><span class="s1">shape=arr.shape)</span>
    <span class="s3">return </span><span class="s1">cls((arr.data</span><span class="s3">, </span><span class="s1">indices</span><span class="s3">, </span><span class="s1">indptr)</span><span class="s3">, </span><span class="s1">shape=arr.shape)</span>

  <span class="s1">@classmethod</span>
  <span class="s3">def </span><span class="s1">from_scipy_sparse(cls</span><span class="s3">, </span><span class="s1">mat</span><span class="s3">, </span><span class="s1">*</span><span class="s3">, </span><span class="s1">index_dtype=</span><span class="s3">None, </span><span class="s1">n_dense=</span><span class="s4">0</span><span class="s3">, </span><span class="s1">n_batch=</span><span class="s4">0</span><span class="s1">):</span>
    <span class="s2">&quot;&quot;&quot;Create a BCSR array from a :mod:`scipy.sparse` array.&quot;&quot;&quot;</span>
    <span class="s3">if </span><span class="s1">n_dense != </span><span class="s4">0 </span><span class="s3">or </span><span class="s1">n_batch != </span><span class="s4">0</span><span class="s1">:</span>
      <span class="s3">raise </span><span class="s1">NotImplementedError(</span><span class="s5">&quot;BCSR from_scipy_sparse with nonzero n_dense/n_batch.&quot;</span><span class="s1">)</span>

    <span class="s3">if </span><span class="s1">mat.ndim != </span><span class="s4">2</span><span class="s1">:</span>
      <span class="s3">raise </span><span class="s1">ValueError(</span><span class="s5">f&quot;BCSR from_scipy_sparse requires 2D array; </span><span class="s3">{</span><span class="s1">mat.ndim</span><span class="s3">}</span><span class="s5">D is given.&quot;</span><span class="s1">)</span>

    <span class="s1">mat = mat.tocsr()</span>
    <span class="s1">data = jnp.asarray(mat.data)</span>
    <span class="s1">indices = jnp.asarray(mat.indices).astype(index_dtype </span><span class="s3">or </span><span class="s1">jnp.int32)</span>
    <span class="s1">indptr = jnp.asarray(mat.indptr).astype(index_dtype </span><span class="s3">or </span><span class="s1">jnp.int32)</span>
    <span class="s3">return </span><span class="s1">cls((data</span><span class="s3">, </span><span class="s1">indices</span><span class="s3">, </span><span class="s1">indptr)</span><span class="s3">, </span><span class="s1">shape=mat.shape)</span>

<span class="s0">#--------------------------------------------------------------------</span>
<span class="s0"># vmappable handlers</span>
<span class="s3">def </span><span class="s1">_bcsr_to_elt(cont</span><span class="s3">, </span><span class="s1">_</span><span class="s3">, </span><span class="s1">val</span><span class="s3">, </span><span class="s1">axis):</span>
  <span class="s3">if </span><span class="s1">axis </span><span class="s3">is None</span><span class="s1">:</span>
    <span class="s3">return </span><span class="s1">val</span>
  <span class="s3">if </span><span class="s1">axis &gt;= val.n_batch:</span>
    <span class="s3">raise </span><span class="s1">ValueError(</span><span class="s5">f&quot;Cannot map in_axis=</span><span class="s3">{</span><span class="s1">axis</span><span class="s3">} </span><span class="s5">for BCSR array with n_batch=&quot;</span>
                     <span class="s5">f&quot;</span><span class="s3">{</span><span class="s1">val.n_batch</span><span class="s3">}</span><span class="s5">. in_axes for batched BCSR operations must &quot;</span>
                     <span class="s5">&quot;correspond to a batched dimension.&quot;</span><span class="s1">)</span>
  <span class="s3">return </span><span class="s1">BCSR((cont(val.data</span><span class="s3">, </span><span class="s1">axis)</span><span class="s3">,</span>
               <span class="s1">cont(val.indices</span><span class="s3">, </span><span class="s1">axis)</span><span class="s3">,</span>
               <span class="s1">cont(val.indptr</span><span class="s3">, </span><span class="s1">axis))</span><span class="s3">,</span>
              <span class="s1">shape=val.shape[:axis] + val.shape[axis + </span><span class="s4">1</span><span class="s1">:])</span>


<span class="s3">def </span><span class="s1">_bcsr_from_elt(cont</span><span class="s3">, </span><span class="s1">axis_size</span><span class="s3">, </span><span class="s1">elt</span><span class="s3">, </span><span class="s1">axis):</span>
  <span class="s3">if </span><span class="s1">axis </span><span class="s3">is None</span><span class="s1">:</span>
    <span class="s3">return </span><span class="s1">elt</span>
  <span class="s3">if </span><span class="s1">axis &gt; elt.n_batch:</span>
    <span class="s3">raise </span><span class="s1">ValueError(</span><span class="s5">f&quot;BCSR: cannot add out_axis=</span><span class="s3">{</span><span class="s1">axis</span><span class="s3">} </span><span class="s5">for BCSR array with &quot;</span>
                     <span class="s5">f&quot;n_batch=</span><span class="s3">{</span><span class="s1">elt.n_batch</span><span class="s3">}</span><span class="s5">. BCSR batch axes must be a &quot;</span>
                     <span class="s5">&quot;contiguous block of leading dimensions.&quot;</span><span class="s1">)</span>
  <span class="s3">return </span><span class="s1">BCSR((cont(axis_size</span><span class="s3">, </span><span class="s1">elt.data</span><span class="s3">, </span><span class="s1">axis)</span><span class="s3">,</span>
               <span class="s1">cont(axis_size</span><span class="s3">, </span><span class="s1">elt.indices</span><span class="s3">, </span><span class="s1">axis)</span><span class="s3">,</span>
               <span class="s1">cont(axis_size</span><span class="s3">, </span><span class="s1">elt.indptr</span><span class="s3">, </span><span class="s1">axis))</span><span class="s3">,</span>
              <span class="s1">shape=elt.shape[:axis] + (axis_size</span><span class="s3">,</span><span class="s1">) + elt.shape[axis:])</span>

<span class="s1">batching.register_vmappable(BCSR</span><span class="s3">, </span><span class="s1">int</span><span class="s3">, </span><span class="s1">int</span><span class="s3">, </span><span class="s1">_bcsr_to_elt</span><span class="s3">, </span><span class="s1">_bcsr_from_elt</span><span class="s3">, None</span><span class="s1">)</span>
</pre>
</body>
</html>