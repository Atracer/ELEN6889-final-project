<html>
<head>
<title>ann.py</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<style type="text/css">
.s0 { color: #808080;}
.s1 { color: #a9b7c6;}
.s2 { color: #629755; font-style: italic;}
.s3 { color: #cc7832;}
.s4 { color: #6897bb;}
.s5 { color: #6a8759;}
</style>
</head>
<body bgcolor="#2b2b2b">
<table CELLSPACING=0 CELLPADDING=5 COLS=1 WIDTH="100%" BGCOLOR="#606060" >
<tr><td><center>
<font face="Arial, Helvetica" color="#000000">
ann.py</font>
</center></td></tr></table>
<pre><span class="s0"># Copyright 2021 The JAX Authors.</span>
<span class="s0">#</span>
<span class="s0"># Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);</span>
<span class="s0"># you may not use this file except in compliance with the License.</span>
<span class="s0"># You may obtain a copy of the License at</span>
<span class="s0">#</span>
<span class="s0">#     https://www.apache.org/licenses/LICENSE-2.0</span>
<span class="s0">#</span>
<span class="s0"># Unless required by applicable law or agreed to in writing, software</span>
<span class="s0"># distributed under the License is distributed on an &quot;AS IS&quot; BASIS,</span>
<span class="s0"># WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span>
<span class="s0"># See the License for the specific language governing permissions and</span>
<span class="s0"># limitations under the License.</span>

<span class="s2">&quot;&quot;&quot;ANN (Approximate Nearest Neighbor) computes top-k with a configurable recall rate. 
 
This package only optimizes the TPU backend. For other device types it fallbacks 
to sort and slice. 
 
Usage:: 
 
  import functools 
  import jax 
 
  # MIPS := maximal inner product search 
  # Inputs: 
  #   qy: f32[qy_size, feature_dim] 
  #   db: f32[db_size, feature_dim] 
  # 
  # Returns: 
  #   (f32[qy_size, k], i32[qy_size, k]) 
  @functools.partial(jax.jit, static_argnames=[&quot;k&quot;, &quot;recall_target&quot;]) 
  def mips(qy, db, k=10, recall_target=0.95): 
    dists = jax.lax.dot(qy, db.transpose()) 
    # Computes max_k along the last dimension 
    # returns (f32[qy_size, k], i32[qy_size, k]) 
    return jax.lax.approx_max_k(dists, k=k, recall_target=recall_target) 
 
  # Multi-core example 
  # Inputs: 
  #   qy: f32[num_devices, qy_size, feature_dim] 
  #   db: f32[num_devices, per_device_db_size, feature_dim] 
  #   db_offset: i32[num_devices] 
  #   db_size = num_devices * per_device_db_size 
  # 
  # Returns: 
  #   (f32[qy_size, num_devices, k], i32[qy_size, num_devices, k]) 
  @functools.partial( 
      jax.pmap, 
      # static args: db_size, k, recall_target 
      static_broadcasted_argnums=[3, 4, 5], 
      out_axes=(1, 1)) 
  def pmap_mips(qy, db, db_offset, db_size, k, recall_target): 
    dists = jax.lax.dot(qy, db.transpose()) 
    dists, neighbors = jax.lax.approx_max_k( 
        dists, k=k, recall_target=recall_target, 
        reduction_input_size_override=db_size) 
    return (dists, neighbors + db_offset) 
 
  # i32[qy_size, num_devices, k] 
  pmap_neighbors = pmap_mips(qy, db, db_offset, db_size, 10, 0.95)[1] 
  # i32[qy_size, num_devices * k] 
  neighbors = jax.lax.collapse(pmap_neighbors, start_dimension=1, stop_dimension=3) 
 
Todos:: 
 
  * On host top-k aggregation 
  * Inaccurate but fast differentiation 
 
&quot;&quot;&quot;</span>

<span class="s3">from </span><span class="s1">functools </span><span class="s3">import </span><span class="s1">partial</span>
<span class="s3">from </span><span class="s1">typing </span><span class="s3">import </span><span class="s1">(Any</span><span class="s3">, </span><span class="s1">Tuple)</span>

<span class="s3">import </span><span class="s1">numpy </span><span class="s3">as </span><span class="s1">np</span>

<span class="s3">from </span><span class="s1">jax.interpreters </span><span class="s3">import </span><span class="s1">xla</span>

<span class="s3">from </span><span class="s1">jax._src </span><span class="s3">import </span><span class="s1">ad_util</span>
<span class="s3">from </span><span class="s1">jax._src </span><span class="s3">import </span><span class="s1">core</span>
<span class="s3">from </span><span class="s1">jax._src </span><span class="s3">import </span><span class="s1">dispatch</span>
<span class="s3">from </span><span class="s1">jax._src </span><span class="s3">import </span><span class="s1">dtypes</span>
<span class="s3">from </span><span class="s1">jax._src.interpreters </span><span class="s3">import </span><span class="s1">ad</span>
<span class="s3">from </span><span class="s1">jax._src.interpreters </span><span class="s3">import </span><span class="s1">batching</span>
<span class="s3">from </span><span class="s1">jax._src.lax </span><span class="s3">import </span><span class="s1">lax</span>
<span class="s3">from </span><span class="s1">jax._src.lib </span><span class="s3">import </span><span class="s1">xla_client </span><span class="s3">as </span><span class="s1">xc</span>


<span class="s1">Array = Any</span>


<span class="s3">def </span><span class="s1">approx_max_k(operand: Array</span><span class="s3">,</span>
                 <span class="s1">k: int</span><span class="s3">,</span>
                 <span class="s1">reduction_dimension: int = -</span><span class="s4">1</span><span class="s3">,</span>
                 <span class="s1">recall_target: float = </span><span class="s4">0.95</span><span class="s3">,</span>
                 <span class="s1">reduction_input_size_override: int = -</span><span class="s4">1</span><span class="s3">,</span>
                 <span class="s1">aggregate_to_topk: bool = </span><span class="s3">True</span><span class="s1">) -&gt; Tuple[Array</span><span class="s3">, </span><span class="s1">Array]:</span>
  <span class="s2">&quot;&quot;&quot;Returns max ``k`` values and their indices of the ``operand`` in an approximate manner. 
 
  See https://arxiv.org/abs/2206.14286 for the algorithm details. 
 
  Args: 
    operand : Array to search for max-k. Must be a floating number type. 
    k : Specifies the number of max-k. 
    reduction_dimension : Integer dimension along which to search. Default: -1. 
    recall_target : Recall target for the approximation. 
    reduction_input_size_override : When set to a positive value, it overrides 
      the size determined by ``operand[reduction_dim]`` for evaluating the 
      recall. This option is useful when the given ``operand`` is only a subset 
      of the overall computation in SPMD or distributed pipelines, where the 
      true input size cannot be deferred by the operand shape. 
    aggregate_to_topk : When true, aggregates approximate results to the top-k 
      in sorted order. When false, returns the approximate results unsorted. In 
      this case, the number of the approximate results is implementation defined 
      and is greater or equal to the specified ``k``. 
 
  Returns: 
    Tuple of two arrays. The arrays are the max ``k`` values and the 
    corresponding indices along the ``reduction_dimension`` of the input 
    ``operand``. The arrays' dimensions are the same as the input ``operand`` 
    except for the ``reduction_dimension``: when ``aggregate_to_topk`` is true, 
    the reduction dimension is ``k``; otherwise, it is greater equals to ``k`` 
    where the size is implementation-defined. 
 
  We encourage users to wrap ``approx_max_k`` with jit. See the following 
  example for maximal inner production search (MIPS): 
 
  &gt;&gt;&gt; import functools 
  &gt;&gt;&gt; import jax 
  &gt;&gt;&gt; import numpy as np 
  &gt;&gt;&gt; @functools.partial(jax.jit, static_argnames=[&quot;k&quot;, &quot;recall_target&quot;]) 
  ... def mips(qy, db, k=10, recall_target=0.95): 
  ...   dists = jax.lax.dot(qy, db.transpose()) 
  ...   # returns (f32[qy_size, k], i32[qy_size, k]) 
  ...   return jax.lax.approx_max_k(dists, k=k, recall_target=recall_target) 
  &gt;&gt;&gt; 
  &gt;&gt;&gt; qy = jax.numpy.array(np.random.rand(50, 64)) 
  &gt;&gt;&gt; db = jax.numpy.array(np.random.rand(1024, 64)) 
  &gt;&gt;&gt; dot_products, neighbors = mips(qy, db, k=10) 
  &quot;&quot;&quot;</span>
  <span class="s3">return </span><span class="s1">approx_top_k_p.bind(</span>
      <span class="s1">operand</span><span class="s3">,</span>
      <span class="s1">k=k</span><span class="s3">,</span>
      <span class="s1">reduction_dimension=reduction_dimension</span><span class="s3">,</span>
      <span class="s1">recall_target=recall_target</span><span class="s3">,</span>
      <span class="s1">is_max_k=</span><span class="s3">True,</span>
      <span class="s1">reduction_input_size_override=reduction_input_size_override</span><span class="s3">,</span>
      <span class="s1">aggregate_to_topk=aggregate_to_topk)</span>


<span class="s3">def </span><span class="s1">approx_min_k(operand: Array</span><span class="s3">,</span>
                 <span class="s1">k: int</span><span class="s3">,</span>
                 <span class="s1">reduction_dimension: int = -</span><span class="s4">1</span><span class="s3">,</span>
                 <span class="s1">recall_target: float = </span><span class="s4">0.95</span><span class="s3">,</span>
                 <span class="s1">reduction_input_size_override: int = -</span><span class="s4">1</span><span class="s3">,</span>
                 <span class="s1">aggregate_to_topk: bool = </span><span class="s3">True</span><span class="s1">) -&gt; Tuple[Array</span><span class="s3">, </span><span class="s1">Array]:</span>
  <span class="s2">&quot;&quot;&quot;Returns min ``k`` values and their indices of the ``operand`` in an approximate manner. 
 
  See https://arxiv.org/abs/2206.14286 for the algorithm details. 
 
  Args: 
    operand : Array to search for min-k. Must be a floating number type. 
    k : Specifies the number of min-k. 
    reduction_dimension: Integer dimension along which to search. Default: -1. 
    recall_target: Recall target for the approximation. 
    reduction_input_size_override : When set to a positive value, it overrides 
      the size determined by ``operand[reduction_dim]`` for evaluating the 
      recall. This option is useful when the given operand is only a subset of 
      the overall computation in SPMD or distributed pipelines, where the true 
      input size cannot be deferred by the ``operand`` shape. 
    aggregate_to_topk : When true, aggregates approximate results to the top-k 
      in sorted order. When false, returns the approximate results unsorted. In 
      this case, the number of the approximate results is implementation defined 
      and is greater or equal to the specified ``k``. 
 
  Returns: 
    Tuple of two arrays. The arrays are the least ``k`` values and the 
    corresponding indices along the ``reduction_dimension`` of the input 
    ``operand``.  The arrays' dimensions are the same as the input ``operand`` 
    except for the ``reduction_dimension``: when ``aggregate_to_topk`` is true, 
    the reduction dimension is ``k``; otherwise, it is greater equals to ``k`` 
    where the size is implementation-defined. 
 
  We encourage users to wrap ``approx_min_k`` with jit. See the following example 
  for nearest neighbor search over the squared l2 distance: 
 
  &gt;&gt;&gt; import functools 
  &gt;&gt;&gt; import jax 
  &gt;&gt;&gt; import numpy as np 
  &gt;&gt;&gt; @functools.partial(jax.jit, static_argnames=[&quot;k&quot;, &quot;recall_target&quot;]) 
  ... def l2_ann(qy, db, half_db_norms, k=10, recall_target=0.95): 
  ...   dists = half_db_norms - jax.lax.dot(qy, db.transpose()) 
  ...   return jax.lax.approx_min_k(dists, k=k, recall_target=recall_target) 
  &gt;&gt;&gt; 
  &gt;&gt;&gt; qy = jax.numpy.array(np.random.rand(50, 64)) 
  &gt;&gt;&gt; db = jax.numpy.array(np.random.rand(1024, 64)) 
  &gt;&gt;&gt; half_db_norm_sq = jax.numpy.linalg.norm(db, axis=1)**2 / 2 
  &gt;&gt;&gt; dists, neighbors = l2_ann(qy, db, half_db_norm_sq, k=10) 
 
  In the example above, we compute ``db^2/2 - dot(qy, db^T)`` instead of 
  ``qy^2 - 2 dot(qy, db^T) + db^2`` for performance reason. The former uses less 
  arithmetics and produces the same set of neighbors. 
  &quot;&quot;&quot;</span>
  <span class="s3">return </span><span class="s1">approx_top_k_p.bind(</span>
      <span class="s1">operand</span><span class="s3">,</span>
      <span class="s1">k=k</span><span class="s3">,</span>
      <span class="s1">reduction_dimension=reduction_dimension</span><span class="s3">,</span>
      <span class="s1">recall_target=recall_target</span><span class="s3">,</span>
      <span class="s1">is_max_k=</span><span class="s3">False,</span>
      <span class="s1">reduction_input_size_override=reduction_input_size_override</span><span class="s3">,</span>
      <span class="s1">aggregate_to_topk=aggregate_to_topk)</span>


<span class="s3">def </span><span class="s1">_approx_top_k_abstract_eval(operand</span><span class="s3">, </span><span class="s1">*</span><span class="s3">, </span><span class="s1">k</span><span class="s3">, </span><span class="s1">reduction_dimension</span><span class="s3">,</span>
                                <span class="s1">recall_target</span><span class="s3">, </span><span class="s1">is_max_k</span><span class="s3">,</span>
                                <span class="s1">reduction_input_size_override</span><span class="s3">,</span>
                                <span class="s1">aggregate_to_topk):</span>
  <span class="s3">if </span><span class="s1">k &lt;= </span><span class="s4">0</span><span class="s1">:</span>
    <span class="s3">raise </span><span class="s1">ValueError(</span><span class="s5">f'k must be positive, got </span><span class="s3">{</span><span class="s1">k</span><span class="s3">}</span><span class="s5">'</span><span class="s1">)</span>
  <span class="s3">if </span><span class="s1">len(operand.shape) == </span><span class="s4">0</span><span class="s1">:</span>
    <span class="s3">raise </span><span class="s1">TypeError(</span><span class="s5">'approx_top_k operand must have &gt;= 1 dimension, got {}'</span><span class="s1">.format(</span>
        <span class="s1">operand.shape))</span>
  <span class="s1">dims = list(operand.shape)</span>
  <span class="s3">if </span><span class="s1">dims[reduction_dimension] &lt; k:</span>
    <span class="s3">raise </span><span class="s1">ValueError(</span>
        <span class="s5">'k must be smaller than the size of reduction_dim {}, got {}'</span><span class="s1">.format(</span>
            <span class="s1">dims[reduction_dimension]</span><span class="s3">, </span><span class="s1">k))</span>
  <span class="s3">if not </span><span class="s1">dtypes.issubdtype(operand.dtype</span><span class="s3">, </span><span class="s1">np.floating):</span>
    <span class="s3">raise </span><span class="s1">ValueError(</span><span class="s5">'operand must be a floating type'</span><span class="s1">)</span>
  <span class="s1">reduction_input_size = dims[reduction_dimension]</span>
  <span class="s1">dims[reduction_dimension] = xc.ops.ApproxTopKReductionOutputSize(</span>
      <span class="s1">reduction_input_size</span><span class="s3">, </span><span class="s1">len(dims)</span><span class="s3">, </span><span class="s1">k</span><span class="s3">, </span><span class="s1">recall_target</span><span class="s3">, </span><span class="s1">aggregate_to_topk</span><span class="s3">,</span>
      <span class="s1">reduction_input_size_override)[</span><span class="s4">0</span><span class="s1">]</span>
  <span class="s3">return </span><span class="s1">(operand.update(</span>
      <span class="s1">shape=dims</span><span class="s3">, </span><span class="s1">dtype=operand.dtype</span><span class="s3">, </span><span class="s1">weak_type=operand.weak_type)</span><span class="s3">,</span>
          <span class="s1">operand.update(shape=dims</span><span class="s3">, </span><span class="s1">dtype=np.dtype(np.int32)))</span>


<span class="s3">def </span><span class="s1">_comparator_builder(op_type</span><span class="s3">, </span><span class="s1">is_max_k):</span>
  <span class="s1">c = xc.XlaBuilder(</span>
      <span class="s5">'top_k_{}_comparator'</span><span class="s1">.format(</span><span class="s5">'gt' </span><span class="s3">if </span><span class="s1">is_max_k </span><span class="s3">else </span><span class="s5">'lt'</span><span class="s1">))</span>
  <span class="s1">p0 = xla.parameter(c</span><span class="s3">, </span><span class="s4">0</span><span class="s3">, </span><span class="s1">xc.Shape.scalar_shape(op_type))</span>
  <span class="s1">p1 = xla.parameter(c</span><span class="s3">, </span><span class="s4">1</span><span class="s3">, </span><span class="s1">xc.Shape.scalar_shape(op_type))</span>
  <span class="s1">xla.parameter(c</span><span class="s3">, </span><span class="s4">2</span><span class="s3">, </span><span class="s1">xc.Shape.scalar_shape(np.dtype(np.int32)))</span>
  <span class="s1">xla.parameter(c</span><span class="s3">, </span><span class="s4">3</span><span class="s3">, </span><span class="s1">xc.Shape.scalar_shape(np.dtype(np.int32)))</span>
  <span class="s3">if </span><span class="s1">is_max_k:</span>
    <span class="s1">cmp_result = xc.ops.Gt(p0</span><span class="s3">, </span><span class="s1">p1)</span>
  <span class="s3">else</span><span class="s1">:</span>
    <span class="s1">cmp_result = xc.ops.Lt(p0</span><span class="s3">, </span><span class="s1">p1)</span>
  <span class="s3">return </span><span class="s1">c.build(cmp_result)</span>


<span class="s3">def </span><span class="s1">_get_init_val_literal(op_type</span><span class="s3">, </span><span class="s1">is_max_k):</span>
  <span class="s3">return </span><span class="s1">np.array(np.NINF </span><span class="s3">if </span><span class="s1">is_max_k </span><span class="s3">else </span><span class="s1">np.Inf</span><span class="s3">, </span><span class="s1">dtype=op_type)</span>

<span class="s3">def </span><span class="s1">_approx_top_k_tpu_translation(ctx</span><span class="s3">, </span><span class="s1">avals_in</span><span class="s3">, </span><span class="s1">avals_out</span><span class="s3">, </span><span class="s1">operand</span><span class="s3">, </span><span class="s1">*</span><span class="s3">, </span><span class="s1">k</span><span class="s3">,</span>
                                  <span class="s1">reduction_dimension</span><span class="s3">, </span><span class="s1">recall_target</span><span class="s3">, </span><span class="s1">is_max_k</span><span class="s3">,</span>
                                  <span class="s1">reduction_input_size_override</span><span class="s3">,</span>
                                  <span class="s1">aggregate_to_topk):</span>
  <span class="s1">c = ctx.builder</span>
  <span class="s1">op_shape = c.get_shape(operand)</span>
  <span class="s3">if not </span><span class="s1">op_shape.is_array():</span>
    <span class="s3">raise </span><span class="s1">ValueError(</span><span class="s5">f'operand must be an array, but was </span><span class="s3">{</span><span class="s1">op_shape</span><span class="s3">}</span><span class="s5">'</span><span class="s1">)</span>
  <span class="s1">op_dims = op_shape.dimensions()</span>
  <span class="s1">op_type = op_shape.element_type()</span>
  <span class="s3">if </span><span class="s1">reduction_dimension &lt; </span><span class="s4">0</span><span class="s1">:</span>
    <span class="s1">reduction_dimension = len(op_dims) + reduction_dimension</span>
  <span class="s1">comparator = _comparator_builder(op_type</span><span class="s3">, </span><span class="s1">is_max_k)</span>
  <span class="s1">init_val_literal = _get_init_val_literal(op_type</span><span class="s3">, </span><span class="s1">is_max_k)</span>
  <span class="s1">iota = xc.ops.Iota(c</span><span class="s3">, </span><span class="s1">xc.Shape.array_shape(np.dtype(np.int32)</span><span class="s3">, </span><span class="s1">op_dims)</span><span class="s3">,</span>
                     <span class="s1">reduction_dimension)</span>
  <span class="s1">init_val = xc.ops.Constant(c</span><span class="s3">, </span><span class="s1">init_val_literal)</span>
  <span class="s1">init_arg = xc.ops.Constant(c</span><span class="s3">, </span><span class="s1">np.int32(-</span><span class="s4">1</span><span class="s1">))</span>
  <span class="s1">out = xc.ops.ApproxTopK(c</span><span class="s3">, </span><span class="s1">[operand</span><span class="s3">, </span><span class="s1">iota]</span><span class="s3">, </span><span class="s1">[init_val</span><span class="s3">, </span><span class="s1">init_arg]</span><span class="s3">, </span><span class="s1">k</span><span class="s3">,</span>
                          <span class="s1">reduction_dimension</span><span class="s3">, </span><span class="s1">comparator</span><span class="s3">, </span><span class="s1">recall_target</span><span class="s3">,</span>
                          <span class="s1">aggregate_to_topk</span><span class="s3">, </span><span class="s1">reduction_input_size_override)</span>
  <span class="s3">return </span><span class="s1">xla.xla_destructure(c</span><span class="s3">, </span><span class="s1">out)</span>


<span class="s3">def </span><span class="s1">_approx_top_k_fallback_translation(ctx</span><span class="s3">, </span><span class="s1">avals_in</span><span class="s3">, </span><span class="s1">avals_out</span><span class="s3">, </span><span class="s1">operand</span><span class="s3">, </span><span class="s1">*</span><span class="s3">, </span><span class="s1">k</span><span class="s3">,</span>
                                       <span class="s1">reduction_dimension</span><span class="s3">, </span><span class="s1">recall_target</span><span class="s3">,</span>
                                       <span class="s1">is_max_k</span><span class="s3">, </span><span class="s1">reduction_input_size_override</span><span class="s3">,</span>
                                       <span class="s1">aggregate_to_topk):</span>
  <span class="s1">c = ctx.builder</span>
  <span class="s1">op_shape = c.get_shape(operand)</span>
  <span class="s3">if not </span><span class="s1">op_shape.is_array():</span>
    <span class="s3">raise </span><span class="s1">ValueError(</span><span class="s5">f'operand must be an array, but was </span><span class="s3">{</span><span class="s1">op_shape</span><span class="s3">}</span><span class="s5">'</span><span class="s1">)</span>
  <span class="s1">op_dims = op_shape.dimensions()</span>
  <span class="s1">op_type = op_shape.element_type()</span>

  <span class="s3">if </span><span class="s1">reduction_dimension &lt; </span><span class="s4">0</span><span class="s1">:</span>
    <span class="s1">reduction_dimension = len(op_dims) + reduction_dimension</span>
  <span class="s1">comparator = _comparator_builder(op_type</span><span class="s3">, </span><span class="s1">is_max_k)</span>
  <span class="s1">iota = xc.ops.Iota(c</span><span class="s3">, </span><span class="s1">xc.Shape.array_shape(np.dtype(np.int32)</span><span class="s3">, </span><span class="s1">op_dims)</span><span class="s3">,</span>
                     <span class="s1">reduction_dimension)</span>
  <span class="s1">init_val_literal = _get_init_val_literal(op_type</span><span class="s3">, </span><span class="s1">is_max_k)</span>
  <span class="s1">init_val = xc.ops.Constant(c</span><span class="s3">, </span><span class="s1">init_val_literal)</span>
  <span class="s1">init_arg = xc.ops.Constant(c</span><span class="s3">, </span><span class="s1">np.int32(-</span><span class="s4">1</span><span class="s1">))</span>
  <span class="s1">out = xc.ops.ApproxTopKFallback(c</span><span class="s3">, </span><span class="s1">[operand</span><span class="s3">, </span><span class="s1">iota]</span><span class="s3">, </span><span class="s1">[init_val</span><span class="s3">, </span><span class="s1">init_arg]</span><span class="s3">, </span><span class="s1">k</span><span class="s3">,</span>
                                  <span class="s1">reduction_dimension</span><span class="s3">, </span><span class="s1">comparator</span><span class="s3">,</span>
                                  <span class="s1">recall_target</span><span class="s3">, </span><span class="s1">aggregate_to_topk</span><span class="s3">,</span>
                                  <span class="s1">reduction_input_size_override)</span>
  <span class="s3">return </span><span class="s1">xla.xla_destructure(c</span><span class="s3">, </span><span class="s1">out)</span>


<span class="s3">def </span><span class="s1">_approx_top_k_batch_rule(batch_operands</span><span class="s3">, </span><span class="s1">batch_axes</span><span class="s3">, </span><span class="s1">*</span><span class="s3">, </span><span class="s1">k</span><span class="s3">,</span>
                             <span class="s1">reduction_dimension</span><span class="s3">, </span><span class="s1">recall_target</span><span class="s3">, </span><span class="s1">is_max_k</span><span class="s3">,</span>
                             <span class="s1">reduction_input_size_override</span><span class="s3">, </span><span class="s1">aggregate_to_topk):</span>
  <span class="s3">assert </span><span class="s1">len(batch_operands) == </span><span class="s4">1</span>
  <span class="s3">assert </span><span class="s1">len(batch_axes) == </span><span class="s4">1</span>
  <span class="s1">operand</span><span class="s3">, </span><span class="s1">= batch_operands</span>
  <span class="s1">batch_axis</span><span class="s3">, </span><span class="s1">= batch_axes</span>
  <span class="s1">dim_map = [d </span><span class="s3">for </span><span class="s1">d </span><span class="s3">in </span><span class="s1">range(operand.ndim) </span><span class="s3">if </span><span class="s1">d </span><span class="s3">is not </span><span class="s1">batch_axis]</span>
  <span class="s1">reduction_dimension = dim_map[reduction_dimension]</span>
  <span class="s3">return </span><span class="s1">approx_top_k_p.bind(</span>
      <span class="s1">operand</span><span class="s3">,</span>
      <span class="s1">k=k</span><span class="s3">,</span>
      <span class="s1">reduction_dimension=reduction_dimension</span><span class="s3">,</span>
      <span class="s1">recall_target=recall_target</span><span class="s3">,</span>
      <span class="s1">is_max_k=is_max_k</span><span class="s3">,</span>
      <span class="s1">reduction_input_size_override=reduction_input_size_override</span><span class="s3">,</span>
      <span class="s1">aggregate_to_topk=aggregate_to_topk)</span><span class="s3">, </span><span class="s1">(batch_axis</span><span class="s3">, </span><span class="s1">batch_axis)</span>


<span class="s0"># Slow jvp implementation using gather.</span>
<span class="s0">#</span>
<span class="s0"># TODO(fchern): Some optimization ideas</span>
<span class="s0"># 1. ApproxTopK is internally a variadic reduce, so we can simply call</span>
<span class="s0">#    ApproxTopK(operand, tangent, iota) for jvp.</span>
<span class="s0"># 2. vjp cannot benefit from the algorithm above. We must run scatter to</span>
<span class="s0">#    distribute the output cotangent to input cotangent. A reasonable way to do</span>
<span class="s0">#    this is to run it on CPU.</span>
<span class="s3">def </span><span class="s1">_approx_top_k_jvp(primals</span><span class="s3">, </span><span class="s1">tangents</span><span class="s3">, </span><span class="s1">*</span><span class="s3">, </span><span class="s1">k</span><span class="s3">, </span><span class="s1">reduction_dimension</span><span class="s3">,</span>
                      <span class="s1">recall_target</span><span class="s3">, </span><span class="s1">is_max_k</span><span class="s3">, </span><span class="s1">reduction_input_size_override</span><span class="s3">,</span>
                      <span class="s1">aggregate_to_topk):</span>
  <span class="s1">operand</span><span class="s3">, </span><span class="s1">= primals</span>
  <span class="s1">tangent</span><span class="s3">, </span><span class="s1">= tangents</span>
  <span class="s3">if </span><span class="s1">is_max_k:</span>
    <span class="s1">val_out</span><span class="s3">, </span><span class="s1">arg_out = approx_max_k(operand</span><span class="s3">, </span><span class="s1">k</span><span class="s3">, </span><span class="s1">reduction_dimension</span><span class="s3">,</span>
                                    <span class="s1">recall_target</span><span class="s3">,</span>
                                    <span class="s1">reduction_input_size_override</span><span class="s3">,</span>
                                    <span class="s1">aggregate_to_topk)</span>
  <span class="s3">else</span><span class="s1">:</span>
    <span class="s1">val_out</span><span class="s3">, </span><span class="s1">arg_out = approx_min_k(operand</span><span class="s3">, </span><span class="s1">k</span><span class="s3">, </span><span class="s1">reduction_dimension</span><span class="s3">,</span>
                                    <span class="s1">recall_target</span><span class="s3">,</span>
                                    <span class="s1">reduction_input_size_override</span><span class="s3">,</span>
                                    <span class="s1">aggregate_to_topk)</span>
  <span class="s3">if </span><span class="s1">type(tangent) </span><span class="s3">is </span><span class="s1">ad_util.Zero:</span>
    <span class="s1">tangent_out = ad_util.Zero.from_value(val_out)</span>
  <span class="s3">else</span><span class="s1">:</span>
    <span class="s1">arg_shape = arg_out.shape</span>
    <span class="s1">rank = len(arg_shape)</span>
    <span class="s3">if </span><span class="s1">reduction_dimension &lt; </span><span class="s4">0</span><span class="s1">:</span>
      <span class="s1">reduction_dimension += rank</span>
    <span class="s1">iotas = [</span>
        <span class="s1">lax.broadcasted_iota(arg_out.dtype</span><span class="s3">, </span><span class="s1">arg_shape</span><span class="s3">, </span><span class="s1">i) </span><span class="s3">for </span><span class="s1">i </span><span class="s3">in </span><span class="s1">range(rank)</span>
    <span class="s1">]</span>
    <span class="s1">idx = tuple(</span>
        <span class="s1">arg_out </span><span class="s3">if </span><span class="s1">i == reduction_dimension </span><span class="s3">else </span><span class="s1">iotas[i] </span><span class="s3">for </span><span class="s1">i </span><span class="s3">in </span><span class="s1">range(rank))</span>
    <span class="s1">tangent_out = tangent[idx]</span>
  <span class="s3">return </span><span class="s1">(val_out</span><span class="s3">, </span><span class="s1">arg_out)</span><span class="s3">, </span><span class="s1">(tangent_out</span><span class="s3">, </span><span class="s1">ad_util.Zero.from_value(arg_out))</span>


<span class="s1">approx_top_k_p = core.Primitive(</span><span class="s5">'approx_top_k'</span><span class="s1">)</span>
<span class="s1">approx_top_k_p.multiple_results = </span><span class="s3">True</span>
<span class="s1">approx_top_k_p.def_impl(partial(dispatch.apply_primitive</span><span class="s3">, </span><span class="s1">approx_top_k_p))</span>
<span class="s1">approx_top_k_p.def_abstract_eval(_approx_top_k_abstract_eval)</span>
<span class="s1">xla.register_translation(approx_top_k_p</span><span class="s3">, </span><span class="s1">_approx_top_k_fallback_translation)</span>
<span class="s1">xla.register_translation(approx_top_k_p</span><span class="s3">, </span><span class="s1">_approx_top_k_tpu_translation</span><span class="s3">,</span>
                         <span class="s1">platform=</span><span class="s5">'tpu'</span><span class="s1">)</span>
<span class="s1">batching.primitive_batchers[approx_top_k_p] = _approx_top_k_batch_rule</span>
<span class="s1">ad.primitive_jvps[approx_top_k_p] = _approx_top_k_jvp</span>
</pre>
</body>
</html>