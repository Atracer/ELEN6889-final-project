<html>
<head>
<title>stax.py</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<style type="text/css">
.s0 { color: #808080;}
.s1 { color: #a9b7c6;}
.s2 { color: #629755; font-style: italic;}
.s3 { color: #cc7832;}
.s4 { color: #6897bb;}
.s5 { color: #6a8759;}
</style>
</head>
<body bgcolor="#2b2b2b">
<table CELLSPACING=0 CELLPADDING=5 COLS=1 WIDTH="100%" BGCOLOR="#606060" >
<tr><td><center>
<font face="Arial, Helvetica" color="#000000">
stax.py</font>
</center></td></tr></table>
<pre><span class="s0"># Copyright 2018 The JAX Authors.</span>
<span class="s0">#</span>
<span class="s0"># Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);</span>
<span class="s0"># you may not use this file except in compliance with the License.</span>
<span class="s0"># You may obtain a copy of the License at</span>
<span class="s0">#</span>
<span class="s0">#     https://www.apache.org/licenses/LICENSE-2.0</span>
<span class="s0">#</span>
<span class="s0"># Unless required by applicable law or agreed to in writing, software</span>
<span class="s0"># distributed under the License is distributed on an &quot;AS IS&quot; BASIS,</span>
<span class="s0"># WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span>
<span class="s0"># See the License for the specific language governing permissions and</span>
<span class="s0"># limitations under the License.</span>

<span class="s2">&quot;&quot;&quot;Stax is a small but flexible neural net specification library from scratch. 
 
You likely do not mean to import this module! Stax is intended as an example 
library only. There are a number of other much more fully-featured neural 
network libraries for JAX, including `Flax`_ from Google, and `Haiku`_ from 
DeepMind. 
 
.. _Haiku: https://github.com/deepmind/dm-haiku 
.. _Flax: https://github.com/google/flax 
&quot;&quot;&quot;</span>

<span class="s3">import </span><span class="s1">functools</span>
<span class="s3">import </span><span class="s1">operator </span><span class="s3">as </span><span class="s1">op</span>

<span class="s3">from </span><span class="s1">jax </span><span class="s3">import </span><span class="s1">lax</span>
<span class="s3">from </span><span class="s1">jax </span><span class="s3">import </span><span class="s1">random</span>
<span class="s3">import </span><span class="s1">jax.numpy </span><span class="s3">as </span><span class="s1">jnp</span>

<span class="s3">from </span><span class="s1">jax.nn </span><span class="s3">import </span><span class="s1">(relu</span><span class="s3">, </span><span class="s1">log_softmax</span><span class="s3">, </span><span class="s1">softmax</span><span class="s3">, </span><span class="s1">softplus</span><span class="s3">, </span><span class="s1">sigmoid</span><span class="s3">, </span><span class="s1">elu</span><span class="s3">,</span>
                    <span class="s1">leaky_relu</span><span class="s3">, </span><span class="s1">selu</span><span class="s3">, </span><span class="s1">gelu</span><span class="s3">, </span><span class="s1">standardize)</span>
<span class="s3">from </span><span class="s1">jax.nn.initializers </span><span class="s3">import </span><span class="s1">glorot_normal</span><span class="s3">, </span><span class="s1">normal</span><span class="s3">, </span><span class="s1">ones</span><span class="s3">, </span><span class="s1">zeros</span>

<span class="s0"># aliases for backwards compatibility</span>
<span class="s1">glorot = glorot_normal</span>
<span class="s1">randn = normal</span>
<span class="s1">logsoftmax = log_softmax</span>

<span class="s0"># Following the convention used in Keras and tf.layers, we use CamelCase for the</span>
<span class="s0"># names of layer constructors, like Conv and Relu, while using snake_case for</span>
<span class="s0"># other functions, like lax.conv and relu.</span>

<span class="s0"># Each layer constructor function returns an (init_fun, apply_fun) pair, where</span>
<span class="s0">#   init_fun: takes an rng key and an input shape and returns an</span>
<span class="s0">#     (output_shape, params) pair,</span>
<span class="s0">#   apply_fun: takes params, inputs, and an rng key and applies the layer.</span>


<span class="s3">def </span><span class="s1">Dense(out_dim</span><span class="s3">, </span><span class="s1">W_init=glorot_normal()</span><span class="s3">, </span><span class="s1">b_init=normal()):</span>
  <span class="s2">&quot;&quot;&quot;Layer constructor function for a dense (fully-connected) layer.&quot;&quot;&quot;</span>
  <span class="s3">def </span><span class="s1">init_fun(rng</span><span class="s3">, </span><span class="s1">input_shape):</span>
    <span class="s1">output_shape = input_shape[:-</span><span class="s4">1</span><span class="s1">] + (out_dim</span><span class="s3">,</span><span class="s1">)</span>
    <span class="s1">k1</span><span class="s3">, </span><span class="s1">k2 = random.split(rng)</span>
    <span class="s1">W</span><span class="s3">, </span><span class="s1">b = W_init(k1</span><span class="s3">, </span><span class="s1">(input_shape[-</span><span class="s4">1</span><span class="s1">]</span><span class="s3">, </span><span class="s1">out_dim))</span><span class="s3">, </span><span class="s1">b_init(k2</span><span class="s3">, </span><span class="s1">(out_dim</span><span class="s3">,</span><span class="s1">))</span>
    <span class="s3">return </span><span class="s1">output_shape</span><span class="s3">, </span><span class="s1">(W</span><span class="s3">, </span><span class="s1">b)</span>
  <span class="s3">def </span><span class="s1">apply_fun(params</span><span class="s3">, </span><span class="s1">inputs</span><span class="s3">, </span><span class="s1">**kwargs):</span>
    <span class="s1">W</span><span class="s3">, </span><span class="s1">b = params</span>
    <span class="s3">return </span><span class="s1">jnp.dot(inputs</span><span class="s3">, </span><span class="s1">W) + b</span>
  <span class="s3">return </span><span class="s1">init_fun</span><span class="s3">, </span><span class="s1">apply_fun</span>


<span class="s3">def </span><span class="s1">GeneralConv(dimension_numbers</span><span class="s3">, </span><span class="s1">out_chan</span><span class="s3">, </span><span class="s1">filter_shape</span><span class="s3">,</span>
                <span class="s1">strides=</span><span class="s3">None, </span><span class="s1">padding=</span><span class="s5">'VALID'</span><span class="s3">, </span><span class="s1">W_init=</span><span class="s3">None,</span>
                <span class="s1">b_init=normal(</span><span class="s4">1e-6</span><span class="s1">)):</span>
  <span class="s2">&quot;&quot;&quot;Layer construction function for a general convolution layer.&quot;&quot;&quot;</span>
  <span class="s1">lhs_spec</span><span class="s3">, </span><span class="s1">rhs_spec</span><span class="s3">, </span><span class="s1">out_spec = dimension_numbers</span>
  <span class="s1">one = (</span><span class="s4">1</span><span class="s3">,</span><span class="s1">) * len(filter_shape)</span>
  <span class="s1">strides = strides </span><span class="s3">or </span><span class="s1">one</span>
  <span class="s1">W_init = W_init </span><span class="s3">or </span><span class="s1">glorot_normal(rhs_spec.index(</span><span class="s5">'I'</span><span class="s1">)</span><span class="s3">, </span><span class="s1">rhs_spec.index(</span><span class="s5">'O'</span><span class="s1">))</span>
  <span class="s3">def </span><span class="s1">init_fun(rng</span><span class="s3">, </span><span class="s1">input_shape):</span>
    <span class="s1">filter_shape_iter = iter(filter_shape)</span>
    <span class="s1">kernel_shape = [out_chan </span><span class="s3">if </span><span class="s1">c == </span><span class="s5">'O' </span><span class="s3">else</span>
                    <span class="s1">input_shape[lhs_spec.index(</span><span class="s5">'C'</span><span class="s1">)] </span><span class="s3">if </span><span class="s1">c == </span><span class="s5">'I' </span><span class="s3">else</span>
                    <span class="s1">next(filter_shape_iter) </span><span class="s3">for </span><span class="s1">c </span><span class="s3">in </span><span class="s1">rhs_spec]</span>
    <span class="s1">output_shape = lax.conv_general_shape_tuple(</span>
        <span class="s1">input_shape</span><span class="s3">, </span><span class="s1">kernel_shape</span><span class="s3">, </span><span class="s1">strides</span><span class="s3">, </span><span class="s1">padding</span><span class="s3">, </span><span class="s1">dimension_numbers)</span>
    <span class="s1">bias_shape = [out_chan </span><span class="s3">if </span><span class="s1">c == </span><span class="s5">'C' </span><span class="s3">else </span><span class="s4">1 </span><span class="s3">for </span><span class="s1">c </span><span class="s3">in </span><span class="s1">out_spec]</span>
    <span class="s1">k1</span><span class="s3">, </span><span class="s1">k2 = random.split(rng)</span>
    <span class="s1">W</span><span class="s3">, </span><span class="s1">b = W_init(k1</span><span class="s3">, </span><span class="s1">kernel_shape)</span><span class="s3">, </span><span class="s1">b_init(k2</span><span class="s3">, </span><span class="s1">bias_shape)</span>
    <span class="s3">return </span><span class="s1">output_shape</span><span class="s3">, </span><span class="s1">(W</span><span class="s3">, </span><span class="s1">b)</span>
  <span class="s3">def </span><span class="s1">apply_fun(params</span><span class="s3">, </span><span class="s1">inputs</span><span class="s3">, </span><span class="s1">**kwargs):</span>
    <span class="s1">W</span><span class="s3">, </span><span class="s1">b = params</span>
    <span class="s3">return </span><span class="s1">lax.conv_general_dilated(inputs</span><span class="s3">, </span><span class="s1">W</span><span class="s3">, </span><span class="s1">strides</span><span class="s3">, </span><span class="s1">padding</span><span class="s3">, </span><span class="s1">one</span><span class="s3">, </span><span class="s1">one</span><span class="s3">,</span>
                                    <span class="s1">dimension_numbers=dimension_numbers) + b</span>
  <span class="s3">return </span><span class="s1">init_fun</span><span class="s3">, </span><span class="s1">apply_fun</span>
<span class="s1">Conv = functools.partial(GeneralConv</span><span class="s3">, </span><span class="s1">(</span><span class="s5">'NHWC'</span><span class="s3">, </span><span class="s5">'HWIO'</span><span class="s3">, </span><span class="s5">'NHWC'</span><span class="s1">))</span>


<span class="s3">def </span><span class="s1">GeneralConvTranspose(dimension_numbers</span><span class="s3">, </span><span class="s1">out_chan</span><span class="s3">, </span><span class="s1">filter_shape</span><span class="s3">,</span>
                         <span class="s1">strides=</span><span class="s3">None, </span><span class="s1">padding=</span><span class="s5">'VALID'</span><span class="s3">, </span><span class="s1">W_init=</span><span class="s3">None,</span>
                         <span class="s1">b_init=normal(</span><span class="s4">1e-6</span><span class="s1">)):</span>
  <span class="s2">&quot;&quot;&quot;Layer construction function for a general transposed-convolution layer.&quot;&quot;&quot;</span>
  <span class="s1">lhs_spec</span><span class="s3">, </span><span class="s1">rhs_spec</span><span class="s3">, </span><span class="s1">out_spec = dimension_numbers</span>
  <span class="s1">one = (</span><span class="s4">1</span><span class="s3">,</span><span class="s1">) * len(filter_shape)</span>
  <span class="s1">strides = strides </span><span class="s3">or </span><span class="s1">one</span>
  <span class="s1">W_init = W_init </span><span class="s3">or </span><span class="s1">glorot_normal(rhs_spec.index(</span><span class="s5">'I'</span><span class="s1">)</span><span class="s3">, </span><span class="s1">rhs_spec.index(</span><span class="s5">'O'</span><span class="s1">))</span>
  <span class="s3">def </span><span class="s1">init_fun(rng</span><span class="s3">, </span><span class="s1">input_shape):</span>
    <span class="s1">filter_shape_iter = iter(filter_shape)</span>
    <span class="s1">kernel_shape = [out_chan </span><span class="s3">if </span><span class="s1">c == </span><span class="s5">'O' </span><span class="s3">else</span>
                    <span class="s1">input_shape[lhs_spec.index(</span><span class="s5">'C'</span><span class="s1">)] </span><span class="s3">if </span><span class="s1">c == </span><span class="s5">'I' </span><span class="s3">else</span>
                    <span class="s1">next(filter_shape_iter) </span><span class="s3">for </span><span class="s1">c </span><span class="s3">in </span><span class="s1">rhs_spec]</span>
    <span class="s1">output_shape = lax.conv_transpose_shape_tuple(</span>
        <span class="s1">input_shape</span><span class="s3">, </span><span class="s1">kernel_shape</span><span class="s3">, </span><span class="s1">strides</span><span class="s3">, </span><span class="s1">padding</span><span class="s3">, </span><span class="s1">dimension_numbers)</span>
    <span class="s1">bias_shape = [out_chan </span><span class="s3">if </span><span class="s1">c == </span><span class="s5">'C' </span><span class="s3">else </span><span class="s4">1 </span><span class="s3">for </span><span class="s1">c </span><span class="s3">in </span><span class="s1">out_spec]</span>
    <span class="s1">k1</span><span class="s3">, </span><span class="s1">k2 = random.split(rng)</span>
    <span class="s1">W</span><span class="s3">, </span><span class="s1">b = W_init(k1</span><span class="s3">, </span><span class="s1">kernel_shape)</span><span class="s3">, </span><span class="s1">b_init(k2</span><span class="s3">, </span><span class="s1">bias_shape)</span>
    <span class="s3">return </span><span class="s1">output_shape</span><span class="s3">, </span><span class="s1">(W</span><span class="s3">, </span><span class="s1">b)</span>
  <span class="s3">def </span><span class="s1">apply_fun(params</span><span class="s3">, </span><span class="s1">inputs</span><span class="s3">, </span><span class="s1">**kwargs):</span>
    <span class="s1">W</span><span class="s3">, </span><span class="s1">b = params</span>
    <span class="s3">return </span><span class="s1">lax.conv_transpose(inputs</span><span class="s3">, </span><span class="s1">W</span><span class="s3">, </span><span class="s1">strides</span><span class="s3">, </span><span class="s1">padding</span><span class="s3">,</span>
                              <span class="s1">dimension_numbers=dimension_numbers) + b</span>
  <span class="s3">return </span><span class="s1">init_fun</span><span class="s3">, </span><span class="s1">apply_fun</span>
<span class="s1">Conv1DTranspose = functools.partial(GeneralConvTranspose</span><span class="s3">, </span><span class="s1">(</span><span class="s5">'NHC'</span><span class="s3">, </span><span class="s5">'HIO'</span><span class="s3">, </span><span class="s5">'NHC'</span><span class="s1">))</span>
<span class="s1">ConvTranspose = functools.partial(GeneralConvTranspose</span><span class="s3">,</span>
                                  <span class="s1">(</span><span class="s5">'NHWC'</span><span class="s3">, </span><span class="s5">'HWIO'</span><span class="s3">, </span><span class="s5">'NHWC'</span><span class="s1">))</span>


<span class="s3">def </span><span class="s1">BatchNorm(axis=(</span><span class="s4">0</span><span class="s3">, </span><span class="s4">1</span><span class="s3">, </span><span class="s4">2</span><span class="s1">)</span><span class="s3">, </span><span class="s1">epsilon=</span><span class="s4">1e-5</span><span class="s3">, </span><span class="s1">center=</span><span class="s3">True, </span><span class="s1">scale=</span><span class="s3">True,</span>
              <span class="s1">beta_init=zeros</span><span class="s3">, </span><span class="s1">gamma_init=ones):</span>
  <span class="s2">&quot;&quot;&quot;Layer construction function for a batch normalization layer.&quot;&quot;&quot;</span>
  <span class="s1">_beta_init = </span><span class="s3">lambda </span><span class="s1">rng</span><span class="s3">, </span><span class="s1">shape: beta_init(rng</span><span class="s3">, </span><span class="s1">shape) </span><span class="s3">if </span><span class="s1">center </span><span class="s3">else </span><span class="s1">()</span>
  <span class="s1">_gamma_init = </span><span class="s3">lambda </span><span class="s1">rng</span><span class="s3">, </span><span class="s1">shape: gamma_init(rng</span><span class="s3">, </span><span class="s1">shape) </span><span class="s3">if </span><span class="s1">scale </span><span class="s3">else </span><span class="s1">()</span>
  <span class="s1">axis = (axis</span><span class="s3">,</span><span class="s1">) </span><span class="s3">if </span><span class="s1">jnp.isscalar(axis) </span><span class="s3">else </span><span class="s1">axis</span>
  <span class="s3">def </span><span class="s1">init_fun(rng</span><span class="s3">, </span><span class="s1">input_shape):</span>
    <span class="s1">shape = tuple(d </span><span class="s3">for </span><span class="s1">i</span><span class="s3">, </span><span class="s1">d </span><span class="s3">in </span><span class="s1">enumerate(input_shape) </span><span class="s3">if </span><span class="s1">i </span><span class="s3">not in </span><span class="s1">axis)</span>
    <span class="s1">k1</span><span class="s3">, </span><span class="s1">k2 = random.split(rng)</span>
    <span class="s1">beta</span><span class="s3">, </span><span class="s1">gamma = _beta_init(k1</span><span class="s3">, </span><span class="s1">shape)</span><span class="s3">, </span><span class="s1">_gamma_init(k2</span><span class="s3">, </span><span class="s1">shape)</span>
    <span class="s3">return </span><span class="s1">input_shape</span><span class="s3">, </span><span class="s1">(beta</span><span class="s3">, </span><span class="s1">gamma)</span>
  <span class="s3">def </span><span class="s1">apply_fun(params</span><span class="s3">, </span><span class="s1">x</span><span class="s3">, </span><span class="s1">**kwargs):</span>
    <span class="s1">beta</span><span class="s3">, </span><span class="s1">gamma = params</span>
    <span class="s0"># TODO(phawkins): jnp.expand_dims should accept an axis tuple.</span>
    <span class="s0"># (https://github.com/numpy/numpy/issues/12290)</span>
    <span class="s1">ed = tuple(</span><span class="s3">None if </span><span class="s1">i </span><span class="s3">in </span><span class="s1">axis </span><span class="s3">else </span><span class="s1">slice(</span><span class="s3">None</span><span class="s1">) </span><span class="s3">for </span><span class="s1">i </span><span class="s3">in </span><span class="s1">range(jnp.ndim(x)))</span>
    <span class="s1">z = standardize(x</span><span class="s3">, </span><span class="s1">axis</span><span class="s3">, </span><span class="s1">epsilon=epsilon)</span>
    <span class="s3">if </span><span class="s1">center </span><span class="s3">and </span><span class="s1">scale: </span><span class="s3">return </span><span class="s1">gamma[ed] * z + beta[ed]</span>
    <span class="s3">if </span><span class="s1">center: </span><span class="s3">return </span><span class="s1">z + beta[ed]</span>
    <span class="s3">if </span><span class="s1">scale: </span><span class="s3">return </span><span class="s1">gamma[ed] * z</span>
    <span class="s3">return </span><span class="s1">z</span>
  <span class="s3">return </span><span class="s1">init_fun</span><span class="s3">, </span><span class="s1">apply_fun</span>


<span class="s3">def </span><span class="s1">elementwise(fun</span><span class="s3">, </span><span class="s1">**fun_kwargs):</span>
  <span class="s2">&quot;&quot;&quot;Layer that applies a scalar function elementwise on its inputs.&quot;&quot;&quot;</span>
  <span class="s1">init_fun = </span><span class="s3">lambda </span><span class="s1">rng</span><span class="s3">, </span><span class="s1">input_shape: (input_shape</span><span class="s3">, </span><span class="s1">())</span>
  <span class="s1">apply_fun = </span><span class="s3">lambda </span><span class="s1">params</span><span class="s3">, </span><span class="s1">inputs</span><span class="s3">, </span><span class="s1">**kwargs: fun(inputs</span><span class="s3">, </span><span class="s1">**fun_kwargs)</span>
  <span class="s3">return </span><span class="s1">init_fun</span><span class="s3">, </span><span class="s1">apply_fun</span>
<span class="s1">Tanh = elementwise(jnp.tanh)</span>
<span class="s1">Relu = elementwise(relu)</span>
<span class="s1">Exp = elementwise(jnp.exp)</span>
<span class="s1">LogSoftmax = elementwise(log_softmax</span><span class="s3">, </span><span class="s1">axis=-</span><span class="s4">1</span><span class="s1">)</span>
<span class="s1">Softmax = elementwise(softmax</span><span class="s3">, </span><span class="s1">axis=-</span><span class="s4">1</span><span class="s1">)</span>
<span class="s1">Softplus = elementwise(softplus)</span>
<span class="s1">Sigmoid = elementwise(sigmoid)</span>
<span class="s1">Elu = elementwise(elu)</span>
<span class="s1">LeakyRelu = elementwise(leaky_relu)</span>
<span class="s1">Selu = elementwise(selu)</span>
<span class="s1">Gelu = elementwise(gelu)</span>


<span class="s3">def </span><span class="s1">_pooling_layer(reducer</span><span class="s3">, </span><span class="s1">init_val</span><span class="s3">, </span><span class="s1">rescaler=</span><span class="s3">None</span><span class="s1">):</span>
  <span class="s3">def </span><span class="s1">PoolingLayer(window_shape</span><span class="s3">, </span><span class="s1">strides=</span><span class="s3">None, </span><span class="s1">padding=</span><span class="s5">'VALID'</span><span class="s3">, </span><span class="s1">spec=</span><span class="s3">None</span><span class="s1">):</span>
    <span class="s2">&quot;&quot;&quot;Layer construction function for a pooling layer.&quot;&quot;&quot;</span>
    <span class="s1">strides = strides </span><span class="s3">or </span><span class="s1">(</span><span class="s4">1</span><span class="s3">,</span><span class="s1">) * len(window_shape)</span>
    <span class="s1">rescale = rescaler(window_shape</span><span class="s3">, </span><span class="s1">strides</span><span class="s3">, </span><span class="s1">padding) </span><span class="s3">if </span><span class="s1">rescaler </span><span class="s3">else None</span>

    <span class="s3">if </span><span class="s1">spec </span><span class="s3">is None</span><span class="s1">:</span>
      <span class="s1">non_spatial_axes = </span><span class="s4">0</span><span class="s3">, </span><span class="s1">len(window_shape) + </span><span class="s4">1</span>
    <span class="s3">else</span><span class="s1">:</span>
      <span class="s1">non_spatial_axes = spec.index(</span><span class="s5">'N'</span><span class="s1">)</span><span class="s3">, </span><span class="s1">spec.index(</span><span class="s5">'C'</span><span class="s1">)</span>

    <span class="s3">for </span><span class="s1">i </span><span class="s3">in </span><span class="s1">sorted(non_spatial_axes):</span>
      <span class="s1">window_shape = window_shape[:i] + (</span><span class="s4">1</span><span class="s3">,</span><span class="s1">) + window_shape[i:]</span>
      <span class="s1">strides = strides[:i] + (</span><span class="s4">1</span><span class="s3">,</span><span class="s1">) + strides[i:]</span>

    <span class="s3">def </span><span class="s1">init_fun(rng</span><span class="s3">, </span><span class="s1">input_shape):</span>
      <span class="s1">padding_vals = lax.padtype_to_pads(input_shape</span><span class="s3">, </span><span class="s1">window_shape</span><span class="s3">,</span>
                                         <span class="s1">strides</span><span class="s3">, </span><span class="s1">padding)</span>
      <span class="s1">ones = (</span><span class="s4">1</span><span class="s3">,</span><span class="s1">) * len(window_shape)</span>
      <span class="s1">out_shape = lax.reduce_window_shape_tuple(</span>
        <span class="s1">input_shape</span><span class="s3">, </span><span class="s1">window_shape</span><span class="s3">, </span><span class="s1">strides</span><span class="s3">, </span><span class="s1">padding_vals</span><span class="s3">, </span><span class="s1">ones</span><span class="s3">, </span><span class="s1">ones)</span>
      <span class="s3">return </span><span class="s1">out_shape</span><span class="s3">, </span><span class="s1">()</span>
    <span class="s3">def </span><span class="s1">apply_fun(params</span><span class="s3">, </span><span class="s1">inputs</span><span class="s3">, </span><span class="s1">**kwargs):</span>
      <span class="s1">out = lax.reduce_window(inputs</span><span class="s3">, </span><span class="s1">init_val</span><span class="s3">, </span><span class="s1">reducer</span><span class="s3">, </span><span class="s1">window_shape</span><span class="s3">,</span>
                              <span class="s1">strides</span><span class="s3">, </span><span class="s1">padding)</span>
      <span class="s3">return </span><span class="s1">rescale(out</span><span class="s3">, </span><span class="s1">inputs</span><span class="s3">, </span><span class="s1">spec) </span><span class="s3">if </span><span class="s1">rescale </span><span class="s3">else </span><span class="s1">out</span>
    <span class="s3">return </span><span class="s1">init_fun</span><span class="s3">, </span><span class="s1">apply_fun</span>
  <span class="s3">return </span><span class="s1">PoolingLayer</span>
<span class="s1">MaxPool = _pooling_layer(lax.max</span><span class="s3">, </span><span class="s1">-jnp.inf)</span>
<span class="s1">SumPool = _pooling_layer(lax.add</span><span class="s3">, </span><span class="s4">0.</span><span class="s1">)</span>


<span class="s3">def </span><span class="s1">_normalize_by_window_size(dims</span><span class="s3">, </span><span class="s1">strides</span><span class="s3">, </span><span class="s1">padding):</span>
  <span class="s3">def </span><span class="s1">rescale(outputs</span><span class="s3">, </span><span class="s1">inputs</span><span class="s3">, </span><span class="s1">spec):</span>
    <span class="s3">if </span><span class="s1">spec </span><span class="s3">is None</span><span class="s1">:</span>
      <span class="s1">non_spatial_axes = </span><span class="s4">0</span><span class="s3">, </span><span class="s1">inputs.ndim - </span><span class="s4">1</span>
    <span class="s3">else</span><span class="s1">:</span>
      <span class="s1">non_spatial_axes = spec.index(</span><span class="s5">'N'</span><span class="s1">)</span><span class="s3">, </span><span class="s1">spec.index(</span><span class="s5">'C'</span><span class="s1">)</span>

    <span class="s1">spatial_shape = tuple(inputs.shape[i]</span>
                          <span class="s3">for </span><span class="s1">i </span><span class="s3">in </span><span class="s1">range(inputs.ndim)</span>
                          <span class="s3">if </span><span class="s1">i </span><span class="s3">not in </span><span class="s1">non_spatial_axes)</span>
    <span class="s1">one = jnp.ones(spatial_shape</span><span class="s3">, </span><span class="s1">dtype=inputs.dtype)</span>
    <span class="s1">window_sizes = lax.reduce_window(one</span><span class="s3">, </span><span class="s4">0.</span><span class="s3">, </span><span class="s1">lax.add</span><span class="s3">, </span><span class="s1">dims</span><span class="s3">, </span><span class="s1">strides</span><span class="s3">, </span><span class="s1">padding)</span>
    <span class="s3">for </span><span class="s1">i </span><span class="s3">in </span><span class="s1">sorted(non_spatial_axes):</span>
      <span class="s1">window_sizes = jnp.expand_dims(window_sizes</span><span class="s3">, </span><span class="s1">i)</span>

    <span class="s3">return </span><span class="s1">outputs / window_sizes</span>
  <span class="s3">return </span><span class="s1">rescale</span>
<span class="s1">AvgPool = _pooling_layer(lax.add</span><span class="s3">, </span><span class="s4">0.</span><span class="s3">, </span><span class="s1">_normalize_by_window_size)</span>


<span class="s3">def </span><span class="s1">Flatten():</span>
  <span class="s2">&quot;&quot;&quot;Layer construction function for flattening all but the leading dim.&quot;&quot;&quot;</span>
  <span class="s3">def </span><span class="s1">init_fun(rng</span><span class="s3">, </span><span class="s1">input_shape):</span>
    <span class="s1">output_shape = input_shape[</span><span class="s4">0</span><span class="s1">]</span><span class="s3">, </span><span class="s1">functools.reduce(op.mul</span><span class="s3">, </span><span class="s1">input_shape[</span><span class="s4">1</span><span class="s1">:]</span><span class="s3">, </span><span class="s4">1</span><span class="s1">)</span>
    <span class="s3">return </span><span class="s1">output_shape</span><span class="s3">, </span><span class="s1">()</span>
  <span class="s3">def </span><span class="s1">apply_fun(params</span><span class="s3">, </span><span class="s1">inputs</span><span class="s3">, </span><span class="s1">**kwargs):</span>
    <span class="s3">return </span><span class="s1">jnp.reshape(inputs</span><span class="s3">, </span><span class="s1">(inputs.shape[</span><span class="s4">0</span><span class="s1">]</span><span class="s3">, </span><span class="s1">-</span><span class="s4">1</span><span class="s1">))</span>
  <span class="s3">return </span><span class="s1">init_fun</span><span class="s3">, </span><span class="s1">apply_fun</span>
<span class="s1">Flatten = Flatten()</span>


<span class="s3">def </span><span class="s1">Identity():</span>
  <span class="s2">&quot;&quot;&quot;Layer construction function for an identity layer.&quot;&quot;&quot;</span>
  <span class="s1">init_fun = </span><span class="s3">lambda </span><span class="s1">rng</span><span class="s3">, </span><span class="s1">input_shape: (input_shape</span><span class="s3">, </span><span class="s1">())</span>
  <span class="s1">apply_fun = </span><span class="s3">lambda </span><span class="s1">params</span><span class="s3">, </span><span class="s1">inputs</span><span class="s3">, </span><span class="s1">**kwargs: inputs</span>
  <span class="s3">return </span><span class="s1">init_fun</span><span class="s3">, </span><span class="s1">apply_fun</span>
<span class="s1">Identity = Identity()</span>


<span class="s3">def </span><span class="s1">FanOut(num):</span>
  <span class="s2">&quot;&quot;&quot;Layer construction function for a fan-out layer.&quot;&quot;&quot;</span>
  <span class="s1">init_fun = </span><span class="s3">lambda </span><span class="s1">rng</span><span class="s3">, </span><span class="s1">input_shape: ([input_shape] * num</span><span class="s3">, </span><span class="s1">())</span>
  <span class="s1">apply_fun = </span><span class="s3">lambda </span><span class="s1">params</span><span class="s3">, </span><span class="s1">inputs</span><span class="s3">, </span><span class="s1">**kwargs: [inputs] * num</span>
  <span class="s3">return </span><span class="s1">init_fun</span><span class="s3">, </span><span class="s1">apply_fun</span>


<span class="s3">def </span><span class="s1">FanInSum():</span>
  <span class="s2">&quot;&quot;&quot;Layer construction function for a fan-in sum layer.&quot;&quot;&quot;</span>
  <span class="s1">init_fun = </span><span class="s3">lambda </span><span class="s1">rng</span><span class="s3">, </span><span class="s1">input_shape: (input_shape[</span><span class="s4">0</span><span class="s1">]</span><span class="s3">, </span><span class="s1">())</span>
  <span class="s1">apply_fun = </span><span class="s3">lambda </span><span class="s1">params</span><span class="s3">, </span><span class="s1">inputs</span><span class="s3">, </span><span class="s1">**kwargs: sum(inputs)</span>
  <span class="s3">return </span><span class="s1">init_fun</span><span class="s3">, </span><span class="s1">apply_fun</span>
<span class="s1">FanInSum = FanInSum()</span>


<span class="s3">def </span><span class="s1">FanInConcat(axis=-</span><span class="s4">1</span><span class="s1">):</span>
  <span class="s2">&quot;&quot;&quot;Layer construction function for a fan-in concatenation layer.&quot;&quot;&quot;</span>
  <span class="s3">def </span><span class="s1">init_fun(rng</span><span class="s3">, </span><span class="s1">input_shape):</span>
    <span class="s1">ax = axis % len(input_shape[</span><span class="s4">0</span><span class="s1">])</span>
    <span class="s1">concat_size = sum(shape[ax] </span><span class="s3">for </span><span class="s1">shape </span><span class="s3">in </span><span class="s1">input_shape)</span>
    <span class="s1">out_shape = input_shape[</span><span class="s4">0</span><span class="s1">][:ax] + (concat_size</span><span class="s3">,</span><span class="s1">) + input_shape[</span><span class="s4">0</span><span class="s1">][ax+</span><span class="s4">1</span><span class="s1">:]</span>
    <span class="s3">return </span><span class="s1">out_shape</span><span class="s3">, </span><span class="s1">()</span>
  <span class="s3">def </span><span class="s1">apply_fun(params</span><span class="s3">, </span><span class="s1">inputs</span><span class="s3">, </span><span class="s1">**kwargs):</span>
    <span class="s3">return </span><span class="s1">jnp.concatenate(inputs</span><span class="s3">, </span><span class="s1">axis)</span>
  <span class="s3">return </span><span class="s1">init_fun</span><span class="s3">, </span><span class="s1">apply_fun</span>


<span class="s3">def </span><span class="s1">Dropout(rate</span><span class="s3">, </span><span class="s1">mode=</span><span class="s5">'train'</span><span class="s1">):</span>
  <span class="s2">&quot;&quot;&quot;Layer construction function for a dropout layer with given rate.&quot;&quot;&quot;</span>
  <span class="s3">def </span><span class="s1">init_fun(rng</span><span class="s3">, </span><span class="s1">input_shape):</span>
    <span class="s3">return </span><span class="s1">input_shape</span><span class="s3">, </span><span class="s1">()</span>
  <span class="s3">def </span><span class="s1">apply_fun(params</span><span class="s3">, </span><span class="s1">inputs</span><span class="s3">, </span><span class="s1">**kwargs):</span>
    <span class="s1">rng = kwargs.get(</span><span class="s5">'rng'</span><span class="s3">, None</span><span class="s1">)</span>
    <span class="s3">if </span><span class="s1">rng </span><span class="s3">is None</span><span class="s1">:</span>
      <span class="s1">msg = (</span><span class="s5">&quot;Dropout layer requires apply_fun to be called with a PRNG key &quot;</span>
             <span class="s5">&quot;argument. That is, instead of `apply_fun(params, inputs)`, call &quot;</span>
             <span class="s5">&quot;it like `apply_fun(params, inputs, rng)` where `rng` is a &quot;</span>
             <span class="s5">&quot;jax.random.PRNGKey value.&quot;</span><span class="s1">)</span>
      <span class="s3">raise </span><span class="s1">ValueError(msg)</span>
    <span class="s3">if </span><span class="s1">mode == </span><span class="s5">'train'</span><span class="s1">:</span>
      <span class="s1">keep = random.bernoulli(rng</span><span class="s3">, </span><span class="s1">rate</span><span class="s3">, </span><span class="s1">inputs.shape)</span>
      <span class="s3">return </span><span class="s1">jnp.where(keep</span><span class="s3">, </span><span class="s1">inputs / rate</span><span class="s3">, </span><span class="s4">0</span><span class="s1">)</span>
    <span class="s3">else</span><span class="s1">:</span>
      <span class="s3">return </span><span class="s1">inputs</span>
  <span class="s3">return </span><span class="s1">init_fun</span><span class="s3">, </span><span class="s1">apply_fun</span>


<span class="s0"># Composing layers via combinators</span>


<span class="s3">def </span><span class="s1">serial(*layers):</span>
  <span class="s2">&quot;&quot;&quot;Combinator for composing layers in serial. 
 
  Args: 
    *layers: a sequence of layers, each an (init_fun, apply_fun) pair. 
 
  Returns: 
    A new layer, meaning an (init_fun, apply_fun) pair, representing the serial 
    composition of the given sequence of layers. 
  &quot;&quot;&quot;</span>
  <span class="s1">nlayers = len(layers)</span>
  <span class="s1">init_funs</span><span class="s3">, </span><span class="s1">apply_funs = zip(*layers)</span>
  <span class="s3">def </span><span class="s1">init_fun(rng</span><span class="s3">, </span><span class="s1">input_shape):</span>
    <span class="s1">params = []</span>
    <span class="s3">for </span><span class="s1">init_fun </span><span class="s3">in </span><span class="s1">init_funs:</span>
      <span class="s1">rng</span><span class="s3">, </span><span class="s1">layer_rng = random.split(rng)</span>
      <span class="s1">input_shape</span><span class="s3">, </span><span class="s1">param = init_fun(layer_rng</span><span class="s3">, </span><span class="s1">input_shape)</span>
      <span class="s1">params.append(param)</span>
    <span class="s3">return </span><span class="s1">input_shape</span><span class="s3">, </span><span class="s1">params</span>
  <span class="s3">def </span><span class="s1">apply_fun(params</span><span class="s3">, </span><span class="s1">inputs</span><span class="s3">, </span><span class="s1">**kwargs):</span>
    <span class="s1">rng = kwargs.pop(</span><span class="s5">'rng'</span><span class="s3">, None</span><span class="s1">)</span>
    <span class="s1">rngs = random.split(rng</span><span class="s3">, </span><span class="s1">nlayers) </span><span class="s3">if </span><span class="s1">rng </span><span class="s3">is not None else </span><span class="s1">(</span><span class="s3">None,</span><span class="s1">) * nlayers</span>
    <span class="s3">for </span><span class="s1">fun</span><span class="s3">, </span><span class="s1">param</span><span class="s3">, </span><span class="s1">rng </span><span class="s3">in </span><span class="s1">zip(apply_funs</span><span class="s3">, </span><span class="s1">params</span><span class="s3">, </span><span class="s1">rngs):</span>
      <span class="s1">inputs = fun(param</span><span class="s3">, </span><span class="s1">inputs</span><span class="s3">, </span><span class="s1">rng=rng</span><span class="s3">, </span><span class="s1">**kwargs)</span>
    <span class="s3">return </span><span class="s1">inputs</span>
  <span class="s3">return </span><span class="s1">init_fun</span><span class="s3">, </span><span class="s1">apply_fun</span>


<span class="s3">def </span><span class="s1">parallel(*layers):</span>
  <span class="s2">&quot;&quot;&quot;Combinator for composing layers in parallel. 
 
  The layer resulting from this combinator is often used with the FanOut and 
  FanInSum layers. 
 
  Args: 
    *layers: a sequence of layers, each an (init_fun, apply_fun) pair. 
 
  Returns: 
    A new layer, meaning an (init_fun, apply_fun) pair, representing the 
    parallel composition of the given sequence of layers. In particular, the 
    returned layer takes a sequence of inputs and returns a sequence of outputs 
    with the same length as the argument `layers`. 
  &quot;&quot;&quot;</span>
  <span class="s1">nlayers = len(layers)</span>
  <span class="s1">init_funs</span><span class="s3">, </span><span class="s1">apply_funs = zip(*layers)</span>
  <span class="s3">def </span><span class="s1">init_fun(rng</span><span class="s3">, </span><span class="s1">input_shape):</span>
    <span class="s1">rngs = random.split(rng</span><span class="s3">, </span><span class="s1">nlayers)</span>
    <span class="s3">return </span><span class="s1">zip(*[init(rng</span><span class="s3">, </span><span class="s1">shape) </span><span class="s3">for </span><span class="s1">init</span><span class="s3">, </span><span class="s1">rng</span><span class="s3">, </span><span class="s1">shape</span>
                 <span class="s3">in </span><span class="s1">zip(init_funs</span><span class="s3">, </span><span class="s1">rngs</span><span class="s3">, </span><span class="s1">input_shape)])</span>
  <span class="s3">def </span><span class="s1">apply_fun(params</span><span class="s3">, </span><span class="s1">inputs</span><span class="s3">, </span><span class="s1">**kwargs):</span>
    <span class="s1">rng = kwargs.pop(</span><span class="s5">'rng'</span><span class="s3">, None</span><span class="s1">)</span>
    <span class="s1">rngs = random.split(rng</span><span class="s3">, </span><span class="s1">nlayers) </span><span class="s3">if </span><span class="s1">rng </span><span class="s3">is not None else </span><span class="s1">(</span><span class="s3">None,</span><span class="s1">) * nlayers</span>
    <span class="s3">return </span><span class="s1">[f(p</span><span class="s3">, </span><span class="s1">x</span><span class="s3">, </span><span class="s1">rng=r</span><span class="s3">, </span><span class="s1">**kwargs) </span><span class="s3">for </span><span class="s1">f</span><span class="s3">, </span><span class="s1">p</span><span class="s3">, </span><span class="s1">x</span><span class="s3">, </span><span class="s1">r </span><span class="s3">in </span><span class="s1">zip(apply_funs</span><span class="s3">, </span><span class="s1">params</span><span class="s3">, </span><span class="s1">inputs</span><span class="s3">, </span><span class="s1">rngs)]</span>
  <span class="s3">return </span><span class="s1">init_fun</span><span class="s3">, </span><span class="s1">apply_fun</span>


<span class="s3">def </span><span class="s1">shape_dependent(make_layer):</span>
  <span class="s2">&quot;&quot;&quot;Combinator to delay layer constructor pair until input shapes are known. 
 
  Args: 
    make_layer: a one-argument function that takes an input shape as an argument 
      (a tuple of positive integers) and returns an (init_fun, apply_fun) pair. 
 
  Returns: 
    A new layer, meaning an (init_fun, apply_fun) pair, representing the same 
    layer as returned by `make_layer` but with its construction delayed until 
    input shapes are known. 
  &quot;&quot;&quot;</span>
  <span class="s3">def </span><span class="s1">init_fun(rng</span><span class="s3">, </span><span class="s1">input_shape):</span>
    <span class="s3">return </span><span class="s1">make_layer(input_shape)[</span><span class="s4">0</span><span class="s1">](rng</span><span class="s3">, </span><span class="s1">input_shape)</span>
  <span class="s3">def </span><span class="s1">apply_fun(params</span><span class="s3">, </span><span class="s1">inputs</span><span class="s3">, </span><span class="s1">**kwargs):</span>
    <span class="s3">return </span><span class="s1">make_layer(inputs.shape)[</span><span class="s4">1</span><span class="s1">](params</span><span class="s3">, </span><span class="s1">inputs</span><span class="s3">, </span><span class="s1">**kwargs)</span>
  <span class="s3">return </span><span class="s1">init_fun</span><span class="s3">, </span><span class="s1">apply_fun</span>
</pre>
</body>
</html>