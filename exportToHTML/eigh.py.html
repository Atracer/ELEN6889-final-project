<html>
<head>
<title>eigh.py</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<style type="text/css">
.s0 { color: #808080;}
.s1 { color: #a9b7c6;}
.s2 { color: #629755; font-style: italic;}
.s3 { color: #cc7832;}
.s4 { color: #6897bb;}
.s5 { color: #6a8759;}
</style>
</head>
<body bgcolor="#2b2b2b">
<table CELLSPACING=0 CELLPADDING=5 COLS=1 WIDTH="100%" BGCOLOR="#606060" >
<tr><td><center>
<font face="Arial, Helvetica" color="#000000">
eigh.py</font>
</center></td></tr></table>
<pre><span class="s0"># Copyright 2021 The JAX Authors.</span>
<span class="s0">#</span>
<span class="s0"># Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);</span>
<span class="s0"># you may not use this file except in compliance with the License.</span>
<span class="s0"># You may obtain a copy of the License at</span>
<span class="s0">#</span>
<span class="s0">#     https://www.apache.org/licenses/LICENSE-2.0</span>
<span class="s0">#</span>
<span class="s0"># Unless required by applicable law or agreed to in writing, software</span>
<span class="s0"># distributed under the License is distributed on an &quot;AS IS&quot; BASIS,</span>
<span class="s0"># WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span>
<span class="s0"># See the License for the specific language governing permissions and</span>
<span class="s0"># limitations under the License</span>

<span class="s2">&quot;&quot;&quot;Symmetric (Hermitian) eigendecomposition using QDWH 
 
References: 
Nakatsukasa, Yuji, and Nicholas J. Higham. 
&quot;Stable and efficient spectral divide and conquer algorithms for the symmetric 
eigenvalue decomposition and the SVD.&quot; SIAM Journal on Scientific Computing 35, 
no. 3 (2013): A1325-A1349. 
https://epubs.siam.org/doi/abs/10.1137/120876605 
 
This implementation is primarily used on TPU, but it can in principle work on 
CPU and GPU also. 
&quot;&quot;&quot;</span>

<span class="s3">from </span><span class="s1">__future__ </span><span class="s3">import </span><span class="s1">annotations</span>

<span class="s3">from </span><span class="s1">functools </span><span class="s3">import </span><span class="s1">partial</span>
<span class="s3">from </span><span class="s1">typing </span><span class="s3">import </span><span class="s1">NamedTuple</span>

<span class="s3">import </span><span class="s1">jax</span>
<span class="s3">import </span><span class="s1">jax._src.numpy.lax_numpy </span><span class="s3">as </span><span class="s1">jnp</span>
<span class="s3">import </span><span class="s1">jax._src.numpy.linalg </span><span class="s3">as </span><span class="s1">jnp_linalg</span>
<span class="s3">from </span><span class="s1">jax._src.numpy </span><span class="s3">import </span><span class="s1">reductions</span>
<span class="s3">from </span><span class="s1">jax._src.numpy </span><span class="s3">import </span><span class="s1">ufuncs</span>
<span class="s3">from </span><span class="s1">jax </span><span class="s3">import </span><span class="s1">lax</span>
<span class="s3">from </span><span class="s1">jax._src.lax </span><span class="s3">import </span><span class="s1">qdwh</span>
<span class="s3">from </span><span class="s1">jax._src.lax </span><span class="s3">import </span><span class="s1">linalg </span><span class="s3">as </span><span class="s1">lax_linalg</span>
<span class="s3">from </span><span class="s1">jax._src.lax.stack </span><span class="s3">import </span><span class="s1">Stack</span>


<span class="s0"># QDWH-eigh is a recursive algorithm where the structure of the recursion</span>
<span class="s0"># is determined by the eigenspectrum. Neither JAX nor XLA can handle this kind</span>
<span class="s0"># of recursion, so we instead express the recursion as iteration using an</span>
<span class="s0"># explicit stack.</span>


<span class="s0"># TODO(phawkins): consider extracting _mask/_slice/_update_slice into a</span>
<span class="s0"># separate module.</span>

<span class="s3">def </span><span class="s1">_mask(x</span><span class="s3">, </span><span class="s1">dims</span><span class="s3">, </span><span class="s1">alternative=</span><span class="s4">0</span><span class="s1">):</span>
  <span class="s2">&quot;&quot;&quot;Masks `x` up to the dynamic shape `dims`. 
 
  Replaces values outside those dimensions with `alternative`. `alternative` is 
  broadcast with `x`. 
  &quot;&quot;&quot;</span>
  <span class="s3">assert </span><span class="s1">jnp.ndim(x) == len(dims)</span>
  <span class="s1">mask = </span><span class="s3">None</span>
  <span class="s3">for </span><span class="s1">i</span><span class="s3">, </span><span class="s1">d </span><span class="s3">in </span><span class="s1">enumerate(dims):</span>
    <span class="s3">if </span><span class="s1">d </span><span class="s3">is not None</span><span class="s1">:</span>
      <span class="s1">mask_dim_i = lax.broadcasted_iota(jnp.int32</span><span class="s3">, </span><span class="s1">x.shape</span><span class="s3">, </span><span class="s1">i) &lt; d</span>
      <span class="s1">mask = mask_dim_i </span><span class="s3">if </span><span class="s1">mask </span><span class="s3">is None else </span><span class="s1">(mask &amp; mask_dim_i)</span>
  <span class="s3">return </span><span class="s1">x </span><span class="s3">if </span><span class="s1">mask </span><span class="s3">is None else </span><span class="s1">jnp.where(mask</span><span class="s3">, </span><span class="s1">x</span><span class="s3">, </span><span class="s1">alternative)</span>

<span class="s3">def </span><span class="s1">_slice(operand</span><span class="s3">, </span><span class="s1">start_indices</span><span class="s3">, </span><span class="s1">dynamic_slice_sizes</span><span class="s3">, </span><span class="s1">static_slice_sizes</span><span class="s3">,</span>
           <span class="s1">fill_value=</span><span class="s4">0</span><span class="s1">):</span>
  <span class="s2">&quot;&quot;&quot;Similar to lax.dynamic_slice, but handles arrays with dynamic sizes. 
 
  Returns fill_value instead of clamping start_indices for those elements that 
  would overflow the side of the array. 
 
  Args: 
    operand: the array to slice 
    start_indices: the offset of the start of the slice 
    dynamic_slice_sizes: the true (unpadded) size of the slice 
    static_slice_sizes: the padded size of the slice, which must be known at 
      compile time. The static size must be larger than the dynamic size. 
    fill_value: value with which to replace masked-out elements. 
  Returns: 
    An array with static shape `static_slice_sizes`, padded from its true 
    (dynamic) size `dynamic_slice_sizes`. 
  &quot;&quot;&quot;</span>
  <span class="s0"># We must pad the input array so the dynamic_slice is guaranteed to fall</span>
  <span class="s0"># entirely in bounds.</span>
  <span class="s1">padded = lax.pad(operand</span><span class="s3">,</span>
                   <span class="s1">jnp.array(</span><span class="s4">0</span><span class="s3">, </span><span class="s1">operand.dtype)</span><span class="s3">,</span>
                   <span class="s1">[(</span><span class="s4">0</span><span class="s3">, </span><span class="s1">d</span><span class="s3">, </span><span class="s4">0</span><span class="s1">) </span><span class="s3">for </span><span class="s1">d </span><span class="s3">in </span><span class="s1">static_slice_sizes])</span>
  <span class="s1">out = lax.dynamic_slice(padded</span><span class="s3">, </span><span class="s1">tuple(jnp.int32(i) </span><span class="s3">for </span><span class="s1">i </span><span class="s3">in </span><span class="s1">start_indices)</span><span class="s3">,</span>
                          <span class="s1">static_slice_sizes)</span>
  <span class="s3">return </span><span class="s1">_mask(out</span><span class="s3">, </span><span class="s1">dynamic_slice_sizes</span><span class="s3">, </span><span class="s1">fill_value)</span>

<span class="s3">def </span><span class="s1">_update_slice(operand</span><span class="s3">, </span><span class="s1">update</span><span class="s3">, </span><span class="s1">start_indices</span><span class="s3">, </span><span class="s1">update_dims):</span>
  <span class="s2">&quot;&quot;&quot; 
  Similar to lax.dynamic_update_slice, but handles padded updates where padding 
  values should not overwrite existing values in the array. 
 
  Args: 
  operand: the array to update 
  update: the padded array to write 
  start_indices: the offset at which to write `update`. 
  update_dims: the true dimensions of the padded update `update`. Only values 
    inside the rectangle given by `update_dims` will be overwritten.&quot;&quot;&quot;</span>
  <span class="s1">operand_shape = operand.shape</span>
  <span class="s1">operand = lax.pad(operand</span><span class="s3">,</span>
                    <span class="s1">jnp.array(</span><span class="s4">0</span><span class="s3">, </span><span class="s1">operand.dtype)</span><span class="s3">,</span>
                    <span class="s1">[(</span><span class="s4">0</span><span class="s3">, </span><span class="s1">d</span><span class="s3">, </span><span class="s4">0</span><span class="s1">) </span><span class="s3">for </span><span class="s1">d </span><span class="s3">in </span><span class="s1">update.shape])</span>
  <span class="s1">start_indices = tuple(jnp.int32(i) </span><span class="s3">for </span><span class="s1">i </span><span class="s3">in </span><span class="s1">start_indices)</span>
  <span class="s1">t = lax.dynamic_slice(operand</span><span class="s3">, </span><span class="s1">start_indices</span><span class="s3">, </span><span class="s1">update.shape)</span>
  <span class="s1">t = _mask(update</span><span class="s3">, </span><span class="s1">update_dims</span><span class="s3">, </span><span class="s1">t)</span>
  <span class="s1">operand = lax.dynamic_update_slice(operand</span><span class="s3">, </span><span class="s1">t</span><span class="s3">, </span><span class="s1">start_indices)</span>
  <span class="s3">return </span><span class="s1">lax.slice(operand</span><span class="s3">, </span><span class="s1">[</span><span class="s4">0</span><span class="s1">] * operand.ndim</span><span class="s3">, </span><span class="s1">operand_shape)</span>


<span class="s3">def </span><span class="s1">_projector_subspace(P</span><span class="s3">, </span><span class="s1">H</span><span class="s3">, </span><span class="s1">n</span><span class="s3">, </span><span class="s1">rank</span><span class="s3">, </span><span class="s1">maxiter=</span><span class="s4">2</span><span class="s1">):</span>
  <span class="s2">&quot;&quot;&quot; Decomposes the `n x n` rank `rank` Hermitian projector `P` into 
  an `n x rank` isometry `V_minus` such that `P = V_minus @ V_minus.conj().T` 
  and an `n x (n - rank)` isometry `V_minus` such that 
  -(I - P) = V_plus @ V_plus.conj().T`. 
 
  The subspaces are computed using the naiive QR eigendecomposition 
  algorithm, which converges very quickly due to the sharp separation 
  between the relevant eigenvalues of the projector. 
 
  Args: 
    P: A rank-`rank` Hermitian projector into the space of `H`'s 
       first `rank` eigenpairs. `P` is padded to NxN. 
    H: The aforementioned Hermitian matrix, which is used to track 
       convergence. 
    n: the true (dynamic) shape of `P`. 
    rank: Rank of `P`. 
    maxiter: Maximum number of iterations. 
  Returns: 
    V_minus, V_plus: Isometries into the eigenspaces described in the docstring. 
  &quot;&quot;&quot;</span>
  <span class="s0"># Choose an initial guess: the `rank` largest-norm columns of P.</span>
  <span class="s1">N</span><span class="s3">, </span><span class="s1">_ = P.shape</span>
  <span class="s1">column_norms = jnp_linalg.norm(P</span><span class="s3">, </span><span class="s1">axis=</span><span class="s4">1</span><span class="s1">)</span>
  <span class="s0"># `jnp.argsort` ensures NaNs sort last, so set masked-out column norms to NaN.</span>
  <span class="s1">column_norms = _mask(column_norms</span><span class="s3">, </span><span class="s1">(n</span><span class="s3">,</span><span class="s1">)</span><span class="s3">, </span><span class="s1">jnp.nan)</span>
  <span class="s1">sort_idxs = jnp.argsort(column_norms)</span>
  <span class="s1">X = P[:</span><span class="s3">, </span><span class="s1">sort_idxs]</span>
  <span class="s0"># X = X[:, :rank]</span>
  <span class="s1">X = _mask(X</span><span class="s3">, </span><span class="s1">(n</span><span class="s3">, </span><span class="s1">rank))</span>

  <span class="s1">H_norm = jnp_linalg.norm(H)</span>
  <span class="s1">thresh = </span><span class="s4">10.0 </span><span class="s1">* float(jnp.finfo(X.dtype).eps) * H_norm</span>

  <span class="s0"># First iteration skips the matmul.</span>
  <span class="s3">def </span><span class="s1">body_f_after_matmul(X):</span>
    <span class="s1">Q</span><span class="s3">, </span><span class="s1">_ = jnp_linalg.qr(X</span><span class="s3">, </span><span class="s1">mode=</span><span class="s5">&quot;complete&quot;</span><span class="s1">)</span>
    <span class="s0"># V1 = Q[:, :rank]</span>
    <span class="s0"># V2 = Q[:, rank:]</span>
    <span class="s1">V1 = _mask(Q</span><span class="s3">, </span><span class="s1">(n</span><span class="s3">, </span><span class="s1">rank))</span>
    <span class="s1">V2 = _slice(Q</span><span class="s3">, </span><span class="s1">(</span><span class="s4">0</span><span class="s3">, </span><span class="s1">rank)</span><span class="s3">, </span><span class="s1">(n</span><span class="s3">, </span><span class="s1">n - rank)</span><span class="s3">, </span><span class="s1">(N</span><span class="s3">, </span><span class="s1">N))</span>

    <span class="s0"># TODO: might be able to get away with lower precision here</span>
    <span class="s1">error_matrix = jnp.dot(V2.conj().T</span><span class="s3">, </span><span class="s1">H)</span>
    <span class="s1">error_matrix = jnp.dot(error_matrix</span><span class="s3">, </span><span class="s1">V1)</span>
    <span class="s1">error = jnp_linalg.norm(error_matrix) / H_norm</span>
    <span class="s3">return </span><span class="s1">V1</span><span class="s3">, </span><span class="s1">V2</span><span class="s3">, </span><span class="s1">error</span>

  <span class="s3">def </span><span class="s1">cond_f(args):</span>
    <span class="s1">_</span><span class="s3">, </span><span class="s1">_</span><span class="s3">, </span><span class="s1">j</span><span class="s3">, </span><span class="s1">error = args</span>
    <span class="s1">still_counting = j &lt; maxiter</span>
    <span class="s1">unconverged = error &gt; thresh</span>
    <span class="s3">return </span><span class="s1">ufuncs.logical_and(still_counting</span><span class="s3">, </span><span class="s1">unconverged)[</span><span class="s4">0</span><span class="s1">]</span>

  <span class="s3">def </span><span class="s1">body_f(args):</span>
    <span class="s1">V1</span><span class="s3">, </span><span class="s1">_</span><span class="s3">, </span><span class="s1">j</span><span class="s3">, </span><span class="s1">_ = args</span>
    <span class="s1">X = jnp.dot(P</span><span class="s3">, </span><span class="s1">V1)</span>
    <span class="s1">V1</span><span class="s3">, </span><span class="s1">V2</span><span class="s3">, </span><span class="s1">error = body_f_after_matmul(X)</span>
    <span class="s3">return </span><span class="s1">V1</span><span class="s3">, </span><span class="s1">V2</span><span class="s3">, </span><span class="s1">j + </span><span class="s4">1</span><span class="s3">, </span><span class="s1">error</span>

  <span class="s1">V1</span><span class="s3">, </span><span class="s1">V2</span><span class="s3">, </span><span class="s1">error = body_f_after_matmul(X)</span>
  <span class="s1">one = jnp.ones(</span><span class="s4">1</span><span class="s3">, </span><span class="s1">dtype=jnp.int32)</span>
  <span class="s1">V1</span><span class="s3">, </span><span class="s1">V2</span><span class="s3">, </span><span class="s1">_</span><span class="s3">, </span><span class="s1">error = lax.while_loop(cond_f</span><span class="s3">, </span><span class="s1">body_f</span><span class="s3">, </span><span class="s1">(V1</span><span class="s3">, </span><span class="s1">V2</span><span class="s3">, </span><span class="s1">one</span><span class="s3">, </span><span class="s1">error))</span>
  <span class="s3">return </span><span class="s1">V1</span><span class="s3">, </span><span class="s1">V2</span>


<span class="s3">def </span><span class="s1">split_spectrum(H</span><span class="s3">, </span><span class="s1">n</span><span class="s3">, </span><span class="s1">split_point</span><span class="s3">, </span><span class="s1">V0=</span><span class="s3">None</span><span class="s1">):</span>
  <span class="s2">&quot;&quot;&quot; The Hermitian matrix `H` is split into two matrices `H_minus` 
  `H_plus`, respectively sharing its eigenspaces beneath and above 
  its `split_point`th eigenvalue. 
 
  Returns, in addition, `V_minus` and `V_plus`, isometries such that 
  `Hi = Vi.conj().T @ H @ Vi`. If `V0` is not None, `V0 @ Vi` are 
  returned instead; this allows the overall isometries mapping from 
  an initial input matrix to progressively smaller blocks to be formed. 
 
  Args: 
    H: The Hermitian matrix to split. 
    split_point: The eigenvalue to split along. 
    V0: Matrix of isometries to be updated. 
  Returns: 
    H_minus: A Hermitian matrix sharing the eigenvalues of `H` beneath 
      `split_point`. 
    V_minus: An isometry from the input space of `V0` to `H_minus`. 
    H_plus: A Hermitian matrix sharing the eigenvalues of `H` above 
      `split_point`. 
    V_plus: An isometry from the input space of `V0` to `H_plus`. 
    rank: The dynamic size of the m subblock. 
  &quot;&quot;&quot;</span>
  <span class="s1">N</span><span class="s3">, </span><span class="s1">_ = H.shape</span>
  <span class="s1">H_shift = H - (split_point * jnp.eye(N</span><span class="s3">, </span><span class="s1">dtype=split_point.dtype)).astype(H.dtype)</span>
  <span class="s1">U</span><span class="s3">, </span><span class="s1">_</span><span class="s3">, </span><span class="s1">_</span><span class="s3">, </span><span class="s1">_ = qdwh.qdwh(H_shift</span><span class="s3">, </span><span class="s1">is_hermitian=</span><span class="s3">True, </span><span class="s1">dynamic_shape=(n</span><span class="s3">, </span><span class="s1">n))</span>
  <span class="s1">P = -</span><span class="s4">0.5 </span><span class="s1">* (U - _mask(jnp.eye(N</span><span class="s3">, </span><span class="s1">dtype=H.dtype)</span><span class="s3">, </span><span class="s1">(n</span><span class="s3">, </span><span class="s1">n)))</span>
  <span class="s1">rank = jnp.round(jnp.trace(ufuncs.real(P))).astype(jnp.int32)</span>

  <span class="s1">V_minus</span><span class="s3">, </span><span class="s1">V_plus = _projector_subspace(P</span><span class="s3">, </span><span class="s1">H</span><span class="s3">, </span><span class="s1">n</span><span class="s3">, </span><span class="s1">rank)</span>
  <span class="s1">H_minus = (V_minus.conj().T @ H) @ V_minus</span>
  <span class="s1">H_plus = (V_plus.conj().T @ H) @ V_plus</span>
  <span class="s3">if </span><span class="s1">V0 </span><span class="s3">is not None</span><span class="s1">:</span>
    <span class="s1">V_minus = jnp.dot(V0</span><span class="s3">, </span><span class="s1">V_minus)</span>
    <span class="s1">V_plus = jnp.dot(V0</span><span class="s3">, </span><span class="s1">V_plus)</span>
  <span class="s3">return </span><span class="s1">H_minus</span><span class="s3">, </span><span class="s1">V_minus</span><span class="s3">, </span><span class="s1">H_plus</span><span class="s3">, </span><span class="s1">V_plus</span><span class="s3">, </span><span class="s1">rank</span>


<span class="s0"># To help understand the iterative version of the algorithm, the original</span>
<span class="s0"># recursive formulation follows.</span>
<span class="s0">#</span>
<span class="s0"># def _eigh_work(H, V=None, termination_size=128):</span>
<span class="s0">#   &quot;&quot;&quot; The main work loop performing the symmetric eigendecomposition of H.</span>
<span class="s0">#   Each step recursively computes a projector into the space of eigenvalues</span>
<span class="s0">#   above jnp.mean(jnp.diag(H)). The result of the projections into and out of</span>
<span class="s0">#   that space, along with the isometries accomplishing these, are then computed.</span>
<span class="s0">#   This is performed recursively until the projections have size 1, and thus</span>
<span class="s0">#   store an eigenvalue of the original input; the corresponding isometry is</span>
<span class="s0">#   the related eigenvector. The results are then composed.</span>
<span class="s0">#</span>
<span class="s0">#   Args:</span>
<span class="s0">#     H: The Hermitian input.</span>
<span class="s0">#     V: Stores the isometries projecting H into its subspaces.</span>
<span class="s0">#     precision: :class:`~jax.lax.Precision` object specifying the matmul precision.</span>
<span class="s0">#</span>
<span class="s0">#   Returns:</span>
<span class="s0">#     H, V: The result of the projection.</span>
<span class="s0">#   &quot;&quot;&quot;</span>
<span class="s0">#   if H.shape[0] &lt;= termination_size:</span>
<span class="s0">#     evals, evecs = jnp_linalg.eigh(H)</span>
<span class="s0">#     if V is not None:</span>
<span class="s0">#       evecs = jnp.dot(V, evecs)</span>
<span class="s0">#     return evals, evecs</span>
<span class="s0">#</span>
<span class="s0">#   split_point = jnp.median(jnp.diag(H))  # TODO: Improve this?</span>
<span class="s0">#   H_minus, V_minus, H_plus, V_plus = split_spectrum(H, split_point, V0=V)</span>
<span class="s0">#   H_minus, V_minus = _eigh_work(H_minus, V=V_minus, termination_size=termination_size)</span>
<span class="s0">#   H_plus, V_plus = _eigh_work(H_plus, V=V_plus, termination_size=termination_size)</span>
<span class="s0">#</span>
<span class="s0">#   evals = jnp.hstack((H_minus, H_plus))</span>
<span class="s0">#   evecs = jnp.hstack((V_minus, V_plus))</span>
<span class="s0">#   return evals, evecs</span>

<span class="s3">class </span><span class="s1">_Subproblem(NamedTuple):</span>
  <span class="s2">&quot;&quot;&quot;Describes a subproblem of _eigh_work. 
 
  Each subproblem is a `size` x `size` Hermitian matrix, starting at `offset` 
  in the workspace. 
  &quot;&quot;&quot;</span>
  <span class="s0"># The row offset of the block in the matrix of blocks.</span>
  <span class="s1">offset: jax.Array</span>

  <span class="s0"># The size of the block.</span>
  <span class="s1">size: jax.Array</span>

<span class="s1">@partial(jax.jit</span><span class="s3">, </span><span class="s1">static_argnames=(</span><span class="s5">'termination_size'</span><span class="s3">,</span><span class="s1">))</span>
<span class="s3">def </span><span class="s1">_eigh_work(H</span><span class="s3">, </span><span class="s1">n</span><span class="s3">, </span><span class="s1">termination_size=</span><span class="s4">256</span><span class="s1">):</span>
  <span class="s2">&quot;&quot;&quot; The main work loop performing the symmetric eigendecomposition of H. 
  Each step recursively computes a projector into the space of eigenvalues 
  above jnp.mean(jnp.diag(H)). The result of the projections into and out of 
  that space, along with the isometries accomplishing these, are then computed. 
  This is performed recursively until the projections have size 1, and thus 
  store an eigenvalue of the original input; the corresponding isometry is 
  the related eigenvector. The results are then composed. 
 
  This function cannot be Jitted because the internal split_spectrum cannot 
  be. 
 
  Args: 
    H: The Hermitian input. 
    n: The true (dynamic) shape of H. 
 
  Returns: 
    H, V: The result of the projection. 
  &quot;&quot;&quot;</span>
  <span class="s0"># We turn what was originally a recursive algorithm into an iterative</span>
  <span class="s0"># algorithm with an explicit stack.</span>
  <span class="s1">N</span><span class="s3">, </span><span class="s1">_ = H.shape</span>
  <span class="s1">n = jnp.asarray(n</span><span class="s3">, </span><span class="s1">jnp.int32)</span>
  <span class="s1">agenda = Stack.create(</span>
    <span class="s1">N + </span><span class="s4">1</span><span class="s3">, </span><span class="s1">_Subproblem(jnp.array(</span><span class="s4">0</span><span class="s3">, </span><span class="s1">jnp.int32)</span><span class="s3">, </span><span class="s1">jnp.array(</span><span class="s4">0</span><span class="s3">, </span><span class="s1">jnp.int32)))</span>
  <span class="s1">agenda = agenda.push(_Subproblem(offset=jnp.int32(</span><span class="s4">0</span><span class="s1">)</span><span class="s3">, </span><span class="s1">size=n))</span>

  <span class="s0"># eigenvectors is the array in which we build the output eigenvectors.</span>
  <span class="s0"># We initialize it with the identity matrix so the initial matrix</span>
  <span class="s0"># multiplications in_split_spectrum_jittable are the identity.</span>
  <span class="s1">eigenvectors = jnp.eye(N</span><span class="s3">, </span><span class="s1">dtype=H.dtype)</span>

  <span class="s0"># blocks is an array representing a stack of Hermitian matrix blocks that we</span>
  <span class="s0"># need to recursively decompose. Subproblems are different sizes, so the stack</span>
  <span class="s0"># of blocks is ragged. Subproblems are left-aligned (i.e. starting at the 0th</span>
  <span class="s0"># column). Here is an ASCII art picture of three blocks A, B, C, embedded</span>
  <span class="s0"># in the larger `blocks` workspace (represented with trailing dots).</span>
  <span class="s0">#</span>
  <span class="s0"># A A A . . .</span>
  <span class="s0"># A A A . . .</span>
  <span class="s0"># A A A . . .</span>
  <span class="s0"># B B . . . .</span>
  <span class="s0"># B B . . . .</span>
  <span class="s0"># C C C C . .</span>
  <span class="s0"># C C C C . .</span>
  <span class="s0"># C C C C . .</span>
  <span class="s0"># C C C C . .</span>
  <span class="s0">#</span>
  <span class="s0"># Each step of the algorithm subdivides a block into two subblocks whose</span>
  <span class="s0"># sizes sum to the original block size. We overwrite the original block with</span>
  <span class="s0"># those two subblocks so we don't need any additional scratch space.</span>
  <span class="s0">#</span>
  <span class="s0"># At termination, &quot;blocks&quot; will contain 1x1 blocks (i.e., the eigenvalues) in</span>
  <span class="s0"># its first column.</span>
  <span class="s1">blocks = H</span>

  <span class="s3">def </span><span class="s1">base_case(B</span><span class="s3">, </span><span class="s1">offset</span><span class="s3">, </span><span class="s1">b</span><span class="s3">, </span><span class="s1">agenda</span><span class="s3">, </span><span class="s1">blocks</span><span class="s3">, </span><span class="s1">eigenvectors):</span>
    <span class="s0"># Base case: for blocks under a minimum size, we cutoff the recursion</span>
    <span class="s0"># and call the TPU Jacobi eigendecomposition implementation. The Jacobi</span>
    <span class="s0"># algorithm works well for small matrices but scales poorly, so the two</span>
    <span class="s0"># complement each other well.</span>
    <span class="s1">H = _slice(blocks</span><span class="s3">, </span><span class="s1">(offset</span><span class="s3">, </span><span class="s4">0</span><span class="s1">)</span><span class="s3">, </span><span class="s1">(b</span><span class="s3">, </span><span class="s1">b)</span><span class="s3">, </span><span class="s1">(B</span><span class="s3">, </span><span class="s1">B))</span>
    <span class="s1">V = _slice(eigenvectors</span><span class="s3">, </span><span class="s1">(</span><span class="s4">0</span><span class="s3">, </span><span class="s1">offset)</span><span class="s3">, </span><span class="s1">(n</span><span class="s3">, </span><span class="s1">b)</span><span class="s3">, </span><span class="s1">(N</span><span class="s3">, </span><span class="s1">B))</span>

    <span class="s0"># We replace the masked-out part of the matrix with the identity matrix.</span>
    <span class="s0"># We know that the TPU Jacobi eigh implementation will not alter the order</span>
    <span class="s0"># of the eigenvalues, so we know the eigendecomposition of the original</span>
    <span class="s0"># matrix is in the top-left corner of the eigendecomposition of the padded</span>
    <span class="s0"># matrix.</span>
    <span class="s0"># It is very important that the underlying eigh implementation does not sort</span>
    <span class="s0"># the eigenvalues for this reason! This is currently not true of JAX's CPU</span>
    <span class="s0"># and GPU eigendecompositions, and for those platforms this algorithm will</span>
    <span class="s0"># only do the right thing if termination_size == 1.</span>
    <span class="s1">H = _mask(H</span><span class="s3">, </span><span class="s1">(b</span><span class="s3">, </span><span class="s1">b)</span><span class="s3">, </span><span class="s1">jnp.eye(B</span><span class="s3">, </span><span class="s1">dtype=H.dtype))</span>
    <span class="s1">eig_vecs</span><span class="s3">, </span><span class="s1">eig_vals = lax.linalg.eigh(H</span><span class="s3">, </span><span class="s1">sort_eigenvalues=</span><span class="s3">False</span><span class="s1">)</span>
    <span class="s1">eig_vecs = _mask(eig_vecs</span><span class="s3">, </span><span class="s1">(b</span><span class="s3">, </span><span class="s1">b))</span>
    <span class="s1">eig_vals = _mask(eig_vals</span><span class="s3">, </span><span class="s1">(b</span><span class="s3">,</span><span class="s1">))</span>
    <span class="s1">eig_vecs = jnp.dot(V</span><span class="s3">, </span><span class="s1">eig_vecs)</span>

    <span class="s1">eig_vals = eig_vals.astype(eig_vecs.dtype)</span>
    <span class="s1">blocks = _update_slice(blocks</span><span class="s3">, </span><span class="s1">eig_vals[:</span><span class="s3">, None</span><span class="s1">]</span><span class="s3">, </span><span class="s1">(offset</span><span class="s3">, </span><span class="s4">0</span><span class="s1">)</span><span class="s3">, </span><span class="s1">(b</span><span class="s3">, </span><span class="s4">1</span><span class="s1">))</span>
    <span class="s1">eigenvectors = _update_slice(eigenvectors</span><span class="s3">, </span><span class="s1">eig_vecs</span><span class="s3">, </span><span class="s1">(</span><span class="s4">0</span><span class="s3">, </span><span class="s1">offset)</span><span class="s3">, </span><span class="s1">(n</span><span class="s3">, </span><span class="s1">b))</span>
    <span class="s3">return </span><span class="s1">agenda</span><span class="s3">, </span><span class="s1">blocks</span><span class="s3">, </span><span class="s1">eigenvectors</span>

  <span class="s3">def </span><span class="s1">recursive_case(B</span><span class="s3">, </span><span class="s1">offset</span><span class="s3">, </span><span class="s1">b</span><span class="s3">, </span><span class="s1">agenda</span><span class="s3">, </span><span class="s1">blocks</span><span class="s3">, </span><span class="s1">eigenvectors):</span>
    <span class="s0"># The recursive case of the algorithm, specialized to a static block size</span>
    <span class="s0"># of B.</span>
    <span class="s1">H = _slice(blocks</span><span class="s3">, </span><span class="s1">(offset</span><span class="s3">, </span><span class="s4">0</span><span class="s1">)</span><span class="s3">, </span><span class="s1">(b</span><span class="s3">, </span><span class="s1">b)</span><span class="s3">, </span><span class="s1">(B</span><span class="s3">, </span><span class="s1">B))</span>

    <span class="s3">def </span><span class="s1">nearly_diagonal_case(agenda</span><span class="s3">, </span><span class="s1">blocks</span><span class="s3">, </span><span class="s1">eigenvectors):</span>
      <span class="s1">blocks = _update_slice(blocks</span><span class="s3">, </span><span class="s1">jnp.diag(H)[:</span><span class="s3">, None</span><span class="s1">]</span><span class="s3">, </span><span class="s1">(offset</span><span class="s3">, </span><span class="s4">0</span><span class="s1">)</span><span class="s3">, </span><span class="s1">(b</span><span class="s3">, </span><span class="s4">1</span><span class="s1">))</span>
      <span class="s3">return </span><span class="s1">agenda</span><span class="s3">, </span><span class="s1">blocks</span><span class="s3">, </span><span class="s1">eigenvectors</span>

    <span class="s3">def </span><span class="s1">default_case(agenda</span><span class="s3">, </span><span class="s1">blocks</span><span class="s3">, </span><span class="s1">eigenvectors):</span>
      <span class="s1">V = _slice(eigenvectors</span><span class="s3">, </span><span class="s1">(</span><span class="s4">0</span><span class="s3">, </span><span class="s1">offset)</span><span class="s3">, </span><span class="s1">(n</span><span class="s3">, </span><span class="s1">b)</span><span class="s3">, </span><span class="s1">(N</span><span class="s3">, </span><span class="s1">B))</span>
      <span class="s0"># TODO: Improve this?</span>
      <span class="s1">split_point = reductions.nanmedian(_mask(jnp.diag(ufuncs.real(H))</span><span class="s3">, </span><span class="s1">(b</span><span class="s3">,</span><span class="s1">)</span><span class="s3">, </span><span class="s1">jnp.nan))</span>
      <span class="s1">H_minus</span><span class="s3">, </span><span class="s1">V_minus</span><span class="s3">, </span><span class="s1">H_plus</span><span class="s3">, </span><span class="s1">V_plus</span><span class="s3">, </span><span class="s1">rank = split_spectrum(</span>
          <span class="s1">H</span><span class="s3">, </span><span class="s1">b</span><span class="s3">, </span><span class="s1">split_point</span><span class="s3">, </span><span class="s1">V0=V)</span>

      <span class="s1">blocks = _update_slice(blocks</span><span class="s3">, </span><span class="s1">H_minus</span><span class="s3">, </span><span class="s1">(offset</span><span class="s3">, </span><span class="s4">0</span><span class="s1">)</span><span class="s3">, </span><span class="s1">(rank</span><span class="s3">, </span><span class="s1">rank))</span>
      <span class="s1">blocks = _update_slice(blocks</span><span class="s3">, </span><span class="s1">H_plus</span><span class="s3">, </span><span class="s1">(offset + rank</span><span class="s3">, </span><span class="s4">0</span><span class="s1">)</span><span class="s3">,</span>
                             <span class="s1">(b - rank</span><span class="s3">, </span><span class="s1">b - rank))</span>
      <span class="s1">eigenvectors = _update_slice(eigenvectors</span><span class="s3">, </span><span class="s1">V_minus</span><span class="s3">, </span><span class="s1">(</span><span class="s4">0</span><span class="s3">, </span><span class="s1">offset)</span><span class="s3">,</span>
                                   <span class="s1">(n</span><span class="s3">, </span><span class="s1">rank))</span>
      <span class="s1">eigenvectors = _update_slice(eigenvectors</span><span class="s3">, </span><span class="s1">V_plus</span><span class="s3">, </span><span class="s1">(</span><span class="s4">0</span><span class="s3">, </span><span class="s1">offset + rank)</span><span class="s3">,</span>
                                  <span class="s1">(n</span><span class="s3">, </span><span class="s1">b - rank))</span>

      <span class="s1">agenda = agenda.push(_Subproblem(offset + rank</span><span class="s3">, </span><span class="s1">(b - rank)))</span>
      <span class="s1">agenda = agenda.push(_Subproblem(offset</span><span class="s3">, </span><span class="s1">rank))</span>
      <span class="s3">return </span><span class="s1">agenda</span><span class="s3">, </span><span class="s1">blocks</span><span class="s3">, </span><span class="s1">eigenvectors</span>

    <span class="s0"># If the matrix is nearly diagonal, terminate the execution. This is</span>
    <span class="s0"># necessary to handle matrices with clusters of eigenvalues. See Nakatsukasa</span>
    <span class="s0"># and Higham section 5.2.</span>
    <span class="s1">norm = jnp_linalg.norm(H)</span>
    <span class="s1">tol = jnp.asarray(</span><span class="s4">10 </span><span class="s1">* jnp.finfo(H.dtype).eps / </span><span class="s4">2</span><span class="s3">, </span><span class="s1">dtype=norm.dtype)</span>
    <span class="s1">off_diag_norm = jnp_linalg.norm(</span>
        <span class="s1">H - jnp.diag(jnp.diag(ufuncs.real(H)).astype(H.dtype)))</span>
    <span class="s0"># We also handle nearly-all-zero matrices matrices here.</span>
    <span class="s1">nearly_diagonal = (norm &lt; tol) | (off_diag_norm / norm &lt; tol)</span>
    <span class="s3">return </span><span class="s1">lax.cond(nearly_diagonal</span><span class="s3">, </span><span class="s1">nearly_diagonal_case</span><span class="s3">, </span><span class="s1">default_case</span><span class="s3">,</span>
                    <span class="s1">agenda</span><span class="s3">, </span><span class="s1">blocks</span><span class="s3">, </span><span class="s1">eigenvectors)</span>

  <span class="s3">def </span><span class="s1">loop_cond(state):</span>
    <span class="s1">agenda</span><span class="s3">, </span><span class="s1">_</span><span class="s3">, </span><span class="s1">_ = state</span>
    <span class="s3">return </span><span class="s1">~agenda.empty()</span>

  <span class="s0"># It would be wasteful to perform all computation padded up to the original</span>
  <span class="s0"># matrix size. Instead, we form buckets of padded sizes e.g.,</span>
  <span class="s0"># [256, 512, 1024, ..., N], aiming for a balance between compilation time</span>
  <span class="s0"># and runtime.</span>
  <span class="s1">cutoff = min(N</span><span class="s3">, </span><span class="s1">termination_size)</span>
  <span class="s1">buckets = [cutoff]</span>
  <span class="s1">branches = [partial(base_case</span><span class="s3">, </span><span class="s1">cutoff)]</span>
  <span class="s1">i = cutoff</span>
  <span class="s3">while </span><span class="s1">i &lt; N:</span>
    <span class="s1">i = min(</span><span class="s4">2 </span><span class="s1">* i</span><span class="s3">, </span><span class="s1">N)</span>
    <span class="s1">buckets.append(i)</span>
    <span class="s1">branches.append(partial(recursive_case</span><span class="s3">, </span><span class="s1">i))</span>
  <span class="s1">buckets = jnp.array(buckets</span><span class="s3">, </span><span class="s1">dtype=</span><span class="s5">'int32'</span><span class="s1">)</span>

  <span class="s3">def </span><span class="s1">loop_body(state):</span>
    <span class="s1">agenda</span><span class="s3">, </span><span class="s1">blocks</span><span class="s3">, </span><span class="s1">eigenvectors = state</span>
    <span class="s1">(offset</span><span class="s3">, </span><span class="s1">b)</span><span class="s3">, </span><span class="s1">agenda = agenda.pop()</span>
    <span class="s1">which = jnp.where(buckets &lt; b</span><span class="s3">, </span><span class="s1">jnp.iinfo(jnp.int32).max</span><span class="s3">, </span><span class="s1">buckets)</span>
    <span class="s1">choice = jnp.argmin(which)</span>
    <span class="s3">return </span><span class="s1">lax.switch(choice</span><span class="s3">, </span><span class="s1">branches</span><span class="s3">, </span><span class="s1">offset</span><span class="s3">, </span><span class="s1">b</span><span class="s3">, </span><span class="s1">agenda</span><span class="s3">, </span><span class="s1">blocks</span><span class="s3">, </span><span class="s1">eigenvectors)</span>

  <span class="s1">_</span><span class="s3">, </span><span class="s1">blocks</span><span class="s3">, </span><span class="s1">eigenvectors = lax.while_loop(</span>
      <span class="s1">loop_cond</span><span class="s3">, </span><span class="s1">loop_body</span><span class="s3">, </span><span class="s1">(agenda</span><span class="s3">, </span><span class="s1">blocks</span><span class="s3">, </span><span class="s1">eigenvectors))</span>
  <span class="s3">return </span><span class="s1">blocks[:</span><span class="s3">, </span><span class="s4">0</span><span class="s1">]</span><span class="s3">, </span><span class="s1">eigenvectors</span>


<span class="s3">def </span><span class="s1">eigh(H</span><span class="s3">, </span><span class="s1">*</span><span class="s3">, </span><span class="s1">precision=</span><span class="s5">&quot;float32&quot;</span><span class="s3">, </span><span class="s1">termination_size=</span><span class="s4">256</span><span class="s3">, </span><span class="s1">n=</span><span class="s3">None,</span>
         <span class="s1">sort_eigenvalues=</span><span class="s3">True</span><span class="s1">):</span>
  <span class="s2">&quot;&quot;&quot; Computes the eigendecomposition of the symmetric/Hermitian matrix H. 
 
  Args: 
    H: The `n x n` Hermitian input, padded to `N x N`. 
    precision: :class:`~jax.lax.Precision` object specifying the matmul precision. 
    termination_size: Recursion ends once the blocks reach this linear size. 
    n: the true (dynamic) size of the matrix. 
    sort_eigenvalues: If `True`, the eigenvalues will be sorted from lowest to 
      highest. 
  Returns: 
    vals: The `n` eigenvalues of `H`. 
    vecs: A unitary matrix such that `vecs[:, i]` is a normalized eigenvector 
      of `H` corresponding to `vals[i]`. We have `H @ vecs = vals * vecs` up 
      to numerical error. 
  &quot;&quot;&quot;</span>
  <span class="s1">M</span><span class="s3">, </span><span class="s1">N = H.shape</span>
  <span class="s3">if </span><span class="s1">M != N:</span>
    <span class="s3">raise </span><span class="s1">TypeError(</span><span class="s5">f&quot;Input H of shape </span><span class="s3">{</span><span class="s1">H.shape</span><span class="s3">} </span><span class="s5">must be square.&quot;</span><span class="s1">)</span>

  <span class="s3">if </span><span class="s1">N &lt;= termination_size:</span>
    <span class="s3">if </span><span class="s1">n </span><span class="s3">is not None</span><span class="s1">:</span>
      <span class="s1">H = _mask(H</span><span class="s3">, </span><span class="s1">(n</span><span class="s3">, </span><span class="s1">n)</span><span class="s3">, </span><span class="s1">jnp.eye(N</span><span class="s3">, </span><span class="s1">dtype=H.dtype))</span>
    <span class="s3">return </span><span class="s1">lax_linalg.eigh_jacobi(</span>
        <span class="s1">H</span><span class="s3">, </span><span class="s1">sort_eigenvalues=sort_eigenvalues)</span>

  <span class="s0"># TODO(phawkins): consider rounding N up to a larger size to maximize reuse</span>
  <span class="s0"># between matrices.</span>

  <span class="s1">n = N </span><span class="s3">if </span><span class="s1">n </span><span class="s3">is None else </span><span class="s1">n</span>
  <span class="s3">with </span><span class="s1">jax.default_matmul_precision(precision):</span>
    <span class="s1">eig_vals</span><span class="s3">, </span><span class="s1">eig_vecs = _eigh_work(H</span><span class="s3">, </span><span class="s1">n</span><span class="s3">, </span><span class="s1">termination_size=termination_size)</span>
  <span class="s1">eig_vals = _mask(ufuncs.real(eig_vals)</span><span class="s3">, </span><span class="s1">(n</span><span class="s3">,</span><span class="s1">)</span><span class="s3">, </span><span class="s1">jnp.nan)</span>
  <span class="s3">if </span><span class="s1">sort_eigenvalues:</span>
    <span class="s1">sort_idxs = jnp.argsort(eig_vals)</span>
    <span class="s1">eig_vals = eig_vals[sort_idxs]</span>
    <span class="s1">eig_vecs = eig_vecs[:</span><span class="s3">, </span><span class="s1">sort_idxs]</span>
  <span class="s3">return </span><span class="s1">eig_vals</span><span class="s3">, </span><span class="s1">eig_vecs</span>
</pre>
</body>
</html>