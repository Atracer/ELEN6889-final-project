<html>
<head>
<title>bcoo.py</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<style type="text/css">
.s0 { color: #808080;}
.s1 { color: #a9b7c6;}
.s2 { color: #629755; font-style: italic;}
.s3 { color: #cc7832;}
.s4 { color: #6897bb;}
.s5 { color: #6a8759;}
</style>
</head>
<body bgcolor="#2b2b2b">
<table CELLSPACING=0 CELLPADDING=5 COLS=1 WIDTH="100%" BGCOLOR="#606060" >
<tr><td><center>
<font face="Arial, Helvetica" color="#000000">
bcoo.py</font>
</center></td></tr></table>
<pre><span class="s0"># Copyright 2021 The JAX Authors.</span>
<span class="s0">#</span>
<span class="s0"># Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);</span>
<span class="s0"># you may not use this file except in compliance with the License.</span>
<span class="s0"># You may obtain a copy of the License at</span>
<span class="s0">#</span>
<span class="s0">#     https://www.apache.org/licenses/LICENSE-2.0</span>
<span class="s0">#</span>
<span class="s0"># Unless required by applicable law or agreed to in writing, software</span>
<span class="s0"># distributed under the License is distributed on an &quot;AS IS&quot; BASIS,</span>
<span class="s0"># WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span>
<span class="s0"># See the License for the specific language governing permissions and</span>
<span class="s0"># limitations under the License.</span>

<span class="s2">&quot;&quot;&quot;BCOO (Bached coordinate format) matrix object and associated primitives.&quot;&quot;&quot;</span>
<span class="s3">from </span><span class="s1">__future__ </span><span class="s3">import </span><span class="s1">annotations</span>

<span class="s3">import </span><span class="s1">functools</span>
<span class="s3">from </span><span class="s1">functools </span><span class="s3">import </span><span class="s1">partial</span>
<span class="s3">import </span><span class="s1">math</span>
<span class="s3">import </span><span class="s1">operator</span>
<span class="s3">from </span><span class="s1">typing </span><span class="s3">import </span><span class="s1">Any</span><span class="s3">, </span><span class="s1">List</span><span class="s3">, </span><span class="s1">NamedTuple</span><span class="s3">, </span><span class="s1">Optional</span><span class="s3">, </span><span class="s1">Protocol</span><span class="s3">, </span><span class="s1">Sequence</span><span class="s3">, </span><span class="s1">Tuple</span><span class="s3">, </span><span class="s1">Union</span>
<span class="s3">import </span><span class="s1">warnings</span>

<span class="s3">import </span><span class="s1">numpy </span><span class="s3">as </span><span class="s1">np</span>

<span class="s3">import </span><span class="s1">jax</span>
<span class="s3">from </span><span class="s1">jax </span><span class="s3">import </span><span class="s1">lax</span>
<span class="s3">from </span><span class="s1">jax </span><span class="s3">import </span><span class="s1">tree_util</span>
<span class="s3">from </span><span class="s1">jax </span><span class="s3">import </span><span class="s1">vmap</span>
<span class="s3">from </span><span class="s1">jax.config </span><span class="s3">import </span><span class="s1">config</span>
<span class="s3">from </span><span class="s1">jax.experimental.sparse._base </span><span class="s3">import </span><span class="s1">JAXSparse</span>
<span class="s3">from </span><span class="s1">jax.experimental.sparse.util </span><span class="s3">import </span><span class="s1">(</span>
  <span class="s1">nfold_vmap</span><span class="s3">, </span><span class="s1">_count_stored_elements</span><span class="s3">,</span>
  <span class="s1">_dot_general_validated_shape</span><span class="s3">, </span><span class="s1">CuSparseEfficiencyWarning</span><span class="s3">,</span>
  <span class="s1">SparseEfficiencyError</span><span class="s3">, </span><span class="s1">SparseEfficiencyWarning</span><span class="s3">, </span><span class="s1">Shape</span><span class="s3">,</span>
  <span class="s1">SparseInfo)</span>
<span class="s3">from </span><span class="s1">jax.experimental.sparse._lowerings </span><span class="s3">import </span><span class="s1">coo_spmv_p</span><span class="s3">, </span><span class="s1">coo_spmm_p</span>
<span class="s3">from </span><span class="s1">jax._src.interpreters </span><span class="s3">import </span><span class="s1">mlir</span>
<span class="s3">import </span><span class="s1">jax.numpy </span><span class="s3">as </span><span class="s1">jnp</span>
<span class="s3">from </span><span class="s1">jax.util </span><span class="s3">import </span><span class="s1">safe_zip</span><span class="s3">, </span><span class="s1">unzip2</span><span class="s3">, </span><span class="s1">split_list</span>
<span class="s3">from </span><span class="s1">jax._src </span><span class="s3">import </span><span class="s1">api_util</span>
<span class="s3">from </span><span class="s1">jax._src </span><span class="s3">import </span><span class="s1">core</span>
<span class="s3">from </span><span class="s1">jax._src </span><span class="s3">import </span><span class="s1">dispatch</span>
<span class="s3">from </span><span class="s1">jax._src.interpreters </span><span class="s3">import </span><span class="s1">ad</span>
<span class="s3">from </span><span class="s1">jax._src.interpreters </span><span class="s3">import </span><span class="s1">batching</span>
<span class="s3">from </span><span class="s1">jax._src.interpreters </span><span class="s3">import </span><span class="s1">partial_eval </span><span class="s3">as </span><span class="s1">pe</span>
<span class="s3">from </span><span class="s1">jax._src.lax.lax </span><span class="s3">import </span><span class="s1">(</span>
  <span class="s1">_const</span><span class="s3">, </span><span class="s1">ranges_like</span><span class="s3">, </span><span class="s1">remaining</span><span class="s3">, </span><span class="s1">_dot_general_batch_dim_nums</span><span class="s3">, </span><span class="s1">DotDimensionNumbers)</span>
<span class="s3">from </span><span class="s1">jax._src.lax.slicing </span><span class="s3">import </span><span class="s1">GatherDimensionNumbers</span><span class="s3">, </span><span class="s1">GatherScatterMode</span>
<span class="s3">from </span><span class="s1">jax._src.lib.mlir </span><span class="s3">import </span><span class="s1">ir</span>
<span class="s3">from </span><span class="s1">jax._src.lib </span><span class="s3">import </span><span class="s1">gpu_sparse</span>
<span class="s3">from </span><span class="s1">jax._src.lib.mlir.dialects </span><span class="s3">import </span><span class="s1">hlo</span>
<span class="s3">from </span><span class="s1">jax._src.numpy.setops </span><span class="s3">import </span><span class="s1">_unique</span>
<span class="s3">from </span><span class="s1">jax._src.typing </span><span class="s3">import </span><span class="s1">Array</span><span class="s3">, </span><span class="s1">ArrayLike</span><span class="s3">, </span><span class="s1">DTypeLike</span>
<span class="s3">from </span><span class="s1">jax._src.util </span><span class="s3">import </span><span class="s1">canonicalize_axis</span>


<span class="s1">CUSPARSE_DATA_DTYPES = [np.float32</span><span class="s3">, </span><span class="s1">np.float64</span><span class="s3">, </span><span class="s1">np.complex64</span><span class="s3">, </span><span class="s1">np.complex128]</span>
<span class="s1">CUSPARSE_INDEX_DTYPES = [np.int32]</span>


<span class="s3">def </span><span class="s1">_bcoo_batch_dims_to_front(batched_args</span><span class="s3">, </span><span class="s1">batch_dims</span><span class="s3">, </span><span class="s1">spinfo</span><span class="s3">, </span><span class="s1">batch_size=</span><span class="s3">None</span><span class="s1">):</span>
  <span class="s1">data</span><span class="s3">, </span><span class="s1">indices = batched_args</span>
  <span class="s1">data_bdim</span><span class="s3">, </span><span class="s1">indices_bdim = batch_dims</span>
  <span class="s1">n_batch = indices.ndim - </span><span class="s4">2 </span><span class="s1">+ bool(indices_bdim </span><span class="s3">is None</span><span class="s1">)</span>
  <span class="s3">if not </span><span class="s1">all(b </span><span class="s3">is None or </span><span class="s4">0 </span><span class="s1">&lt;= b &lt; n_batch </span><span class="s3">for </span><span class="s1">b </span><span class="s3">in </span><span class="s1">batch_dims):</span>
    <span class="s3">raise </span><span class="s1">NotImplementedError(</span><span class="s5">&quot;batch_dims must be None or satisfy 0 &lt; dim &lt; n_batch. &quot;</span>
                              <span class="s5">f&quot;Got </span><span class="s3">{</span><span class="s1">batch_dims=</span><span class="s3">} </span><span class="s5">for </span><span class="s3">{</span><span class="s1">n_batch=</span><span class="s3">}</span><span class="s5">.&quot;</span><span class="s1">)</span>
  <span class="s1">batched_data</span><span class="s3">, </span><span class="s1">batched_indices = [</span>
      <span class="s1">lax.expand_dims(arg</span><span class="s3">, </span><span class="s1">[</span><span class="s4">0</span><span class="s1">]) </span><span class="s3">if </span><span class="s1">bdim </span><span class="s3">is None else </span><span class="s1">jnp.moveaxis(arg</span><span class="s3">, </span><span class="s1">bdim</span><span class="s3">, </span><span class="s4">0</span><span class="s1">)</span>
      <span class="s3">for </span><span class="s1">arg</span><span class="s3">, </span><span class="s1">bdim </span><span class="s3">in </span><span class="s1">[(data</span><span class="s3">, </span><span class="s1">data_bdim)</span><span class="s3">, </span><span class="s1">(indices</span><span class="s3">, </span><span class="s1">indices_bdim)]]</span>
  <span class="s3">if </span><span class="s1">batch_size </span><span class="s3">is None</span><span class="s1">:</span>
    <span class="s1">batch_size = max(arg.shape[dim] </span><span class="s3">for </span><span class="s1">arg</span><span class="s3">, </span><span class="s1">dim </span><span class="s3">in </span><span class="s1">zip((data</span><span class="s3">, </span><span class="s1">indices)</span><span class="s3">, </span><span class="s1">batch_dims) </span><span class="s3">if </span><span class="s1">dim </span><span class="s3">is not None</span><span class="s1">)</span>
  <span class="s1">batched_spinfo = SparseInfo((batch_size</span><span class="s3">, </span><span class="s1">*spinfo.shape)</span><span class="s3">,</span>
                              <span class="s1">indices_sorted=spinfo.indices_sorted</span><span class="s3">,</span>
                              <span class="s1">unique_indices=spinfo.unique_indices)</span>
  <span class="s3">return </span><span class="s1">batched_data</span><span class="s3">, </span><span class="s1">batched_indices</span><span class="s3">, </span><span class="s1">batched_spinfo</span>


<span class="s0">#----------------------------------------------------------------------</span>
<span class="s0"># BCOO primitives: batched extension of COO.</span>

<span class="s3">def </span><span class="s1">_bcoo_set_nse(mat: BCOO</span><span class="s3">, </span><span class="s1">nse: int) -&gt; BCOO:</span>
  <span class="s2">&quot;&quot;&quot;Return a copy of `mat` with the specified nse. 
  Note that if nse &lt; mat.nse, this will potentially discard data. 
  &quot;&quot;&quot;</span>
  <span class="s1">nse = operator.index(nse)</span>
  <span class="s3">assert </span><span class="s1">nse &gt;= </span><span class="s4">0</span>
  <span class="s3">if </span><span class="s1">mat.nse == nse:</span>
    <span class="s3">return </span><span class="s1">mat</span>
  <span class="s3">if </span><span class="s1">nse &lt;= mat.nse:</span>
    <span class="s1">data = mat.data[(*(slice(</span><span class="s3">None</span><span class="s1">) </span><span class="s3">for </span><span class="s1">i </span><span class="s3">in </span><span class="s1">range(mat.n_batch))</span><span class="s3">, </span><span class="s1">slice(nse))]</span>
    <span class="s1">indices = mat.indices[...</span><span class="s3">, </span><span class="s1">:nse</span><span class="s3">, </span><span class="s1">:]</span>
  <span class="s3">else</span><span class="s1">:</span>
    <span class="s1">data = jnp.zeros_like(mat.data</span><span class="s3">, </span><span class="s1">shape=(*mat.data.shape[:mat.n_batch]</span><span class="s3">, </span><span class="s1">nse</span><span class="s3">, </span><span class="s1">*mat.data.shape[mat.n_batch + </span><span class="s4">1</span><span class="s1">:]))</span>
    <span class="s1">data = data.at[(*(slice(</span><span class="s3">None</span><span class="s1">) </span><span class="s3">for </span><span class="s1">i </span><span class="s3">in </span><span class="s1">range(mat.n_batch))</span><span class="s3">, </span><span class="s1">slice(mat.nse))].set(mat.data)</span>
    <span class="s1">indices = jnp.zeros_like(mat.indices</span><span class="s3">, </span><span class="s1">shape=(*mat.indices.shape[:-</span><span class="s4">2</span><span class="s1">]</span><span class="s3">, </span><span class="s1">nse</span><span class="s3">, </span><span class="s1">mat.indices.shape[-</span><span class="s4">1</span><span class="s1">]))</span>
    <span class="s1">indices = indices.at[...</span><span class="s3">, </span><span class="s1">:mat.nse</span><span class="s3">, </span><span class="s1">:].set(mat.indices)</span>
    <span class="s1">indices = indices.at[...</span><span class="s3">, </span><span class="s1">mat.nse:</span><span class="s3">, </span><span class="s1">:].set(jnp.array(mat.shape[mat.n_batch:mat.n_batch + mat.n_sparse]</span><span class="s3">,</span>
                                                         <span class="s1">dtype=indices.dtype))</span>
  <span class="s3">return </span><span class="s1">BCOO((data</span><span class="s3">, </span><span class="s1">indices)</span><span class="s3">, </span><span class="s1">shape=mat.shape</span><span class="s3">,</span>
              <span class="s1">indices_sorted=mat.indices_sorted</span><span class="s3">,</span>
              <span class="s1">unique_indices=mat.unique_indices)</span>

<span class="s0"># TODO(jakevdp) this can be problematic when used with autodiff; see</span>
<span class="s0"># https://github.com/google/jax/issues/10163. Should this be a primitive?</span>
<span class="s0"># Alternatively, maybe roll this into bcoo_sum_duplicates as an optional argument.</span>
<span class="s3">def </span><span class="s1">bcoo_eliminate_zeros(mat: BCOO</span><span class="s3">, </span><span class="s1">nse: Optional[int] = </span><span class="s3">None</span><span class="s1">) -&gt; BCOO:</span>
  <span class="s1">data</span><span class="s3">, </span><span class="s1">indices</span><span class="s3">, </span><span class="s1">shape = mat.data</span><span class="s3">, </span><span class="s1">mat.indices</span><span class="s3">, </span><span class="s1">mat.shape</span>
  <span class="s1">props = _validate_bcoo(data</span><span class="s3">, </span><span class="s1">indices</span><span class="s3">, </span><span class="s1">shape)</span>
  <span class="s1">mask = (data == </span><span class="s4">0</span><span class="s1">).all(tuple(range(props.n_batch + </span><span class="s4">1</span><span class="s3">, </span><span class="s1">data.ndim)))</span>
  <span class="s1">dims_to_contract = tuple(i </span><span class="s3">for </span><span class="s1">i</span><span class="s3">, </span><span class="s1">s </span><span class="s3">in </span><span class="s1">enumerate(indices.shape[:props.n_batch]) </span><span class="s3">if </span><span class="s1">s == </span><span class="s4">1</span><span class="s1">)</span>
  <span class="s1">mask = mask.all(dims_to_contract</span><span class="s3">, </span><span class="s1">keepdims=</span><span class="s3">True</span><span class="s1">)</span>
  <span class="s1">fill_value = jnp.array(shape[props.n_batch:props.n_batch + props.n_sparse]</span><span class="s3">, </span><span class="s1">dtype=indices.dtype)</span>
  <span class="s1">f = </span><span class="s3">lambda </span><span class="s1">i</span><span class="s3">, </span><span class="s1">m: jnp.where(m[:</span><span class="s3">, None</span><span class="s1">]</span><span class="s3">, </span><span class="s1">fill_value[</span><span class="s3">None, </span><span class="s1">:]</span><span class="s3">, </span><span class="s1">i)</span>
  <span class="s1">indices = nfold_vmap(f</span><span class="s3">, </span><span class="s1">props.n_batch)(indices</span><span class="s3">, </span><span class="s1">mask)</span>
  <span class="s3">return </span><span class="s1">bcoo_sum_duplicates(BCOO((data</span><span class="s3">, </span><span class="s1">indices)</span><span class="s3">, </span><span class="s1">shape=shape)</span><span class="s3">, </span><span class="s1">nse=nse)</span>


<span class="s3">class </span><span class="s1">BCOOProperties(NamedTuple):</span>
  <span class="s1">n_batch: int</span>
  <span class="s1">n_sparse: int</span>
  <span class="s1">n_dense: int</span>
  <span class="s1">nse: int</span>

<span class="s3">class </span><span class="s1">Buffer(Protocol):</span>
  <span class="s1">@property</span>
  <span class="s3">def </span><span class="s1">shape(self) -&gt; Shape: ...</span>
  <span class="s1">@property</span>
  <span class="s3">def </span><span class="s1">dtype(self) -&gt; Any: ...</span>


<span class="s3">def </span><span class="s1">_validate_bcoo(data: Buffer</span><span class="s3">, </span><span class="s1">indices: Buffer</span><span class="s3">, </span><span class="s1">shape: Sequence[int]) -&gt; BCOOProperties:</span>
  <span class="s1">props = _validate_bcoo_indices(indices</span><span class="s3">, </span><span class="s1">shape)</span>
  <span class="s1">n_batch</span><span class="s3">, </span><span class="s1">n_sparse</span><span class="s3">, </span><span class="s1">n_dense</span><span class="s3">, </span><span class="s1">nse = props</span>
  <span class="s1">shape = tuple(shape)</span>
  <span class="s3">if </span><span class="s1">any(s1 </span><span class="s3">not in </span><span class="s1">(</span><span class="s4">1</span><span class="s3">, </span><span class="s1">s2) </span><span class="s3">for </span><span class="s1">s1</span><span class="s3">, </span><span class="s1">s2 </span><span class="s3">in </span><span class="s1">safe_zip(data.shape[:n_batch]</span><span class="s3">, </span><span class="s1">shape[:n_batch])):</span>
    <span class="s3">raise </span><span class="s1">ValueError(</span><span class="s5">f&quot;data batch dimensions not compatible for </span><span class="s3">{</span><span class="s1">data.shape=</span><span class="s3">}</span><span class="s5">, </span><span class="s3">{</span><span class="s1">shape=</span><span class="s3">}</span><span class="s5">&quot;</span><span class="s1">)</span>
  <span class="s3">if </span><span class="s1">data.shape[n_batch:] != (nse</span><span class="s3">,</span><span class="s1">) + shape[n_batch + n_sparse:]:</span>
    <span class="s3">raise </span><span class="s1">ValueError(</span><span class="s5">f&quot;Invalid </span><span class="s3">{</span><span class="s1">data.shape=</span><span class="s3">} </span><span class="s5">for </span><span class="s3">{</span><span class="s1">nse=</span><span class="s3">}</span><span class="s5">, </span><span class="s3">{</span><span class="s1">n_batch=</span><span class="s3">}</span><span class="s5">, </span><span class="s3">{</span><span class="s1">n_dense=</span><span class="s3">}</span><span class="s5">&quot;</span><span class="s1">)</span>
  <span class="s3">return </span><span class="s1">props</span>


<span class="s3">def </span><span class="s1">_validate_bcoo_indices(indices: Buffer</span><span class="s3">, </span><span class="s1">shape: Sequence[int]) -&gt; BCOOProperties:</span>
  <span class="s3">assert </span><span class="s1">jnp.issubdtype(indices.dtype</span><span class="s3">, </span><span class="s1">jnp.integer)</span>
  <span class="s1">shape = tuple(shape)</span>
  <span class="s1">nse</span><span class="s3">, </span><span class="s1">n_sparse = indices.shape[-</span><span class="s4">2</span><span class="s1">:]</span>
  <span class="s1">n_batch = len(indices.shape) - </span><span class="s4">2</span>
  <span class="s1">n_dense = len(shape) - n_batch - n_sparse</span>
  <span class="s3">assert </span><span class="s1">n_dense &gt;= </span><span class="s4">0</span>
  <span class="s3">if </span><span class="s1">any(s1 </span><span class="s3">not in </span><span class="s1">(</span><span class="s4">1</span><span class="s3">, </span><span class="s1">s2) </span><span class="s3">for </span><span class="s1">s1</span><span class="s3">, </span><span class="s1">s2 </span><span class="s3">in </span><span class="s1">safe_zip(indices.shape[:n_batch]</span><span class="s3">, </span><span class="s1">shape[:n_batch])):</span>
    <span class="s3">raise </span><span class="s1">ValueError(</span><span class="s5">f&quot;indices batch dimensions not compatible for </span><span class="s3">{</span><span class="s1">indices.shape=</span><span class="s3">}</span><span class="s5">, </span><span class="s3">{</span><span class="s1">shape=</span><span class="s3">}</span><span class="s5">&quot;</span><span class="s1">)</span>
  <span class="s3">if </span><span class="s1">indices.shape[n_batch:] != (nse</span><span class="s3">, </span><span class="s1">n_sparse):</span>
    <span class="s3">raise </span><span class="s1">ValueError(</span><span class="s5">f&quot;Invalid =</span><span class="s3">{</span><span class="s1">indices.shape=</span><span class="s3">} </span><span class="s5">for </span><span class="s3">{</span><span class="s1">nse=</span><span class="s3">}</span><span class="s5">, </span><span class="s3">{</span><span class="s1">n_batch=</span><span class="s3">}</span><span class="s5">, </span><span class="s3">{</span><span class="s1">n_dense=</span><span class="s3">}</span><span class="s5">&quot;</span><span class="s1">)</span>
  <span class="s3">return </span><span class="s1">BCOOProperties(n_batch=n_batch</span><span class="s3">, </span><span class="s1">n_sparse=n_sparse</span><span class="s3">, </span><span class="s1">n_dense=n_dense</span><span class="s3">, </span><span class="s1">nse=nse)</span>


<span class="s0">#----------------------------------------------------------------------</span>
<span class="s0"># bcoo_todense</span>

<span class="s1">bcoo_todense_p = core.Primitive(</span><span class="s5">'bcoo_todense'</span><span class="s1">)</span>

<span class="s3">def </span><span class="s1">bcoo_todense(mat: BCOO) -&gt; Array:</span>
  <span class="s2">&quot;&quot;&quot;Convert batched sparse matrix to a dense matrix. 
 
  Args: 
    mat: BCOO matrix. 
 
  Returns: 
    mat_dense: dense version of ``mat``. 
  &quot;&quot;&quot;</span>
  <span class="s3">return </span><span class="s1">_bcoo_todense(mat.data</span><span class="s3">, </span><span class="s1">mat.indices</span><span class="s3">, </span><span class="s1">spinfo=mat._info)</span>

<span class="s3">def </span><span class="s1">_bcoo_todense(data: Array</span><span class="s3">, </span><span class="s1">indices: Array</span><span class="s3">, </span><span class="s1">*</span><span class="s3">, </span><span class="s1">spinfo: SparseInfo</span>
                  <span class="s1">) -&gt; Array:</span>
  <span class="s2">&quot;&quot;&quot;Convert batched sparse matrix to a dense matrix. 
 
  Args: 
    data : array of shape ``batch_dims + (nse,) + block_dims``. 
    indices : array of shape ``batch_dims + (n_sparse, nse)`` 
    spinfo : SparseInfo. In particular, this includes the shape 
      of the matrix, which is equal to ``batch_dims + sparse_dims + block_dims`` 
      where ``len(sparse_dims) == n_sparse`` 
 
  Returns: 
    mat : array with specified shape and dtype matching ``data`` 
  &quot;&quot;&quot;</span>
  <span class="s3">return </span><span class="s1">bcoo_todense_p.bind(jnp.asarray(data)</span><span class="s3">, </span><span class="s1">jnp.asarray(indices)</span><span class="s3">, </span><span class="s1">spinfo=spinfo)</span>

<span class="s1">@bcoo_todense_p.def_impl</span>
<span class="s3">def </span><span class="s1">_bcoo_todense_impl(data</span><span class="s3">, </span><span class="s1">indices</span><span class="s3">, </span><span class="s1">*</span><span class="s3">, </span><span class="s1">spinfo):</span>
  <span class="s1">shape = spinfo.shape</span>
  <span class="s1">n_batch</span><span class="s3">, </span><span class="s1">n_sparse</span><span class="s3">, </span><span class="s1">_</span><span class="s3">, </span><span class="s1">_ = _validate_bcoo(data</span><span class="s3">, </span><span class="s1">indices</span><span class="s3">, </span><span class="s1">shape)</span>

  <span class="s1">ind_slices = tuple(np.zeros(s</span><span class="s3">, </span><span class="s1">int) </span><span class="s3">if </span><span class="s1">i_s == </span><span class="s4">1 </span><span class="s3">else </span><span class="s1">np.arange(s)</span>
                     <span class="s3">for </span><span class="s1">s</span><span class="s3">, </span><span class="s1">i_s </span><span class="s3">in </span><span class="s1">zip(shape[:n_batch]</span><span class="s3">, </span><span class="s1">indices.shape[:n_batch]))</span>
  <span class="s1">grid = tuple(np.meshgrid(*ind_slices</span><span class="s3">, </span><span class="s1">indexing=</span><span class="s5">'ij'</span><span class="s3">, </span><span class="s1">sparse=</span><span class="s3">True</span><span class="s1">))</span>
  <span class="s1">sparse_ind = tuple(indices[grid + (slice(</span><span class="s3">None</span><span class="s1">)</span><span class="s3">, </span><span class="s1">i)] </span><span class="s3">for </span><span class="s1">i </span><span class="s3">in </span><span class="s1">range(n_sparse))</span>

  <span class="s1">batch_slices = tuple(np.arange(s) </span><span class="s3">for </span><span class="s1">s </span><span class="s3">in </span><span class="s1">shape[:n_batch])</span>
  <span class="s1">grid = np.meshgrid(*batch_slices</span><span class="s3">, </span><span class="s1">np.arange(</span><span class="s4">1</span><span class="s1">)</span><span class="s3">, </span><span class="s1">indexing=</span><span class="s5">'ij'</span><span class="s3">, </span><span class="s1">sparse=</span><span class="s3">True</span><span class="s1">)</span>
  <span class="s1">batch_ind = tuple(grid)[:-</span><span class="s4">1</span><span class="s1">]</span>

  <span class="s3">if not </span><span class="s1">sparse_ind:</span>
    <span class="s1">data = data.sum(n_batch</span><span class="s3">, </span><span class="s1">keepdims=bool(batch_ind)</span><span class="s3">, </span><span class="s1">dtype=data.dtype)</span>
  <span class="s3">return </span><span class="s1">jnp.zeros(shape</span><span class="s3">, </span><span class="s1">data.dtype).at[batch_ind + sparse_ind].add(data)</span>

<span class="s1">@bcoo_todense_p.def_abstract_eval</span>
<span class="s3">def </span><span class="s1">_bcoo_todense_abstract_eval(data</span><span class="s3">, </span><span class="s1">indices</span><span class="s3">, </span><span class="s1">*</span><span class="s3">, </span><span class="s1">spinfo):</span>
  <span class="s1">shape = spinfo.shape</span>
  <span class="s1">_validate_bcoo(data</span><span class="s3">, </span><span class="s1">indices</span><span class="s3">, </span><span class="s1">shape)</span>
  <span class="s3">return </span><span class="s1">core.ShapedArray(shape</span><span class="s3">, </span><span class="s1">data.dtype)</span>

<span class="s3">def </span><span class="s1">_bcoo_todense_jvp(data_dot</span><span class="s3">, </span><span class="s1">data</span><span class="s3">, </span><span class="s1">indices</span><span class="s3">, </span><span class="s1">*</span><span class="s3">, </span><span class="s1">spinfo):</span>
  <span class="s3">return </span><span class="s1">_bcoo_todense(data_dot</span><span class="s3">, </span><span class="s1">indices</span><span class="s3">, </span><span class="s1">spinfo=spinfo)</span>

<span class="s3">def </span><span class="s1">_bcoo_todense_transpose(ct</span><span class="s3">, </span><span class="s1">data</span><span class="s3">, </span><span class="s1">indices</span><span class="s3">, </span><span class="s1">*</span><span class="s3">, </span><span class="s1">spinfo):</span>
  <span class="s1">shape = spinfo.shape</span>
  <span class="s3">assert </span><span class="s1">ad.is_undefined_primal(data)</span>
  <span class="s3">if </span><span class="s1">ad.is_undefined_primal(indices):</span>
    <span class="s3">raise </span><span class="s1">ValueError(</span><span class="s5">&quot;Cannot transpose with respect to sparse indices&quot;</span><span class="s1">)</span>
  <span class="s3">assert </span><span class="s1">ct.shape == shape</span>
  <span class="s3">assert </span><span class="s1">ct.dtype == data.aval.dtype</span>
  <span class="s3">return </span><span class="s1">_bcoo_extract(indices</span><span class="s3">, </span><span class="s1">ct)</span><span class="s3">, </span><span class="s1">indices</span>

<span class="s3">def </span><span class="s1">_bcoo_todense_batching_rule(batched_args</span><span class="s3">, </span><span class="s1">batch_dims</span><span class="s3">, </span><span class="s1">*</span><span class="s3">, </span><span class="s1">spinfo):</span>
  <span class="s1">data</span><span class="s3">, </span><span class="s1">indices</span><span class="s3">, </span><span class="s1">spinfo = _bcoo_batch_dims_to_front(batched_args</span><span class="s3">, </span><span class="s1">batch_dims</span><span class="s3">, </span><span class="s1">spinfo)</span>
  <span class="s3">return </span><span class="s1">_bcoo_todense(data</span><span class="s3">, </span><span class="s1">indices</span><span class="s3">, </span><span class="s1">spinfo=spinfo)</span><span class="s3">, </span><span class="s4">0</span>

<span class="s1">ad.defjvp(bcoo_todense_p</span><span class="s3">, </span><span class="s1">_bcoo_todense_jvp</span><span class="s3">, None</span><span class="s1">)</span>
<span class="s1">ad.primitive_transposes[bcoo_todense_p] = _bcoo_todense_transpose</span>
<span class="s1">batching.primitive_batchers[bcoo_todense_p] = _bcoo_todense_batching_rule</span>
<span class="s1">mlir.register_lowering(bcoo_todense_p</span><span class="s3">, </span><span class="s1">mlir.lower_fun(</span>
    <span class="s1">_bcoo_todense_impl</span><span class="s3">, </span><span class="s1">multiple_results=</span><span class="s3">False</span><span class="s1">))</span>

<span class="s0">#--------------------------------------------------------------------</span>
<span class="s0"># bcoo_fromdense</span>

<span class="s1">bcoo_fromdense_p = core.Primitive(</span><span class="s5">'bcoo_fromdense'</span><span class="s1">)</span>
<span class="s1">bcoo_fromdense_p.multiple_results = </span><span class="s3">True</span>

<span class="s1">_TRACED_NSE_ERROR = </span><span class="s5">&quot;&quot;&quot; 
The error arose for the nse argument of bcoo_fromdense. In order for 
BCOO.fromdense() to be used in traced/compiled code, you must pass a concrete 
value to the nse (number of stored elements) argument. 
&quot;&quot;&quot;</span>

<span class="s3">def </span><span class="s1">bcoo_fromdense(mat: Array</span><span class="s3">, </span><span class="s1">*</span><span class="s3">, </span><span class="s1">nse: Optional[int] = </span><span class="s3">None, </span><span class="s1">n_batch: int = </span><span class="s4">0</span><span class="s3">,</span>
                   <span class="s1">n_dense: int = </span><span class="s4">0</span><span class="s3">, </span><span class="s1">index_dtype: DTypeLike = jnp.int32) -&gt; BCOO:</span>
  <span class="s2">&quot;&quot;&quot;Create BCOO-format sparse matrix from a dense matrix. 
 
  Args: 
    mat : array to be converted to BCOO. 
    nse : number of specified elements in each batch 
    n_batch : number of batch dimensions (default: 0) 
    n_dense : number of block_dimensions (default: 0) 
    index_dtype : dtype of sparse indices (default: int32) 
 
  Returns: 
    mat_bcoo: BCOO representation of the matrix. 
  &quot;&quot;&quot;</span>
  <span class="s1">mat = jnp.asarray(mat)</span>
  <span class="s3">if </span><span class="s1">nse </span><span class="s3">is None</span><span class="s1">:</span>
    <span class="s1">nse = _count_stored_elements(mat</span><span class="s3">, </span><span class="s1">n_batch</span><span class="s3">, </span><span class="s1">n_dense)</span>
  <span class="s1">nse_int = core.concrete_or_error(operator.index</span><span class="s3">, </span><span class="s1">nse</span><span class="s3">, </span><span class="s1">_TRACED_NSE_ERROR)</span>
  <span class="s3">return </span><span class="s1">BCOO(_bcoo_fromdense(mat</span><span class="s3">, </span><span class="s1">nse=nse_int</span><span class="s3">, </span><span class="s1">n_batch=n_batch</span><span class="s3">, </span><span class="s1">n_dense=n_dense</span><span class="s3">,</span>
                              <span class="s1">index_dtype=index_dtype)</span><span class="s3">,</span>
              <span class="s1">shape=mat.shape</span><span class="s3">, </span><span class="s1">indices_sorted=</span><span class="s3">True, </span><span class="s1">unique_indices=</span><span class="s3">True</span><span class="s1">)</span>

<span class="s3">def </span><span class="s1">_bcoo_fromdense(mat: Array</span><span class="s3">, </span><span class="s1">*</span><span class="s3">, </span><span class="s1">nse: int</span><span class="s3">, </span><span class="s1">n_batch: int = </span><span class="s4">0</span><span class="s3">, </span><span class="s1">n_dense: int = </span><span class="s4">0</span><span class="s3">,</span>
                    <span class="s1">index_dtype: DTypeLike = jnp.int32) -&gt; Tuple[Array</span><span class="s3">, </span><span class="s1">Array]:</span>
  <span class="s2">&quot;&quot;&quot;Create BCOO-format sparse matrix from a dense matrix. 
 
  Args: 
    mat : array to be converted to BCOO, with ``ndim = n_batch + n_sparse + n_dense``. 
    nse : number of specified elements in each batch 
    n_batch : number of batch dimensions (default: 0) 
    n_dense : number of block_dimensions (default: 0) 
    index_dtype : dtype of sparse indices (default: int32) 
 
  Returns: 
    data : array of shape ``mat.shape[:n_batch] + (nse,) + mat.shape[mat.ndim - n_dense:]`` 
      and dtype ``mat.dtype`` 
    indices : array of shape ``mat.shape[:n_batch] + (n_sparse, nse)`` 
  &quot;&quot;&quot;</span>
  <span class="s1">mat = jnp.asarray(mat)</span>
  <span class="s1">nse = core.concrete_or_error(operator.index</span><span class="s3">, </span><span class="s1">nse</span><span class="s3">, </span><span class="s1">_TRACED_NSE_ERROR)</span>
  <span class="s3">return </span><span class="s1">bcoo_fromdense_p.bind(mat</span><span class="s3">, </span><span class="s1">nse=nse</span><span class="s3">, </span><span class="s1">n_batch=n_batch</span><span class="s3">, </span><span class="s1">n_dense=n_dense</span><span class="s3">,</span>
                               <span class="s1">index_dtype=index_dtype)</span>

<span class="s1">@bcoo_fromdense_p.def_impl</span>
<span class="s3">def </span><span class="s1">_bcoo_fromdense_impl(mat</span><span class="s3">, </span><span class="s1">*</span><span class="s3">, </span><span class="s1">nse</span><span class="s3">, </span><span class="s1">n_batch</span><span class="s3">, </span><span class="s1">n_dense</span><span class="s3">, </span><span class="s1">index_dtype):</span>
  <span class="s1">mat = jnp.asarray(mat)</span>
  <span class="s1">n_sparse = mat.ndim - n_dense - n_batch</span>
  <span class="s1">mask = (mat != </span><span class="s4">0</span><span class="s1">)</span>
  <span class="s3">if </span><span class="s1">n_dense &gt; </span><span class="s4">0</span><span class="s1">:</span>
    <span class="s1">mask = mask.any([-(i + </span><span class="s4">1</span><span class="s1">) </span><span class="s3">for </span><span class="s1">i </span><span class="s3">in </span><span class="s1">range(n_dense)])</span>
  <span class="s1">@partial(nfold_vmap</span><span class="s3">, </span><span class="s1">N=n_batch</span><span class="s3">, </span><span class="s1">broadcasted=</span><span class="s3">False</span><span class="s1">)</span>
  <span class="s3">def </span><span class="s1">_nonzero(a):</span>
    <span class="s3">if </span><span class="s1">a.ndim:</span>
      <span class="s3">return </span><span class="s1">jnp.nonzero(a</span><span class="s3">, </span><span class="s1">size=nse</span><span class="s3">, </span><span class="s1">fill_value=a.shape[:n_sparse])</span>
    <span class="s3">return </span><span class="s1">()</span>
  <span class="s1">indices = _nonzero(mask)</span>
  <span class="s3">if not </span><span class="s1">indices:</span>
    <span class="s1">indices = jnp.zeros(mask.shape[:n_batch] + (nse</span><span class="s3">, </span><span class="s4">0</span><span class="s1">)</span><span class="s3">, </span><span class="s1">index_dtype)</span>
  <span class="s3">else</span><span class="s1">:</span>
    <span class="s1">indices = jnp.moveaxis(jnp.array(indices</span><span class="s3">, </span><span class="s1">index_dtype)</span><span class="s3">, </span><span class="s4">0</span><span class="s3">, </span><span class="s1">n_batch + </span><span class="s4">1</span><span class="s1">)</span>
  <span class="s1">data = _bcoo_extract(indices</span><span class="s3">, </span><span class="s1">mat)</span>

  <span class="s1">true_nse = mask.sum(list(range(n_batch</span><span class="s3">, </span><span class="s1">mask.ndim)))[...</span><span class="s3">, None</span><span class="s1">]</span>
  <span class="s1">true_nonzeros = lax.broadcasted_iota(true_nse.dtype</span><span class="s3">, </span><span class="s1">(</span><span class="s4">1</span><span class="s3">,</span><span class="s1">) * n_batch + (nse</span><span class="s3">,</span><span class="s1">)</span><span class="s3">, </span><span class="s1">n_batch) &lt; true_nse</span>
  <span class="s1">true_nonzeros = true_nonzeros[(n_batch + </span><span class="s4">1</span><span class="s1">) * (slice(</span><span class="s3">None</span><span class="s1">)</span><span class="s3">,</span><span class="s1">) + n_dense * (</span><span class="s3">None,</span><span class="s1">)]</span>
  <span class="s1">data = jnp.where(true_nonzeros</span><span class="s3">, </span><span class="s1">data</span><span class="s3">, </span><span class="s4">0</span><span class="s1">)</span>

  <span class="s3">return </span><span class="s1">data</span><span class="s3">, </span><span class="s1">indices</span>

<span class="s1">@bcoo_fromdense_p.def_abstract_eval</span>
<span class="s3">def </span><span class="s1">_bcoo_fromdense_abstract_eval(mat</span><span class="s3">, </span><span class="s1">*</span><span class="s3">, </span><span class="s1">nse</span><span class="s3">, </span><span class="s1">n_batch</span><span class="s3">, </span><span class="s1">n_dense</span><span class="s3">, </span><span class="s1">index_dtype):</span>
  <span class="s1">n_sparse = mat.ndim - n_batch - n_dense</span>
  <span class="s1">data_shape = mat.shape[:n_batch] + (nse</span><span class="s3">,</span><span class="s1">) + mat.shape[n_batch + n_sparse:]</span>
  <span class="s1">index_shape = mat.shape[:n_batch] + (nse</span><span class="s3">, </span><span class="s1">n_sparse)</span>
  <span class="s3">return </span><span class="s1">core.ShapedArray(data_shape</span><span class="s3">, </span><span class="s1">mat.dtype)</span><span class="s3">, </span><span class="s1">core.ShapedArray(index_shape</span><span class="s3">, </span><span class="s1">index_dtype)</span>

<span class="s3">def </span><span class="s1">_bcoo_fromdense_jvp(primals</span><span class="s3">, </span><span class="s1">tangents</span><span class="s3">, </span><span class="s1">*</span><span class="s3">, </span><span class="s1">nse</span><span class="s3">, </span><span class="s1">n_batch</span><span class="s3">, </span><span class="s1">n_dense</span><span class="s3">, </span><span class="s1">index_dtype):</span>
  <span class="s1">M</span><span class="s3">, </span><span class="s1">= primals</span>
  <span class="s1">Mdot</span><span class="s3">, </span><span class="s1">= tangents</span>

  <span class="s1">primals_out = _bcoo_fromdense(M</span><span class="s3">, </span><span class="s1">nse=nse</span><span class="s3">, </span><span class="s1">n_batch=n_batch</span><span class="s3">, </span><span class="s1">n_dense=n_dense</span><span class="s3">, </span><span class="s1">index_dtype=index_dtype)</span>
  <span class="s1">data</span><span class="s3">, </span><span class="s1">indices = primals_out</span>

  <span class="s3">if </span><span class="s1">type(Mdot) </span><span class="s3">is </span><span class="s1">ad.Zero:</span>
    <span class="s1">data_dot = ad.Zero.from_value(data)</span>
  <span class="s3">else</span><span class="s1">:</span>
    <span class="s1">data_dot = _bcoo_extract(indices</span><span class="s3">, </span><span class="s1">Mdot)</span>

  <span class="s1">tangents_out = (data_dot</span><span class="s3">, </span><span class="s1">ad.Zero.from_value(indices))</span>

  <span class="s3">return </span><span class="s1">primals_out</span><span class="s3">, </span><span class="s1">tangents_out</span>

<span class="s3">def </span><span class="s1">_bcoo_fromdense_transpose(ct</span><span class="s3">, </span><span class="s1">M</span><span class="s3">, </span><span class="s1">*</span><span class="s3">, </span><span class="s1">nse</span><span class="s3">, </span><span class="s1">n_batch</span><span class="s3">, </span><span class="s1">n_dense</span><span class="s3">, </span><span class="s1">index_dtype):</span>
  <span class="s1">data</span><span class="s3">, </span><span class="s1">indices = ct</span>
  <span class="s1">n_sparse = M.ndim - n_batch - n_dense</span>
  <span class="s3">assert </span><span class="s1">data.shape == M.shape[:n_batch] + (nse</span><span class="s3">,</span><span class="s1">) + M.shape[n_batch + n_sparse:]</span>
  <span class="s3">assert </span><span class="s1">indices.shape == M.shape[:n_batch] + (n_sparse</span><span class="s3">, </span><span class="s1">nse)</span>
  <span class="s3">assert </span><span class="s1">indices.dtype == index_dtype</span>
  <span class="s3">if </span><span class="s1">isinstance(indices</span><span class="s3">, </span><span class="s1">ad.Zero):</span>
    <span class="s3">raise </span><span class="s1">ValueError(</span><span class="s5">&quot;Cannot transpose with respect to sparse indices&quot;</span><span class="s1">)</span>
  <span class="s3">assert </span><span class="s1">ad.is_undefined_primal(M)</span>
  <span class="s3">return </span><span class="s1">_bcoo_todense(data</span><span class="s3">, </span><span class="s1">indices</span><span class="s3">, </span><span class="s1">spinfo=SparseInfo(M.aval.shape))</span>

<span class="s3">def </span><span class="s1">_bcoo_fromdense_batching_rule(batched_args</span><span class="s3">, </span><span class="s1">batch_dims</span><span class="s3">, </span><span class="s1">*</span><span class="s3">, </span><span class="s1">nse</span><span class="s3">, </span><span class="s1">n_batch</span><span class="s3">, </span><span class="s1">n_dense</span><span class="s3">, </span><span class="s1">index_dtype):</span>
  <span class="s1">M</span><span class="s3">, </span><span class="s1">= batched_args</span>
  <span class="s1">bdim</span><span class="s3">, </span><span class="s1">= batch_dims</span>
  <span class="s3">if not </span><span class="s1">(</span><span class="s4">0 </span><span class="s1">&lt;= bdim &lt;= n_batch):</span>
    <span class="s3">raise </span><span class="s1">ValueError(</span><span class="s5">f&quot;Expected 0 &lt; bdim &lt;= n_batch; got </span><span class="s3">{</span><span class="s1">bdim=</span><span class="s3">}</span><span class="s5">, </span><span class="s3">{</span><span class="s1">n_batch=</span><span class="s3">}</span><span class="s5">&quot;</span><span class="s1">)</span>
  <span class="s3">return </span><span class="s1">_bcoo_fromdense(M</span><span class="s3">, </span><span class="s1">nse=nse</span><span class="s3">, </span><span class="s1">n_batch=n_batch + </span><span class="s4">1</span><span class="s3">, </span><span class="s1">n_dense=n_dense</span><span class="s3">, </span><span class="s1">index_dtype=index_dtype)</span><span class="s3">, </span><span class="s1">(bdim</span><span class="s3">, </span><span class="s1">bdim)</span>

<span class="s1">ad.primitive_jvps[bcoo_fromdense_p] = _bcoo_fromdense_jvp</span>
<span class="s1">ad.primitive_transposes[bcoo_fromdense_p] = _bcoo_fromdense_transpose</span>
<span class="s1">batching.primitive_batchers[bcoo_fromdense_p] = _bcoo_fromdense_batching_rule</span>
<span class="s1">mlir.register_lowering(bcoo_fromdense_p</span><span class="s3">, </span><span class="s1">mlir.lower_fun(</span>
    <span class="s1">_bcoo_fromdense_impl</span><span class="s3">, </span><span class="s1">multiple_results=</span><span class="s3">True</span><span class="s1">))</span>

<span class="s0">#----------------------------------------------------------------------</span>
<span class="s0"># bcoo_extract</span>

<span class="s1">bcoo_extract_p = core.Primitive(</span><span class="s5">'bcoo_extract'</span><span class="s1">)</span>


<span class="s3">def </span><span class="s1">bcoo_extract(sparr: BCOO</span><span class="s3">, </span><span class="s1">arr: ArrayLike</span><span class="s3">, </span><span class="s1">*</span><span class="s3">, </span><span class="s1">assume_unique: Optional[bool] = </span><span class="s3">None</span><span class="s1">) -&gt; BCOO:</span>
  <span class="s2">&quot;&quot;&quot;Extract values from a dense array according to the sparse array's indices. 
 
  Args: 
    sparr : BCOO array whose indices will be used for the output. 
    arr : ArrayLike with shape equal to self.shape 
    assume_unique : bool, defaults to sparr.unique_indices 
      If True, extract values for every index, even if index contains duplicates. 
      If False, duplicate indices will have their values summed and returned in 
      the position of the first index. 
 
  Returns: 
    extracted : a BCOO array with the same sparsity pattern as self. 
  &quot;&quot;&quot;</span>
  <span class="s3">if not </span><span class="s1">isinstance(sparr</span><span class="s3">, </span><span class="s1">BCOO):</span>
    <span class="s3">raise </span><span class="s1">ValueError(</span><span class="s5">f&quot;First argument to bcoo_extract should be a BCOO array. Got </span><span class="s3">{</span><span class="s1">type(sparr)=</span><span class="s3">}</span><span class="s5">&quot;</span><span class="s1">)</span>
  <span class="s1">arr = jnp.asarray(arr)</span>
  <span class="s3">if </span><span class="s1">arr.shape != sparr.shape:</span>
    <span class="s3">raise </span><span class="s1">ValueError(</span><span class="s5">f&quot;shape mismatch: </span><span class="s3">{</span><span class="s1">sparr.shape=</span><span class="s3">} {</span><span class="s1">arr.shape=</span><span class="s3">}</span><span class="s5">&quot;</span><span class="s1">)</span>
  <span class="s3">if </span><span class="s1">assume_unique </span><span class="s3">is None</span><span class="s1">:</span>
    <span class="s1">assume_unique = sparr.unique_indices</span>
  <span class="s1">data = _bcoo_extract(sparr.indices</span><span class="s3">, </span><span class="s1">arr</span><span class="s3">, </span><span class="s1">assume_unique=assume_unique)</span>
  <span class="s3">return </span><span class="s1">BCOO((data</span><span class="s3">, </span><span class="s1">sparr.indices)</span><span class="s3">, </span><span class="s1">**sparr._info._asdict())</span>


<span class="s3">def </span><span class="s1">_bcoo_extract(indices: Array</span><span class="s3">, </span><span class="s1">arr: Array</span><span class="s3">, </span><span class="s1">*</span><span class="s3">, </span><span class="s1">assume_unique=</span><span class="s3">True</span><span class="s1">) -&gt; Array:</span>
  <span class="s2">&quot;&quot;&quot;Extract BCOO data values from a dense array at given BCOO indices. 
 
  Args: 
    indices: An ndarray; see BCOO indices. 
    arr: A dense array. 
    assume_unique: bool, default=True 
      If True, then indices will be assumed unique and a value will be extracted 
      from arr for each index. Otherwise, extra work will be done to de-duplicate 
      indices to zero-out duplicate extracted values. 
 
  Returns: 
    An ndarray; see BCOO data. 
  &quot;&quot;&quot;</span>
  <span class="s3">return </span><span class="s1">bcoo_extract_p.bind(indices</span><span class="s3">, </span><span class="s1">arr</span><span class="s3">, </span><span class="s1">assume_unique=assume_unique)</span>

<span class="s1">@bcoo_extract_p.def_impl</span>
<span class="s3">def </span><span class="s1">_bcoo_extract_impl(indices</span><span class="s3">, </span><span class="s1">arr</span><span class="s3">, </span><span class="s1">*</span><span class="s3">, </span><span class="s1">assume_unique):</span>
  <span class="s1">arr = jnp.asarray(arr)</span>
  <span class="s1">props = _validate_bcoo_indices(indices</span><span class="s3">, </span><span class="s1">arr.shape)</span>
  <span class="s3">if not </span><span class="s1">assume_unique:</span>
    <span class="s1">indices</span><span class="s3">, </span><span class="s1">sort_ind = _unique_indices(indices</span><span class="s3">, </span><span class="s1">shape=arr.shape</span><span class="s3">, </span><span class="s1">return_index=</span><span class="s3">True</span><span class="s1">)</span>
    <span class="s1">original_props = props</span>
    <span class="s1">props = _validate_bcoo_indices(indices</span><span class="s3">, </span><span class="s1">arr.shape)</span>

  <span class="s1">ind_slices = tuple(np.zeros(s</span><span class="s3">, </span><span class="s1">int) </span><span class="s3">if </span><span class="s1">i_s == </span><span class="s4">1 </span><span class="s3">else </span><span class="s1">np.arange(s)</span>
                     <span class="s3">for </span><span class="s1">s</span><span class="s3">, </span><span class="s1">i_s </span><span class="s3">in </span><span class="s1">zip(arr.shape[:props.n_batch]</span><span class="s3">, </span><span class="s1">indices.shape[:props.n_batch]))</span>
  <span class="s1">grid = tuple(np.meshgrid(*ind_slices</span><span class="s3">, </span><span class="s1">indexing=</span><span class="s5">'ij'</span><span class="s3">, </span><span class="s1">sparse=</span><span class="s3">True</span><span class="s1">))</span>
  <span class="s1">sparse_ind = tuple(indices[grid + (slice(</span><span class="s3">None</span><span class="s1">)</span><span class="s3">, </span><span class="s1">i)] </span><span class="s3">for </span><span class="s1">i </span><span class="s3">in </span><span class="s1">range(props.n_sparse))</span>

  <span class="s1">batch_slices = tuple(np.arange(s) </span><span class="s3">for </span><span class="s1">s </span><span class="s3">in </span><span class="s1">arr.shape[:props.n_batch])</span>
  <span class="s1">grid = np.meshgrid(*batch_slices</span><span class="s3">, </span><span class="s1">np.arange(</span><span class="s4">1</span><span class="s1">)</span><span class="s3">, </span><span class="s1">indexing=</span><span class="s5">'ij'</span><span class="s3">, </span><span class="s1">sparse=</span><span class="s3">True</span><span class="s1">)</span>
  <span class="s1">batch_ind = tuple(grid)[:-</span><span class="s4">1</span><span class="s1">]</span>

  <span class="s3">if not </span><span class="s1">sparse_ind + batch_ind:</span>
    <span class="s1">result = arr[</span><span class="s3">None</span><span class="s1">]</span>
  <span class="s3">else</span><span class="s1">:</span>
    <span class="s1">result = arr.at[batch_ind + sparse_ind].get(mode=</span><span class="s5">'fill'</span><span class="s3">, </span><span class="s1">fill_value=</span><span class="s4">0</span><span class="s1">)</span>
  <span class="s3">if </span><span class="s1">props.n_sparse == </span><span class="s4">0 </span><span class="s3">and </span><span class="s1">props.nse != </span><span class="s4">1</span><span class="s1">:</span>
    <span class="s3">if </span><span class="s1">assume_unique:</span>
      <span class="s1">result = lax.broadcast_in_dim(</span>
        <span class="s1">result</span><span class="s3">, </span><span class="s1">_tuple_replace(result.shape</span><span class="s3">, </span><span class="s1">props.n_batch</span><span class="s3">, </span><span class="s1">props.nse)</span><span class="s3">, </span><span class="s1">range(result.ndim))</span>
    <span class="s3">else</span><span class="s1">:</span>
      <span class="s1">out_shape = _tuple_replace(result.shape</span><span class="s3">, </span><span class="s1">props.n_batch</span><span class="s3">, </span><span class="s1">original_props.nse)</span>
      <span class="s1">ind = props.n_batch * (slice(</span><span class="s3">None</span><span class="s1">)</span><span class="s3">,</span><span class="s1">) + (slice(</span><span class="s4">1</span><span class="s1">)</span><span class="s3">,</span><span class="s1">)</span>
      <span class="s1">result = jnp.zeros_like(result</span><span class="s3">, </span><span class="s1">shape=out_shape).at[ind].set(result)</span>
  <span class="s3">if not </span><span class="s1">assume_unique:</span>
    <span class="s1">unbatched_out_shape = (original_props.nse</span><span class="s3">, </span><span class="s1">*result.shape[props.n_batch + </span><span class="s4">1</span><span class="s1">:])</span>
    <span class="s3">def </span><span class="s1">f(r</span><span class="s3">, </span><span class="s1">i):</span>
      <span class="s3">return </span><span class="s1">jnp.zeros_like(r</span><span class="s3">, </span><span class="s1">shape=unbatched_out_shape).at[i].add(r)</span>
    <span class="s3">for </span><span class="s1">_ </span><span class="s3">in </span><span class="s1">range(props.n_batch):</span>
      <span class="s1">f = vmap(f)</span>
    <span class="s1">result = f(result</span><span class="s3">, </span><span class="s1">sort_ind)</span>
  <span class="s3">return </span><span class="s1">result</span>

<span class="s1">@bcoo_extract_p.def_abstract_eval</span>
<span class="s3">def </span><span class="s1">_bcoo_extract_abstract_eval(indices</span><span class="s3">, </span><span class="s1">arr</span><span class="s3">, </span><span class="s1">*</span><span class="s3">, </span><span class="s1">assume_unique):</span>
  <span class="s1">_ = bool(assume_unique)</span>
  <span class="s1">n_batch</span><span class="s3">, </span><span class="s1">_</span><span class="s3">, </span><span class="s1">n_dense</span><span class="s3">, </span><span class="s1">nse = _validate_bcoo_indices(indices</span><span class="s3">, </span><span class="s1">arr.shape)</span>
  <span class="s1">out_shape = arr.shape[:n_batch] + (nse</span><span class="s3">,</span><span class="s1">) + arr.shape[arr.ndim - n_dense:]</span>
  <span class="s3">return </span><span class="s1">core.ShapedArray(out_shape</span><span class="s3">, </span><span class="s1">arr.dtype)</span>

<span class="s3">def </span><span class="s1">_bcoo_extract_jvp(arr_dot</span><span class="s3">, </span><span class="s1">indices</span><span class="s3">, </span><span class="s1">arr</span><span class="s3">, </span><span class="s1">*</span><span class="s3">, </span><span class="s1">assume_unique):</span>
  <span class="s3">assert </span><span class="s1">arr_dot.shape == arr.shape</span>
  <span class="s3">return </span><span class="s1">_bcoo_extract(indices</span><span class="s3">, </span><span class="s1">arr_dot</span><span class="s3">, </span><span class="s1">assume_unique=assume_unique)</span>

<span class="s3">def </span><span class="s1">_bcoo_extract_transpose(ct</span><span class="s3">, </span><span class="s1">indices</span><span class="s3">, </span><span class="s1">arr</span><span class="s3">, </span><span class="s1">*</span><span class="s3">, </span><span class="s1">assume_unique):</span>
  <span class="s3">if not </span><span class="s1">assume_unique:</span>
    <span class="s3">raise </span><span class="s1">NotImplementedError(</span><span class="s5">&quot;transpose of bcoo_extract with assume_unique=False&quot;</span><span class="s1">)</span>
  <span class="s3">assert </span><span class="s1">ad.is_undefined_primal(arr)</span>
  <span class="s3">if </span><span class="s1">ad.is_undefined_primal(indices):</span>
    <span class="s3">raise </span><span class="s1">ValueError(</span><span class="s5">&quot;Cannot transpose with respect to sparse indices&quot;</span><span class="s1">)</span>
  <span class="s3">assert </span><span class="s1">ct.dtype == arr.aval.dtype</span>
  <span class="s3">return </span><span class="s1">indices</span><span class="s3">, </span><span class="s1">_bcoo_todense(ct</span><span class="s3">, </span><span class="s1">indices</span><span class="s3">, </span><span class="s1">spinfo=SparseInfo(arr.aval.shape))</span>

<span class="s3">def </span><span class="s1">_bcoo_extract_batching_rule(batched_args</span><span class="s3">, </span><span class="s1">batch_dims</span><span class="s3">, </span><span class="s1">*</span><span class="s3">, </span><span class="s1">assume_unique):</span>
  <span class="s1">indices</span><span class="s3">, </span><span class="s1">arr = batched_args</span>
  <span class="s3">assert </span><span class="s1">any(b </span><span class="s3">is not None for </span><span class="s1">b </span><span class="s3">in </span><span class="s1">batch_dims)</span>
  <span class="s3">if </span><span class="s1">batch_dims[</span><span class="s4">0</span><span class="s1">] </span><span class="s3">is None</span><span class="s1">:</span>
    <span class="s1">bdim = batch_dims[</span><span class="s4">1</span><span class="s1">]</span>
    <span class="s1">indices = lax.expand_dims(indices</span><span class="s3">, </span><span class="s1">(bdim</span><span class="s3">,</span><span class="s1">))</span>
  <span class="s3">elif </span><span class="s1">batch_dims[</span><span class="s4">1</span><span class="s1">] </span><span class="s3">is None</span><span class="s1">:</span>
    <span class="s0"># TODO(jakevdp) can we handle this case without explicit broadcasting?</span>
    <span class="s1">bdim = batch_dims[</span><span class="s4">0</span><span class="s1">]</span>
    <span class="s1">result_shape = list(arr.shape)</span>
    <span class="s1">result_shape.insert(bdim</span><span class="s3">, </span><span class="s1">indices.shape[bdim])</span>
    <span class="s1">arr = lax.broadcast_in_dim(arr</span><span class="s3">, </span><span class="s1">result_shape</span><span class="s3">, </span><span class="s1">(bdim</span><span class="s3">,</span><span class="s1">))</span>
  <span class="s3">else</span><span class="s1">:</span>
    <span class="s3">if </span><span class="s1">batch_dims[</span><span class="s4">0</span><span class="s1">] != batch_dims[</span><span class="s4">1</span><span class="s1">]:</span>
      <span class="s3">raise </span><span class="s1">NotImplementedError(</span><span class="s5">&quot;bcoo_extract with unequal batch dimensions.&quot;</span><span class="s1">)</span>
    <span class="s1">bdim = batch_dims[</span><span class="s4">0</span><span class="s1">]</span>
  <span class="s1">n_batch = indices.ndim - </span><span class="s4">2</span>
  <span class="s3">if </span><span class="s1">bdim &gt;= n_batch:</span>
    <span class="s3">raise </span><span class="s1">ValueError(</span><span class="s5">f&quot;</span><span class="s3">{</span><span class="s1">batch_dims=</span><span class="s3">} </span><span class="s5">out of range for indices with </span><span class="s3">{</span><span class="s1">n_batch=</span><span class="s3">}</span><span class="s5">&quot;</span><span class="s1">)</span>
  <span class="s3">return </span><span class="s1">_bcoo_extract(indices</span><span class="s3">, </span><span class="s1">arr</span><span class="s3">, </span><span class="s1">assume_unique=assume_unique)</span><span class="s3">, </span><span class="s1">bdim</span>

<span class="s1">ad.defjvp(bcoo_extract_p</span><span class="s3">, None, </span><span class="s1">_bcoo_extract_jvp)</span>
<span class="s1">ad.primitive_transposes[bcoo_extract_p] = _bcoo_extract_transpose</span>
<span class="s1">batching.primitive_batchers[bcoo_extract_p] = _bcoo_extract_batching_rule</span>
<span class="s1">mlir.register_lowering(bcoo_extract_p</span><span class="s3">, </span><span class="s1">mlir.lower_fun(</span>
    <span class="s1">_bcoo_extract_impl</span><span class="s3">, </span><span class="s1">multiple_results=</span><span class="s3">False</span><span class="s1">))</span>

<span class="s0">#----------------------------------------------------------------------</span>
<span class="s0"># bcoo_transpose</span>
<span class="s0"># transpose of a BCOO array</span>

<span class="s1">bcoo_transpose_p = core.Primitive(</span><span class="s5">'bcoo_transpose'</span><span class="s1">)</span>
<span class="s1">bcoo_transpose_p.multiple_results = </span><span class="s3">True</span>

<span class="s3">def </span><span class="s1">bcoo_transpose(mat: BCOO</span><span class="s3">, </span><span class="s1">*</span><span class="s3">, </span><span class="s1">permutation: Sequence[int]) -&gt; BCOO:</span>
  <span class="s2">&quot;&quot;&quot;Transpose a BCOO-format array. 
 
  Args: 
    mat: A BCOO-format array. 
    permutation:  A tuple or list or ndarray which contains a permutation of 
      [0,1,..,N-1] where N is the number of axes of ``mat`` in the order of 
      batch, sparse, and dense dimensions. The i’th axis of the returned array 
      corresponds to the axis numbered permutation[i] of ``mat``. Transpose 
      permutation currently does not support permuting batch axes with non-batch 
      axes nor permutating dense axes with non-dense axes. 
 
  Returns: 
    A BCOO-format array. 
  &quot;&quot;&quot;</span>
  <span class="s1">buffers = _bcoo_transpose(mat.data</span><span class="s3">, </span><span class="s1">mat.indices</span><span class="s3">, </span><span class="s1">permutation=permutation</span><span class="s3">, </span><span class="s1">spinfo=mat._info)</span>
  <span class="s1">out_shape = tuple(mat.shape[p] </span><span class="s3">for </span><span class="s1">p </span><span class="s3">in </span><span class="s1">permutation)</span>
  <span class="s3">return </span><span class="s1">BCOO(buffers</span><span class="s3">, </span><span class="s1">shape=out_shape</span><span class="s3">, </span><span class="s1">unique_indices=mat.unique_indices)</span>

<span class="s3">def </span><span class="s1">_bcoo_transpose(data: Array</span><span class="s3">, </span><span class="s1">indices: Array</span><span class="s3">, </span><span class="s1">*</span><span class="s3">,</span>
                    <span class="s1">permutation: Sequence[int]</span><span class="s3">, </span><span class="s1">spinfo: SparseInfo) -&gt; Tuple[Array</span><span class="s3">, </span><span class="s1">Array]:</span>
  <span class="s1">permutation = tuple(permutation)</span>
  <span class="s3">if </span><span class="s1">permutation == tuple(range(len(spinfo.shape))):</span>
    <span class="s3">return </span><span class="s1">data</span><span class="s3">, </span><span class="s1">indices</span>
  <span class="s3">else</span><span class="s1">:</span>
    <span class="s3">return </span><span class="s1">bcoo_transpose_p.bind(data</span><span class="s3">, </span><span class="s1">indices</span><span class="s3">, </span><span class="s1">permutation=permutation</span><span class="s3">,</span>
                                 <span class="s1">spinfo=spinfo)</span>

<span class="s3">def </span><span class="s1">_validate_permutation(data</span><span class="s3">, </span><span class="s1">indices</span><span class="s3">, </span><span class="s1">permutation</span><span class="s3">, </span><span class="s1">shape):</span>
  <span class="s3">if not </span><span class="s1">isinstance(permutation</span><span class="s3">, </span><span class="s1">(tuple</span><span class="s3">, </span><span class="s1">list</span><span class="s3">, </span><span class="s1">np.ndarray)):</span>
    <span class="s3">raise </span><span class="s1">TypeError(</span><span class="s5">f&quot;transpose permutation must be a tuple/list/ndarray, got </span><span class="s3">{</span><span class="s1">type(permutation)</span><span class="s3">}</span><span class="s5">.&quot;</span><span class="s1">)</span>
  <span class="s3">if </span><span class="s1">tuple(sorted(permutation)) != tuple(range(len(shape))):</span>
    <span class="s3">raise </span><span class="s1">TypeError(</span><span class="s5">&quot;transpose permutation isn't a permutation of operand dimensions, &quot;</span>
                    <span class="s5">f&quot;got permutation </span><span class="s3">{</span><span class="s1">permutation</span><span class="s3">} </span><span class="s5">for shape </span><span class="s3">{</span><span class="s1">shape</span><span class="s3">}</span><span class="s5">.&quot;</span><span class="s1">)</span>
  <span class="s1">n_batch</span><span class="s3">, </span><span class="s1">n_sparse</span><span class="s3">, </span><span class="s1">n_dense</span><span class="s3">, </span><span class="s1">_ = _validate_bcoo(data</span><span class="s3">, </span><span class="s1">indices</span><span class="s3">, </span><span class="s1">shape)</span>
  <span class="s1">batch_perm = permutation[:n_batch]</span>
  <span class="s1">sparse_perm = [p - n_batch </span><span class="s3">for </span><span class="s1">p </span><span class="s3">in </span><span class="s1">permutation[n_batch: n_batch + n_sparse]]</span>
  <span class="s1">dense_perm = [p - n_sparse - n_batch </span><span class="s3">for </span><span class="s1">p </span><span class="s3">in </span><span class="s1">permutation[n_batch + n_sparse:]]</span>
  <span class="s3">if </span><span class="s1">n_batch </span><span class="s3">and </span><span class="s1">tuple(sorted(batch_perm)) != tuple(range(n_batch)):</span>
    <span class="s3">raise </span><span class="s1">NotImplementedError(</span><span class="s5">&quot;transpose permutation cannot permute batch axes with non-batch axes; &quot;</span>
                              <span class="s5">f&quot;got permutation </span><span class="s3">{</span><span class="s1">permutation</span><span class="s3">}</span><span class="s5">, with </span><span class="s3">{</span><span class="s1">n_batch=</span><span class="s3">}</span><span class="s5">.&quot;</span><span class="s1">)</span>
  <span class="s3">if </span><span class="s1">n_dense </span><span class="s3">and </span><span class="s1">tuple(sorted(dense_perm)) != tuple(range(n_dense)):</span>
    <span class="s3">raise </span><span class="s1">NotImplementedError(</span><span class="s5">&quot;transpose permutation cannot permute dense axes with non-dense axes; &quot;</span>
                              <span class="s5">f&quot;got permutation </span><span class="s3">{</span><span class="s1">permutation</span><span class="s3">}</span><span class="s5">, with </span><span class="s3">{</span><span class="s1">n_dense=</span><span class="s3">}</span><span class="s5">.&quot;</span><span class="s1">)</span>
  <span class="s3">return </span><span class="s1">batch_perm</span><span class="s3">, </span><span class="s1">sparse_perm</span><span class="s3">, </span><span class="s1">dense_perm</span>

<span class="s1">@bcoo_transpose_p.def_impl</span>
<span class="s3">def </span><span class="s1">_bcoo_transpose_impl(data</span><span class="s3">, </span><span class="s1">indices</span><span class="s3">, </span><span class="s1">*</span><span class="s3">, </span><span class="s1">permutation: Sequence[int]</span><span class="s3">, </span><span class="s1">spinfo: SparseInfo):</span>
  <span class="s1">batch_perm</span><span class="s3">, </span><span class="s1">sparse_perm</span><span class="s3">, </span><span class="s1">dense_perm = _validate_permutation(data</span><span class="s3">, </span><span class="s1">indices</span><span class="s3">, </span><span class="s1">permutation</span><span class="s3">, </span><span class="s1">spinfo.shape)</span>
  <span class="s1">n_batch = len(batch_perm)</span>
  <span class="s1">indices = indices[...</span><span class="s3">, </span><span class="s1">sparse_perm].transpose(*batch_perm</span><span class="s3">, </span><span class="s1">n_batch</span><span class="s3">, </span><span class="s1">n_batch + </span><span class="s4">1</span><span class="s1">)</span>
  <span class="s1">data = data.transpose(*batch_perm</span><span class="s3">, </span><span class="s1">n_batch</span><span class="s3">, </span><span class="s1">*(d + n_batch + </span><span class="s4">1 </span><span class="s3">for </span><span class="s1">d </span><span class="s3">in </span><span class="s1">dense_perm))</span>
  <span class="s3">return </span><span class="s1">data</span><span class="s3">, </span><span class="s1">indices</span>

<span class="s1">@bcoo_transpose_p.def_abstract_eval</span>
<span class="s3">def </span><span class="s1">_bcoo_transpose_abstract_eval(data</span><span class="s3">, </span><span class="s1">indices</span><span class="s3">, </span><span class="s1">*</span><span class="s3">, </span><span class="s1">permutation: Sequence[int]</span><span class="s3">, </span><span class="s1">spinfo: SparseInfo):</span>
  <span class="s1">batch_perm</span><span class="s3">, </span><span class="s1">_</span><span class="s3">, </span><span class="s1">dense_perm = _validate_permutation(data</span><span class="s3">, </span><span class="s1">indices</span><span class="s3">, </span><span class="s1">permutation</span><span class="s3">, </span><span class="s1">spinfo.shape)</span>
  <span class="s1">n_batch = len(batch_perm)</span>
  <span class="s1">indices_shape = np.array(indices.shape)[[*batch_perm</span><span class="s3">, </span><span class="s1">n_batch</span><span class="s3">, </span><span class="s1">n_batch + </span><span class="s4">1</span><span class="s1">]]</span>
  <span class="s1">data_shape = np.array(data.shape)[[*batch_perm</span><span class="s3">, </span><span class="s1">n_batch</span><span class="s3">, </span><span class="s1">*(d + n_batch + </span><span class="s4">1 </span><span class="s3">for </span><span class="s1">d </span><span class="s3">in </span><span class="s1">dense_perm)]]</span>
  <span class="s3">return </span><span class="s1">core.ShapedArray(data_shape</span><span class="s3">, </span><span class="s1">data.dtype)</span><span class="s3">, </span><span class="s1">core.ShapedArray(indices_shape</span><span class="s3">, </span><span class="s1">indices.dtype)</span>

<span class="s3">def </span><span class="s1">_bcoo_transpose_jvp(primals</span><span class="s3">, </span><span class="s1">tangents</span><span class="s3">, </span><span class="s1">*</span><span class="s3">, </span><span class="s1">permutation: Sequence[int]</span><span class="s3">, </span><span class="s1">spinfo: SparseInfo):</span>
  <span class="s1">data</span><span class="s3">, </span><span class="s1">indices = primals</span>
  <span class="s1">data_dot</span><span class="s3">, </span><span class="s1">_ = tangents</span>
  <span class="s1">primals_out = _bcoo_transpose(data</span><span class="s3">, </span><span class="s1">indices</span><span class="s3">, </span><span class="s1">permutation=permutation</span><span class="s3">, </span><span class="s1">spinfo=spinfo)</span>
  <span class="s1">data_dot_out</span><span class="s3">, </span><span class="s1">_ = _bcoo_transpose(data_dot</span><span class="s3">, </span><span class="s1">indices</span><span class="s3">, </span><span class="s1">permutation=permutation</span><span class="s3">, </span><span class="s1">spinfo=spinfo)</span>
  <span class="s3">return </span><span class="s1">primals_out</span><span class="s3">, </span><span class="s1">(data_dot_out</span><span class="s3">, </span><span class="s1">ad.Zero.from_value(indices))</span>

<span class="s3">def </span><span class="s1">_bcoo_transpose_transpose(ct</span><span class="s3">, </span><span class="s1">data</span><span class="s3">, </span><span class="s1">indices</span><span class="s3">, </span><span class="s1">*</span><span class="s3">, </span><span class="s1">permutation: Sequence[int]</span><span class="s3">, </span><span class="s1">spinfo: SparseInfo):</span>
  <span class="s1">data_ct</span><span class="s3">, </span><span class="s1">indices_ct = ct</span>
  <span class="s3">assert </span><span class="s1">isinstance(indices_ct</span><span class="s3">, </span><span class="s1">ad.Zero)</span>
  <span class="s3">if </span><span class="s1">ad.is_undefined_primal(indices):</span>
    <span class="s3">raise </span><span class="s1">ValueError(</span><span class="s5">&quot;Cannot transpose with respect to sparse indices&quot;</span><span class="s1">)</span>
  <span class="s3">assert </span><span class="s1">data_ct.dtype == data.aval.dtype</span>
  <span class="s1">ct_spinfo = SparseInfo(tuple(spinfo.shape[p] </span><span class="s3">for </span><span class="s1">p </span><span class="s3">in </span><span class="s1">permutation))</span>
  <span class="s1">rev_permutation = list(np.argsort(permutation))</span>
  <span class="s0"># TODO(jakevdp) avoid dummy indices?</span>
  <span class="s1">dummy_indices = jnp.zeros([</span><span class="s4">1 </span><span class="s3">for </span><span class="s1">i </span><span class="s3">in </span><span class="s1">range(indices.ndim - </span><span class="s4">2</span><span class="s1">)] + list(indices.shape[-</span><span class="s4">2</span><span class="s1">:])</span><span class="s3">, </span><span class="s1">dtype=int)</span>
  <span class="s1">data_trans</span><span class="s3">, </span><span class="s1">_ = _bcoo_transpose(data_ct</span><span class="s3">, </span><span class="s1">dummy_indices</span><span class="s3">, </span><span class="s1">permutation=rev_permutation</span><span class="s3">, </span><span class="s1">spinfo=ct_spinfo)</span>
  <span class="s3">return </span><span class="s1">data_trans</span><span class="s3">, </span><span class="s1">indices_ct</span>

<span class="s3">def </span><span class="s1">_bcoo_transpose_batch_rule(batched_args</span><span class="s3">, </span><span class="s1">batch_dims</span><span class="s3">, </span><span class="s1">*</span><span class="s3">, </span><span class="s1">permutation: Sequence[int]</span><span class="s3">, </span><span class="s1">spinfo: SparseInfo):</span>
  <span class="s1">data</span><span class="s3">, </span><span class="s1">indices</span><span class="s3">, </span><span class="s1">spinfo = _bcoo_batch_dims_to_front(batched_args</span><span class="s3">, </span><span class="s1">batch_dims</span><span class="s3">, </span><span class="s1">spinfo)</span>
  <span class="s1">batched_permutation = (</span><span class="s4">0</span><span class="s3">, </span><span class="s1">*(p + </span><span class="s4">1 </span><span class="s3">for </span><span class="s1">p </span><span class="s3">in </span><span class="s1">permutation))</span>
  <span class="s1">data</span><span class="s3">, </span><span class="s1">indices = _bcoo_transpose(data</span><span class="s3">, </span><span class="s1">indices</span><span class="s3">, </span><span class="s1">permutation=batched_permutation</span><span class="s3">, </span><span class="s1">spinfo=spinfo)</span>
  <span class="s1">batch_dims_out = [</span><span class="s3">None if </span><span class="s1">bdim </span><span class="s3">is None else </span><span class="s4">0 </span><span class="s3">for </span><span class="s1">bdim </span><span class="s3">in </span><span class="s1">batch_dims]</span>
  <span class="s1">args_out = [lax.squeeze(arg</span><span class="s3">, </span><span class="s1">[</span><span class="s4">0</span><span class="s1">]) </span><span class="s3">if </span><span class="s1">bdim </span><span class="s3">is None else </span><span class="s1">arg</span>
              <span class="s3">for </span><span class="s1">arg</span><span class="s3">, </span><span class="s1">bdim </span><span class="s3">in </span><span class="s1">zip((data</span><span class="s3">, </span><span class="s1">indices)</span><span class="s3">, </span><span class="s1">batch_dims_out)]</span>
  <span class="s3">return </span><span class="s1">args_out</span><span class="s3">, </span><span class="s1">batch_dims_out</span>

<span class="s1">ad.primitive_jvps[bcoo_transpose_p] = _bcoo_transpose_jvp</span>
<span class="s1">ad.primitive_transposes[bcoo_transpose_p] = _bcoo_transpose_transpose</span>
<span class="s1">batching.primitive_batchers[bcoo_transpose_p] = _bcoo_transpose_batch_rule</span>
<span class="s1">mlir.register_lowering(bcoo_transpose_p</span><span class="s3">, </span><span class="s1">mlir.lower_fun(</span>
    <span class="s1">_bcoo_transpose_impl</span><span class="s3">, </span><span class="s1">multiple_results=</span><span class="s3">True</span><span class="s1">))</span>

<span class="s0">#----------------------------------------------------------------------</span>
<span class="s0"># bcoo_dot_general</span>
<span class="s0"># (batched) general dot product of a BCOO sparse ND array and a dense ND array,</span>
<span class="s0"># returning a dense ND array.</span>

<span class="s1">bcoo_dot_general_p = core.Primitive(</span><span class="s5">'bcoo_dot_general'</span><span class="s1">)</span>

<span class="s3">def </span><span class="s1">bcoo_dot_general(lhs: Union[BCOO</span><span class="s3">, </span><span class="s1">Array]</span><span class="s3">, </span><span class="s1">rhs: Union[BCOO</span><span class="s3">, </span><span class="s1">Array]</span><span class="s3">, </span><span class="s1">*</span><span class="s3">, </span><span class="s1">dimension_numbers: DotDimensionNumbers</span><span class="s3">,</span>
                     <span class="s1">precision: </span><span class="s3">None </span><span class="s1">= </span><span class="s3">None, </span><span class="s1">preferred_element_type: </span><span class="s3">None </span><span class="s1">= </span><span class="s3">None</span><span class="s1">) -&gt; Union[BCOO</span><span class="s3">, </span><span class="s1">Array]:</span>
  <span class="s2">&quot;&quot;&quot;A general contraction operation. 
 
  Args: 
    lhs: An ndarray or BCOO-format sparse array. 
    rhs: An ndarray or BCOO-format sparse array.. 
    dimension_numbers: a tuple of tuples of the form 
      `((lhs_contracting_dims, rhs_contracting_dims), 
      (lhs_batch_dims, rhs_batch_dims))`. 
    precision: unused 
    preferred_element_type: unused 
 
  Returns: 
    An ndarray or BCOO-format sparse array containing the result. If both inputs 
    are sparse, the result will be sparse, of type BCOO. If either input is dense, 
    the result will be dense, of type ndarray. 
  &quot;&quot;&quot;</span>
  <span class="s0"># TODO(jakevdp) make use of these?</span>
  <span class="s3">del </span><span class="s1">precision</span><span class="s3">, </span><span class="s1">preferred_element_type  </span><span class="s0"># unused</span>
  <span class="s3">if </span><span class="s1">isinstance(lhs</span><span class="s3">, </span><span class="s1">BCOO) </span><span class="s3">and </span><span class="s1">isinstance(rhs</span><span class="s3">, </span><span class="s1">BCOO):</span>
    <span class="s1">shape = _dot_general_validated_shape(lhs.shape</span><span class="s3">, </span><span class="s1">rhs.shape</span><span class="s3">,</span>
                                         <span class="s1">dimension_numbers)</span>
    <span class="s1">bufs = _bcoo_spdot_general(lhs.data</span><span class="s3">, </span><span class="s1">lhs.indices</span><span class="s3">, </span><span class="s1">rhs.data</span><span class="s3">, </span><span class="s1">rhs.indices</span><span class="s3">,</span>
                               <span class="s1">lhs_spinfo=lhs._info</span><span class="s3">, </span><span class="s1">rhs_spinfo=rhs._info</span><span class="s3">,</span>
                               <span class="s1">dimension_numbers=dimension_numbers)</span>
    <span class="s3">return </span><span class="s1">BCOO(bufs</span><span class="s3">, </span><span class="s1">shape=shape)</span>
  <span class="s3">elif </span><span class="s1">isinstance(lhs</span><span class="s3">, </span><span class="s1">BCOO):</span>
    <span class="s3">return </span><span class="s1">_bcoo_dot_general(lhs.data</span><span class="s3">, </span><span class="s1">lhs.indices</span><span class="s3">, </span><span class="s1">rhs</span><span class="s3">, </span><span class="s1">dimension_numbers=dimension_numbers</span><span class="s3">,  </span><span class="s0"># type: ignore[arg-type]</span>
                             <span class="s1">lhs_spinfo=lhs._info)</span>
  <span class="s3">elif </span><span class="s1">isinstance(rhs</span><span class="s3">, </span><span class="s1">BCOO):</span>
    <span class="s3">return </span><span class="s1">_bcoo_rdot_general(lhs</span><span class="s3">, </span><span class="s1">rhs.data</span><span class="s3">, </span><span class="s1">rhs.indices</span><span class="s3">, </span><span class="s1">dimension_numbers=dimension_numbers</span><span class="s3">,  </span><span class="s0"># type: ignore[arg-type]</span>
                              <span class="s1">rhs_spinfo=rhs._info)</span>
  <span class="s3">else</span><span class="s1">:</span>
    <span class="s3">return </span><span class="s1">lax.dot_general(lhs</span><span class="s3">, </span><span class="s1">rhs</span><span class="s3">, </span><span class="s1">dimension_numbers=dimension_numbers)</span>

<span class="s3">def </span><span class="s1">_bcoo_dot_general(lhs_data: Array</span><span class="s3">, </span><span class="s1">lhs_indices: Array</span><span class="s3">, </span><span class="s1">rhs: Array</span><span class="s3">, </span><span class="s1">*</span><span class="s3">,</span>
                      <span class="s1">dimension_numbers: DotDimensionNumbers</span><span class="s3">, </span><span class="s1">lhs_spinfo: SparseInfo) -&gt; Array:</span>
  <span class="s1">(lhs_contract</span><span class="s3">, </span><span class="s1">rhs_contract)</span><span class="s3">, </span><span class="s1">(lhs_batch</span><span class="s3">, </span><span class="s1">rhs_batch) = dimension_numbers</span>
  <span class="s1">cdims = (api_util._ensure_index_tuple(lhs_contract)</span><span class="s3">,</span>
           <span class="s1">api_util._ensure_index_tuple(rhs_contract))</span>
  <span class="s1">bdims = (api_util._ensure_index_tuple(lhs_batch)</span><span class="s3">,</span>
           <span class="s1">api_util._ensure_index_tuple(rhs_batch))</span>
  <span class="s3">return </span><span class="s1">bcoo_dot_general_p.bind(jnp.asarray(lhs_data)</span><span class="s3">, </span><span class="s1">jnp.asarray(lhs_indices)</span><span class="s3">, </span><span class="s1">jnp.asarray(rhs)</span><span class="s3">,</span>
                                 <span class="s1">dimension_numbers=(cdims</span><span class="s3">, </span><span class="s1">bdims)</span><span class="s3">,</span>
                                 <span class="s1">lhs_spinfo=lhs_spinfo)</span>

<span class="s3">def </span><span class="s1">_bcoo_rdot_general(lhs: Array</span><span class="s3">, </span><span class="s1">rhs_data: Array</span><span class="s3">, </span><span class="s1">rhs_indices: Array</span><span class="s3">, </span><span class="s1">*</span><span class="s3">,</span>
                       <span class="s1">dimension_numbers: DotDimensionNumbers</span><span class="s3">, </span><span class="s1">rhs_spinfo: SparseInfo) -&gt; Array:</span>
  <span class="s0"># TODO(jakevdp): perhaps this should be part of the bcoo_dot_general primitive?</span>
  <span class="s1">dimension_numbers_reversed: DotDimensionNumbers = tuple(d[::-</span><span class="s4">1</span><span class="s1">] </span><span class="s3">for </span><span class="s1">d </span><span class="s3">in </span><span class="s1">dimension_numbers)  </span><span class="s0"># type: ignore[assignment]</span>
  <span class="s1">result = _bcoo_dot_general(rhs_data</span><span class="s3">, </span><span class="s1">rhs_indices</span><span class="s3">, </span><span class="s1">lhs</span><span class="s3">, </span><span class="s1">lhs_spinfo=rhs_spinfo</span><span class="s3">,</span>
                             <span class="s1">dimension_numbers=dimension_numbers_reversed)</span>
  <span class="s1">n_contract</span><span class="s3">, </span><span class="s1">n_batch = (len(d[</span><span class="s4">0</span><span class="s1">]) </span><span class="s3">for </span><span class="s1">d </span><span class="s3">in </span><span class="s1">dimension_numbers)</span>
  <span class="s1">n_swap = len(rhs_spinfo.shape) - n_contract</span>
  <span class="s1">permutation = tuple([*range(n_batch)</span><span class="s3">, </span><span class="s1">*range(n_swap</span><span class="s3">, </span><span class="s1">result.ndim)</span><span class="s3">, </span><span class="s1">*range(n_batch</span><span class="s3">, </span><span class="s1">n_swap)])</span>
  <span class="s3">return </span><span class="s1">lax.transpose(result</span><span class="s3">, </span><span class="s1">permutation)</span>

<span class="s3">def </span><span class="s1">_bcoo_dot_general_impl(lhs_data</span><span class="s3">, </span><span class="s1">lhs_indices</span><span class="s3">, </span><span class="s1">rhs</span><span class="s3">, </span><span class="s1">*</span><span class="s3">, </span><span class="s1">dimension_numbers</span><span class="s3">, </span><span class="s1">lhs_spinfo: SparseInfo):</span>
  <span class="s1">lhs_data = jnp.asarray(lhs_data)</span>
  <span class="s1">lhs_indices = jnp.asarray(lhs_indices)</span>
  <span class="s1">rhs = jnp.asarray(rhs)</span>
  <span class="s0"># Validate all inputs via abstract_eval</span>
  <span class="s1">out_aval = _bcoo_dot_general_abstract_eval(lhs_data.aval</span><span class="s3">, </span><span class="s1">lhs_indices.aval</span><span class="s3">, </span><span class="s1">rhs.aval</span><span class="s3">,</span>
                                             <span class="s1">dimension_numbers=dimension_numbers</span><span class="s3">,</span>
                                             <span class="s1">lhs_spinfo=lhs_spinfo)</span>
  <span class="s1">n_sparse = lhs_indices.shape[-</span><span class="s4">1</span><span class="s1">]</span>
  <span class="s1">n_batch = lhs_indices.ndim - </span><span class="s4">2</span>

  <span class="s1">(lhs_contracting</span><span class="s3">, </span><span class="s1">rhs_contracting)</span><span class="s3">, </span><span class="s1">(lhs_batch</span><span class="s3">, </span><span class="s1">rhs_batch) = dimension_numbers</span>
  <span class="s1">lhs_contracting_b</span><span class="s3">, </span><span class="s1">rhs_contracting_b = unzip2([</span>
    <span class="s1">(l</span><span class="s3">, </span><span class="s1">r) </span><span class="s3">for </span><span class="s1">l</span><span class="s3">, </span><span class="s1">r </span><span class="s3">in </span><span class="s1">safe_zip(lhs_contracting</span><span class="s3">, </span><span class="s1">rhs_contracting) </span><span class="s3">if </span><span class="s1">l &lt; n_batch])</span>
  <span class="s1">lhs_contracting_s</span><span class="s3">, </span><span class="s1">rhs_contracting_s = unzip2([</span>
    <span class="s1">(l</span><span class="s3">, </span><span class="s1">r) </span><span class="s3">for </span><span class="s1">l</span><span class="s3">, </span><span class="s1">r </span><span class="s3">in </span><span class="s1">safe_zip(lhs_contracting</span><span class="s3">, </span><span class="s1">rhs_contracting) </span><span class="s3">if </span><span class="s1">l &gt;= n_batch])</span>

  <span class="s0"># Reorder lhs batch dimensions</span>
  <span class="s3">if </span><span class="s1">lhs_batch </span><span class="s3">or </span><span class="s1">lhs_contracting_b:</span>
    <span class="s1">batch_perm = [*lhs_batch</span><span class="s3">, </span><span class="s1">*remaining(range(n_batch)</span><span class="s3">, </span><span class="s1">lhs_batch</span><span class="s3">, </span><span class="s1">lhs_contracting_b)</span><span class="s3">, </span><span class="s1">*lhs_contracting_b]</span>
    <span class="s1">lhs_data = lhs_data.transpose([*batch_perm</span><span class="s3">, </span><span class="s1">*range(n_batch</span><span class="s3">, </span><span class="s1">lhs_data.ndim)])</span>
    <span class="s1">lhs_indices = lhs_indices.transpose([*batch_perm</span><span class="s3">, </span><span class="s1">*range(n_batch</span><span class="s3">, </span><span class="s1">lhs_indices.ndim)])</span>

  <span class="s0"># Reorder lhs sparse dimensions</span>
  <span class="s3">if </span><span class="s1">lhs_contracting_s:</span>
    <span class="s1">lhs_contracting_s = tuple(d - n_batch </span><span class="s3">for </span><span class="s1">d </span><span class="s3">in </span><span class="s1">lhs_contracting_s)</span>
    <span class="s1">sparse_perm = jnp.array([*lhs_contracting_s</span><span class="s3">, </span><span class="s1">*remaining(range(n_sparse)</span><span class="s3">, </span><span class="s1">lhs_contracting_s)])</span>
    <span class="s1">lhs_indices = lhs_indices[...</span><span class="s3">, </span><span class="s1">sparse_perm]</span>

  <span class="s0"># Reorder rhs dimensions</span>
  <span class="s1">rhs_perm = [*rhs_batch</span><span class="s3">, </span><span class="s1">*rhs_contracting_b</span><span class="s3">, </span><span class="s1">*rhs_contracting_s</span><span class="s3">,</span>
              <span class="s1">*remaining(range(rhs.ndim)</span><span class="s3">, </span><span class="s1">rhs_batch</span><span class="s3">, </span><span class="s1">rhs_contracting)]</span>
  <span class="s1">rhs = rhs.transpose(rhs_perm)</span>

  <span class="s3">def </span><span class="s1">result(out_array</span><span class="s3">, </span><span class="s1">lhs_data</span><span class="s3">, </span><span class="s1">lhs_indices</span><span class="s3">, </span><span class="s1">rhs):</span>
    <span class="s1">idx = tuple(lhs_indices[...</span><span class="s3">, </span><span class="s1">i] </span><span class="s3">for </span><span class="s1">i </span><span class="s3">in </span><span class="s1">range(n_sparse))</span>
    <span class="s1">idx_right = idx[:len(lhs_contracting_s)]</span>
    <span class="s1">idx_out = idx[len(lhs_contracting_s):]</span>
    <span class="s3">if </span><span class="s1">idx_right </span><span class="s3">and </span><span class="s1">lhs_indices.ndim &gt; </span><span class="s4">2</span><span class="s1">:</span>
      <span class="s1">idx_batch = jnp.meshgrid(</span>
          <span class="s1">*(jnp.arange(n) </span><span class="s3">for </span><span class="s1">n </span><span class="s3">in </span><span class="s1">lhs_indices.shape[:-</span><span class="s4">1</span><span class="s1">])</span><span class="s3">,</span>
          <span class="s1">indexing=</span><span class="s5">'ij'</span><span class="s1">)[:lhs_indices.ndim - </span><span class="s4">2</span><span class="s1">]</span>
      <span class="s1">idx_right = (*idx_batch</span><span class="s3">, </span><span class="s1">*idx_right)</span>
    <span class="s1">batch_dims = list(range(len(lhs_contracting_b) + bool(lhs_contracting_s)))</span>
    <span class="s1">prod = lax.dot_general(lhs_data</span><span class="s3">, </span><span class="s1">rhs.at[idx_right].get(mode=</span><span class="s5">'fill'</span><span class="s3">, </span><span class="s1">fill_value=</span><span class="s4">0</span><span class="s1">)</span><span class="s3">,</span>
                           <span class="s1">(([]</span><span class="s3">, </span><span class="s1">[])</span><span class="s3">, </span><span class="s1">(batch_dims</span><span class="s3">, </span><span class="s1">batch_dims)))</span>
    <span class="s3">if </span><span class="s1">idx_out:</span>
      <span class="s3">return </span><span class="s1">out_array.at[idx_out].add(prod)</span>
    <span class="s3">else</span><span class="s1">:</span>
      <span class="s3">return </span><span class="s1">prod.sum(tuple(range(prod.ndim - out_array.ndim))</span><span class="s3">, </span><span class="s1">dtype=out_array.dtype)</span>
  <span class="s1">result = nfold_vmap(result</span><span class="s3">, </span><span class="s1">n_batch - len(lhs_contracting_b))</span>
  <span class="s1">rhs = lax.expand_dims(rhs</span><span class="s3">, </span><span class="s1">range(len(rhs_batch)</span><span class="s3">, </span><span class="s1">n_batch - len(lhs_contracting_b)))</span>
  <span class="s1">out_array = jnp.zeros(out_aval.shape</span><span class="s3">, </span><span class="s1">out_aval.dtype)</span>
  <span class="s3">return </span><span class="s1">result(out_array</span><span class="s3">, </span><span class="s1">lhs_data</span><span class="s3">, </span><span class="s1">lhs_indices</span><span class="s3">, </span><span class="s1">rhs)</span>

<span class="s1">@bcoo_dot_general_p.def_abstract_eval</span>
<span class="s3">def </span><span class="s1">_bcoo_dot_general_abstract_eval(lhs_data</span><span class="s3">, </span><span class="s1">lhs_indices</span><span class="s3">, </span><span class="s1">rhs</span><span class="s3">, </span><span class="s1">*</span><span class="s3">, </span><span class="s1">dimension_numbers</span><span class="s3">, </span><span class="s1">lhs_spinfo: SparseInfo):</span>
  <span class="s3">if </span><span class="s1">lhs_data.dtype != rhs.dtype:</span>
    <span class="s3">raise </span><span class="s1">ValueError(</span><span class="s5">&quot;bcoo_dot_general requires arguments to have matching dtypes; &quot;</span>
                     <span class="s5">f&quot;got lhs.dtype=</span><span class="s3">{</span><span class="s1">lhs_data.dtype</span><span class="s3">}</span><span class="s5">, rhs.dtype=</span><span class="s3">{</span><span class="s1">rhs.dtype</span><span class="s3">}</span><span class="s5">&quot;</span><span class="s1">)</span>

  <span class="s1">(lhs_contracting</span><span class="s3">, </span><span class="s1">_)</span><span class="s3">, </span><span class="s1">(lhs_batch</span><span class="s3">, </span><span class="s1">_) = dimension_numbers</span>
  <span class="s1">n_batch</span><span class="s3">, </span><span class="s1">n_sparse</span><span class="s3">, </span><span class="s1">_</span><span class="s3">, </span><span class="s1">_ = _validate_bcoo(lhs_data</span><span class="s3">, </span><span class="s1">lhs_indices</span><span class="s3">, </span><span class="s1">lhs_spinfo.shape)</span>
  <span class="s1">out_shape = _dot_general_validated_shape(lhs_spinfo.shape</span><span class="s3">, </span><span class="s1">rhs.shape</span><span class="s3">,</span>
                                           <span class="s1">dimension_numbers)</span>

  <span class="s3">if </span><span class="s1">lhs_batch </span><span class="s3">and </span><span class="s1">max(lhs_batch) &gt;= n_batch:</span>
    <span class="s3">raise </span><span class="s1">NotImplementedError(</span>
      <span class="s5">&quot;bcoo_dot_general batch dimensions must be among the batch dimensions in the sparse representtaion.</span><span class="s3">\n</span><span class="s5">&quot;</span>
      <span class="s5">f&quot;got </span><span class="s3">{</span><span class="s1">lhs_batch=</span><span class="s3">}</span><span class="s5">, </span><span class="s3">{</span><span class="s1">n_batch=</span><span class="s3">}</span><span class="s5">&quot;</span><span class="s1">)</span>

  <span class="s0"># TODO: support contraction of dense dimensions?</span>
  <span class="s3">if </span><span class="s1">any(d &gt;= n_batch + n_sparse </span><span class="s3">for </span><span class="s1">d </span><span class="s3">in </span><span class="s1">lhs_contracting):</span>
    <span class="s3">raise </span><span class="s1">NotImplementedError(</span><span class="s5">&quot;bcoo_dot_general: contracting over dense dimensions.&quot;</span><span class="s1">)</span>

  <span class="s3">return </span><span class="s1">core.ShapedArray(out_shape</span><span class="s3">, </span><span class="s1">lhs_data.dtype)</span>

<span class="s1">_bcoo_dot_general_default_lowering = mlir.lower_fun(</span>
    <span class="s1">_bcoo_dot_general_impl</span><span class="s3">, </span><span class="s1">multiple_results=</span><span class="s3">False</span><span class="s1">)</span>


<span class="s3">def </span><span class="s1">_hlo_reshape(x</span><span class="s3">, </span><span class="s1">shape):</span>
  <span class="s1">x_type = ir.RankedTensorType(x.type)</span>
  <span class="s3">return </span><span class="s1">hlo.ReshapeOp(</span>
      <span class="s1">ir.RankedTensorType.get(shape</span><span class="s3">, </span><span class="s1">x_type.element_type)</span><span class="s3">, </span><span class="s1">x).result</span>

<span class="s3">def </span><span class="s1">_bcoo_get_row_col(ctx</span><span class="s3">, </span><span class="s1">indices):</span>
  <span class="s1">shape = ir.RankedTensorType(indices.type).shape</span>
  <span class="s3">assert </span><span class="s1">len(shape) == </span><span class="s4">2</span>
  <span class="s3">if </span><span class="s1">shape[</span><span class="s4">1</span><span class="s1">] == </span><span class="s4">1</span><span class="s1">:</span>
    <span class="s1">col = _hlo_reshape(indices</span><span class="s3">, </span><span class="s1">shape[:</span><span class="s4">1</span><span class="s1">])</span>
    <span class="s1">row = mlir.full_like_aval(</span>
        <span class="s1">ctx</span><span class="s3">, </span><span class="s4">0</span><span class="s3">, </span><span class="s1">core.ShapedArray(ir.RankedTensorType(col.type).shape</span><span class="s3">,</span>
                                 <span class="s1">np.dtype(np.int32)))</span>
  <span class="s3">elif </span><span class="s1">shape[</span><span class="s4">1</span><span class="s1">] == </span><span class="s4">2</span><span class="s1">:</span>
    <span class="s1">row = _hlo_reshape(</span>
        <span class="s1">hlo.SliceOp(</span>
            <span class="s1">indices</span><span class="s3">,</span>
            <span class="s1">start_indices=mlir.dense_int_elements([</span><span class="s4">0</span><span class="s3">, </span><span class="s4">0</span><span class="s1">])</span><span class="s3">,</span>
            <span class="s1">limit_indices=mlir.dense_int_elements([shape[</span><span class="s4">0</span><span class="s1">]</span><span class="s3">, </span><span class="s4">1</span><span class="s1">])</span><span class="s3">,</span>
            <span class="s1">strides=mlir.dense_int_elements([</span><span class="s4">1</span><span class="s3">, </span><span class="s4">1</span><span class="s1">])).result</span><span class="s3">,</span>
        <span class="s1">shape[:</span><span class="s4">1</span><span class="s1">])</span>
    <span class="s1">col = _hlo_reshape(</span>
        <span class="s1">hlo.SliceOp(</span>
            <span class="s1">indices</span><span class="s3">,</span>
            <span class="s1">start_indices=mlir.dense_int_elements([</span><span class="s4">0</span><span class="s3">, </span><span class="s4">1</span><span class="s1">])</span><span class="s3">,</span>
            <span class="s1">limit_indices=mlir.dense_int_elements([shape[</span><span class="s4">0</span><span class="s1">]</span><span class="s3">, </span><span class="s4">2</span><span class="s1">])</span><span class="s3">,</span>
            <span class="s1">strides=mlir.dense_int_elements([</span><span class="s4">1</span><span class="s3">, </span><span class="s4">1</span><span class="s1">])).result</span><span class="s3">,</span>
        <span class="s1">shape[:</span><span class="s4">1</span><span class="s1">])</span>
  <span class="s3">else</span><span class="s1">:</span>
    <span class="s3">raise </span><span class="s1">ValueError(</span><span class="s5">f&quot;expected shape[1] in (1, 2), got </span><span class="s3">{</span><span class="s1">shape=</span><span class="s3">}</span><span class="s5">&quot;</span><span class="s1">)</span>

  <span class="s3">return </span><span class="s1">row</span><span class="s3">, </span><span class="s1">col</span>

<span class="s3">def </span><span class="s1">_bcoo_dot_general_fallback(data</span><span class="s3">, </span><span class="s1">indices</span><span class="s3">, </span><span class="s1">spinfo):</span>
  <span class="s3">if </span><span class="s1">data.dtype </span><span class="s3">not in </span><span class="s1">CUSPARSE_DATA_DTYPES:</span>
    <span class="s1">warnings.warn(</span><span class="s5">'bcoo_dot_general cusparse/hipsparse lowering not available '</span>
                  <span class="s5">f'for </span><span class="s3">{</span><span class="s1">data.dtype=</span><span class="s3">}</span><span class="s5">. Falling back to default implementation.'</span><span class="s3">,</span>
                  <span class="s1">CuSparseEfficiencyWarning)</span>
    <span class="s3">return True</span>
  <span class="s3">elif </span><span class="s1">indices.dtype </span><span class="s3">not in </span><span class="s1">CUSPARSE_INDEX_DTYPES:</span>
    <span class="s1">warnings.warn(</span><span class="s5">'bcoo_dot_general cusparse/hipsparse lowering not available '</span>
                  <span class="s5">f'for </span><span class="s3">{</span><span class="s1">indices.dtype=</span><span class="s3">}</span><span class="s5">. Falling back to default implementation.'</span><span class="s3">,</span>
                  <span class="s1">CuSparseEfficiencyWarning)</span>
    <span class="s3">return True</span>
  <span class="s3">elif not </span><span class="s1">spinfo.indices_sorted:</span>
    <span class="s1">warnings.warn(</span><span class="s5">&quot;bcoo_dot_general GPU lowering requires matrices with &quot;</span>
                  <span class="s5">&quot;sorted indices. To sort the rows in your matrix, use e.g. &quot;</span>
                  <span class="s5">&quot;mat = mat.sort_indices(). Falling back to the default &quot;</span>
                  <span class="s5">&quot;implementation.&quot;</span><span class="s3">, </span><span class="s1">CuSparseEfficiencyWarning)</span>
    <span class="s3">return True</span>
  <span class="s3">else</span><span class="s1">:</span>
    <span class="s3">return False</span>

<span class="s3">def </span><span class="s1">_bcoo_dot_general_gpu_impl(lhs_data</span><span class="s3">, </span><span class="s1">lhs_indices</span><span class="s3">, </span><span class="s1">rhs</span><span class="s3">, </span><span class="s1">*</span><span class="s3">,</span>
                               <span class="s1">dimension_numbers</span><span class="s3">, </span><span class="s1">lhs_spinfo):</span>
  <span class="s3">if not </span><span class="s1">config.jax_bcoo_cusparse_lowering:</span>
    <span class="s3">return </span><span class="s1">_bcoo_dot_general_impl(lhs_data</span><span class="s3">, </span><span class="s1">lhs_indices</span><span class="s3">, </span><span class="s1">rhs</span><span class="s3">,</span>
        <span class="s1">dimension_numbers=dimension_numbers</span><span class="s3">, </span><span class="s1">lhs_spinfo=lhs_spinfo)</span>

  <span class="s1">(lhs_contract</span><span class="s3">, </span><span class="s1">rhs_contract)</span><span class="s3">, </span><span class="s1">(lhs_batch</span><span class="s3">, </span><span class="s1">_) = dimension_numbers</span>
  <span class="s1">n_batch</span><span class="s3">, </span><span class="s1">n_sparse</span><span class="s3">, </span><span class="s1">n_dense</span><span class="s3">, </span><span class="s1">_ = _validate_bcoo(</span>
      <span class="s1">lhs_data</span><span class="s3">, </span><span class="s1">lhs_indices</span><span class="s3">, </span><span class="s1">lhs_spinfo.shape)</span>
  <span class="s1">coo_matmul_p = coo_spmv_p </span><span class="s3">if </span><span class="s1">rhs.ndim == </span><span class="s4">1 </span><span class="s3">else </span><span class="s1">coo_spmm_p</span>

  <span class="s0"># TODO(jakevdp, tianjianlu): add support for batched lowerings</span>
  <span class="s3">if </span><span class="s1">(len(lhs_contract) == </span><span class="s4">1 </span><span class="s3">and </span><span class="s1">len(lhs_batch) == </span><span class="s4">0 </span><span class="s3">and </span><span class="s1">rhs.ndim </span><span class="s3">in </span><span class="s1">(</span><span class="s4">1</span><span class="s3">, </span><span class="s4">2</span><span class="s1">)</span>
      <span class="s3">and </span><span class="s1">(n_batch</span><span class="s3">, </span><span class="s1">n_sparse</span><span class="s3">, </span><span class="s1">n_dense) == (</span><span class="s4">0</span><span class="s3">, </span><span class="s4">1</span><span class="s3">, </span><span class="s4">0</span><span class="s1">)</span>
      <span class="s3">and not </span><span class="s1">_bcoo_dot_general_fallback(lhs_data</span><span class="s3">, </span><span class="s1">lhs_indices</span><span class="s3">, </span><span class="s1">lhs_spinfo)):</span>
    <span class="s1">row</span><span class="s3">, </span><span class="s1">col = jnp.zeros(lhs_indices.shape[</span><span class="s4">0</span><span class="s1">]</span><span class="s3">, </span><span class="s1">lhs_indices.dtype)</span><span class="s3">, </span><span class="s1">lhs_indices.ravel()</span>
    <span class="s1">transpose = </span><span class="s3">False</span>
    <span class="s1">shape = (</span><span class="s4">1</span><span class="s3">, </span><span class="s1">*lhs_spinfo.shape)</span>
    <span class="s1">row</span><span class="s3">, </span><span class="s1">col</span><span class="s3">, </span><span class="s1">shape = _coo_correct_out_of_bound_indices(row</span><span class="s3">, </span><span class="s1">col</span><span class="s3">, </span><span class="s1">shape</span><span class="s3">, </span><span class="s1">transpose)</span>
    <span class="s1">out = coo_matmul_p.bind(lhs_data</span><span class="s3">, </span><span class="s1">row</span><span class="s3">, </span><span class="s1">col</span><span class="s3">,</span>
                            <span class="s1">rhs.T </span><span class="s3">if </span><span class="s1">rhs_contract[</span><span class="s4">0</span><span class="s1">] == </span><span class="s4">1 </span><span class="s3">else </span><span class="s1">rhs</span><span class="s3">,</span>
                            <span class="s1">transpose=transpose</span><span class="s3">, </span><span class="s1">shape=shape)</span>
    <span class="s3">return </span><span class="s1">out[</span><span class="s4">0</span><span class="s1">]</span>
  <span class="s3">elif </span><span class="s1">(len(lhs_contract) == </span><span class="s4">1 </span><span class="s3">and </span><span class="s1">len(lhs_batch) == </span><span class="s4">0 </span><span class="s3">and </span><span class="s1">rhs.ndim </span><span class="s3">in </span><span class="s1">(</span><span class="s4">1</span><span class="s3">, </span><span class="s4">2</span><span class="s1">)</span>
        <span class="s3">and </span><span class="s1">(n_batch</span><span class="s3">, </span><span class="s1">n_sparse</span><span class="s3">, </span><span class="s1">n_dense) == (</span><span class="s4">0</span><span class="s3">, </span><span class="s4">2</span><span class="s3">, </span><span class="s4">0</span><span class="s1">)</span>
        <span class="s3">and not </span><span class="s1">_bcoo_dot_general_fallback(lhs_data</span><span class="s3">, </span><span class="s1">lhs_indices</span><span class="s3">, </span><span class="s1">lhs_spinfo)):</span>
    <span class="s1">row</span><span class="s3">, </span><span class="s1">col = lhs_indices[:</span><span class="s3">, </span><span class="s4">0</span><span class="s1">]</span><span class="s3">, </span><span class="s1">lhs_indices[:</span><span class="s3">, </span><span class="s4">1</span><span class="s1">]</span>
    <span class="s1">transpose = (lhs_contract[</span><span class="s4">0</span><span class="s1">] == </span><span class="s4">0</span><span class="s1">)</span>
    <span class="s1">shape = lhs_spinfo.shape</span>
    <span class="s1">row</span><span class="s3">, </span><span class="s1">col</span><span class="s3">, </span><span class="s1">shape = _coo_correct_out_of_bound_indices(row</span><span class="s3">, </span><span class="s1">col</span><span class="s3">, </span><span class="s1">shape</span><span class="s3">, </span><span class="s1">transpose)</span>
    <span class="s1">out = coo_matmul_p.bind(lhs_data</span><span class="s3">, </span><span class="s1">row</span><span class="s3">, </span><span class="s1">col</span><span class="s3">,</span>
                            <span class="s1">rhs.T </span><span class="s3">if </span><span class="s1">rhs_contract[</span><span class="s4">0</span><span class="s1">] == </span><span class="s4">1 </span><span class="s3">else </span><span class="s1">rhs</span><span class="s3">,</span>
                            <span class="s1">transpose=transpose</span><span class="s3">, </span><span class="s1">shape=shape)</span>
    <span class="s3">return </span><span class="s1">out[:-</span><span class="s4">1</span><span class="s1">]</span>
  <span class="s3">else</span><span class="s1">:</span>
    <span class="s3">return </span><span class="s1">_bcoo_dot_general_impl(lhs_data</span><span class="s3">, </span><span class="s1">lhs_indices</span><span class="s3">, </span><span class="s1">rhs</span><span class="s3">,</span>
        <span class="s1">dimension_numbers=dimension_numbers</span><span class="s3">, </span><span class="s1">lhs_spinfo=lhs_spinfo)</span>

<span class="s1">_bcoo_dot_general_gpu_lowering = mlir.lower_fun(</span>
    <span class="s1">_bcoo_dot_general_gpu_impl</span><span class="s3">, </span><span class="s1">multiple_results=</span><span class="s3">False</span><span class="s1">)</span>

<span class="s3">def </span><span class="s1">_bcoo_dot_general_jvp_lhs(lhs_data_dot</span><span class="s3">, </span><span class="s1">lhs_data</span><span class="s3">, </span><span class="s1">lhs_indices</span><span class="s3">, </span><span class="s1">rhs</span><span class="s3">, </span><span class="s1">*</span><span class="s3">, </span><span class="s1">dimension_numbers</span><span class="s3">, </span><span class="s1">lhs_spinfo: SparseInfo):</span>
  <span class="s3">return </span><span class="s1">_bcoo_dot_general(lhs_data_dot</span><span class="s3">, </span><span class="s1">lhs_indices</span><span class="s3">, </span><span class="s1">rhs</span><span class="s3">, </span><span class="s1">dimension_numbers=dimension_numbers</span><span class="s3">, </span><span class="s1">lhs_spinfo=lhs_spinfo)</span>

<span class="s3">def </span><span class="s1">_bcoo_dot_general_jvp_rhs(rhs_dot</span><span class="s3">, </span><span class="s1">lhs_data</span><span class="s3">, </span><span class="s1">lhs_indices</span><span class="s3">, </span><span class="s1">rhs</span><span class="s3">, </span><span class="s1">*</span><span class="s3">, </span><span class="s1">dimension_numbers</span><span class="s3">, </span><span class="s1">lhs_spinfo: SparseInfo):</span>
  <span class="s3">return </span><span class="s1">_bcoo_dot_general(lhs_data</span><span class="s3">, </span><span class="s1">lhs_indices</span><span class="s3">, </span><span class="s1">rhs_dot</span><span class="s3">, </span><span class="s1">dimension_numbers=dimension_numbers</span><span class="s3">, </span><span class="s1">lhs_spinfo=lhs_spinfo)</span>

<span class="s3">def </span><span class="s1">_bcoo_dot_general_transpose(ct</span><span class="s3">, </span><span class="s1">lhs_data</span><span class="s3">, </span><span class="s1">lhs_indices</span><span class="s3">, </span><span class="s1">rhs</span><span class="s3">, </span><span class="s1">*</span><span class="s3">, </span><span class="s1">dimension_numbers</span><span class="s3">, </span><span class="s1">lhs_spinfo: SparseInfo):</span>
  <span class="s3">assert not </span><span class="s1">ad.is_undefined_primal(lhs_indices)</span>
  <span class="s1">(lhs_contract</span><span class="s3">, </span><span class="s1">rhs_contract)</span><span class="s3">, </span><span class="s1">(lhs_batch</span><span class="s3">, </span><span class="s1">rhs_batch) = dimension_numbers</span>
  <span class="s1">lhs_ndim = len(lhs_spinfo.shape)</span>
  <span class="s1">rhs_ndim = rhs.aval.ndim </span><span class="s3">if </span><span class="s1">ad.is_undefined_primal(rhs) </span><span class="s3">else </span><span class="s1">rhs.ndim</span>
  <span class="s1">lhs_kept = remaining(range(lhs_ndim)</span><span class="s3">, </span><span class="s1">lhs_contract</span><span class="s3">, </span><span class="s1">lhs_batch)</span>
  <span class="s1">rhs_kept = remaining(range(rhs_ndim)</span><span class="s3">, </span><span class="s1">rhs_contract</span><span class="s3">, </span><span class="s1">rhs_batch)</span>
  <span class="s1">ans_batch</span><span class="s3">, </span><span class="s1">ans_lhs</span><span class="s3">, </span><span class="s1">ans_rhs = map(list</span><span class="s3">, </span><span class="s1">ranges_like(lhs_batch</span><span class="s3">, </span><span class="s1">lhs_kept</span><span class="s3">, </span><span class="s1">rhs_kept))</span>
  <span class="s3">if </span><span class="s1">ad.is_undefined_primal(lhs_data):</span>
    <span class="s1">dims: DotDimensionNumbers = ((ans_rhs</span><span class="s3">, </span><span class="s1">rhs_kept)</span><span class="s3">, </span><span class="s1">(ans_batch</span><span class="s3">, </span><span class="s1">rhs_batch))  </span><span class="s0"># type: ignore[assignment]</span>
    <span class="s1">lhs_contract_sorted_by_rhs = list(np.take(lhs_contract</span><span class="s3">, </span><span class="s1">np.argsort(rhs_contract)))</span>
    <span class="s1">permutation = list(lhs_batch) + lhs_kept + lhs_contract_sorted_by_rhs</span>
    <span class="s1">out_axes = list(np.argsort(permutation))</span>

    <span class="s0"># Determine whether efficient approach is possible:</span>
    <span class="s1">placeholder_data = jnp.empty((lhs_indices.ndim - </span><span class="s4">2</span><span class="s1">) * (</span><span class="s4">1</span><span class="s3">,</span><span class="s1">) + (lhs_indices.shape[-</span><span class="s4">2</span><span class="s1">]</span><span class="s3">,</span><span class="s1">))</span>
    <span class="s1">placeholder_shape = tuple(lhs_indices.shape[:-</span><span class="s4">2</span><span class="s1">]) + lhs_indices.shape[-</span><span class="s4">1</span><span class="s1">] * (</span><span class="s4">1</span><span class="s3">,</span><span class="s1">)</span>
    <span class="s3">try</span><span class="s1">:</span>
      <span class="s1">_validate_permutation(placeholder_data</span><span class="s3">, </span><span class="s1">lhs_indices</span><span class="s3">, </span><span class="s1">permutation</span><span class="s3">, </span><span class="s1">placeholder_shape)</span>
    <span class="s3">except </span><span class="s1">NotImplementedError:</span>
      <span class="s1">indices_can_be_untransposed = </span><span class="s3">False</span>
    <span class="s3">else</span><span class="s1">:</span>
      <span class="s1">indices_can_be_untransposed = </span><span class="s3">True</span>

    <span class="s0"># TODO(jakevdp): explore implementing the efficient approach without actually un-transposing</span>
    <span class="s0"># the indices. Could this be done by un-permuting ct, rhs, and dims?</span>

    <span class="s3">if </span><span class="s1">indices_can_be_untransposed:</span>
      <span class="s0"># Efficient approach: (1) un-transpose indices, (2) compute SDDMM, (3) re-transpose result.</span>
      <span class="s1">_</span><span class="s3">, </span><span class="s1">lhs_indices_T = _bcoo_transpose(placeholder_data</span><span class="s3">, </span><span class="s1">lhs_indices</span><span class="s3">, </span><span class="s1">permutation=permutation</span><span class="s3">,</span>
                                         <span class="s1">spinfo=SparseInfo(placeholder_shape))</span>
      <span class="s1">result_T_shape = tuple(placeholder_shape[i] </span><span class="s3">for </span><span class="s1">i </span><span class="s3">in </span><span class="s1">permutation)</span>
      <span class="s1">result_T = bcoo_dot_general_sampled(ct</span><span class="s3">, </span><span class="s1">rhs</span><span class="s3">, </span><span class="s1">lhs_indices_T</span><span class="s3">, </span><span class="s1">dimension_numbers=dims)</span>
      <span class="s1">result</span><span class="s3">, </span><span class="s1">_ = _bcoo_transpose(result_T</span><span class="s3">, </span><span class="s1">lhs_indices_T</span><span class="s3">, </span><span class="s1">permutation=out_axes</span><span class="s3">,</span>
                                  <span class="s1">spinfo=SparseInfo(result_T_shape))</span>
    <span class="s3">else</span><span class="s1">:</span>
      <span class="s0"># Fallback to direct approach when above is not possible.</span>
      <span class="s1">out_dense_T = lax.dot_general(ct</span><span class="s3">, </span><span class="s1">rhs</span><span class="s3">, </span><span class="s1">dimension_numbers=dims)</span>
      <span class="s1">out_dense = lax.transpose(out_dense_T</span><span class="s3">, </span><span class="s1">out_axes)</span>
      <span class="s1">result = _bcoo_extract(lhs_indices</span><span class="s3">, </span><span class="s1">out_dense)</span>
    <span class="s3">return </span><span class="s1">result</span><span class="s3">, </span><span class="s1">lhs_indices</span><span class="s3">, </span><span class="s1">rhs</span>
  <span class="s3">else</span><span class="s1">:</span>
    <span class="s1">dims = ((lhs_kept</span><span class="s3">, </span><span class="s1">ans_lhs)</span><span class="s3">, </span><span class="s1">(lhs_batch</span><span class="s3">, </span><span class="s1">ans_batch))  </span><span class="s0"># type: ignore[assignment]</span>
    <span class="s1">rhs_contract_sorted_by_lhs = list(np.take(rhs_contract</span><span class="s3">, </span><span class="s1">np.argsort(lhs_contract)))</span>
    <span class="s1">out_axes = list(np.argsort(list(rhs_batch) + rhs_contract_sorted_by_lhs + rhs_kept))</span>
    <span class="s1">result = _bcoo_dot_general(lhs_data</span><span class="s3">, </span><span class="s1">lhs_indices</span><span class="s3">, </span><span class="s1">ct</span><span class="s3">, </span><span class="s1">lhs_spinfo=lhs_spinfo</span><span class="s3">, </span><span class="s1">dimension_numbers=dims)</span>
    <span class="s3">return </span><span class="s1">lhs_data</span><span class="s3">, </span><span class="s1">lhs_indices</span><span class="s3">, </span><span class="s1">lax.transpose(result</span><span class="s3">, </span><span class="s1">out_axes)</span>

<span class="s3">def </span><span class="s1">_bcoo_dot_general_batch_rule(batched_args</span><span class="s3">, </span><span class="s1">batch_dims</span><span class="s3">, </span><span class="s1">*</span><span class="s3">, </span><span class="s1">dimension_numbers</span><span class="s3">, </span><span class="s1">lhs_spinfo: SparseInfo):</span>
  <span class="s1">_</span><span class="s3">, </span><span class="s1">_</span><span class="s3">, </span><span class="s1">rhs = batched_args</span>
  <span class="s1">_</span><span class="s3">, </span><span class="s1">_</span><span class="s3">, </span><span class="s1">rhs_bdim = batch_dims</span>
  <span class="s1">new_lhs_data</span><span class="s3">, </span><span class="s1">new_lhs_indices</span><span class="s3">, </span><span class="s1">new_lhs_spinfo = _bcoo_batch_dims_to_front(</span>
    <span class="s1">batched_args[:</span><span class="s4">2</span><span class="s1">]</span><span class="s3">, </span><span class="s1">batch_dims[:</span><span class="s4">2</span><span class="s1">]</span><span class="s3">, </span><span class="s1">lhs_spinfo</span><span class="s3">,</span>
    <span class="s1">batch_size=</span><span class="s3">None if </span><span class="s1">rhs_bdim </span><span class="s3">is None else </span><span class="s1">rhs.shape[rhs_bdim])</span>
  <span class="s1">new_dimension_numbers</span><span class="s3">, </span><span class="s1">result_batch_dim = _dot_general_batch_dim_nums(</span>
      <span class="s1">(len(lhs_spinfo.shape)</span><span class="s3">, </span><span class="s1">rhs.ndim)</span><span class="s3">, </span><span class="s1">(</span><span class="s4">0</span><span class="s3">, </span><span class="s1">rhs_bdim)</span><span class="s3">, </span><span class="s1">dimension_numbers)</span>
  <span class="s1">batched_out = _bcoo_dot_general(new_lhs_data</span><span class="s3">, </span><span class="s1">new_lhs_indices</span><span class="s3">, </span><span class="s1">rhs</span><span class="s3">, </span><span class="s1">lhs_spinfo=new_lhs_spinfo</span><span class="s3">,</span>
                                  <span class="s1">dimension_numbers=new_dimension_numbers)</span>
  <span class="s3">return </span><span class="s1">batched_out</span><span class="s3">, </span><span class="s1">result_batch_dim</span>

<span class="s1">ad.defjvp(bcoo_dot_general_p</span><span class="s3">, </span><span class="s1">_bcoo_dot_general_jvp_lhs</span><span class="s3">, None, </span><span class="s1">_bcoo_dot_general_jvp_rhs)</span>
<span class="s1">ad.primitive_transposes[bcoo_dot_general_p] = _bcoo_dot_general_transpose</span>
<span class="s1">batching.primitive_batchers[bcoo_dot_general_p] = _bcoo_dot_general_batch_rule</span>
<span class="s1">mlir.register_lowering(bcoo_dot_general_p</span><span class="s3">, </span><span class="s1">_bcoo_dot_general_default_lowering)</span>
<span class="s1">dispatch.simple_impl(bcoo_dot_general_p)</span>

<span class="s3">if </span><span class="s1">gpu_sparse.cuda_is_supported:</span>
  <span class="s1">mlir.register_lowering(</span>
      <span class="s1">bcoo_dot_general_p</span><span class="s3">, </span><span class="s1">_bcoo_dot_general_gpu_lowering</span><span class="s3">, </span><span class="s1">platform=</span><span class="s5">'cuda'</span><span class="s1">)</span>
<span class="s3">if </span><span class="s1">gpu_sparse.rocm_is_supported:</span>
  <span class="s1">mlir.register_lowering(</span>
      <span class="s1">bcoo_dot_general_p</span><span class="s3">, </span><span class="s1">_bcoo_dot_general_gpu_lowering</span><span class="s3">, </span><span class="s1">platform=</span><span class="s5">'rocm'</span><span class="s1">)</span>


<span class="s0">#----------------------------------------------------------------------</span>
<span class="s0"># bcoo_dot_general_sampled</span>
<span class="s0"># (batched) general sampled dot product of two dense ND arrays, with</span>
<span class="s0"># output computed only at a given set of sparse indices.</span>

<span class="s1">bcoo_dot_general_sampled_p = core.Primitive(</span><span class="s5">&quot;bcoo_dot_general_sampled&quot;</span><span class="s1">)</span>

<span class="s3">def </span><span class="s1">bcoo_dot_general_sampled(A: Array</span><span class="s3">, </span><span class="s1">B: Array</span><span class="s3">, </span><span class="s1">indices: Array</span><span class="s3">, </span><span class="s1">*</span><span class="s3">, </span><span class="s1">dimension_numbers: DotDimensionNumbers) -&gt; Array:</span>
  <span class="s2">&quot;&quot;&quot;A contraction operation with output computed at given sparse indices. 
 
  Args: 
    lhs: An ndarray. 
    rhs: An ndarray. 
    indices: BCOO indices. 
    dimension_numbers: a tuple of tuples of the form 
      `((lhs_contracting_dims, rhs_contracting_dims), 
      (lhs_batch_dims, rhs_batch_dims))`. 
 
  Returns: 
    BCOO data, an ndarray containing the result. 
  &quot;&quot;&quot;</span>
  <span class="s1">(lhs_contract</span><span class="s3">, </span><span class="s1">rhs_contract)</span><span class="s3">, </span><span class="s1">(lhs_batch</span><span class="s3">, </span><span class="s1">rhs_batch) = dimension_numbers</span>
  <span class="s1">cdims = (api_util._ensure_index_tuple(lhs_contract)</span><span class="s3">,</span>
           <span class="s1">api_util._ensure_index_tuple(rhs_contract))</span>
  <span class="s1">bdims = (api_util._ensure_index_tuple(lhs_batch)</span><span class="s3">,</span>
           <span class="s1">api_util._ensure_index_tuple(rhs_batch))</span>
  <span class="s3">return </span><span class="s1">bcoo_dot_general_sampled_p.bind(A</span><span class="s3">, </span><span class="s1">B</span><span class="s3">, </span><span class="s1">indices</span><span class="s3">,</span>
                                         <span class="s1">dimension_numbers=(cdims</span><span class="s3">, </span><span class="s1">bdims))</span>

<span class="s3">def </span><span class="s1">_bcoo_dot_general_sampled_slow(A</span><span class="s3">, </span><span class="s1">B</span><span class="s3">, </span><span class="s1">indices</span><span class="s3">, </span><span class="s1">*</span><span class="s3">, </span><span class="s1">dimension_numbers</span><span class="s3">, </span><span class="s1">precision):</span>
  <span class="s3">return </span><span class="s1">_bcoo_extract(indices</span><span class="s3">, </span><span class="s1">lax.dot_general(A</span><span class="s3">, </span><span class="s1">B</span><span class="s3">, </span><span class="s1">dimension_numbers=dimension_numbers</span><span class="s3">, </span><span class="s1">precision=precision))</span>

<span class="s3">def </span><span class="s1">_bcoo_dot_general_sampled_simple(A</span><span class="s3">, </span><span class="s1">B</span><span class="s3">, </span><span class="s1">indices</span><span class="s3">, </span><span class="s1">*</span><span class="s3">, </span><span class="s1">dimension_numbers</span><span class="s3">, </span><span class="s1">precision):</span>
  <span class="s0"># This case used in transpose of sparse matvec</span>
  <span class="s0"># TODO(jakevdp) generalize this</span>
  <span class="s3">del </span><span class="s1">precision  </span><span class="s0"># Unused here</span>
  <span class="s1">(lhs_contract</span><span class="s3">, </span><span class="s1">rhs_contract)</span><span class="s3">, </span><span class="s1">(lhs_batch</span><span class="s3">, </span><span class="s1">rhs_batch) = dimension_numbers</span>
  <span class="s3">assert not </span><span class="s1">(lhs_contract </span><span class="s3">or </span><span class="s1">rhs_contract </span><span class="s3">or </span><span class="s1">lhs_batch </span><span class="s3">or </span><span class="s1">rhs_batch)</span>
  <span class="s3">assert </span><span class="s1">A.ndim == B.ndim == </span><span class="s4">1</span>
  <span class="s1">n_batch = indices.ndim - </span><span class="s4">2</span>
  <span class="s1">n_sparse = indices.shape[-</span><span class="s4">1</span><span class="s1">]</span>
  <span class="s1">nse = indices.shape[-</span><span class="s4">2</span><span class="s1">]</span>
  <span class="s3">assert </span><span class="s1">n_batch + n_sparse == </span><span class="s4">2</span>
  <span class="s3">if </span><span class="s1">n_batch == </span><span class="s4">0</span><span class="s1">:</span>
    <span class="s3">return </span><span class="s1">(A.at[indices[:</span><span class="s3">, </span><span class="s4">0</span><span class="s1">]].get(mode=</span><span class="s5">'fill'</span><span class="s3">, </span><span class="s1">fill_value=</span><span class="s4">0</span><span class="s1">)</span>
            <span class="s1">* B.at[indices[:</span><span class="s3">, </span><span class="s4">1</span><span class="s1">]].get(mode=</span><span class="s5">'fill'</span><span class="s3">, </span><span class="s1">fill_value=</span><span class="s4">0</span><span class="s1">))</span>
  <span class="s3">elif </span><span class="s1">n_batch == </span><span class="s4">1</span><span class="s1">:</span>
    <span class="s3">return </span><span class="s1">A[:</span><span class="s3">, None</span><span class="s1">] * B.at[indices[...</span><span class="s3">, </span><span class="s4">0</span><span class="s1">]].get(mode=</span><span class="s5">'fill'</span><span class="s3">, </span><span class="s1">fill_value=</span><span class="s4">0</span><span class="s1">)</span>
  <span class="s3">elif </span><span class="s1">n_batch == </span><span class="s4">2</span><span class="s1">:</span>
    <span class="s1">out = A[:</span><span class="s3">, None, None</span><span class="s1">] * B[</span><span class="s3">None, </span><span class="s1">:</span><span class="s3">, None</span><span class="s1">]</span>
    <span class="s3">return </span><span class="s1">lax.broadcast_in_dim(out</span><span class="s3">, </span><span class="s1">(len(A)</span><span class="s3">, </span><span class="s1">len(B)</span><span class="s3">, </span><span class="s1">nse)</span><span class="s3">, </span><span class="s1">(</span><span class="s4">0</span><span class="s3">, </span><span class="s4">1</span><span class="s3">, </span><span class="s4">2</span><span class="s1">))</span>
  <span class="s3">else</span><span class="s1">:</span>
    <span class="s3">raise </span><span class="s1">ValueError(</span><span class="s5">&quot;too many batch dimensions.&quot;</span><span class="s1">)</span>

<span class="s3">def </span><span class="s1">_bcoo_dot_general_sampled_simple2(A</span><span class="s3">, </span><span class="s1">B</span><span class="s3">, </span><span class="s1">indices</span><span class="s3">, </span><span class="s1">*</span><span class="s3">, </span><span class="s1">dimension_numbers</span><span class="s3">, </span><span class="s1">precision):</span>
  <span class="s0"># This case used in transpose of sparse matmat</span>
  <span class="s0"># TODO(jakevdp) generalize this</span>
  <span class="s1">(lhs_contract</span><span class="s3">, </span><span class="s1">rhs_contract)</span><span class="s3">, </span><span class="s1">(lhs_batch</span><span class="s3">, </span><span class="s1">rhs_batch) = dimension_numbers</span>
  <span class="s3">assert not </span><span class="s1">(lhs_batch </span><span class="s3">or </span><span class="s1">rhs_batch)</span>
  <span class="s3">assert </span><span class="s1">len(lhs_contract) == len(rhs_contract) == </span><span class="s4">1</span>
  <span class="s3">assert </span><span class="s1">A.ndim == B.ndim == </span><span class="s4">2</span>
  <span class="s1">n_batch = indices.ndim - </span><span class="s4">2</span>
  <span class="s1">n_sparse = indices.shape[-</span><span class="s4">1</span><span class="s1">]</span>
  <span class="s1">nse = indices.shape[-</span><span class="s4">2</span><span class="s1">]</span>
  <span class="s3">assert </span><span class="s1">n_batch + n_sparse == </span><span class="s4">2</span>
  <span class="s3">if </span><span class="s1">n_batch == </span><span class="s4">0</span><span class="s1">:</span>
    <span class="s1">lhs_batch = [</span><span class="s4">1</span><span class="s1">] </span><span class="s3">if </span><span class="s1">lhs_contract[</span><span class="s4">0</span><span class="s1">] == </span><span class="s4">0 </span><span class="s3">else </span><span class="s1">[</span><span class="s4">0</span><span class="s1">]</span>
    <span class="s1">rhs_batch = [</span><span class="s4">1</span><span class="s1">] </span><span class="s3">if </span><span class="s1">rhs_contract[</span><span class="s4">0</span><span class="s1">] == </span><span class="s4">0 </span><span class="s3">else </span><span class="s1">[</span><span class="s4">0</span><span class="s1">]</span>
    <span class="s1">A = A.at[_tuple_replace((slice(</span><span class="s3">None</span><span class="s1">)</span><span class="s3">, </span><span class="s1">slice(</span><span class="s3">None</span><span class="s1">))</span><span class="s3">, </span><span class="s1">lhs_batch[</span><span class="s4">0</span><span class="s1">]</span><span class="s3">, </span><span class="s1">indices[:</span><span class="s3">, </span><span class="s4">0</span><span class="s1">])].get(mode=</span><span class="s5">'fill'</span><span class="s3">, </span><span class="s1">fill_value=</span><span class="s4">0</span><span class="s1">)</span>
    <span class="s1">B = B.at[_tuple_replace((slice(</span><span class="s3">None</span><span class="s1">)</span><span class="s3">, </span><span class="s1">slice(</span><span class="s3">None</span><span class="s1">))</span><span class="s3">, </span><span class="s1">rhs_batch[</span><span class="s4">0</span><span class="s1">]</span><span class="s3">, </span><span class="s1">indices[:</span><span class="s3">, </span><span class="s4">1</span><span class="s1">])].get(mode=</span><span class="s5">'fill'</span><span class="s3">, </span><span class="s1">fill_value=</span><span class="s4">0</span><span class="s1">)</span>
    <span class="s3">return </span><span class="s1">lax.dot_general(A</span><span class="s3">, </span><span class="s1">B</span><span class="s3">, </span><span class="s1">dimension_numbers=((lhs_contract</span><span class="s3">, </span><span class="s1">rhs_contract)</span><span class="s3">, </span><span class="s1">(lhs_batch</span><span class="s3">, </span><span class="s1">rhs_batch))</span><span class="s3">,</span>
                           <span class="s1">precision=precision)</span>
  <span class="s3">if </span><span class="s1">n_batch == </span><span class="s4">1</span><span class="s1">:</span>
    <span class="s1">lhs_batch = [</span><span class="s4">1</span><span class="s1">] </span><span class="s3">if </span><span class="s1">lhs_contract[</span><span class="s4">0</span><span class="s1">] == </span><span class="s4">0 </span><span class="s3">else </span><span class="s1">[</span><span class="s4">0</span><span class="s1">]</span>
    <span class="s1">rhs_batch = [</span><span class="s4">1</span><span class="s1">] </span><span class="s3">if </span><span class="s1">rhs_contract[</span><span class="s4">0</span><span class="s1">] == </span><span class="s4">0 </span><span class="s3">else </span><span class="s1">[</span><span class="s4">0</span><span class="s1">]</span>
    <span class="s1">B = B.at[_tuple_replace((slice(</span><span class="s3">None</span><span class="s1">)</span><span class="s3">, </span><span class="s1">slice(</span><span class="s3">None</span><span class="s1">))</span><span class="s3">, </span><span class="s1">rhs_batch[</span><span class="s4">0</span><span class="s1">]</span><span class="s3">, </span><span class="s1">indices[...</span><span class="s3">, </span><span class="s4">0</span><span class="s1">])].get(mode=</span><span class="s5">'fill'</span><span class="s3">, </span><span class="s1">fill_value=</span><span class="s4">0</span><span class="s1">)</span>
    <span class="s3">if </span><span class="s1">rhs_contract[</span><span class="s4">0</span><span class="s1">] == </span><span class="s4">1</span><span class="s1">:</span>
      <span class="s1">rhs_contract = [</span><span class="s4">2</span><span class="s1">]</span>
    <span class="s3">return </span><span class="s1">lax.dot_general(A</span><span class="s3">, </span><span class="s1">B</span><span class="s3">, </span><span class="s1">dimension_numbers=((lhs_contract</span><span class="s3">, </span><span class="s1">rhs_contract)</span><span class="s3">, </span><span class="s1">(lhs_batch</span><span class="s3">, </span><span class="s1">rhs_batch))</span><span class="s3">,</span>
                           <span class="s1">precision=precision)</span>
  <span class="s3">if </span><span class="s1">n_batch == </span><span class="s4">2</span><span class="s1">:</span>
    <span class="s1">out = lax.dot_general(A</span><span class="s3">, </span><span class="s1">B</span><span class="s3">, </span><span class="s1">dimension_numbers=((lhs_contract</span><span class="s3">, </span><span class="s1">rhs_contract)</span><span class="s3">, </span><span class="s1">(lhs_batch</span><span class="s3">, </span><span class="s1">rhs_batch))</span><span class="s3">,</span>
                          <span class="s1">precision=precision)</span>
    <span class="s3">return </span><span class="s1">lax.broadcast_in_dim(lax.expand_dims(out</span><span class="s3">, </span><span class="s1">(</span><span class="s4">2</span><span class="s3">,</span><span class="s1">))</span><span class="s3">, </span><span class="s1">(*out.shape</span><span class="s3">, </span><span class="s1">nse)</span><span class="s3">, </span><span class="s1">(</span><span class="s4">0</span><span class="s3">, </span><span class="s4">1</span><span class="s3">, </span><span class="s4">2</span><span class="s1">))</span>
  <span class="s3">else</span><span class="s1">:</span>
    <span class="s3">raise </span><span class="s1">ValueError(</span><span class="s5">&quot;too many batch dimensions.&quot;</span><span class="s1">)</span>


<span class="s1">@bcoo_dot_general_sampled_p.def_impl</span>
<span class="s3">def </span><span class="s1">_bcoo_dot_general_sampled_impl(A</span><span class="s3">, </span><span class="s1">B</span><span class="s3">, </span><span class="s1">indices</span><span class="s3">, </span><span class="s1">*</span><span class="s3">, </span><span class="s1">dimension_numbers):</span>
  <span class="s1">A = jnp.asarray(A)</span>
  <span class="s1">B = jnp.asarray(B)</span>
  <span class="s1">indices = jnp.asarray(indices)</span>
  <span class="s1">(lhs_contract</span><span class="s3">, </span><span class="s1">rhs_contract)</span><span class="s3">, </span><span class="s1">(lhs_batch</span><span class="s3">, </span><span class="s1">rhs_batch) = dimension_numbers</span>
  <span class="s1">n_batch = indices.ndim - </span><span class="s4">2</span>
  <span class="s1">n_sparse = indices.shape[-</span><span class="s4">1</span><span class="s1">]</span>
  <span class="s1">precision = lax.Precision.HIGHEST</span>

  <span class="s0"># TODO(jakevdp): add fast approach for more general cases / combine the following:</span>
  <span class="s3">if </span><span class="s1">(</span><span class="s3">not </span><span class="s1">(lhs_contract </span><span class="s3">or </span><span class="s1">rhs_contract </span><span class="s3">or </span><span class="s1">lhs_batch </span><span class="s3">or </span><span class="s1">rhs_batch)</span>
      <span class="s3">and </span><span class="s1">A.ndim == B.ndim == </span><span class="s4">1 </span><span class="s3">and </span><span class="s1">n_sparse + n_batch == </span><span class="s4">2</span><span class="s1">):</span>
    <span class="s3">return </span><span class="s1">_bcoo_dot_general_sampled_simple(A</span><span class="s3">, </span><span class="s1">B</span><span class="s3">, </span><span class="s1">indices</span><span class="s3">, </span><span class="s1">dimension_numbers=dimension_numbers</span><span class="s3">, </span><span class="s1">precision=precision)</span>
  <span class="s3">if </span><span class="s1">len(lhs_contract) == </span><span class="s4">1 </span><span class="s3">and not </span><span class="s1">lhs_batch </span><span class="s3">and </span><span class="s1">A.ndim == B.ndim == </span><span class="s4">2 </span><span class="s3">and </span><span class="s1">n_sparse + n_batch == </span><span class="s4">2</span><span class="s1">:</span>
    <span class="s3">return </span><span class="s1">_bcoo_dot_general_sampled_simple2(A</span><span class="s3">, </span><span class="s1">B</span><span class="s3">, </span><span class="s1">indices</span><span class="s3">, </span><span class="s1">dimension_numbers=dimension_numbers</span><span class="s3">, </span><span class="s1">precision=precision)</span>


  <span class="s3">return </span><span class="s1">_bcoo_dot_general_sampled_slow(A</span><span class="s3">, </span><span class="s1">B</span><span class="s3">, </span><span class="s1">indices</span><span class="s3">, </span><span class="s1">dimension_numbers=dimension_numbers</span><span class="s3">, </span><span class="s1">precision=precision)</span>


<span class="s1">@bcoo_dot_general_sampled_p.def_abstract_eval</span>
<span class="s3">def </span><span class="s1">_bcoo_dot_general_sampled_abstract_eval(A</span><span class="s3">, </span><span class="s1">B</span><span class="s3">, </span><span class="s1">indices</span><span class="s3">, </span><span class="s1">*</span><span class="s3">, </span><span class="s1">dimension_numbers):</span>
  <span class="s1">dense_result</span><span class="s3">, </span><span class="s1">= pe.abstract_eval_fun(</span><span class="s3">lambda </span><span class="s1">*args: [lax.dot_general(*args</span><span class="s3">, </span><span class="s1">dimension_numbers=dimension_numbers)]</span><span class="s3">, </span><span class="s1">A</span><span class="s3">, </span><span class="s1">B)</span>
  <span class="s1">sparse_result</span><span class="s3">, </span><span class="s1">= pe.abstract_eval_fun(</span><span class="s3">lambda </span><span class="s1">*args: [_bcoo_extract(*args)]</span><span class="s3">, </span><span class="s1">indices</span><span class="s3">, </span><span class="s1">dense_result)</span>
  <span class="s3">return </span><span class="s1">sparse_result</span>

<span class="s3">def </span><span class="s1">_bcoo_dot_general_sampled_transpose(ct</span><span class="s3">, </span><span class="s1">A</span><span class="s3">, </span><span class="s1">B</span><span class="s3">, </span><span class="s1">indices</span><span class="s3">, </span><span class="s1">*</span><span class="s3">, </span><span class="s1">dimension_numbers):</span>
  <span class="s1">A_shape = A.aval.shape </span><span class="s3">if </span><span class="s1">hasattr(A</span><span class="s3">, </span><span class="s5">'aval'</span><span class="s1">) </span><span class="s3">else </span><span class="s1">A.shape</span>
  <span class="s1">B_shape = B.aval.shape </span><span class="s3">if </span><span class="s1">hasattr(B</span><span class="s3">, </span><span class="s5">'aval'</span><span class="s1">) </span><span class="s3">else </span><span class="s1">B.shape</span>
  <span class="s1">mat_shape = _dot_general_validated_shape(A_shape</span><span class="s3">, </span><span class="s1">B_shape</span><span class="s3">, </span><span class="s1">dimension_numbers)</span>
  <span class="s1">mat = ad.UndefinedPrimal(core.ShapedArray(mat_shape</span><span class="s3">, </span><span class="s1">ct.dtype))</span>
  <span class="s1">indices</span><span class="s3">, </span><span class="s1">ct = _bcoo_extract_transpose(ct</span><span class="s3">, </span><span class="s1">indices</span><span class="s3">, </span><span class="s1">mat</span><span class="s3">, </span><span class="s1">assume_unique=</span><span class="s3">True</span><span class="s1">)</span>
  <span class="s1">kwds = {</span><span class="s5">'dimension_numbers'</span><span class="s1">: dimension_numbers</span><span class="s3">,</span>
          <span class="s5">'precision'</span><span class="s1">: </span><span class="s3">None,</span>
          <span class="s5">'preferred_element_type'</span><span class="s1">: </span><span class="s3">None</span><span class="s1">}</span>
  <span class="s1">A</span><span class="s3">, </span><span class="s1">B = ad.get_primitive_transpose(lax.dot_general_p)(ct</span><span class="s3">, </span><span class="s1">A</span><span class="s3">, </span><span class="s1">B</span><span class="s3">, </span><span class="s1">**kwds)</span>
  <span class="s3">return </span><span class="s1">A</span><span class="s3">, </span><span class="s1">B</span><span class="s3">, </span><span class="s1">indices</span>

<span class="s3">def </span><span class="s1">_bcoo_dot_general_sampled_jvp_A(A_dot</span><span class="s3">, </span><span class="s1">A</span><span class="s3">, </span><span class="s1">B</span><span class="s3">, </span><span class="s1">indices</span><span class="s3">, </span><span class="s1">*</span><span class="s3">, </span><span class="s1">dimension_numbers):</span>
  <span class="s3">return </span><span class="s1">bcoo_dot_general_sampled(A_dot</span><span class="s3">, </span><span class="s1">B</span><span class="s3">, </span><span class="s1">indices</span><span class="s3">, </span><span class="s1">dimension_numbers=dimension_numbers)</span>

<span class="s3">def </span><span class="s1">_bcoo_dot_general_sampled_jvp_B(B_dot</span><span class="s3">, </span><span class="s1">A</span><span class="s3">, </span><span class="s1">B</span><span class="s3">, </span><span class="s1">indices</span><span class="s3">, </span><span class="s1">*</span><span class="s3">, </span><span class="s1">dimension_numbers):</span>
  <span class="s3">return </span><span class="s1">bcoo_dot_general_sampled(A</span><span class="s3">, </span><span class="s1">B_dot</span><span class="s3">, </span><span class="s1">indices</span><span class="s3">, </span><span class="s1">dimension_numbers=dimension_numbers)</span>

<span class="s3">def </span><span class="s1">_bcoo_dot_general_sampled_batch_rule(batched_args</span><span class="s3">, </span><span class="s1">batch_dims</span><span class="s3">, </span><span class="s1">*</span><span class="s3">, </span><span class="s1">dimension_numbers):</span>
  <span class="s3">def </span><span class="s1">impl(A</span><span class="s3">, </span><span class="s1">B</span><span class="s3">, </span><span class="s1">indices):</span>
    <span class="s3">return </span><span class="s1">_bcoo_dot_general_sampled_impl(A</span><span class="s3">, </span><span class="s1">B</span><span class="s3">, </span><span class="s1">indices</span><span class="s3">, </span><span class="s1">dimension_numbers=dimension_numbers)</span>
  <span class="s3">return </span><span class="s1">vmap(impl</span><span class="s3">, </span><span class="s1">in_axes=batch_dims</span><span class="s3">, </span><span class="s1">out_axes=</span><span class="s4">0</span><span class="s1">)(*batched_args)</span><span class="s3">, </span><span class="s4">0</span>

<span class="s1">ad.defjvp(bcoo_dot_general_sampled_p</span><span class="s3">, </span><span class="s1">_bcoo_dot_general_sampled_jvp_A</span><span class="s3">,</span>
          <span class="s1">_bcoo_dot_general_sampled_jvp_B</span><span class="s3">, None</span><span class="s1">)</span>
<span class="s1">ad.primitive_transposes[bcoo_dot_general_sampled_p] = _bcoo_dot_general_sampled_transpose</span>
<span class="s1">batching.primitive_batchers[bcoo_dot_general_sampled_p] = _bcoo_dot_general_sampled_batch_rule</span>
<span class="s1">mlir.register_lowering(</span>
    <span class="s1">bcoo_dot_general_sampled_p</span><span class="s3">,</span>
    <span class="s1">mlir.lower_fun(_bcoo_dot_general_sampled_impl</span><span class="s3">, </span><span class="s1">multiple_results=</span><span class="s3">False</span><span class="s1">))</span>

<span class="s0">#----------------------------------------------------------------------</span>
<span class="s0"># bcoo_spdot_general</span>
<span class="s0"># (batched) general dot product of two BCOO sparse arrays returning a</span>
<span class="s0"># Dense ND array.</span>

<span class="s1">bcoo_spdot_general_p = core.Primitive(</span><span class="s5">'bcoo_spdot_general'</span><span class="s1">)</span>
<span class="s1">bcoo_spdot_general_p.multiple_results = </span><span class="s3">True</span>

<span class="s3">def </span><span class="s1">_bcoo_spdot_general(lhs_data: Array</span><span class="s3">, </span><span class="s1">lhs_indices: Array</span><span class="s3">, </span><span class="s1">rhs_data: Array</span><span class="s3">, </span><span class="s1">rhs_indices: Array</span><span class="s3">, </span><span class="s1">*</span><span class="s3">,</span>
                        <span class="s1">lhs_spinfo: SparseInfo</span><span class="s3">, </span><span class="s1">rhs_spinfo: SparseInfo</span><span class="s3">, </span><span class="s1">dimension_numbers: DotDimensionNumbers) -&gt; Tuple[Array</span><span class="s3">, </span><span class="s1">Array]:</span>
  <span class="s1">(lhs_contract</span><span class="s3">, </span><span class="s1">rhs_contract)</span><span class="s3">, </span><span class="s1">(lhs_batch</span><span class="s3">, </span><span class="s1">rhs_batch) = dimension_numbers</span>
  <span class="s1">cdims = (api_util._ensure_index_tuple(lhs_contract)</span><span class="s3">,</span>
           <span class="s1">api_util._ensure_index_tuple(rhs_contract))</span>
  <span class="s1">bdims = (api_util._ensure_index_tuple(lhs_batch)</span><span class="s3">,</span>
           <span class="s1">api_util._ensure_index_tuple(rhs_batch))</span>
  <span class="s3">return </span><span class="s1">bcoo_spdot_general_p.bind(lhs_data</span><span class="s3">, </span><span class="s1">lhs_indices</span><span class="s3">, </span><span class="s1">rhs_data</span><span class="s3">, </span><span class="s1">rhs_indices</span><span class="s3">,</span>
                                   <span class="s1">lhs_spinfo=lhs_spinfo</span><span class="s3">, </span><span class="s1">rhs_spinfo=rhs_spinfo</span><span class="s3">,</span>
                                   <span class="s1">dimension_numbers=(cdims</span><span class="s3">, </span><span class="s1">bdims))</span>

<span class="s3">def </span><span class="s1">_bcoo_spdot_general_unbatched(lhs_data</span><span class="s3">, </span><span class="s1">lhs_indices</span><span class="s3">, </span><span class="s1">rhs_data</span><span class="s3">, </span><span class="s1">rhs_indices</span><span class="s3">, </span><span class="s1">*</span><span class="s3">, </span><span class="s1">lhs_spinfo</span><span class="s3">, </span><span class="s1">rhs_spinfo</span><span class="s3">, </span><span class="s1">lhs_contracting</span><span class="s3">, </span><span class="s1">rhs_contracting</span><span class="s3">, </span><span class="s1">out_nse):</span>
  <span class="s1">lhs_shape = lhs_spinfo.shape</span>
  <span class="s1">rhs_shape = rhs_spinfo.shape</span>

  <span class="s1">lhs = _validate_bcoo(lhs_data</span><span class="s3">, </span><span class="s1">lhs_indices</span><span class="s3">, </span><span class="s1">lhs_shape)</span>
  <span class="s1">rhs = _validate_bcoo(rhs_data</span><span class="s3">, </span><span class="s1">rhs_indices</span><span class="s3">, </span><span class="s1">rhs_shape)</span>

  <span class="s3">assert </span><span class="s1">lhs.n_batch == rhs.n_batch == </span><span class="s4">0</span>
  <span class="s3">assert </span><span class="s1">lhs.n_dense == rhs.n_dense == </span><span class="s4">0</span>
  <span class="s3">assert </span><span class="s1">[lhs_shape[d] </span><span class="s3">for </span><span class="s1">d </span><span class="s3">in </span><span class="s1">lhs_contracting] == [rhs_shape[d] </span><span class="s3">for </span><span class="s1">d </span><span class="s3">in </span><span class="s1">rhs_contracting]</span>
  <span class="s3">assert </span><span class="s1">max(lhs_contracting</span><span class="s3">, </span><span class="s1">default=-</span><span class="s4">1</span><span class="s1">) &lt; lhs.n_sparse</span>
  <span class="s3">assert </span><span class="s1">max(rhs_contracting</span><span class="s3">, </span><span class="s1">default=-</span><span class="s4">1</span><span class="s1">) &lt; rhs.n_sparse</span>

  <span class="s1">out_shape = (</span>
    <span class="s1">*(s </span><span class="s3">for </span><span class="s1">i</span><span class="s3">, </span><span class="s1">s </span><span class="s3">in </span><span class="s1">enumerate(lhs_shape) </span><span class="s3">if </span><span class="s1">i </span><span class="s3">not in </span><span class="s1">lhs_contracting)</span><span class="s3">,</span>
    <span class="s1">*(s </span><span class="s3">for </span><span class="s1">i</span><span class="s3">, </span><span class="s1">s </span><span class="s3">in </span><span class="s1">enumerate(rhs_shape) </span><span class="s3">if </span><span class="s1">i </span><span class="s3">not in </span><span class="s1">rhs_contracting))</span>

  <span class="s1">lhs_i = lhs_indices[:</span><span class="s3">, </span><span class="s1">jnp.array(lhs_contracting</span><span class="s3">, </span><span class="s1">dtype=int)]</span>
  <span class="s1">rhs_i = rhs_indices[:</span><span class="s3">, </span><span class="s1">jnp.array(rhs_contracting</span><span class="s3">, </span><span class="s1">dtype=int)]</span>
  <span class="s1">lhs_j = lhs_indices[:</span><span class="s3">, </span><span class="s1">jnp.array(remaining(range(lhs.n_sparse)</span><span class="s3">, </span><span class="s1">lhs_contracting)</span><span class="s3">, </span><span class="s1">dtype=int)]</span>
  <span class="s1">rhs_j = rhs_indices[:</span><span class="s3">, </span><span class="s1">jnp.array(remaining(range(rhs.n_sparse)</span><span class="s3">, </span><span class="s1">rhs_contracting)</span><span class="s3">, </span><span class="s1">dtype=int)]</span>

  <span class="s0"># TODO(jakevdp): can we do this more efficiently than using an outer product? Note that</span>
  <span class="s0">#   jnp.isin() currently doesn't help much, because it also does all() over an outer</span>
  <span class="s0">#   comparison.</span>
  <span class="s1">overlap = (lhs_i[:</span><span class="s3">, None</span><span class="s1">] == rhs_i[</span><span class="s3">None, </span><span class="s1">:]).all(-</span><span class="s4">1</span><span class="s1">)</span>
  <span class="s1">lhs_fill_value = jnp.expand_dims(</span>
    <span class="s1">jnp.array([lhs_shape[d] </span><span class="s3">for </span><span class="s1">d </span><span class="s3">in </span><span class="s1">lhs_contracting]</span><span class="s3">, </span><span class="s1">dtype=lhs_i.dtype)</span><span class="s3">,</span>
    <span class="s1">range(lhs_i.ndim - </span><span class="s4">1</span><span class="s1">))</span>
  <span class="s1">rhs_fill_value = jnp.expand_dims(</span>
    <span class="s1">jnp.array([rhs_shape[d] </span><span class="s3">for </span><span class="s1">d </span><span class="s3">in </span><span class="s1">rhs_contracting]</span><span class="s3">, </span><span class="s1">dtype=rhs_i.dtype)</span><span class="s3">,</span>
    <span class="s1">range(rhs_i.ndim - </span><span class="s4">1</span><span class="s1">))</span>
  <span class="s1">lhs_valid = (lhs_i &lt; lhs_fill_value).all(-</span><span class="s4">1</span><span class="s1">)</span>
  <span class="s1">rhs_valid = (rhs_i &lt; rhs_fill_value).all(-</span><span class="s4">1</span><span class="s1">)</span>
  <span class="s1">out_data = jnp.where(overlap &amp; lhs_valid[:</span><span class="s3">, None</span><span class="s1">] &amp; rhs_valid[</span><span class="s3">None, </span><span class="s1">:]</span><span class="s3">,</span>
                       <span class="s1">lhs_data[:</span><span class="s3">, None</span><span class="s1">] * rhs_data[</span><span class="s3">None, </span><span class="s1">:]</span><span class="s3">, </span><span class="s4">0</span><span class="s1">).ravel()</span>

  <span class="s1">out_indices = jnp.empty([lhs.nse</span><span class="s3">, </span><span class="s1">rhs.nse</span><span class="s3">, </span><span class="s1">lhs_j.shape[-</span><span class="s4">1</span><span class="s1">] + rhs_j.shape[-</span><span class="s4">1</span><span class="s1">]]</span><span class="s3">,</span>
                          <span class="s1">dtype=jnp.result_type(lhs_indices</span><span class="s3">, </span><span class="s1">rhs_indices))</span>
  <span class="s1">out_indices = out_indices.at[:</span><span class="s3">, </span><span class="s1">:</span><span class="s3">, </span><span class="s1">:lhs_j.shape[-</span><span class="s4">1</span><span class="s1">]].set(lhs_j[:</span><span class="s3">, None</span><span class="s1">])</span>
  <span class="s1">out_indices = out_indices.at[:</span><span class="s3">, </span><span class="s1">:</span><span class="s3">, </span><span class="s1">lhs_j.shape[-</span><span class="s4">1</span><span class="s1">]:].set(rhs_j[</span><span class="s3">None, </span><span class="s1">:])</span>
  <span class="s1">out_indices = out_indices.reshape(len(out_data)</span><span class="s3">, </span><span class="s1">out_indices.shape[-</span><span class="s4">1</span><span class="s1">])</span>
  <span class="s0"># Note: we do not eliminate zeros here, because it can cause issues with autodiff.</span>
  <span class="s0"># See https://github.com/google/jax/issues/10163.</span>
  <span class="s3">return </span><span class="s1">_bcoo_sum_duplicates(out_data</span><span class="s3">, </span><span class="s1">out_indices</span><span class="s3">, </span><span class="s1">spinfo=SparseInfo(shape=out_shape)</span><span class="s3">, </span><span class="s1">nse=out_nse)</span>

<span class="s1">@bcoo_spdot_general_p.def_impl</span>
<span class="s3">def </span><span class="s1">_bcoo_spdot_general_impl(lhs_data</span><span class="s3">, </span><span class="s1">lhs_indices</span><span class="s3">, </span><span class="s1">rhs_data</span><span class="s3">, </span><span class="s1">rhs_indices</span><span class="s3">, </span><span class="s1">*</span><span class="s3">, </span><span class="s1">lhs_spinfo: SparseInfo</span><span class="s3">, </span><span class="s1">rhs_spinfo: SparseInfo</span><span class="s3">, </span><span class="s1">dimension_numbers):</span>
  <span class="s1">lhs_shape = lhs_spinfo.shape</span>
  <span class="s1">rhs_shape = rhs_spinfo.shape</span>

  <span class="s1">lhs = _validate_bcoo(lhs_data</span><span class="s3">, </span><span class="s1">lhs_indices</span><span class="s3">, </span><span class="s1">lhs_shape)</span>
  <span class="s1">rhs = _validate_bcoo(rhs_data</span><span class="s3">, </span><span class="s1">rhs_indices</span><span class="s3">, </span><span class="s1">rhs_shape)</span>
  <span class="s3">assert </span><span class="s1">lhs.n_dense == rhs.n_dense == </span><span class="s4">0</span>
  <span class="s1">data_aval</span><span class="s3">, </span><span class="s1">_ = _bcoo_spdot_general_abstract_eval(</span>
    <span class="s1">lhs_data.aval</span><span class="s3">, </span><span class="s1">lhs_indices.aval</span><span class="s3">, </span><span class="s1">rhs_data.aval</span><span class="s3">, </span><span class="s1">rhs_indices.aval</span><span class="s3">,</span>
    <span class="s1">lhs_spinfo=lhs_spinfo</span><span class="s3">, </span><span class="s1">rhs_spinfo=rhs_spinfo</span><span class="s3">, </span><span class="s1">dimension_numbers=dimension_numbers)</span>
  <span class="s1">out_nse = data_aval.shape[-</span><span class="s4">1</span><span class="s1">]</span>

  <span class="s1">(lhs_contracting</span><span class="s3">, </span><span class="s1">rhs_contracting)</span><span class="s3">, </span><span class="s1">(lhs_batch</span><span class="s3">, </span><span class="s1">rhs_batch) = dimension_numbers</span>

  <span class="s0"># Move batch dimensions to front of each array.</span>
  <span class="s1">lhs_batch_perm = [*lhs_batch</span><span class="s3">, </span><span class="s1">*remaining(range(lhs.n_batch)</span><span class="s3">, </span><span class="s1">lhs_batch)]</span>
  <span class="s1">rhs_batch_perm = [*rhs_batch</span><span class="s3">, </span><span class="s1">*remaining(range(rhs.n_batch)</span><span class="s3">, </span><span class="s1">rhs_batch)]</span>
  <span class="s1">lhs_data = lhs_data.transpose([*lhs_batch_perm</span><span class="s3">, </span><span class="s1">*range(lhs.n_batch</span><span class="s3">, </span><span class="s1">lhs_data.ndim)])</span>
  <span class="s1">rhs_data = rhs_data.transpose([*rhs_batch_perm</span><span class="s3">, </span><span class="s1">*range(rhs.n_batch</span><span class="s3">, </span><span class="s1">rhs_data.ndim)])</span>
  <span class="s1">lhs_indices = lhs_indices.transpose([*lhs_batch_perm</span><span class="s3">, </span><span class="s1">*range(lhs.n_batch</span><span class="s3">, </span><span class="s1">lhs_indices.ndim)])</span>
  <span class="s1">rhs_indices = rhs_indices.transpose([*rhs_batch_perm</span><span class="s3">, </span><span class="s1">*range(rhs.n_batch</span><span class="s3">, </span><span class="s1">rhs_indices.ndim)])</span>

  <span class="s0"># Implement batched dot product via vmap</span>
  <span class="s1">func = functools.partial(_bcoo_spdot_general_unbatched</span><span class="s3">,</span>
      <span class="s1">lhs_spinfo=SparseInfo(lhs_shape[lhs.n_batch:])</span><span class="s3">,</span>
      <span class="s1">rhs_spinfo=SparseInfo(rhs_shape[rhs.n_batch:])</span><span class="s3">,</span>
      <span class="s1">lhs_contracting=[d - lhs.n_batch </span><span class="s3">for </span><span class="s1">d </span><span class="s3">in </span><span class="s1">lhs_contracting]</span><span class="s3">,</span>
      <span class="s1">rhs_contracting=[d - rhs.n_batch </span><span class="s3">for </span><span class="s1">d </span><span class="s3">in </span><span class="s1">rhs_contracting]</span><span class="s3">,</span>
      <span class="s1">out_nse=out_nse)</span>

  <span class="s1">func = nfold_vmap(func</span><span class="s3">, </span><span class="s1">rhs.n_batch - len(rhs_batch)</span><span class="s3">, </span><span class="s1">in_axes=(</span><span class="s3">None, None, </span><span class="s4">0</span><span class="s3">, </span><span class="s4">0</span><span class="s1">))</span>
  <span class="s1">func = nfold_vmap(func</span><span class="s3">, </span><span class="s1">lhs.n_batch - len(lhs_batch)</span><span class="s3">, </span><span class="s1">in_axes=(</span><span class="s4">0</span><span class="s3">, </span><span class="s4">0</span><span class="s3">, None, None</span><span class="s1">))</span>
  <span class="s1">func = nfold_vmap(func</span><span class="s3">, </span><span class="s1">len(lhs_batch))</span>
  <span class="s3">return </span><span class="s1">func(lhs_data</span><span class="s3">, </span><span class="s1">lhs_indices</span><span class="s3">, </span><span class="s1">rhs_data</span><span class="s3">, </span><span class="s1">rhs_indices)</span>

<span class="s1">@bcoo_spdot_general_p.def_abstract_eval</span>
<span class="s3">def </span><span class="s1">_bcoo_spdot_general_abstract_eval(lhs_data</span><span class="s3">, </span><span class="s1">lhs_indices</span><span class="s3">, </span><span class="s1">rhs_data</span><span class="s3">, </span><span class="s1">rhs_indices</span><span class="s3">, </span><span class="s1">*</span><span class="s3">, </span><span class="s1">lhs_spinfo: SparseInfo</span><span class="s3">, </span><span class="s1">rhs_spinfo: SparseInfo</span><span class="s3">, </span><span class="s1">dimension_numbers):</span>
  <span class="s1">lhs_shape = lhs_spinfo.shape</span>
  <span class="s1">rhs_shape = rhs_spinfo.shape</span>
  <span class="s1">out_shape = _dot_general_validated_shape(lhs_shape</span><span class="s3">, </span><span class="s1">rhs_shape</span><span class="s3">,</span>
                                           <span class="s1">dimension_numbers)</span>

  <span class="s3">if </span><span class="s1">lhs_data.dtype != rhs_data.dtype:</span>
    <span class="s3">raise </span><span class="s1">ValueError(</span><span class="s5">&quot;bcoo_spdot_general requires inputs to have matching dtypes; &quot;</span>
                     <span class="s5">f&quot;got lhs.dtype=</span><span class="s3">{</span><span class="s1">lhs_data.dtype</span><span class="s3">}</span><span class="s5">, rhs.dtype=</span><span class="s3">{</span><span class="s1">rhs_data.dtype</span><span class="s3">}</span><span class="s5">&quot;</span><span class="s1">)</span>
  <span class="s1">lhs = _validate_bcoo(lhs_data</span><span class="s3">, </span><span class="s1">lhs_indices</span><span class="s3">, </span><span class="s1">lhs_shape)</span>
  <span class="s1">rhs = _validate_bcoo(rhs_data</span><span class="s3">, </span><span class="s1">rhs_indices</span><span class="s3">, </span><span class="s1">rhs_shape)</span>
  <span class="s1">(lhs_contracting</span><span class="s3">, </span><span class="s1">rhs_contracting)</span><span class="s3">, </span><span class="s1">(lhs_batch</span><span class="s3">, </span><span class="s1">rhs_batch) = dimension_numbers</span>
  <span class="s1">_ = _dot_general_validated_shape(lhs_shape</span><span class="s3">, </span><span class="s1">rhs_shape</span><span class="s3">, </span><span class="s1">dimension_numbers)</span>

  <span class="s3">if </span><span class="s1">lhs.n_dense </span><span class="s3">or </span><span class="s1">rhs.n_dense:</span>
    <span class="s0"># TODO(jakevdp): handle dense dimensions</span>
    <span class="s3">raise </span><span class="s1">NotImplementedError(</span><span class="s5">&quot;bcoo_spdot_general with dense dimensions.&quot;</span><span class="s1">)</span>

  <span class="s3">if </span><span class="s1">(lhs_batch </span><span class="s3">and </span><span class="s1">max(lhs_batch) &gt;= lhs.n_batch) </span><span class="s3">or </span><span class="s1">(rhs_batch </span><span class="s3">and </span><span class="s1">max(rhs_batch) &gt;= rhs.n_batch):</span>
    <span class="s3">raise </span><span class="s1">NotImplementedError(</span><span class="s5">&quot;bcoo_spdot_general: batch_dims must correspond to batch dimensions of the sparse representation.&quot;</span><span class="s1">)</span>

  <span class="s3">if </span><span class="s1">lhs_contracting </span><span class="s3">and </span><span class="s1">(min(lhs_contracting) &lt; lhs.n_batch </span><span class="s3">or </span><span class="s1">max(lhs_contracting) &gt;= lhs.n_batch + lhs.n_sparse):</span>
    <span class="s3">raise </span><span class="s1">NotImplementedError(</span><span class="s5">&quot;bcoo_spdot_general only supports contraction of sparse indices.&quot;</span><span class="s1">)</span>

  <span class="s3">if </span><span class="s1">rhs_contracting </span><span class="s3">and </span><span class="s1">(min(rhs_contracting) &lt; rhs.n_batch </span><span class="s3">or </span><span class="s1">max(rhs_contracting) &gt;= rhs.n_batch + rhs.n_sparse):</span>
    <span class="s3">raise </span><span class="s1">NotImplementedError(</span><span class="s5">&quot;bcoo_spdot_general only supports contraction of sparse indices.&quot;</span><span class="s1">)</span>

  <span class="s3">if </span><span class="s1">rhs.n_batch &gt; len(rhs_batch) </span><span class="s3">and </span><span class="s1">lhs.n_sparse &gt; len(lhs_contracting):</span>
    <span class="s3">raise </span><span class="s1">ValueError(</span><span class="s5">&quot;bcoo_spdot_general: cannot have unused batch dims on rhs with unused sparse dims on lhs.&quot;</span><span class="s1">)</span>

  <span class="s1">out_nse = (</span>
    <span class="s1">(lhs.nse </span><span class="s3">if </span><span class="s1">lhs.n_sparse &gt; len(lhs_contracting) </span><span class="s3">else </span><span class="s4">1</span><span class="s1">) *</span>
    <span class="s1">(rhs.nse </span><span class="s3">if </span><span class="s1">rhs.n_sparse &gt; len(rhs_contracting) </span><span class="s3">else </span><span class="s4">1</span><span class="s1">)</span>
  <span class="s1">)</span>

  <span class="s0"># Ensure we're not storing more output elements than necessary.</span>
  <span class="s0"># TODO(jakevdp): should we warn here if output is effectively dense?</span>
  <span class="s1">out_n_batch = lhs.n_batch + rhs.n_batch - len(lhs_batch)</span>
  <span class="s1">out_nse = min(out_nse</span><span class="s3">, </span><span class="s1">math.prod(out_shape[out_n_batch:]))</span>

  <span class="s1">data_shape = (</span>
    <span class="s1">*(lhs_shape[dim] </span><span class="s3">for </span><span class="s1">dim </span><span class="s3">in </span><span class="s1">lhs_batch)</span><span class="s3">,</span>
    <span class="s1">*(lhs_data.shape[dim] </span><span class="s3">for </span><span class="s1">dim </span><span class="s3">in </span><span class="s1">range(lhs.n_batch) </span><span class="s3">if </span><span class="s1">dim </span><span class="s3">not in </span><span class="s1">lhs_batch)</span><span class="s3">,</span>
    <span class="s1">*(rhs_data.shape[dim] </span><span class="s3">for </span><span class="s1">dim </span><span class="s3">in </span><span class="s1">range(rhs.n_batch) </span><span class="s3">if </span><span class="s1">dim </span><span class="s3">not in </span><span class="s1">rhs_batch)</span><span class="s3">,</span>
    <span class="s1">out_nse)</span>
  <span class="s1">indices_shape = (</span>
    <span class="s1">*(lhs_shape[dim] </span><span class="s3">for </span><span class="s1">dim </span><span class="s3">in </span><span class="s1">lhs_batch)</span><span class="s3">,</span>
    <span class="s1">*(lhs_indices.shape[dim] </span><span class="s3">for </span><span class="s1">dim </span><span class="s3">in </span><span class="s1">range(lhs.n_batch) </span><span class="s3">if </span><span class="s1">dim </span><span class="s3">not in </span><span class="s1">lhs_batch)</span><span class="s3">,</span>
    <span class="s1">*(rhs_indices.shape[dim] </span><span class="s3">for </span><span class="s1">dim </span><span class="s3">in </span><span class="s1">range(rhs.n_batch) </span><span class="s3">if </span><span class="s1">dim </span><span class="s3">not in </span><span class="s1">rhs_batch)</span><span class="s3">,</span>
    <span class="s1">out_nse</span><span class="s3">, </span><span class="s1">lhs.n_sparse + rhs.n_sparse - </span><span class="s4">2 </span><span class="s1">* len(lhs_contracting))</span>

  <span class="s1">data_aval = core.ShapedArray(data_shape</span><span class="s3">, </span><span class="s1">lhs_data.dtype)</span>
  <span class="s1">indices_aval = core.ShapedArray(indices_shape</span><span class="s3">, </span><span class="s1">lhs_indices.dtype)</span>
  <span class="s1">_validate_bcoo(data_aval</span><span class="s3">, </span><span class="s1">indices_aval</span><span class="s3">, </span><span class="s1">out_shape)  </span><span class="s0"># pytype: disable=wrong-arg-types  # always-use-return-annotations</span>

  <span class="s3">return </span><span class="s1">data_aval</span><span class="s3">, </span><span class="s1">indices_aval</span>

<span class="s3">def </span><span class="s1">_bcoo_spdot_general_batch_rule(batched_args</span><span class="s3">, </span><span class="s1">batch_dims</span><span class="s3">, </span><span class="s1">*</span><span class="s3">, </span><span class="s1">lhs_spinfo: SparseInfo</span><span class="s3">, </span><span class="s1">rhs_spinfo: SparseInfo</span><span class="s3">, </span><span class="s1">dimension_numbers):</span>
  <span class="s1">lhs_ndim = len(lhs_spinfo.shape)</span>
  <span class="s1">rhs_ndim = len(rhs_spinfo.shape)</span>
  <span class="s1">batch_size = max(arg.shape[dim] </span><span class="s3">for </span><span class="s1">arg</span><span class="s3">, </span><span class="s1">dim </span><span class="s3">in </span><span class="s1">zip(batched_args</span><span class="s3">, </span><span class="s1">batch_dims) </span><span class="s3">if </span><span class="s1">dim </span><span class="s3">is not None</span><span class="s1">)</span>
  <span class="s1">lhs_data</span><span class="s3">, </span><span class="s1">lhs_indices</span><span class="s3">, </span><span class="s1">lhs_spinfo = _bcoo_batch_dims_to_front(</span>
    <span class="s1">batched_args[:</span><span class="s4">2</span><span class="s1">]</span><span class="s3">, </span><span class="s1">batch_dims[:</span><span class="s4">2</span><span class="s1">]</span><span class="s3">, </span><span class="s1">lhs_spinfo</span><span class="s3">, </span><span class="s1">batch_size=batch_size)</span>
  <span class="s1">rhs_data</span><span class="s3">, </span><span class="s1">rhs_indices</span><span class="s3">, </span><span class="s1">rhs_spinfo = _bcoo_batch_dims_to_front(</span>
    <span class="s1">batched_args[</span><span class="s4">2</span><span class="s1">:]</span><span class="s3">, </span><span class="s1">batch_dims[</span><span class="s4">2</span><span class="s1">:]</span><span class="s3">, </span><span class="s1">rhs_spinfo</span><span class="s3">, </span><span class="s1">batch_size=batch_size)</span>
  <span class="s1">dimension_numbers</span><span class="s3">, </span><span class="s1">result_batch_dim = _dot_general_batch_dim_nums(</span>
      <span class="s1">(lhs_ndim</span><span class="s3">, </span><span class="s1">rhs_ndim)</span><span class="s3">, </span><span class="s1">(</span><span class="s4">0</span><span class="s3">, </span><span class="s4">0</span><span class="s1">)</span><span class="s3">, </span><span class="s1">dimension_numbers)</span>
  <span class="s1">batched_out = _bcoo_spdot_general(lhs_data</span><span class="s3">, </span><span class="s1">lhs_indices</span><span class="s3">, </span><span class="s1">rhs_data</span><span class="s3">, </span><span class="s1">rhs_indices</span><span class="s3">,</span>
                                    <span class="s1">dimension_numbers=dimension_numbers</span><span class="s3">,</span>
                                    <span class="s1">lhs_spinfo=lhs_spinfo</span><span class="s3">, </span><span class="s1">rhs_spinfo=rhs_spinfo)</span>
  <span class="s3">return </span><span class="s1">batched_out</span><span class="s3">, </span><span class="s1">(result_batch_dim</span><span class="s3">, </span><span class="s1">result_batch_dim)</span>


<span class="s3">def </span><span class="s1">_bcoo_spdot_general_jvp(primals</span><span class="s3">, </span><span class="s1">tangents</span><span class="s3">, </span><span class="s1">**kwds):</span>
  <span class="s1">lhs_data</span><span class="s3">, </span><span class="s1">lhs_indices</span><span class="s3">, </span><span class="s1">rhs_data</span><span class="s3">, </span><span class="s1">rhs_indices = primals</span>
  <span class="s1">lhs_data_dot</span><span class="s3">, </span><span class="s1">lhs_indices_dot</span><span class="s3">, </span><span class="s1">rhs_data_dot</span><span class="s3">, </span><span class="s1">rhs_indices_dot = tangents</span>
  <span class="s1">primals_out = _bcoo_spdot_general(*primals</span><span class="s3">, </span><span class="s1">**kwds)</span>
  <span class="s3">assert </span><span class="s1">type(lhs_indices_dot) </span><span class="s3">is </span><span class="s1">ad.Zero</span>
  <span class="s3">assert </span><span class="s1">type(rhs_indices_dot) </span><span class="s3">is </span><span class="s1">ad.Zero</span>
  <span class="s1">data_dot_out = </span><span class="s4">0</span>
  <span class="s3">if </span><span class="s1">type(lhs_data_dot) </span><span class="s3">is not </span><span class="s1">ad.Zero:</span>
    <span class="s1">data_dot_out += _bcoo_spdot_general(lhs_data_dot</span><span class="s3">, </span><span class="s1">lhs_indices</span><span class="s3">, </span><span class="s1">rhs_data</span><span class="s3">, </span><span class="s1">rhs_indices</span><span class="s3">, </span><span class="s1">**kwds)[</span><span class="s4">0</span><span class="s1">]</span>
  <span class="s3">if </span><span class="s1">type(rhs_data_dot) </span><span class="s3">is not </span><span class="s1">ad.Zero:</span>
    <span class="s1">data_dot_out += _bcoo_spdot_general(lhs_data</span><span class="s3">, </span><span class="s1">lhs_indices</span><span class="s3">, </span><span class="s1">rhs_data_dot</span><span class="s3">, </span><span class="s1">rhs_indices</span><span class="s3">, </span><span class="s1">**kwds)[</span><span class="s4">0</span><span class="s1">]</span>
  <span class="s3">return </span><span class="s1">primals_out</span><span class="s3">, </span><span class="s1">[data_dot_out</span><span class="s3">, </span><span class="s1">ad.Zero.from_value(primals_out[</span><span class="s4">1</span><span class="s1">])]</span>

<span class="s0"># TODO(JVP): transpose rule</span>
<span class="s1">batching.primitive_batchers[bcoo_spdot_general_p] = _bcoo_spdot_general_batch_rule</span>
<span class="s1">ad.primitive_jvps[bcoo_spdot_general_p] = _bcoo_spdot_general_jvp</span>
<span class="s1">mlir.register_lowering(bcoo_spdot_general_p</span><span class="s3">, </span><span class="s1">mlir.lower_fun(</span>
    <span class="s1">_bcoo_spdot_general_impl</span><span class="s3">, </span><span class="s1">multiple_results=</span><span class="s3">True</span><span class="s1">))</span>


<span class="s0">#----------------------------------------------------------------------</span>
<span class="s0"># bcoo_sort_indices</span>
<span class="s0"># Utility to sort the indices of a BCOO representation. This primitive</span>
<span class="s0"># does not support deduplication or removing of zeros; see bcoo_sum_duplicates.</span>

<span class="s1">bcoo_sort_indices_p = core.Primitive(</span><span class="s5">&quot;bcoo_sort_indices&quot;</span><span class="s1">)</span>
<span class="s1">bcoo_sort_indices_p.multiple_results = </span><span class="s3">True</span>

<span class="s3">def </span><span class="s1">bcoo_sort_indices(mat: BCOO) -&gt; BCOO:</span>
  <span class="s2">&quot;&quot;&quot;Sort indices of a BCOO array. 
 
  Args: 
    mat : BCOO array 
 
  Returns: 
    mat_out : BCOO array with sorted indices. 
  &quot;&quot;&quot;</span>
  <span class="s1">data</span><span class="s3">, </span><span class="s1">indices = bcoo_sort_indices_p.bind(*mat._bufs</span><span class="s3">, </span><span class="s1">spinfo=mat._info)</span>
  <span class="s3">return </span><span class="s1">BCOO((data</span><span class="s3">, </span><span class="s1">indices)</span><span class="s3">, </span><span class="s1">shape=mat.shape</span><span class="s3">, </span><span class="s1">indices_sorted=</span><span class="s3">True,</span>
              <span class="s1">unique_indices=mat.unique_indices)</span>

<span class="s1">@bcoo_sort_indices_p.def_impl</span>
<span class="s3">def </span><span class="s1">_bcoo_sort_indices_impl(data</span><span class="s3">, </span><span class="s1">indices</span><span class="s3">, </span><span class="s1">*</span><span class="s3">, </span><span class="s1">spinfo):</span>
  <span class="s1">props = _validate_bcoo(data</span><span class="s3">, </span><span class="s1">indices</span><span class="s3">, </span><span class="s1">spinfo.shape)</span>
  <span class="s3">if </span><span class="s1">props.n_sparse == </span><span class="s4">0</span><span class="s1">:</span>
    <span class="s3">return </span><span class="s1">data</span><span class="s3">, </span><span class="s1">indices</span>
  <span class="s1">f = nfold_vmap(_bcoo_sort_indices_unbatched</span><span class="s3">, </span><span class="s1">props.n_batch</span><span class="s3">, </span><span class="s1">broadcasted=</span><span class="s3">False</span><span class="s1">)</span>
  <span class="s1">indices</span><span class="s3">, </span><span class="s1">perm = f(indices)</span>
  <span class="s1">permute = nfold_vmap(</span><span class="s3">lambda </span><span class="s1">d</span><span class="s3">, </span><span class="s1">p: d[p]</span><span class="s3">, </span><span class="s1">props.n_batch)</span>
  <span class="s1">data = permute(data</span><span class="s3">, </span><span class="s1">perm)</span>
  <span class="s3">return </span><span class="s1">data</span><span class="s3">, </span><span class="s1">indices</span>

<span class="s3">def </span><span class="s1">_bcoo_sort_indices_unbatched(indices):</span>
  <span class="s0"># sort indices without summing duplicates</span>
  <span class="s1">nse</span><span class="s3">, </span><span class="s1">N = indices.shape</span>
  <span class="s1">idx_cols = (indices[:</span><span class="s3">, </span><span class="s1">i] </span><span class="s3">for </span><span class="s1">i </span><span class="s3">in </span><span class="s1">range(N))</span>
  <span class="s1">*indices</span><span class="s3">, </span><span class="s1">perm = lax.sort((*idx_cols</span><span class="s3">, </span><span class="s1">lax.iota(indices.dtype</span><span class="s3">, </span><span class="s1">nse))</span><span class="s3">, </span><span class="s1">num_keys=N)</span>
  <span class="s3">return </span><span class="s1">jnp.column_stack(indices)</span><span class="s3">, </span><span class="s1">perm</span>

<span class="s1">@bcoo_sort_indices_p.def_abstract_eval</span>
<span class="s3">def </span><span class="s1">_bcoo_sort_indices_abstract_eval(data</span><span class="s3">, </span><span class="s1">indices</span><span class="s3">, </span><span class="s1">*</span><span class="s3">, </span><span class="s1">spinfo):</span>
  <span class="s1">props = _validate_bcoo(data</span><span class="s3">, </span><span class="s1">indices</span><span class="s3">, </span><span class="s1">spinfo.shape)</span>
  <span class="s3">if </span><span class="s1">props.n_sparse == </span><span class="s4">0</span><span class="s1">:</span>
    <span class="s3">return </span><span class="s1">data</span><span class="s3">, </span><span class="s1">indices</span>
  <span class="s1">data_out = core.ShapedArray(</span>
    <span class="s1">(*map(max</span><span class="s3">, </span><span class="s1">indices.shape[:props.n_batch]</span><span class="s3">, </span><span class="s1">data.shape[:props.n_batch])</span><span class="s3">,</span>
     <span class="s1">props.nse</span><span class="s3">, </span><span class="s1">*data.shape[props.n_batch + </span><span class="s4">1</span><span class="s1">:])</span><span class="s3">, </span><span class="s1">data.dtype</span><span class="s3">, </span><span class="s1">weak_type=data.weak_type)</span>
  <span class="s3">return </span><span class="s1">data_out</span><span class="s3">, </span><span class="s1">indices</span>

<span class="s3">def </span><span class="s1">_bcoo_sort_indices_batching_rule(batched_args</span><span class="s3">, </span><span class="s1">batch_dims</span><span class="s3">, </span><span class="s1">*</span><span class="s3">, </span><span class="s1">spinfo):</span>
  <span class="s1">data</span><span class="s3">, </span><span class="s1">indices</span><span class="s3">, </span><span class="s1">spinfo = _bcoo_batch_dims_to_front(batched_args</span><span class="s3">, </span><span class="s1">batch_dims</span><span class="s3">, </span><span class="s1">spinfo)</span>
  <span class="s1">data_out</span><span class="s3">, </span><span class="s1">indices_out = bcoo_sort_indices_p.bind(data</span><span class="s3">, </span><span class="s1">indices</span><span class="s3">, </span><span class="s1">spinfo=spinfo)</span>
  <span class="s1">out_axes = (</span><span class="s4">0</span><span class="s3">, </span><span class="s4">0</span><span class="s1">)</span>
  <span class="s0"># Note: if data is unbatched on input, it will be batched on output.</span>
  <span class="s0"># However, if indices are unbatched on input, they will be unbatched on output.</span>
  <span class="s3">if </span><span class="s1">batch_dims[</span><span class="s4">1</span><span class="s1">] </span><span class="s3">is None</span><span class="s1">:</span>
    <span class="s1">indices_out = indices_out[</span><span class="s4">0</span><span class="s1">]</span>
    <span class="s1">out_axes = (</span><span class="s4">0</span><span class="s3">, None</span><span class="s1">)</span>
  <span class="s3">return </span><span class="s1">(data_out</span><span class="s3">, </span><span class="s1">indices_out)</span><span class="s3">, </span><span class="s1">out_axes</span>

<span class="s3">def </span><span class="s1">_bcoo_sort_indices_jvp(primals</span><span class="s3">, </span><span class="s1">tangents</span><span class="s3">, </span><span class="s1">*</span><span class="s3">, </span><span class="s1">spinfo):</span>
  <span class="s1">props = _validate_bcoo(*primals</span><span class="s3">, </span><span class="s1">spinfo.shape)</span>
  <span class="s3">if </span><span class="s1">props.n_sparse == </span><span class="s4">0</span><span class="s1">:</span>
    <span class="s3">return </span><span class="s1">primals</span><span class="s3">, </span><span class="s1">tangents</span>

  <span class="s1">data</span><span class="s3">, </span><span class="s1">indices = primals</span>
  <span class="s1">data_dot</span><span class="s3">, </span><span class="s1">_ = tangents</span>
  <span class="s1">f = nfold_vmap(_bcoo_sort_indices_unbatched</span><span class="s3">, </span><span class="s1">props.n_batch)</span>
  <span class="s1">indices_out</span><span class="s3">, </span><span class="s1">perm = f(indices)</span>
  <span class="s1">permute = nfold_vmap(</span><span class="s3">lambda </span><span class="s1">d</span><span class="s3">, </span><span class="s1">p: d[p]</span><span class="s3">, </span><span class="s1">props.n_batch)</span>
  <span class="s1">data_out = permute(data</span><span class="s3">, </span><span class="s1">perm)</span>

  <span class="s1">indices_dot_out = ad.Zero.from_value(indices)</span>
  <span class="s1">data_dot_out = ad.Zero.from_value(data_out) </span><span class="s3">if </span><span class="s1">type(data_dot) </span><span class="s3">is </span><span class="s1">ad.Zero </span><span class="s3">else </span><span class="s1">permute(data_dot</span><span class="s3">, </span><span class="s1">perm)</span>
  <span class="s3">return </span><span class="s1">(data_out</span><span class="s3">, </span><span class="s1">indices_out)</span><span class="s3">, </span><span class="s1">(data_dot_out</span><span class="s3">, </span><span class="s1">indices_dot_out)</span>

<span class="s1">_bcoo_sort_indices_hlo = mlir.lower_fun(</span>
    <span class="s1">_bcoo_sort_indices_impl</span><span class="s3">, </span><span class="s1">multiple_results=</span><span class="s3">True</span><span class="s1">)</span>

<span class="s1">ad.primitive_jvps[bcoo_sort_indices_p] = _bcoo_sort_indices_jvp</span>
<span class="s1">batching.primitive_batchers[bcoo_sort_indices_p] = _bcoo_sort_indices_batching_rule</span>
<span class="s1">mlir.register_lowering(bcoo_sort_indices_p</span><span class="s3">, </span><span class="s1">_bcoo_sort_indices_hlo)</span>


<span class="s0">#----------------------------------------------------------------------</span>
<span class="s0"># bcoo_sum_duplicates</span>
<span class="s0"># Utility to sum duplicate indices in a BCOO array representation.</span>

<span class="s1">bcoo_sum_duplicates_p = core.Primitive(</span><span class="s5">&quot;bcoo_sum_duplicates&quot;</span><span class="s1">)</span>
<span class="s1">bcoo_sum_duplicates_p.multiple_results = </span><span class="s3">True</span>

<span class="s3">def </span><span class="s1">bcoo_sum_duplicates(mat: BCOO</span><span class="s3">, </span><span class="s1">nse: Optional[int] = </span><span class="s3">None</span><span class="s1">) -&gt; BCOO:</span>
  <span class="s2">&quot;&quot;&quot;Sums duplicate indices within a BCOO array, returning an array with sorted indices. 
 
  Args: 
    mat : BCOO array 
    nse : integer (optional). The number of specified elements in the output matrix. This must 
      be specified for bcoo_sum_duplicates to be compatible with JIT and other JAX transformations. 
      If not specified, the optimal nse will be computed based on the contents of the data and 
      index arrays. If specified nse is larger than necessary, data and index arrays will be padded 
      with standard fill values. If smaller than necessary, data elements will be dropped from the 
      output matrix. 
 
  Returns: 
    mat_out : BCOO array with sorted indices and no duplicate indices. 
  &quot;&quot;&quot;</span>
  <span class="s1">data</span><span class="s3">, </span><span class="s1">indices = _bcoo_sum_duplicates(mat.data</span><span class="s3">, </span><span class="s1">mat.indices</span><span class="s3">, </span><span class="s1">spinfo=mat._info</span><span class="s3">, </span><span class="s1">nse=nse)</span>
  <span class="s3">return </span><span class="s1">BCOO((data</span><span class="s3">, </span><span class="s1">indices)</span><span class="s3">, </span><span class="s1">shape=mat.shape</span><span class="s3">, </span><span class="s1">indices_sorted=</span><span class="s3">True,</span>
              <span class="s1">unique_indices=</span><span class="s3">True</span><span class="s1">)</span>

<span class="s3">def </span><span class="s1">_bcoo_sum_duplicates(data: Array</span><span class="s3">, </span><span class="s1">indices: Array</span><span class="s3">, </span><span class="s1">*</span><span class="s3">, </span><span class="s1">spinfo: SparseInfo</span><span class="s3">, </span><span class="s1">nse: Optional[int]) -&gt; Tuple[Array</span><span class="s3">, </span><span class="s1">Array]:</span>
  <span class="s3">if </span><span class="s1">nse </span><span class="s3">is not None</span><span class="s1">:</span>
    <span class="s1">nse = core.concrete_or_error(operator.index</span><span class="s3">, </span><span class="s1">nse</span><span class="s3">, </span><span class="s5">&quot;nse argument of bcoo_sum_duplicates.&quot;</span><span class="s1">)</span>
  <span class="s3">return </span><span class="s1">bcoo_sum_duplicates_p.bind(data</span><span class="s3">, </span><span class="s1">indices</span><span class="s3">, </span><span class="s1">spinfo=spinfo</span><span class="s3">, </span><span class="s1">nse=nse)</span>

<span class="s1">@bcoo_sum_duplicates_p.def_impl</span>
<span class="s3">def </span><span class="s1">_bcoo_sum_duplicates_impl(data</span><span class="s3">, </span><span class="s1">indices</span><span class="s3">, </span><span class="s1">*</span><span class="s3">, </span><span class="s1">spinfo</span><span class="s3">, </span><span class="s1">nse):</span>
  <span class="s1">props = _validate_bcoo(data</span><span class="s3">, </span><span class="s1">indices</span><span class="s3">, </span><span class="s1">spinfo.shape)</span>
  <span class="s1">indices_out</span><span class="s3">, </span><span class="s1">mapping</span><span class="s3">, </span><span class="s1">nse_batched = _unique_indices(</span>
    <span class="s1">indices</span><span class="s3">, </span><span class="s1">shape=spinfo.shape</span><span class="s3">, </span><span class="s1">return_inverse=</span><span class="s3">True, </span><span class="s1">return_true_size=</span><span class="s3">True</span><span class="s1">)</span>
  <span class="s3">if </span><span class="s1">nse </span><span class="s3">is None</span><span class="s1">:</span>
    <span class="s1">nse = </span><span class="s4">1 </span><span class="s3">if </span><span class="s1">props.n_sparse == </span><span class="s4">0 </span><span class="s3">else </span><span class="s1">nse_batched.max()</span>
  <span class="s1">indices_out = _adjust_indices_nse(indices_out</span><span class="s3">, </span><span class="s1">nse=nse</span><span class="s3">, </span><span class="s1">shape=spinfo.shape)</span>
  <span class="s3">if </span><span class="s1">props.n_sparse == </span><span class="s4">0</span><span class="s1">:</span>
    <span class="s1">data = data.sum(props.n_batch</span><span class="s3">, </span><span class="s1">keepdims=</span><span class="s3">True</span><span class="s1">)</span>
  <span class="s1">data_out = jnp.empty((*map(max</span><span class="s3">, </span><span class="s1">indices.shape[:props.n_batch]</span><span class="s3">, </span><span class="s1">data.shape[:props.n_batch])</span><span class="s3">,</span>
                        <span class="s1">nse</span><span class="s3">, </span><span class="s1">*data.shape[props.n_batch + </span><span class="s4">1</span><span class="s1">:])</span><span class="s3">, </span><span class="s1">dtype=data.dtype)</span>
  <span class="s1">permute = </span><span class="s3">lambda </span><span class="s1">d_out</span><span class="s3">, </span><span class="s1">m</span><span class="s3">, </span><span class="s1">d: d_out.at[m].add(d</span><span class="s3">, </span><span class="s1">mode=</span><span class="s5">'drop'</span><span class="s1">)</span>
  <span class="s1">permute = nfold_vmap(permute</span><span class="s3">, </span><span class="s1">props.n_batch)</span>
  <span class="s1">data_out = permute(data_out</span><span class="s3">, </span><span class="s1">mapping</span><span class="s3">, </span><span class="s1">data)</span>
  <span class="s3">return </span><span class="s1">data_out</span><span class="s3">, </span><span class="s1">indices_out</span>

<span class="s3">def </span><span class="s1">_adjust_indices_nse(indices</span><span class="s3">, </span><span class="s1">*</span><span class="s3">, </span><span class="s1">nse</span><span class="s3">, </span><span class="s1">shape):</span>
  <span class="s1">props = _validate_bcoo_indices(indices</span><span class="s3">, </span><span class="s1">shape)</span>
  <span class="s3">if </span><span class="s1">nse &lt;= props.nse:</span>
    <span class="s1">indices = indices[...</span><span class="s3">, </span><span class="s1">:nse</span><span class="s3">, </span><span class="s1">:]</span>
  <span class="s3">else</span><span class="s1">:</span>
    <span class="s1">fill = lax.broadcast_in_dim(</span>
      <span class="s1">operand=jnp.array(shape[props.n_batch:props.n_batch + props.n_sparse]</span><span class="s3">, </span><span class="s1">dtype=indices.dtype)</span><span class="s3">,</span>
      <span class="s1">shape=(*indices.shape[:-</span><span class="s4">2</span><span class="s1">]</span><span class="s3">, </span><span class="s1">nse - props.nse</span><span class="s3">, </span><span class="s1">indices.shape[-</span><span class="s4">1</span><span class="s1">])</span><span class="s3">,</span>
      <span class="s1">broadcast_dimensions=(indices.ndim - </span><span class="s4">1</span><span class="s3">,</span><span class="s1">)</span>
    <span class="s1">)</span>
    <span class="s1">indices = lax.concatenate([indices</span><span class="s3">, </span><span class="s1">fill]</span><span class="s3">, </span><span class="s1">dimension=indices.ndim - </span><span class="s4">2</span><span class="s1">)</span>
  <span class="s3">return </span><span class="s1">indices</span>

<span class="s3">def </span><span class="s1">_unique_indices(indices</span><span class="s3">, </span><span class="s1">*</span><span class="s3">, </span><span class="s1">shape</span><span class="s3">, </span><span class="s1">return_inverse=</span><span class="s3">False,</span>
                    <span class="s1">return_index=</span><span class="s3">False, </span><span class="s1">return_true_size=</span><span class="s3">False</span><span class="s1">):</span>
  <span class="s1">props = _validate_bcoo_indices(indices</span><span class="s3">, </span><span class="s1">shape)</span>
  <span class="s1">f = partial(_unique_indices_unbatched</span><span class="s3">, </span><span class="s1">shape=shape[props.n_batch:]</span><span class="s3">,</span>
              <span class="s1">return_inverse=return_inverse</span><span class="s3">, </span><span class="s1">return_index=return_index</span><span class="s3">,</span>
              <span class="s1">return_true_size=return_true_size)</span>
  <span class="s1">f = nfold_vmap(f</span><span class="s3">, </span><span class="s1">props.n_batch</span><span class="s3">, </span><span class="s1">broadcasted=</span><span class="s3">False</span><span class="s1">)</span>
  <span class="s3">return </span><span class="s1">f(indices)</span>

<span class="s3">def </span><span class="s1">_unique_indices_unbatched(indices</span><span class="s3">, </span><span class="s1">*</span><span class="s3">, </span><span class="s1">shape</span><span class="s3">, </span><span class="s1">return_inverse=</span><span class="s3">False,</span>
                              <span class="s1">return_index=</span><span class="s3">False, </span><span class="s1">return_true_size=</span><span class="s3">False</span><span class="s1">):</span>
  <span class="s1">props = _validate_bcoo_indices(indices</span><span class="s3">, </span><span class="s1">shape)</span>
  <span class="s3">if </span><span class="s1">props.n_sparse == </span><span class="s4">0</span><span class="s1">:</span>
    <span class="s1">nse = </span><span class="s4">1</span>
    <span class="s1">indices_out = jnp.zeros_like(indices</span><span class="s3">, </span><span class="s1">shape=(nse</span><span class="s3">, </span><span class="s4">0</span><span class="s1">))</span>
    <span class="s1">out = (indices_out</span><span class="s3">,</span><span class="s1">)</span>
    <span class="s3">if </span><span class="s1">return_index:</span>
      <span class="s1">out = (*out</span><span class="s3">, </span><span class="s1">jnp.zeros(nse</span><span class="s3">, </span><span class="s1">dtype=</span><span class="s5">'int32'</span><span class="s1">))</span>
    <span class="s3">if </span><span class="s1">return_inverse:</span>
      <span class="s1">out = (*out</span><span class="s3">, </span><span class="s1">jnp.zeros(nse</span><span class="s3">, </span><span class="s1">dtype=</span><span class="s5">'int32'</span><span class="s1">))</span>
    <span class="s3">if </span><span class="s1">return_true_size:</span>
      <span class="s1">out = (*out</span><span class="s3">, </span><span class="s1">nse)</span>
    <span class="s3">return </span><span class="s1">out[</span><span class="s4">0</span><span class="s1">] </span><span class="s3">if </span><span class="s1">len(out) == </span><span class="s4">1 </span><span class="s3">else </span><span class="s1">out</span>
  <span class="s1">fill_value = jnp.expand_dims(jnp.array(shape[:props.n_sparse]</span><span class="s3">, </span><span class="s1">dtype=indices.dtype)</span><span class="s3">, </span><span class="s1">(</span><span class="s4">0</span><span class="s3">,</span><span class="s1">))</span>
  <span class="s1">out_of_bounds = (indices &gt;= fill_value).any(-</span><span class="s4">1</span><span class="s3">, </span><span class="s1">keepdims=</span><span class="s3">True</span><span class="s1">)</span>
  <span class="s1">indices = jnp.where(out_of_bounds</span><span class="s3">, </span><span class="s1">fill_value</span><span class="s3">, </span><span class="s1">indices)</span>
  <span class="s0"># TODO: check if `indices_sorted` is True.</span>
  <span class="s1">out = _unique(indices</span><span class="s3">, </span><span class="s1">axis=</span><span class="s4">0</span><span class="s3">, </span><span class="s1">return_inverse=return_inverse</span><span class="s3">, </span><span class="s1">return_index=return_index</span><span class="s3">,</span>
                <span class="s1">return_true_size=return_true_size</span><span class="s3">, </span><span class="s1">size=props.nse</span><span class="s3">, </span><span class="s1">fill_value=fill_value)</span>
  <span class="s3">if </span><span class="s1">return_true_size:</span>
    <span class="s1">nse = out[-</span><span class="s4">1</span><span class="s1">]</span>
    <span class="s1">nse = nse - (indices == fill_value).any().astype(nse.dtype)</span>
    <span class="s1">out = (*out[:-</span><span class="s4">1</span><span class="s1">]</span><span class="s3">, </span><span class="s1">nse)</span>
  <span class="s3">return </span><span class="s1">out</span>

<span class="s3">def </span><span class="s1">_coo_correct_out_of_bound_indices(row</span><span class="s3">, </span><span class="s1">col</span><span class="s3">, </span><span class="s1">shape</span><span class="s3">, </span><span class="s1">transpose):</span>
  <span class="s0"># Since cusparse does not have any well-tested support for padded indices,</span>
  <span class="s0"># we push them into an extra row/col of the matrix, which will then be</span>
  <span class="s0"># sliced away in the output.</span>
  <span class="s3">assert </span><span class="s1">row.ndim == col.ndim</span><span class="s3">, </span><span class="s5">f&quot;</span><span class="s3">{</span><span class="s1">row.ndim</span><span class="s3">} </span><span class="s5">!= </span><span class="s3">{</span><span class="s1">col.ndim</span><span class="s3">}</span><span class="s5">&quot;</span>
  <span class="s3">assert </span><span class="s1">len(shape) == row.ndim + </span><span class="s4">1</span><span class="s3">, </span><span class="s5">f&quot;</span><span class="s3">{</span><span class="s1">len(shape)</span><span class="s3">} </span><span class="s5">!= </span><span class="s3">{</span><span class="s1">row.ndim + </span><span class="s4">1</span><span class="s3">}</span><span class="s5">&quot;</span>
  <span class="s3">if </span><span class="s1">row.ndim &gt; </span><span class="s4">1</span><span class="s1">:</span>
    <span class="s1">f = partial(_coo_correct_out_of_bound_indices</span><span class="s3">,</span>
                <span class="s1">shape=shape[row.ndim:]</span><span class="s3">, </span><span class="s1">transpose=transpose)</span>
    <span class="s3">return </span><span class="s1">nfold_vmap(f</span><span class="s3">, </span><span class="s1">row.ndim)(row</span><span class="s3">, </span><span class="s1">col)</span>
  <span class="s1">mask = (row &gt;= shape[</span><span class="s4">0</span><span class="s1">]) | (col &gt;= shape[</span><span class="s4">1</span><span class="s1">])</span>
  <span class="s3">if </span><span class="s1">transpose:</span>
    <span class="s1">row = jnp.where(mask</span><span class="s3">, </span><span class="s4">0</span><span class="s3">, </span><span class="s1">row)</span>
    <span class="s1">col = jnp.where(mask</span><span class="s3">, </span><span class="s1">shape[</span><span class="s4">1</span><span class="s1">]</span><span class="s3">, </span><span class="s1">col)</span>
    <span class="s1">shape = (shape[</span><span class="s4">0</span><span class="s1">]</span><span class="s3">, </span><span class="s1">shape[</span><span class="s4">1</span><span class="s1">] + </span><span class="s4">1</span><span class="s1">)</span>
  <span class="s3">else</span><span class="s1">:</span>
    <span class="s1">row = jnp.where(mask</span><span class="s3">, </span><span class="s1">shape[</span><span class="s4">0</span><span class="s1">]</span><span class="s3">, </span><span class="s1">row)</span>
    <span class="s1">col = jnp.where(mask</span><span class="s3">, </span><span class="s4">0</span><span class="s3">, </span><span class="s1">col)</span>
    <span class="s1">shape = (shape[</span><span class="s4">0</span><span class="s1">] + </span><span class="s4">1</span><span class="s3">, </span><span class="s1">shape[</span><span class="s4">1</span><span class="s1">])</span>
  <span class="s3">return </span><span class="s1">row</span><span class="s3">, </span><span class="s1">col</span><span class="s3">, </span><span class="s1">shape</span>

<span class="s1">@bcoo_sum_duplicates_p.def_abstract_eval</span>
<span class="s3">def </span><span class="s1">_bcoo_sum_duplicates_abstract_eval(data</span><span class="s3">, </span><span class="s1">indices</span><span class="s3">, </span><span class="s1">*</span><span class="s3">, </span><span class="s1">spinfo</span><span class="s3">, </span><span class="s1">nse):</span>
  <span class="s3">if </span><span class="s1">nse </span><span class="s3">is None</span><span class="s1">:</span>
    <span class="s3">raise </span><span class="s1">ValueError(</span><span class="s5">&quot;bcoo_sum_duplicates: nse must be specified when using the function within &quot;</span>
                     <span class="s5">&quot;jit, vmap, and other transformations requiring abstract evaluation.&quot;</span><span class="s1">)</span>
  <span class="s1">props = _validate_bcoo(data</span><span class="s3">, </span><span class="s1">indices</span><span class="s3">, </span><span class="s1">spinfo.shape)</span>
  <span class="s1">indices_out = core.ShapedArray((*indices.shape[:props.n_batch]</span><span class="s3">, </span><span class="s1">nse</span><span class="s3">, </span><span class="s1">props.n_sparse)</span><span class="s3">,</span>
                                  <span class="s1">dtype=indices.dtype</span><span class="s3">, </span><span class="s1">weak_type=indices.weak_type)</span>
  <span class="s1">data_out = core.ShapedArray(</span>
    <span class="s1">(*map(max</span><span class="s3">, </span><span class="s1">indices.shape[:props.n_batch]</span><span class="s3">, </span><span class="s1">data.shape[:props.n_batch])</span><span class="s3">,</span>
     <span class="s1">nse</span><span class="s3">, </span><span class="s1">*data.shape[props.n_batch + </span><span class="s4">1</span><span class="s1">:])</span><span class="s3">, </span><span class="s1">data.dtype</span><span class="s3">, </span><span class="s1">weak_type=data.weak_type)</span>
  <span class="s3">return </span><span class="s1">data_out</span><span class="s3">, </span><span class="s1">indices_out</span>

<span class="s3">def </span><span class="s1">_bcoo_sum_duplicates_batching_rule(batched_args</span><span class="s3">, </span><span class="s1">batch_dims</span><span class="s3">, </span><span class="s1">*</span><span class="s3">, </span><span class="s1">spinfo</span><span class="s3">, </span><span class="s1">nse):</span>
  <span class="s1">data</span><span class="s3">, </span><span class="s1">indices</span><span class="s3">, </span><span class="s1">new_spinfo = _bcoo_batch_dims_to_front(batched_args</span><span class="s3">, </span><span class="s1">batch_dims</span><span class="s3">, </span><span class="s1">spinfo)</span>
  <span class="s1">data_out</span><span class="s3">, </span><span class="s1">indices_out = bcoo_sum_duplicates_p.bind(data</span><span class="s3">, </span><span class="s1">indices</span><span class="s3">, </span><span class="s1">spinfo=new_spinfo</span><span class="s3">, </span><span class="s1">nse=nse)</span>
  <span class="s0"># Note: if data is unbatched on input, it will be batched on output.</span>
  <span class="s0"># However, if indices are unbatched on input, they will be unbatched on output.</span>
  <span class="s3">if </span><span class="s1">batch_dims[</span><span class="s4">1</span><span class="s1">] </span><span class="s3">is None</span><span class="s1">:</span>
    <span class="s1">indices_out = lax.squeeze(indices_out</span><span class="s3">, </span><span class="s1">[</span><span class="s4">0</span><span class="s1">])</span>
    <span class="s1">out_axes = (</span><span class="s4">0</span><span class="s3">, None</span><span class="s1">)</span>
  <span class="s3">else</span><span class="s1">:</span>
    <span class="s1">out_axes = (</span><span class="s4">0</span><span class="s3">, </span><span class="s4">0</span><span class="s1">)</span>
  <span class="s3">return </span><span class="s1">(data_out</span><span class="s3">, </span><span class="s1">indices_out)</span><span class="s3">, </span><span class="s1">out_axes</span>

<span class="s3">def </span><span class="s1">_bcoo_sum_duplicates_jvp(primals</span><span class="s3">, </span><span class="s1">tangents</span><span class="s3">, </span><span class="s1">*</span><span class="s3">, </span><span class="s1">spinfo</span><span class="s3">, </span><span class="s1">nse):</span>
  <span class="s1">props = _validate_bcoo(*primals</span><span class="s3">, </span><span class="s1">spinfo.shape)</span>

  <span class="s1">data</span><span class="s3">, </span><span class="s1">indices = primals</span>
  <span class="s1">data_dot</span><span class="s3">, </span><span class="s1">_ = tangents</span>
  <span class="s1">indices_out</span><span class="s3">, </span><span class="s1">mapping</span><span class="s3">, </span><span class="s1">nse_batched = _unique_indices(</span>
    <span class="s1">indices</span><span class="s3">, </span><span class="s1">shape=spinfo.shape</span><span class="s3">, </span><span class="s1">return_inverse=</span><span class="s3">True, </span><span class="s1">return_true_size=</span><span class="s3">True</span><span class="s1">)</span>
  <span class="s3">if </span><span class="s1">nse </span><span class="s3">is None</span><span class="s1">:</span>
    <span class="s1">nse = jnp.sum(nse_batched)</span>
  <span class="s3">try</span><span class="s1">:</span>
    <span class="s1">nse = core.concrete_or_error(operator.index</span><span class="s3">, </span><span class="s1">nse</span><span class="s3">, </span><span class="s5">&quot;nse argument of bcoo_sum_duplicates.&quot;</span><span class="s1">)</span>
  <span class="s3">except </span><span class="s1">core.ConcretizationTypeError:</span>
    <span class="s3">raise </span><span class="s1">ValueError(</span><span class="s5">&quot;bcoo_sum_duplicates: nse must be specified when using the function within &quot;</span>
                     <span class="s5">&quot;jit, vmap, and other transformations requiring abstract evaluation.&quot;</span><span class="s1">)</span>
  <span class="s1">indices_out = _adjust_indices_nse(indices_out</span><span class="s3">, </span><span class="s1">nse=nse</span><span class="s3">, </span><span class="s1">shape=spinfo.shape)</span>
  <span class="s3">if </span><span class="s1">props.n_sparse == </span><span class="s4">0</span><span class="s1">:</span>
    <span class="s1">data = data.sum(props.n_batch</span><span class="s3">, </span><span class="s1">keepdims=</span><span class="s3">True</span><span class="s1">)</span>
    <span class="s1">data_dot = data_dot.sum(props.n_batch</span><span class="s3">, </span><span class="s1">keepdims=</span><span class="s3">True</span><span class="s1">)</span>
  <span class="s1">data_out = jnp.empty((*map(max</span><span class="s3">, </span><span class="s1">indices.shape[:props.n_batch]</span><span class="s3">, </span><span class="s1">data.shape[:props.n_batch])</span><span class="s3">,</span>
                        <span class="s1">nse</span><span class="s3">, </span><span class="s1">*data.shape[props.n_batch + </span><span class="s4">1</span><span class="s1">:])</span><span class="s3">, </span><span class="s1">dtype=data.dtype)</span>
  <span class="s1">data_dot_out = data_out</span>
  <span class="s0"># This check is because scatter-add on zero-sized arrays has poorly defined</span>
  <span class="s0"># semantics; see https://github.com/google/jax/issues/13656.</span>
  <span class="s3">if </span><span class="s1">data_out.size:</span>
    <span class="s1">permute = </span><span class="s3">lambda </span><span class="s1">x</span><span class="s3">, </span><span class="s1">i</span><span class="s3">, </span><span class="s1">y: x.at[i].add(y</span><span class="s3">, </span><span class="s1">mode=</span><span class="s5">'drop'</span><span class="s1">)</span>
  <span class="s3">else</span><span class="s1">:</span>
    <span class="s1">permute = </span><span class="s3">lambda </span><span class="s1">x</span><span class="s3">, </span><span class="s1">i</span><span class="s3">, </span><span class="s1">y: x</span>
  <span class="s1">permute = nfold_vmap(permute</span><span class="s3">, </span><span class="s1">props.n_batch)</span>
  <span class="s1">data_out = permute(data_out</span><span class="s3">, </span><span class="s1">mapping</span><span class="s3">, </span><span class="s1">data)</span>
  <span class="s1">indices_dot_out = ad.Zero.from_value(indices_out)</span>
  <span class="s1">data_dot_out = ad.Zero.from_value(data_out) </span><span class="s3">if </span><span class="s1">type(data_dot) </span><span class="s3">is </span><span class="s1">ad.Zero </span><span class="s3">else </span><span class="s1">permute(data_dot_out</span><span class="s3">, </span><span class="s1">mapping</span><span class="s3">, </span><span class="s1">data_dot)</span>
  <span class="s3">return </span><span class="s1">(data_out</span><span class="s3">, </span><span class="s1">indices_out)</span><span class="s3">, </span><span class="s1">(data_dot_out</span><span class="s3">, </span><span class="s1">indices_dot_out)</span>

<span class="s1">_bcoo_sum_duplicates_hlo = mlir.lower_fun(</span>
    <span class="s1">_bcoo_sum_duplicates_impl</span><span class="s3">, </span><span class="s1">multiple_results=</span><span class="s3">True</span><span class="s1">)</span>

<span class="s1">ad.primitive_jvps[bcoo_sum_duplicates_p] = _bcoo_sum_duplicates_jvp</span>
<span class="s1">batching.primitive_batchers[bcoo_sum_duplicates_p] = _bcoo_sum_duplicates_batching_rule</span>
<span class="s1">mlir.register_lowering(bcoo_sum_duplicates_p</span><span class="s3">, </span><span class="s1">_bcoo_sum_duplicates_hlo)</span>

<span class="s0">#----------------------------------------------------------------------</span>
<span class="s0"># BCOO functions that maybe should be primitives?</span>

<span class="s3">def </span><span class="s1">bcoo_update_layout(mat: BCOO</span><span class="s3">, </span><span class="s1">*</span><span class="s3">, </span><span class="s1">n_batch: Optional[int] = </span><span class="s3">None, </span><span class="s1">n_dense: Optional[int] = </span><span class="s3">None,</span>
                       <span class="s1">on_inefficient: Optional[str] = </span><span class="s5">'error'</span><span class="s1">) -&gt; BCOO:</span>
  <span class="s2">&quot;&quot;&quot;Update the storage layout (i.e. n_batch &amp; n_dense) of a BCOO matrix. 
 
  In many cases this can be done without introducing undue storage overhead. However, 
  increasing ``mat.n_batch`` or ``mat.n_dense`` will lead to very inefficient storage, 
  with many explicitly-stored zeros, unless the new batch or dense dimensions have size 
  0 or 1. In such cases, ``bcoo_update_layout`` will raise a :class:`SparseEfficiencyError`. 
  This can be silenced by specifying the ``on_inefficient`` argument. 
 
  Args: 
    mat : BCOO array 
    n_batch : optional(int) the number of batch dimensions in the output matrix. If None, 
      then n_batch = mat.n_batch. 
    n_dense : optional(int) the number of dense dimensions in the output matrix. If None, 
      then n_dense = mat.n_dense. 
    on_inefficient : optional(string), one of ``['error', 'warn', None]``. Specify the 
      behavior in case of an inefficient reconfiguration. This is defined as a reconfiguration 
      where the size of the resulting representation is much larger than the size of the 
      input representation. 
 
  Returns: 
    mat_out : BCOO array 
      A BCOO array representing the same sparse array as the input, with the specified 
      layout. ``mat_out.todense()`` will match ``mat.todense()`` up to appropriate precision. 
  &quot;&quot;&quot;</span>
  <span class="s0"># TODO(jakevdp): allow specification of nse?</span>
  <span class="s0"># TODO(jakevdp): there is room for some improvements here:</span>
  <span class="s0"># - we could probably do better in the case of converting a dense dim to</span>
  <span class="s0">#   a batch dim or vice-versa. Worth adding that special case?</span>
  <span class="s0"># - we could work to preserve broadcasted batch dimensions when possible.</span>
  <span class="s0"># - if indices are known to be unique, we can convert them to batch/dense</span>
  <span class="s0">#   dimensions more efficiently.</span>
  <span class="s1">n_batch = mat.n_batch </span><span class="s3">if </span><span class="s1">n_batch </span><span class="s3">is None else </span><span class="s1">operator.index(n_batch)</span>
  <span class="s1">n_dense = mat.n_dense </span><span class="s3">if </span><span class="s1">n_dense </span><span class="s3">is None else </span><span class="s1">operator.index(n_dense)</span>

  <span class="s3">if </span><span class="s1">(n_batch</span><span class="s3">, </span><span class="s1">n_dense) == (mat.n_batch</span><span class="s3">, </span><span class="s1">mat.n_dense):</span>
    <span class="s3">return </span><span class="s1">mat</span>

  <span class="s1">n_sparse = mat.ndim - n_batch - n_dense</span>
  <span class="s3">if </span><span class="s1">on_inefficient </span><span class="s3">not in </span><span class="s1">[</span><span class="s5">'error'</span><span class="s3">, </span><span class="s5">'warn'</span><span class="s3">, None</span><span class="s1">]:</span>
    <span class="s3">raise </span><span class="s1">ValueError(</span><span class="s5">&quot;on_inefficent={on_inefficient!r}; expected one of ['error', 'warn', None].&quot;</span><span class="s1">)</span>

  <span class="s3">if </span><span class="s1">n_batch &lt; </span><span class="s4">0</span><span class="s1">:</span>
    <span class="s3">raise </span><span class="s1">ValueError(</span><span class="s5">f&quot;n_batch must be non-negative; got </span><span class="s3">{</span><span class="s1">n_batch</span><span class="s3">}</span><span class="s5">&quot;</span><span class="s1">)</span>
  <span class="s3">if </span><span class="s1">n_dense &lt; </span><span class="s4">0</span><span class="s1">:</span>
    <span class="s3">raise </span><span class="s1">ValueError(</span><span class="s5">f&quot;n_dense must be non-negative; got </span><span class="s3">{</span><span class="s1">n_dense</span><span class="s3">}</span><span class="s5">&quot;</span><span class="s1">)</span>
  <span class="s3">if </span><span class="s1">n_sparse &lt; </span><span class="s4">0</span><span class="s1">:</span>
    <span class="s3">raise </span><span class="s1">ValueError(</span><span class="s5">f&quot;sum of </span><span class="s3">{</span><span class="s1">n_batch=</span><span class="s3">} </span><span class="s5">and </span><span class="s3">{</span><span class="s1">n_dense=</span><span class="s3">} </span><span class="s5">&quot;</span>
                     <span class="s5">f&quot;cannot be larger than mat.ndim=</span><span class="s3">{</span><span class="s1">mat.ndim</span><span class="s3">}</span><span class="s5">.&quot;</span><span class="s1">)</span>

  <span class="s3">def </span><span class="s1">_maybe_err_or_warn(msg):</span>
    <span class="s3">if </span><span class="s1">on_inefficient == </span><span class="s5">'error'</span><span class="s1">:</span>
      <span class="s1">msg += (</span><span class="s5">&quot; To disable this error, set the on_inefficient argument &quot;</span>
              <span class="s5">&quot;of bcoo_update_layout to 'warn' or None.&quot;</span><span class="s1">)</span>
      <span class="s3">raise </span><span class="s1">SparseEfficiencyError(msg)</span>
    <span class="s3">elif </span><span class="s1">on_inefficient == </span><span class="s5">'warn'</span><span class="s1">:</span>
      <span class="s1">msg += (</span><span class="s5">&quot; To disable this warning, set the on_inefficient argument &quot;</span>
              <span class="s5">&quot;of bcoo_update_layout to None.&quot;</span><span class="s1">)</span>
      <span class="s1">warnings.warn(msg</span><span class="s3">, </span><span class="s1">category=SparseEfficiencyWarning)</span>

  <span class="s0"># TODO(jakevdp): are efficiency warnings necessary when nse is 0 or 1?</span>
  <span class="s3">if </span><span class="s1">(n_dense &gt; mat.n_dense </span><span class="s3">and</span>
      <span class="s1">any(d &gt; </span><span class="s4">1 </span><span class="s3">for </span><span class="s1">d </span><span class="s3">in </span><span class="s1">mat.shape[-n_dense:mat.ndim - mat.n_dense])):</span>
    <span class="s1">_maybe_err_or_warn(</span><span class="s5">f&quot;For matrix of shape </span><span class="s3">{</span><span class="s1">mat.shape</span><span class="s3">}</span><span class="s5">, increasing n_dense from &quot;</span>
                       <span class="s5">f&quot;</span><span class="s3">{</span><span class="s1">mat.n_dense</span><span class="s3">} </span><span class="s5">to </span><span class="s3">{</span><span class="s1">n_dense</span><span class="s3">} </span><span class="s5">results in inefficient storage.&quot;</span><span class="s1">)</span>
  <span class="s3">if </span><span class="s1">n_batch &gt; mat.n_batch </span><span class="s3">and </span><span class="s1">any(d &gt; </span><span class="s4">1 </span><span class="s3">for </span><span class="s1">d </span><span class="s3">in </span><span class="s1">mat.shape[mat.n_batch:n_batch]):</span>
    <span class="s1">_maybe_err_or_warn(</span><span class="s5">f&quot;For matrix of shape </span><span class="s3">{</span><span class="s1">mat.shape</span><span class="s3">}</span><span class="s5">, increasing n_batch from &quot;</span>
                       <span class="s5">f&quot;</span><span class="s3">{</span><span class="s1">mat.n_batch</span><span class="s3">} </span><span class="s5">to </span><span class="s3">{</span><span class="s1">n_batch</span><span class="s3">} </span><span class="s5">results in inefficient storage.&quot;</span><span class="s1">)</span>

  <span class="s1">new_data</span><span class="s3">, </span><span class="s1">new_indices = mat.data</span><span class="s3">, </span><span class="s1">mat.indices</span>
  <span class="s1">shape = mat.shape</span>
  <span class="s1">current_n_batch = mat.n_batch</span>
  <span class="s1">current_n_dense = mat.n_dense</span>

  <span class="s3">if </span><span class="s1">n_dense &lt; current_n_dense:</span>
    <span class="s1">n = current_n_dense - n_dense</span>
    <span class="s1">@partial(nfold_vmap</span><span class="s3">, </span><span class="s1">N=current_n_batch + </span><span class="s4">1</span><span class="s1">)</span>
    <span class="s3">def </span><span class="s1">_update(d</span><span class="s3">, </span><span class="s1">i):</span>
      <span class="s1">new_d = d.reshape(np.prod(d.shape[:n])</span><span class="s3">, </span><span class="s1">*d.shape[n:])</span>
      <span class="s1">meshes = jnp.meshgrid(*(jnp.arange(s</span><span class="s3">, </span><span class="s1">dtype=i.dtype) </span><span class="s3">for </span><span class="s1">s </span><span class="s3">in </span><span class="s1">d.shape[:n])</span><span class="s3">,</span>
                            <span class="s1">indexing=</span><span class="s5">'ij'</span><span class="s1">)</span>
      <span class="s1">new_i = jnp.column_stack([jnp.broadcast_to(i</span><span class="s3">, </span><span class="s1">(new_d.shape[</span><span class="s4">0</span><span class="s1">]</span><span class="s3">, </span><span class="s1">i.size))</span><span class="s3">,</span>
                                <span class="s1">*map(jnp.ravel</span><span class="s3">, </span><span class="s1">meshes)])</span>
      <span class="s3">return </span><span class="s1">new_d</span><span class="s3">, </span><span class="s1">new_i</span>
    <span class="s1">new_data</span><span class="s3">, </span><span class="s1">new_indices = _update(new_data</span><span class="s3">, </span><span class="s1">new_indices)</span>
    <span class="s1">new_data = new_data.reshape(*new_data.shape[:current_n_batch]</span><span class="s3">,</span>
                                <span class="s1">np.prod(new_data.shape[current_n_batch:current_n_batch + </span><span class="s4">2</span><span class="s1">])</span><span class="s3">,</span>
                                <span class="s1">*new_data.shape[current_n_batch + </span><span class="s4">2</span><span class="s1">:])</span>
    <span class="s1">new_indices = new_indices.reshape(*new_indices.shape[:current_n_batch]</span><span class="s3">,</span>
                                      <span class="s1">np.prod(new_indices.shape[current_n_batch: current_n_batch + </span><span class="s4">2</span><span class="s1">])</span><span class="s3">,</span>
                                      <span class="s1">*new_indices.shape[current_n_batch + </span><span class="s4">2</span><span class="s1">:])</span>
    <span class="s1">current_n_dense = n_dense</span>

  <span class="s3">if </span><span class="s1">n_batch &lt; current_n_batch:</span>
    <span class="s1">n = current_n_batch - n_batch</span>
    <span class="s1">@partial(nfold_vmap</span><span class="s3">, </span><span class="s1">N=n_batch)</span>
    <span class="s3">def </span><span class="s1">_update(d</span><span class="s3">, </span><span class="s1">i):</span>
      <span class="s1">nse = i.shape[-</span><span class="s4">2</span><span class="s1">]</span>
      <span class="s1">new_d = d.reshape(np.prod(d.shape[:n + </span><span class="s4">1</span><span class="s1">])</span><span class="s3">, </span><span class="s1">*d.shape[n + </span><span class="s4">1</span><span class="s1">:])</span>
      <span class="s1">meshes = jnp.meshgrid(*(jnp.arange(d</span><span class="s3">, </span><span class="s1">dtype=i.dtype) </span><span class="s3">for </span><span class="s1">d </span><span class="s3">in </span><span class="s1">(*i.shape[:n]</span><span class="s3">, </span><span class="s1">nse))</span><span class="s3">,</span>
                            <span class="s1">indexing=</span><span class="s5">'ij'</span><span class="s1">)</span>
      <span class="s1">new_i = i.reshape(np.prod(i.shape[:n + </span><span class="s4">1</span><span class="s1">])</span><span class="s3">, </span><span class="s1">*i.shape[n + </span><span class="s4">1</span><span class="s1">:])</span>
      <span class="s1">new_i = jnp.column_stack((*(m.ravel() </span><span class="s3">for </span><span class="s1">m </span><span class="s3">in </span><span class="s1">meshes[:-</span><span class="s4">1</span><span class="s1">])</span><span class="s3">, </span><span class="s1">new_i))</span>
      <span class="s3">return </span><span class="s1">new_d</span><span class="s3">, </span><span class="s1">new_i</span>
    <span class="s1">new_data</span><span class="s3">, </span><span class="s1">new_indices = _update(new_data</span><span class="s3">, </span><span class="s1">new_indices)</span>
    <span class="s1">current_n_batch = n_batch</span>

  <span class="s3">if </span><span class="s1">n_dense &gt; current_n_dense:</span>
    <span class="s1">n = n_dense - current_n_dense</span>
    <span class="s1">@partial(nfold_vmap</span><span class="s3">, </span><span class="s1">N=current_n_batch + </span><span class="s4">1</span><span class="s1">)</span>
    <span class="s3">def </span><span class="s1">_update(d</span><span class="s3">, </span><span class="s1">i):</span>
      <span class="s1">new_d = jnp.zeros_like(d</span><span class="s3">, </span><span class="s1">shape=shape[-n_dense:]).at[tuple(i[-n:])].set(d)</span>
      <span class="s1">new_i = i[:-n]</span>
      <span class="s3">return </span><span class="s1">new_d</span><span class="s3">, </span><span class="s1">new_i</span>
    <span class="s1">new_data</span><span class="s3">, </span><span class="s1">new_indices = _update(new_data</span><span class="s3">, </span><span class="s1">new_indices)</span>
    <span class="s1">current_n_dense = n_dense</span>

  <span class="s3">if </span><span class="s1">n_batch &gt; current_n_batch:</span>
    <span class="s1">n = n_batch - current_n_batch</span>
    <span class="s1">@partial(nfold_vmap</span><span class="s3">, </span><span class="s1">N=current_n_batch)</span>
    <span class="s3">def </span><span class="s1">_update(d</span><span class="s3">, </span><span class="s1">i):</span>
      <span class="s1">nse = i.shape[-</span><span class="s4">2</span><span class="s1">]</span>
      <span class="s1">idx = tuple(i[:</span><span class="s3">, </span><span class="s1">j] </span><span class="s3">for </span><span class="s1">j </span><span class="s3">in </span><span class="s1">range(n)) + (jnp.arange(nse)</span><span class="s3">,</span><span class="s1">)</span>
      <span class="s1">new_i_shape = (*shape[current_n_batch:n_batch]</span><span class="s3">, </span><span class="s1">nse</span><span class="s3">, </span><span class="s1">i.shape[-</span><span class="s4">1</span><span class="s1">] - n)</span>
      <span class="s1">new_i = jnp.broadcast_to(i[:</span><span class="s3">, </span><span class="s1">n:]</span><span class="s3">, </span><span class="s1">new_i_shape)</span>
      <span class="s1">new_d_shape = (*shape[current_n_batch:n_batch]</span><span class="s3">, </span><span class="s1">nse</span><span class="s3">, </span><span class="s1">*d.shape[d.ndim - n_dense:])</span>
      <span class="s1">new_d = jnp.zeros_like(d</span><span class="s3">, </span><span class="s1">shape=new_d_shape).at[idx].set(d)</span>
      <span class="s3">return </span><span class="s1">new_d</span><span class="s3">, </span><span class="s1">new_i</span>
    <span class="s1">new_data</span><span class="s3">, </span><span class="s1">new_indices = _update(new_data</span><span class="s3">, </span><span class="s1">new_indices)</span>
    <span class="s1">current_n_batch = n_batch</span>

  <span class="s3">return </span><span class="s1">BCOO((new_data</span><span class="s3">, </span><span class="s1">new_indices)</span><span class="s3">, </span><span class="s1">shape=shape)</span>


<span class="s3">def </span><span class="s1">bcoo_broadcast_in_dim(mat: BCOO</span><span class="s3">, </span><span class="s1">*</span><span class="s3">, </span><span class="s1">shape: Shape</span><span class="s3">, </span><span class="s1">broadcast_dimensions: Sequence[int]) -&gt; BCOO:</span>
  <span class="s2">&quot;&quot;&quot;Expand the size and rank of a BCOO array by duplicating the data. 
 
  A BCOO equivalence to jax.lax.broadcast_in_dim. 
 
  Args: 
    mat: A BCOO-format array. 
    shape: The shape of the target array. 
    broadcast_dimensions: The dimension in the shape of the target array which 
      each dimension of the operand (``mat``) shape corresponds to. 
 
  Returns: 
    A BCOO-format array containing the target array. 
  &quot;&quot;&quot;</span>
  <span class="s3">return </span><span class="s1">BCOO(_bcoo_broadcast_in_dim(mat.data</span><span class="s3">, </span><span class="s1">mat.indices</span><span class="s3">, </span><span class="s1">spinfo=mat._info</span><span class="s3">,</span>
                                     <span class="s1">shape=shape</span><span class="s3">,</span>
                                     <span class="s1">broadcast_dimensions=broadcast_dimensions)</span><span class="s3">,</span>
              <span class="s1">shape=shape)</span>

<span class="s3">def </span><span class="s1">_bcoo_broadcast_in_dim(data: Array</span><span class="s3">, </span><span class="s1">indices: Array</span><span class="s3">, </span><span class="s1">*</span><span class="s3">, </span><span class="s1">spinfo: SparseInfo</span><span class="s3">, </span><span class="s1">shape: Shape</span><span class="s3">,</span>
                           <span class="s1">broadcast_dimensions: Sequence[int]) -&gt; Tuple[Array</span><span class="s3">, </span><span class="s1">Array]:</span>
  <span class="s2">&quot;&quot;&quot;BCOO equivalent of lax.broadcast_in_dim&quot;&quot;&quot;</span>
  <span class="s3">if </span><span class="s1">len(spinfo.shape) != len(broadcast_dimensions):</span>
    <span class="s3">raise </span><span class="s1">ValueError(</span><span class="s5">f&quot;</span><span class="s3">{</span><span class="s1">spinfo.shape=</span><span class="s3">} </span><span class="s5">and </span><span class="s3">{</span><span class="s1">broadcast_dimensions=</span><span class="s3">} </span><span class="s5">must have the same length&quot;</span><span class="s1">)</span>
  <span class="s1">props = _validate_bcoo(data</span><span class="s3">, </span><span class="s1">indices</span><span class="s3">, </span><span class="s1">spinfo.shape)</span>
  <span class="s1">batch_dims</span><span class="s3">, </span><span class="s1">sparse_dims</span><span class="s3">, </span><span class="s1">dense_dims = split_list(broadcast_dimensions</span><span class="s3">, </span><span class="s1">[props.n_batch</span><span class="s3">, </span><span class="s1">props.n_sparse])</span>

  <span class="s3">if </span><span class="s1">max(batch_dims</span><span class="s3">, </span><span class="s1">default=</span><span class="s4">0</span><span class="s1">) &gt; min(sparse_dims</span><span class="s3">, </span><span class="s1">default=len(shape)):</span>
    <span class="s3">raise </span><span class="s1">ValueError(</span><span class="s5">&quot;Cannot mix batch and sparse dimensions during broadcast_in_dim&quot;</span><span class="s1">)</span>
  <span class="s3">if </span><span class="s1">max(sparse_dims</span><span class="s3">, </span><span class="s1">default=</span><span class="s4">0</span><span class="s1">) &gt; min(dense_dims</span><span class="s3">, </span><span class="s1">default=len(shape)):</span>
    <span class="s3">raise </span><span class="s1">ValueError(</span><span class="s5">&quot;Cannot mix sparse and dense dimensions during broadcast_in_dim&quot;</span><span class="s1">)</span>

  <span class="s0"># All new dimensions preceding a sparse or dense dimension are batch dimensions:</span>
  <span class="s1">new_n_batch = min(broadcast_dimensions[props.n_batch:]</span><span class="s3">, </span><span class="s1">default=len(shape))</span>
  <span class="s0"># TODO(jakevdp): Should new trailing dimensions be dense by default?</span>
  <span class="s1">new_n_dense = props.n_dense </span><span class="s3">and </span><span class="s1">len(shape) - min(broadcast_dimensions[-props.n_dense:])</span>
  <span class="s1">new_n_sparse = len(shape) - new_n_batch - new_n_dense</span>

  <span class="s3">if </span><span class="s1">np.prod(spinfo.shape[props.n_batch: props.n_batch + props.n_sparse]) != np.prod(shape[new_n_batch:new_n_batch + new_n_sparse]):</span>
    <span class="s3">raise </span><span class="s1">NotImplementedError(</span><span class="s5">&quot;Adding sparse dimensions with lengths != 1&quot;</span><span class="s1">)</span>
  <span class="s1">new_data</span><span class="s3">, </span><span class="s1">new_indices = data</span><span class="s3">, </span><span class="s1">indices</span>

  <span class="s0"># batch &amp; dense dimensions</span>
  <span class="s3">if </span><span class="s1">(new_n_batch</span><span class="s3">, </span><span class="s1">new_n_dense) != (props.n_batch</span><span class="s3">, </span><span class="s1">props.n_dense):</span>
    <span class="s1">new_data = lax.broadcast_in_dim(new_data</span><span class="s3">,</span>
        <span class="s1">shape=(*shape[:new_n_batch]</span><span class="s3">, </span><span class="s1">props.nse</span><span class="s3">, </span><span class="s1">*shape[new_n_batch + new_n_sparse:])</span><span class="s3">,</span>
        <span class="s1">broadcast_dimensions=(*batch_dims</span><span class="s3">, </span><span class="s1">new_n_batch</span><span class="s3">, </span><span class="s1">*(b + </span><span class="s4">1 </span><span class="s1">- new_n_sparse </span><span class="s3">for </span><span class="s1">b </span><span class="s3">in </span><span class="s1">dense_dims)))</span>
    <span class="s1">new_indices = lax.broadcast_in_dim(new_indices</span><span class="s3">,</span>
        <span class="s1">shape=(*shape[:new_n_batch]</span><span class="s3">, </span><span class="s1">props.nse</span><span class="s3">, </span><span class="s1">props.n_sparse)</span><span class="s3">,</span>
        <span class="s1">broadcast_dimensions=(*batch_dims</span><span class="s3">, </span><span class="s1">new_n_batch</span><span class="s3">, </span><span class="s1">new_n_batch + </span><span class="s4">1</span><span class="s1">))</span>

  <span class="s0"># sparse dimensions</span>
  <span class="s3">if </span><span class="s1">new_n_sparse != props.n_sparse:</span>
    <span class="s1">shape = (*shape[:new_n_batch]</span><span class="s3">, </span><span class="s1">props.nse</span><span class="s3">, </span><span class="s1">new_n_sparse)</span>
    <span class="s1">ind = jnp.array(sparse_dims</span><span class="s3">, </span><span class="s1">int) - new_n_batch</span>
    <span class="s1">new_indices = (jnp.zeros_like(new_indices</span><span class="s3">, </span><span class="s1">shape=shape).at[...</span><span class="s3">, </span><span class="s1">ind].set(new_indices))</span>

  <span class="s3">return </span><span class="s1">new_data</span><span class="s3">, </span><span class="s1">new_indices</span>

<span class="s3">def </span><span class="s1">bcoo_concatenate(operands: Sequence[BCOO]</span><span class="s3">, </span><span class="s1">*</span><span class="s3">, </span><span class="s1">dimension: int) -&gt; BCOO:</span>
  <span class="s2">&quot;&quot;&quot;Sparse implementation of :func:`jax.lax.concatenate` 
 
  Args: 
    operands : Sequence of BCOO arrays to concatenate. The arrays must have equal 
      shapes, except in the `dimension` axis. Additionally, the arrays must have 
      have equivalent batch, sparse, and dense dimensions. 
    dimension : Positive integer specifying the dimension along which to concatenate 
      the arrays. The dimension must be among batch or sparse dimensions of the input; 
      concatenation along dense dimensions is not supported. 
 
  Returns: 
    A BCOO array containing the concatenation of the inputs. 
  &quot;&quot;&quot;</span>
  <span class="s1">dimension = operator.index(dimension)</span>
  <span class="s3">if not </span><span class="s1">all(isinstance(op</span><span class="s3">, </span><span class="s1">BCOO) </span><span class="s3">for </span><span class="s1">op </span><span class="s3">in </span><span class="s1">operands):</span>
    <span class="s3">raise </span><span class="s1">ValueError(</span><span class="s5">&quot;bcoo_concatenate: expected operands to be a sequence of BCOO arrays. &quot;</span>
                     <span class="s5">f&quot;Got </span><span class="s3">{</span><span class="s1">operands</span><span class="s3">}</span><span class="s5">&quot;</span><span class="s1">)</span>
  <span class="s0"># Validate inputs using lax.concatenate abstract evaluation.</span>
  <span class="s1">out_aval = jax.eval_shape(</span>
    <span class="s1">functools.partial(lax.concatenate</span><span class="s3">, </span><span class="s1">dimension=dimension)</span><span class="s3">,</span>
    <span class="s1">[core.ShapedArray(op.shape</span><span class="s3">, </span><span class="s1">op.dtype) </span><span class="s3">for </span><span class="s1">op </span><span class="s3">in </span><span class="s1">operands])</span>
  <span class="s3">if </span><span class="s1">len({op.n_dense </span><span class="s3">for </span><span class="s1">op </span><span class="s3">in </span><span class="s1">operands}) &gt; </span><span class="s4">1</span><span class="s1">:</span>
    <span class="s3">raise </span><span class="s1">ValueError(</span><span class="s5">&quot;bcoo_concatenate requires inputs to have matching nse dimensions.&quot;</span><span class="s1">)</span>

  <span class="s1">n_batches = {op.n_batch </span><span class="s3">for </span><span class="s1">op </span><span class="s3">in </span><span class="s1">operands}</span>
  <span class="s0"># Correct for the common case, where op[None, :] adds a single batch dimension and we</span>
  <span class="s0"># need to align it in order to match the others &amp; concatenate.</span>
  <span class="s3">if </span><span class="s1">len(n_batches) != </span><span class="s4">1 </span><span class="s3">and </span><span class="s1">max(n_batches) == </span><span class="s4">1</span><span class="s1">:</span>
    <span class="s3">if </span><span class="s1">all(op.shape[</span><span class="s4">0</span><span class="s1">] == </span><span class="s4">1 </span><span class="s3">for </span><span class="s1">op </span><span class="s3">in </span><span class="s1">operands </span><span class="s3">if </span><span class="s1">op.n_batch == </span><span class="s4">0</span><span class="s1">):</span>
      <span class="s1">operands = [bcoo_update_layout(op</span><span class="s3">, </span><span class="s1">n_batch=</span><span class="s4">1</span><span class="s1">) </span><span class="s3">if </span><span class="s1">op.n_batch == </span><span class="s4">0 </span><span class="s3">else </span><span class="s1">op </span><span class="s3">for </span><span class="s1">op </span><span class="s3">in </span><span class="s1">operands]</span>
    <span class="s3">elif </span><span class="s1">all(op.shape[</span><span class="s4">0</span><span class="s1">] == </span><span class="s4">1 </span><span class="s3">for </span><span class="s1">op </span><span class="s3">in </span><span class="s1">operands </span><span class="s3">if </span><span class="s1">op.n_batch == </span><span class="s4">1</span><span class="s1">):</span>
      <span class="s1">operands = [bcoo_update_layout(op</span><span class="s3">, </span><span class="s1">n_batch=</span><span class="s4">0</span><span class="s1">) </span><span class="s3">if </span><span class="s1">op.n_batch == </span><span class="s4">1 </span><span class="s3">else </span><span class="s1">op </span><span class="s3">for </span><span class="s1">op </span><span class="s3">in </span><span class="s1">operands]</span>
    <span class="s1">n_batches = {op.n_batch </span><span class="s3">for </span><span class="s1">op </span><span class="s3">in </span><span class="s1">operands}</span>

  <span class="s3">if </span><span class="s1">len(n_batches) != </span><span class="s4">1</span><span class="s1">:</span>
    <span class="s3">raise </span><span class="s1">ValueError(</span><span class="s5">&quot;bcoo_concatenate requires inputs to have matching batch dimensions.&quot;</span><span class="s1">)</span>

  <span class="s1">n_batch</span><span class="s3">, </span><span class="s1">n_sparse = operands[</span><span class="s4">0</span><span class="s1">].n_batch</span><span class="s3">, </span><span class="s1">operands[</span><span class="s4">0</span><span class="s1">].n_sparse</span>

  <span class="s1">index_batches = [op.indices.shape[:n_batch] </span><span class="s3">for </span><span class="s1">op </span><span class="s3">in </span><span class="s1">operands]</span>
  <span class="s1">data_batches = [op.data.shape[:n_batch] </span><span class="s3">for </span><span class="s1">op </span><span class="s3">in </span><span class="s1">operands]</span>
  <span class="s3">if </span><span class="s1">dimension &lt; n_batch:</span>
    <span class="s1">index_batches = [s[:dimension] + s[dimension + </span><span class="s4">1</span><span class="s1">:] </span><span class="s3">for </span><span class="s1">s </span><span class="s3">in </span><span class="s1">index_batches]</span>
    <span class="s1">data_batches = [s[:dimension] + s[dimension + </span><span class="s4">1</span><span class="s1">:] </span><span class="s3">for </span><span class="s1">s </span><span class="s3">in </span><span class="s1">data_batches]</span>
  <span class="s3">if not </span><span class="s1">(len(set(index_batches)) == len(set(data_batches)) == </span><span class="s4">1</span><span class="s1">):</span>
    <span class="s3">raise </span><span class="s1">NotImplementedError(</span><span class="s5">&quot;concatenation of arrays with broadcasted batch indices&quot;</span><span class="s1">)</span>

  <span class="s3">if </span><span class="s1">dimension &lt; n_batch:  </span><span class="s0"># Concatenation along batch axes</span>
    <span class="s0"># Ensure nse of operands match.</span>
    <span class="s1">nses = {op.nse </span><span class="s3">for </span><span class="s1">op </span><span class="s3">in </span><span class="s1">operands}</span>
    <span class="s3">if </span><span class="s1">len(nses) != </span><span class="s4">1</span><span class="s1">:</span>
      <span class="s1">nse = max(nses)</span>
      <span class="s1">operands = [_bcoo_set_nse(op</span><span class="s3">, </span><span class="s1">nse) </span><span class="s3">for </span><span class="s1">op </span><span class="s3">in </span><span class="s1">operands]</span>
    <span class="s1">new_indices = lax.concatenate([op.indices </span><span class="s3">for </span><span class="s1">op </span><span class="s3">in </span><span class="s1">operands]</span><span class="s3">, </span><span class="s1">dimension=dimension)</span>
    <span class="s1">new_data = lax.concatenate([op.data </span><span class="s3">for </span><span class="s1">op </span><span class="s3">in </span><span class="s1">operands]</span><span class="s3">, </span><span class="s1">dimension=dimension)</span>
  <span class="s3">elif </span><span class="s1">dimension &lt; n_batch + n_sparse:  </span><span class="s0"># Concatenation along sparse axes</span>
    <span class="s1">offsets = np.cumsum([</span><span class="s4">0</span><span class="s1">] + [op.shape[dimension] </span><span class="s3">for </span><span class="s1">op </span><span class="s3">in </span><span class="s1">operands[:-</span><span class="s4">1</span><span class="s1">]]</span><span class="s3">,</span>
                        <span class="s1">dtype=operands[</span><span class="s4">0</span><span class="s1">].indices.dtype)</span>
    <span class="s1">new_data = lax.concatenate([op.data </span><span class="s3">for </span><span class="s1">op </span><span class="s3">in </span><span class="s1">operands]</span><span class="s3">, </span><span class="s1">dimension=n_batch)</span>
    <span class="s1">new_indices = lax.concatenate([op.indices.at[...</span><span class="s3">, </span><span class="s1">dimension - n_batch].add(offset)</span>
                                   <span class="s3">for </span><span class="s1">op</span><span class="s3">, </span><span class="s1">offset </span><span class="s3">in </span><span class="s1">safe_zip(operands</span><span class="s3">, </span><span class="s1">offsets)]</span><span class="s3">,</span>
                                  <span class="s1">dimension=n_batch)</span>
  <span class="s3">else</span><span class="s1">:  </span><span class="s0"># Concatenation along dense axes</span>
    <span class="s0"># TODO(jakevdp) should we implement this? In general it results in a wasteful</span>
    <span class="s0"># representation because we cannot assume that the indices match.</span>
    <span class="s3">raise </span><span class="s1">NotImplementedError(</span><span class="s5">&quot;Concatenation along dense dimensions.&quot;</span><span class="s1">)</span>

  <span class="s3">return </span><span class="s1">BCOO((new_data</span><span class="s3">, </span><span class="s1">new_indices)</span><span class="s3">, </span><span class="s1">shape=out_aval.shape)</span>


<span class="s3">def </span><span class="s1">bcoo_reshape(mat: BCOO</span><span class="s3">, </span><span class="s1">*</span><span class="s3">, </span><span class="s1">new_sizes: Sequence[int]</span><span class="s3">, </span><span class="s1">dimensions: Sequence[int]) -&gt; BCOO:</span>
  <span class="s2">&quot;&quot;&quot;Sparse implementation of {func}`jax.lax.reshape`. 
 
  Args: 
    operand: BCOO array to be reshaped. 
    new_sizes: sequence of integers specifying the resulting shape. The size 
      of the final array must match the size of the input. This must be specified 
      such that batch, sparse, and dense dimensions do not mix. 
    dimensions: optional sequence of integers specifying the permutation order of 
      the input shape. If specified, the length must match ``operand.shape``. 
      Additionally, dimensions must only permute among like dimensions of mat: 
      batch, sparse, and dense dimensions cannot be permuted. 
 
  Returns: 
    out: reshaped array. 
  &quot;&quot;&quot;</span>
  <span class="s3">if </span><span class="s1">(mat.indices.shape[:mat.n_batch] != mat.data.shape[:mat.n_batch] != mat.shape[:mat.n_batch]):</span>
    <span class="s0"># TODO(jakevdp) implement this case via broadcast_in_dim</span>
    <span class="s3">raise </span><span class="s1">NotImplementedError(</span><span class="s5">&quot;reshape of arrays with broadacsted batch dimensions.&quot;</span><span class="s1">)</span>

  <span class="s1">batch_shape</span><span class="s3">, </span><span class="s1">sparse_shape</span><span class="s3">, </span><span class="s1">dense_shape = split_list(mat.shape</span><span class="s3">, </span><span class="s1">[mat.n_batch</span><span class="s3">, </span><span class="s1">mat.n_sparse])</span>
  <span class="s1">batch_perm</span><span class="s3">, </span><span class="s1">sparse_perm</span><span class="s3">, </span><span class="s1">dense_perm = _validate_permutation(</span>
    <span class="s1">mat.data</span><span class="s3">, </span><span class="s1">mat.indices</span><span class="s3">, </span><span class="s1">dimensions </span><span class="s3">or </span><span class="s1">tuple(range(mat.ndim))</span><span class="s3">, </span><span class="s1">mat.shape)</span>
  <span class="s1">batch_size = np.prod(batch_shape</span><span class="s3">, </span><span class="s1">dtype=int)</span>
  <span class="s1">sparse_size = np.prod(sparse_shape</span><span class="s3">, </span><span class="s1">dtype=int)</span>

  <span class="s1">cuml_shape = np.cumprod(new_sizes)</span>
  <span class="s3">if </span><span class="s1">batch_size != </span><span class="s4">1 </span><span class="s3">and </span><span class="s1">batch_size </span><span class="s3">not in </span><span class="s1">cuml_shape:</span>
    <span class="s3">raise </span><span class="s1">ValueError(</span><span class="s5">&quot;bcoo_reshape: new shape cannot mix batch and sparse dimensions; &quot;</span>
                     <span class="s5">f&quot;got shape=</span><span class="s3">{</span><span class="s1">mat.shape</span><span class="s3">} </span><span class="s5">new_shape=</span><span class="s3">{</span><span class="s1">new_sizes</span><span class="s3">} </span><span class="s5">with n_batch=</span><span class="s3">{</span><span class="s1">mat.n_batch</span><span class="s3">}</span><span class="s5">&quot;</span><span class="s1">)</span>
  <span class="s3">if </span><span class="s1">sparse_size != </span><span class="s4">1 </span><span class="s3">and </span><span class="s1">batch_size * sparse_size </span><span class="s3">not in </span><span class="s1">cuml_shape:</span>
    <span class="s3">raise </span><span class="s1">ValueError(</span><span class="s5">&quot;bcoo_reshape: new shape cannot mix sparse and dense dimensions; &quot;</span>
                     <span class="s5">f&quot;got shape=</span><span class="s3">{</span><span class="s1">mat.shape</span><span class="s3">} </span><span class="s5">new_shape=</span><span class="s3">{</span><span class="s1">new_sizes</span><span class="s3">} </span><span class="s5">with n_dense=</span><span class="s3">{</span><span class="s1">mat.n_dense</span><span class="s3">}</span><span class="s5">&quot;</span><span class="s1">)</span>

  <span class="s1">i1 = cuml_shape.searchsorted(batch_size</span><span class="s3">, </span><span class="s1">side=</span><span class="s5">'right'</span><span class="s1">)</span>
  <span class="s1">i2 = cuml_shape.searchsorted(batch_size * sparse_size</span><span class="s3">, </span><span class="s1">side=</span><span class="s5">'right'</span><span class="s1">)</span>
  <span class="s1">new_batch_shape</span><span class="s3">, </span><span class="s1">new_sparse_shape</span><span class="s3">, </span><span class="s1">new_dense_shape = split_list(new_sizes</span><span class="s3">, </span><span class="s1">[i1</span><span class="s3">, </span><span class="s1">i2])</span>

  <span class="s0"># Reshape batch &amp; dense dimensions: this is accomplished via a standard reshape.</span>
  <span class="s1">data = lax.reshape(</span>
    <span class="s1">mat.data</span><span class="s3">, </span><span class="s1">new_sizes=(*new_batch_shape</span><span class="s3">, </span><span class="s1">mat.nse</span><span class="s3">, </span><span class="s1">*new_dense_shape)</span><span class="s3">,</span>
    <span class="s1">dimensions=(*batch_perm</span><span class="s3">, </span><span class="s1">mat.n_batch</span><span class="s3">, </span><span class="s1">*(p + mat.n_batch + </span><span class="s4">1 </span><span class="s3">for </span><span class="s1">p </span><span class="s3">in </span><span class="s1">dense_perm)))</span>
  <span class="s1">indices = lax.reshape(</span>
    <span class="s1">mat.indices</span><span class="s3">, </span><span class="s1">new_sizes=(*new_batch_shape</span><span class="s3">, </span><span class="s1">mat.nse</span><span class="s3">, </span><span class="s1">mat.n_sparse)</span><span class="s3">,</span>
    <span class="s1">dimensions=(*batch_perm</span><span class="s3">, </span><span class="s1">mat.n_batch</span><span class="s3">, </span><span class="s1">mat.n_batch + </span><span class="s4">1</span><span class="s1">))</span>

  <span class="s0"># Reshape the sparse dimensions: this is accomplished by re-indexing.</span>
  <span class="s3">if not </span><span class="s1">new_sparse_shape:</span>
    <span class="s1">indices = jnp.empty_like(indices</span><span class="s3">, </span><span class="s1">shape=(*new_batch_shape</span><span class="s3">, </span><span class="s1">mat.nse</span><span class="s3">, </span><span class="s4">0</span><span class="s1">))</span>
  <span class="s3">elif </span><span class="s1">sparse_shape:</span>
    <span class="s1">index_cols = tuple(indices[...</span><span class="s3">, </span><span class="s1">i] </span><span class="s3">for </span><span class="s1">i </span><span class="s3">in </span><span class="s1">sparse_perm)</span>
    <span class="s1">sparse_shape = [int(mat.shape[mat.n_batch + i]) </span><span class="s3">for </span><span class="s1">i </span><span class="s3">in </span><span class="s1">sparse_perm]</span>
    <span class="s1">flat_indices = jnp.ravel_multi_index(index_cols</span><span class="s3">, </span><span class="s1">dims=tuple(sparse_shape)</span><span class="s3">, </span><span class="s1">mode=</span><span class="s5">'clip'</span><span class="s1">)</span>
    <span class="s3">with </span><span class="s1">jax.numpy_rank_promotion(</span><span class="s5">'allow'</span><span class="s1">):</span>
      <span class="s1">oob_indices = (indices &gt;= jnp.array(mat.shape[mat.n_batch: mat.n_batch + mat.n_sparse]</span><span class="s3">,</span>
                                          <span class="s1">dtype=indices.dtype)).any(-</span><span class="s4">1</span><span class="s3">, </span><span class="s1">keepdims=</span><span class="s3">True</span><span class="s1">)</span>
    <span class="s1">new_index_cols = jnp.unravel_index(flat_indices</span><span class="s3">, </span><span class="s1">new_sparse_shape)</span>
    <span class="s1">indices = jnp.concatenate([col[...</span><span class="s3">, None</span><span class="s1">] </span><span class="s3">for </span><span class="s1">col </span><span class="s3">in </span><span class="s1">new_index_cols]</span><span class="s3">, </span><span class="s1">axis=-</span><span class="s4">1</span><span class="s1">)</span>
    <span class="s1">indices = jnp.where(oob_indices</span><span class="s3">, </span><span class="s1">jnp.array(new_sparse_shape</span><span class="s3">, </span><span class="s1">dtype=indices.dtype)</span><span class="s3">, </span><span class="s1">indices)</span>

  <span class="s3">return </span><span class="s1">BCOO((data</span><span class="s3">, </span><span class="s1">indices)</span><span class="s3">, </span><span class="s1">shape=new_sizes)</span>


<span class="s3">def </span><span class="s1">bcoo_rev(operand</span><span class="s3">, </span><span class="s1">dimensions):</span>
  <span class="s2">&quot;&quot;&quot;Sparse implementation of {func}`jax.lax.rev`&quot;&quot;&quot;</span>
  <span class="s0"># Check validity of dimensions via original implementation.</span>
  <span class="s1">_ = jax.eval_shape(partial(lax.rev</span><span class="s3">, </span><span class="s1">dimensions=dimensions)</span><span class="s3">,</span>
                     <span class="s1">jax.ShapeDtypeStruct(operand.shape</span><span class="s3">, </span><span class="s1">operand.dtype))</span>
  <span class="s1">batch_dims = [d </span><span class="s3">for </span><span class="s1">d </span><span class="s3">in </span><span class="s1">dimensions </span><span class="s3">if </span><span class="s1">d &lt; operand.n_batch]</span>
  <span class="s1">sparse_dims = [d </span><span class="s3">for </span><span class="s1">d </span><span class="s3">in </span><span class="s1">dimensions </span><span class="s3">if </span><span class="s1">operand.n_batch &lt;= d &lt; operand.n_batch + operand.n_sparse]</span>
  <span class="s1">dense_dims = [d </span><span class="s3">for </span><span class="s1">d </span><span class="s3">in </span><span class="s1">dimensions </span><span class="s3">if </span><span class="s1">d &gt;= operand.n_batch + operand.n_sparse]</span>

  <span class="s1">data</span><span class="s3">, </span><span class="s1">indices = operand.data</span><span class="s3">, </span><span class="s1">operand.indices</span>

  <span class="s3">if </span><span class="s1">batch_dims:</span>
    <span class="s1">indices = lax.rev(indices</span><span class="s3">, </span><span class="s1">dimensions=batch_dims)</span>
  <span class="s3">if </span><span class="s1">batch_dims </span><span class="s3">or </span><span class="s1">dense_dims:</span>
    <span class="s1">data = lax.rev(data</span><span class="s3">, </span><span class="s1">dimensions=batch_dims + [d + </span><span class="s4">1 </span><span class="s1">- operand.n_sparse </span><span class="s3">for </span><span class="s1">d </span><span class="s3">in </span><span class="s1">dense_dims])</span>

  <span class="s3">if </span><span class="s1">sparse_dims:</span>
    <span class="s1">sparse_shape = jnp.array(operand.shape[operand.n_batch: operand.n_batch + operand.n_sparse]</span><span class="s3">,</span>
                             <span class="s1">dtype=indices.dtype)</span>
    <span class="s1">spdims = jnp.array([d - operand.n_batch </span><span class="s3">for </span><span class="s1">d </span><span class="s3">in </span><span class="s1">sparse_dims])</span>
    <span class="s1">indices = indices.at[...</span><span class="s3">, </span><span class="s1">spdims].mul(-</span><span class="s4">1</span><span class="s1">)</span>
    <span class="s1">indices = indices.at[...</span><span class="s3">, </span><span class="s1">spdims].add(sparse_shape[spdims] - </span><span class="s4">1</span><span class="s1">)</span>
    <span class="s1">indices = jnp.where(indices &lt; </span><span class="s4">0</span><span class="s3">, </span><span class="s1">sparse_shape</span><span class="s3">, </span><span class="s1">indices)</span>

  <span class="s3">return </span><span class="s1">BCOO((data</span><span class="s3">, </span><span class="s1">indices)</span><span class="s3">, </span><span class="s1">shape=operand.shape)</span>


<span class="s3">def </span><span class="s1">bcoo_squeeze(arr: BCOO</span><span class="s3">, </span><span class="s1">*</span><span class="s3">, </span><span class="s1">dimensions: Sequence[int]) -&gt; BCOO:</span>
  <span class="s2">&quot;&quot;&quot;Sparse implementation of {func}`jax.lax.squeeze`. 
 
  Squeeze any number of size 1 dimensions from an array. 
 
  Args: 
    arr: BCOO array to be reshaped. 
    dimensions: sequence of integers specifying dimensions to squeeze. 
 
  Returns: 
    out: reshaped array. 
  &quot;&quot;&quot;</span>
  <span class="s1">dimensions = tuple(canonicalize_axis(dim</span><span class="s3">, </span><span class="s1">arr.ndim) </span><span class="s3">for </span><span class="s1">dim </span><span class="s3">in </span><span class="s1">dimensions)</span>
  <span class="s3">if </span><span class="s1">any(arr.shape[dim] != </span><span class="s4">1 </span><span class="s3">for </span><span class="s1">dim </span><span class="s3">in </span><span class="s1">dimensions):</span>
    <span class="s3">raise </span><span class="s1">ValueError(</span><span class="s5">&quot;cannot select an axis to squeeze out which has size not equal to one, &quot;</span>
                     <span class="s5">f&quot;got shape=</span><span class="s3">{</span><span class="s1">arr.shape</span><span class="s3">} </span><span class="s5">and </span><span class="s3">{</span><span class="s1">dimensions=</span><span class="s3">}</span><span class="s5">&quot;</span><span class="s1">)</span>
  <span class="s1">batch_dims = tuple(d </span><span class="s3">for </span><span class="s1">d </span><span class="s3">in </span><span class="s1">dimensions </span><span class="s3">if </span><span class="s1">d &lt; arr.n_batch)</span>
  <span class="s1">sparse_dims = np.array([i </span><span class="s3">for </span><span class="s1">i </span><span class="s3">in </span><span class="s1">range(arr.n_sparse)</span>
                          <span class="s3">if </span><span class="s1">i + arr.n_batch </span><span class="s3">not in </span><span class="s1">dimensions]</span><span class="s3">, </span><span class="s1">dtype=int)</span>
  <span class="s1">dense_dims = tuple(d - arr.n_sparse + </span><span class="s4">1 </span><span class="s3">for </span><span class="s1">d </span><span class="s3">in </span><span class="s1">dimensions</span>
                     <span class="s3">if </span><span class="s1">d &gt;= arr.n_batch + arr.n_sparse)</span>
  <span class="s1">data_out = lax.squeeze(arr.data</span><span class="s3">, </span><span class="s1">batch_dims + dense_dims)</span>
  <span class="s1">indices_out = lax.squeeze(arr.indices[...</span><span class="s3">, </span><span class="s1">sparse_dims]</span><span class="s3">, </span><span class="s1">batch_dims)</span>
  <span class="s1">out_shape = tuple(s </span><span class="s3">for </span><span class="s1">i</span><span class="s3">, </span><span class="s1">s </span><span class="s3">in </span><span class="s1">enumerate(arr.shape) </span><span class="s3">if </span><span class="s1">i </span><span class="s3">not in </span><span class="s1">dimensions)</span>
  <span class="s3">return </span><span class="s1">BCOO((data_out</span><span class="s3">, </span><span class="s1">indices_out)</span><span class="s3">, </span><span class="s1">shape=out_shape</span><span class="s3">,</span>
              <span class="s1">indices_sorted=arr.indices_sorted</span><span class="s3">, </span><span class="s1">unique_indices=arr.unique_indices)</span>


<span class="s3">def </span><span class="s1">bcoo_slice(mat: BCOO</span><span class="s3">, </span><span class="s1">*</span><span class="s3">, </span><span class="s1">start_indices: Sequence[int]</span><span class="s3">, </span><span class="s1">limit_indices: Sequence[int]</span><span class="s3">,</span>
               <span class="s1">strides: Optional[Sequence[int]] = </span><span class="s3">None</span><span class="s1">) -&gt; BCOO:</span>
  <span class="s2">&quot;&quot;&quot;Sparse implementation of {func}`jax.lax.slice`. 
 
  Args: 
    mat: BCOO array to be reshaped. 
    start_indices: sequence of integers of length `mat.ndim` specifying the starting 
      indices of each slice. 
    limit_indices: sequence of integers of length `mat.ndim` specifying the ending 
      indices of each slice 
    strides: (not implemented) sequence of integers of length `mat.ndim` specifying 
      the stride for each slice 
 
  Returns: 
    out: BCOO array containing the slice. 
  &quot;&quot;&quot;</span>
  <span class="s3">if not </span><span class="s1">isinstance(mat</span><span class="s3">, </span><span class="s1">BCOO):</span>
    <span class="s3">raise </span><span class="s1">ValueError(</span><span class="s5">f&quot;bcoo_slice: input should be BCOO array, got type(mat)=</span><span class="s3">{</span><span class="s1">type(mat)</span><span class="s3">}</span><span class="s5">&quot;</span><span class="s1">)</span>
  <span class="s1">start_indices = [operator.index(i) </span><span class="s3">for </span><span class="s1">i </span><span class="s3">in </span><span class="s1">start_indices]</span>
  <span class="s1">limit_indices = [operator.index(i) </span><span class="s3">for </span><span class="s1">i </span><span class="s3">in </span><span class="s1">limit_indices]</span>
  <span class="s3">if </span><span class="s1">strides </span><span class="s3">is not None</span><span class="s1">:</span>
    <span class="s1">strides = [operator.index(i) </span><span class="s3">for </span><span class="s1">i </span><span class="s3">in </span><span class="s1">strides]</span>
  <span class="s3">else</span><span class="s1">:</span>
    <span class="s1">strides = [</span><span class="s4">1</span><span class="s1">] * mat.ndim</span>
  <span class="s3">if </span><span class="s1">len(start_indices) != len(limit_indices) != len(strides) != mat.ndim:</span>
    <span class="s3">raise </span><span class="s1">ValueError(</span><span class="s5">f&quot;bcoo_slice: indices must have size mat.ndim=</span><span class="s3">{</span><span class="s1">mat.ndim</span><span class="s3">}</span><span class="s5">&quot;</span><span class="s1">)</span>
  <span class="s3">if </span><span class="s1">len(strides) != mat.ndim:</span>
    <span class="s3">raise </span><span class="s1">ValueError(</span><span class="s5">f&quot;len(strides) = </span><span class="s3">{</span><span class="s1">len(strides)</span><span class="s3">}</span><span class="s5">; expected </span><span class="s3">{</span><span class="s1">mat.ndim</span><span class="s3">}</span><span class="s5">&quot;</span><span class="s1">)</span>
  <span class="s3">if </span><span class="s1">any(s &lt;= </span><span class="s4">0 </span><span class="s3">for </span><span class="s1">s </span><span class="s3">in </span><span class="s1">strides):</span>
    <span class="s3">raise </span><span class="s1">ValueError(</span><span class="s5">f&quot;strides must be a sequence of positive integers; got </span><span class="s3">{</span><span class="s1">strides</span><span class="s3">}</span><span class="s5">&quot;</span><span class="s1">)</span>

  <span class="s3">if not </span><span class="s1">all(</span><span class="s4">0 </span><span class="s1">&lt;= start &lt;= end &lt;= size</span>
             <span class="s3">for </span><span class="s1">start</span><span class="s3">, </span><span class="s1">end</span><span class="s3">, </span><span class="s1">size </span><span class="s3">in </span><span class="s1">safe_zip(start_indices</span><span class="s3">, </span><span class="s1">limit_indices</span><span class="s3">, </span><span class="s1">mat.shape)):</span>
    <span class="s3">raise </span><span class="s1">ValueError(</span><span class="s5">f&quot;bcoo_slice: invalid indices. Got </span><span class="s3">{</span><span class="s1">start_indices=</span><span class="s3">}</span><span class="s5">, &quot;</span>
                     <span class="s5">f&quot;</span><span class="s3">{</span><span class="s1">limit_indices=</span><span class="s3">} </span><span class="s5">and shape=</span><span class="s3">{</span><span class="s1">mat.shape</span><span class="s3">}</span><span class="s5">&quot;</span><span class="s1">)</span>

  <span class="s1">start_batch</span><span class="s3">, </span><span class="s1">start_sparse</span><span class="s3">, </span><span class="s1">start_dense = split_list(start_indices</span><span class="s3">, </span><span class="s1">[mat.n_batch</span><span class="s3">, </span><span class="s1">mat.n_sparse])</span>
  <span class="s1">end_batch</span><span class="s3">, </span><span class="s1">end_sparse</span><span class="s3">, </span><span class="s1">end_dense = split_list(limit_indices</span><span class="s3">, </span><span class="s1">[mat.n_batch</span><span class="s3">, </span><span class="s1">mat.n_sparse])</span>
  <span class="s1">stride_batch</span><span class="s3">, </span><span class="s1">stride_sparse</span><span class="s3">, </span><span class="s1">stride_dense = split_list(strides</span><span class="s3">, </span><span class="s1">[mat.n_batch</span><span class="s3">, </span><span class="s1">mat.n_sparse])</span>

  <span class="s1">data_slices = []</span>
  <span class="s1">index_slices = []</span>
  <span class="s3">for </span><span class="s1">i</span><span class="s3">, </span><span class="s1">(start</span><span class="s3">, </span><span class="s1">end</span><span class="s3">, </span><span class="s1">stride) </span><span class="s3">in </span><span class="s1">enumerate(zip(start_batch</span><span class="s3">, </span><span class="s1">end_batch</span><span class="s3">, </span><span class="s1">stride_batch)):</span>
    <span class="s1">data_slices.append(slice(</span><span class="s3">None</span><span class="s1">) </span><span class="s3">if </span><span class="s1">mat.data.shape[i] != mat.shape[i] </span><span class="s3">else </span><span class="s1">slice(start</span><span class="s3">, </span><span class="s1">end</span><span class="s3">, </span><span class="s1">stride))</span>
    <span class="s1">index_slices.append(slice(</span><span class="s3">None</span><span class="s1">) </span><span class="s3">if </span><span class="s1">mat.indices.shape[i] != mat.shape[i] </span><span class="s3">else </span><span class="s1">slice(start</span><span class="s3">, </span><span class="s1">end</span><span class="s3">, </span><span class="s1">stride))</span>
  <span class="s1">data_slices.append(slice(</span><span class="s3">None</span><span class="s1">))</span>
  <span class="s1">index_slices.extend([slice(</span><span class="s3">None</span><span class="s1">)</span><span class="s3">, </span><span class="s1">slice(</span><span class="s3">None</span><span class="s1">)])</span>
  <span class="s3">for </span><span class="s1">i</span><span class="s3">, </span><span class="s1">(start</span><span class="s3">, </span><span class="s1">end</span><span class="s3">, </span><span class="s1">stride) </span><span class="s3">in </span><span class="s1">enumerate(zip(start_dense</span><span class="s3">, </span><span class="s1">end_dense</span><span class="s3">, </span><span class="s1">stride_dense)):</span>
    <span class="s1">data_slices.append(slice(start</span><span class="s3">, </span><span class="s1">end</span><span class="s3">, </span><span class="s1">stride))</span>
  <span class="s1">new_data = mat.data[tuple(data_slices)]</span>
  <span class="s1">new_indices = mat.indices[tuple(index_slices)]</span>
  <span class="s1">new_shape = tuple(</span>
    <span class="s1">(end - start + stride - </span><span class="s4">1</span><span class="s1">) // stride</span>
    <span class="s3">for </span><span class="s1">start</span><span class="s3">, </span><span class="s1">end</span><span class="s3">, </span><span class="s1">stride </span><span class="s3">in </span><span class="s1">safe_zip(start_indices</span><span class="s3">, </span><span class="s1">limit_indices</span><span class="s3">, </span><span class="s1">strides))</span>
  <span class="s1">_</span><span class="s3">, </span><span class="s1">new_shape_sparse</span><span class="s3">, </span><span class="s1">_ = split_list(new_shape</span><span class="s3">, </span><span class="s1">[mat.n_batch</span><span class="s3">, </span><span class="s1">mat.n_sparse])</span>

  <span class="s3">if </span><span class="s1">mat.n_sparse:</span>
    <span class="s1">starts = jnp.expand_dims(jnp.array(start_sparse</span><span class="s3">, </span><span class="s1">dtype=new_indices.dtype)</span><span class="s3">, </span><span class="s1">range(mat.n_batch + </span><span class="s4">1</span><span class="s1">))</span>
    <span class="s1">ends = jnp.expand_dims(jnp.array(end_sparse</span><span class="s3">, </span><span class="s1">dtype=new_indices.dtype)</span><span class="s3">, </span><span class="s1">range(mat.n_batch + </span><span class="s4">1</span><span class="s1">))</span>
    <span class="s1">strides_ = jnp.expand_dims(jnp.array(stride_sparse</span><span class="s3">, </span><span class="s1">dtype=new_indices.dtype)</span><span class="s3">, </span><span class="s1">range(mat.n_batch + </span><span class="s4">1</span><span class="s1">))</span>

    <span class="s1">keep = jnp.all((new_indices &gt;= starts) &amp; (new_indices &lt; ends) &amp;</span>
                   <span class="s1">((new_indices - starts) % strides_ == </span><span class="s4">0</span><span class="s1">)</span><span class="s3">,</span>
                   <span class="s1">axis=-</span><span class="s4">1</span><span class="s3">, </span><span class="s1">keepdims=</span><span class="s3">True</span><span class="s1">)</span>
    <span class="s1">new_indices = jnp.where(keep</span><span class="s3">, </span><span class="s1">(new_indices - starts + strides_ - </span><span class="s4">1</span><span class="s1">) // strides_</span><span class="s3">,</span>
                            <span class="s1">(ends - starts + strides_ - </span><span class="s4">1</span><span class="s1">) // strides_)</span>

    <span class="s1">keep_data = lax.expand_dims(keep[...</span><span class="s3">, </span><span class="s4">0</span><span class="s1">]</span><span class="s3">, </span><span class="s1">range(mat.n_batch + </span><span class="s4">1</span><span class="s3">, </span><span class="s1">mat.n_batch + </span><span class="s4">1 </span><span class="s1">+ mat.n_dense))</span>
    <span class="s1">new_data = jnp.where(keep_data</span><span class="s3">, </span><span class="s1">new_data</span><span class="s3">, </span><span class="s4">0</span><span class="s1">)</span>

    <span class="s1">new_nse = int(np.prod(new_shape_sparse))</span>
    <span class="s3">if </span><span class="s1">mat.nse &gt; new_nse:</span>
      <span class="s1">new_data</span><span class="s3">, </span><span class="s1">new_indices = _bcoo_sum_duplicates(</span>
        <span class="s1">new_data</span><span class="s3">, </span><span class="s1">new_indices</span><span class="s3">, </span><span class="s1">spinfo=SparseInfo(shape=new_shape)</span><span class="s3">, </span><span class="s1">nse=new_nse)</span>

  <span class="s3">return </span><span class="s1">BCOO((new_data</span><span class="s3">, </span><span class="s1">new_indices)</span><span class="s3">, </span><span class="s1">shape=new_shape)</span>

<span class="s3">def </span><span class="s1">bcoo_dynamic_slice(mat: BCOO</span><span class="s3">, </span><span class="s1">start_indices: Sequence[Any]</span><span class="s3">, </span><span class="s1">slice_sizes: Sequence[int]) -&gt; BCOO:</span>
  <span class="s2">&quot;&quot;&quot;Sparse implementation of {func}`jax.lax.dynamic_slice`. 
 
  Args: 
    mat: BCOO array to slice. 
    start_indices: a list of scalar indices, one per dimension. These values 
      may be dynamic. 
    slice_sizes: the size of the slice. Must be a sequence of non-negative 
      integers with length equal to `ndim(operand)`. Inside a JIT compiled 
      function, only static values are supported (all JAX arrays inside JIT 
      must have statically known size). 
 
  Returns: 
    out: BCOO array containing the slice. 
  &quot;&quot;&quot;</span>
  <span class="s0"># Use abstract eval to validate inputs.</span>
  <span class="s1">jax.eval_shape(partial(lax.dynamic_slice</span><span class="s3">, </span><span class="s1">slice_sizes=slice_sizes)</span><span class="s3">,</span>
    <span class="s1">jax.ShapeDtypeStruct(mat.shape</span><span class="s3">, </span><span class="s1">mat.dtype)</span><span class="s3">, </span><span class="s1">start_indices)</span>
  <span class="s3">if not </span><span class="s1">isinstance(mat</span><span class="s3">, </span><span class="s1">BCOO):</span>
    <span class="s3">raise </span><span class="s1">ValueError(</span><span class="s5">f&quot;bcoo_slice: input should be BCOO array, got type(mat)=</span><span class="s3">{</span><span class="s1">type(mat)</span><span class="s3">}</span><span class="s5">&quot;</span><span class="s1">)</span>
  <span class="s1">start_indices = tuple(jnp.asarray(i) </span><span class="s3">for </span><span class="s1">i </span><span class="s3">in </span><span class="s1">start_indices)</span>
  <span class="s3">assert </span><span class="s1">all(jnp.issubdtype(i.dtype</span><span class="s3">, </span><span class="s1">np.integer) </span><span class="s3">for </span><span class="s1">i </span><span class="s3">in </span><span class="s1">start_indices)</span>
  <span class="s3">assert </span><span class="s1">all(i.shape == () </span><span class="s3">for </span><span class="s1">i </span><span class="s3">in </span><span class="s1">start_indices)</span>
  <span class="s1">slice_sizes = tuple(operator.index(i) </span><span class="s3">for </span><span class="s1">i </span><span class="s3">in </span><span class="s1">slice_sizes)</span>
  <span class="s3">if </span><span class="s1">len(start_indices) != len(slice_sizes) != mat.ndim:</span>
    <span class="s3">raise </span><span class="s1">ValueError(</span><span class="s5">f&quot;bcoo_dynamic_slice: indices must have size mat.ndim=</span><span class="s3">{</span><span class="s1">mat.ndim</span><span class="s3">}</span><span class="s5">&quot;</span><span class="s1">)</span>
  <span class="s3">if not </span><span class="s1">all(</span><span class="s4">0 </span><span class="s1">&lt;= slice_size &lt;= axis_size </span><span class="s3">for </span><span class="s1">slice_size</span><span class="s3">, </span><span class="s1">axis_size </span><span class="s3">in </span><span class="s1">zip(slice_sizes</span><span class="s3">, </span><span class="s1">mat.shape)):</span>
    <span class="s3">raise </span><span class="s1">TypeError(</span><span class="s5">&quot;slice_sizes must be less than or equal to operand shape, &quot;</span>
                    <span class="s5">f&quot;got slice_sizes </span><span class="s3">{</span><span class="s1">slice_sizes</span><span class="s3">} </span><span class="s5">for operand shape </span><span class="s3">{</span><span class="s1">mat.shape</span><span class="s3">}</span><span class="s5">&quot;</span><span class="s1">)</span>

  <span class="s1">start_batch</span><span class="s3">, </span><span class="s1">start_sparse</span><span class="s3">, </span><span class="s1">start_dense = split_list(start_indices</span><span class="s3">, </span><span class="s1">[mat.n_batch</span><span class="s3">, </span><span class="s1">mat.n_sparse])</span>
  <span class="s1">size_batch</span><span class="s3">, </span><span class="s1">size_sparse</span><span class="s3">, </span><span class="s1">size_dense = split_list(slice_sizes</span><span class="s3">, </span><span class="s1">[mat.n_batch</span><span class="s3">, </span><span class="s1">mat.n_sparse])</span>

  <span class="s1">data_start = []</span>
  <span class="s1">data_sizes = []</span>
  <span class="s1">indices_start = []</span>
  <span class="s1">indices_sizes = []</span>
  <span class="s1">zero = _const(start_indices[</span><span class="s4">0</span><span class="s1">] </span><span class="s3">if </span><span class="s1">start_indices </span><span class="s3">else </span><span class="s1">np.int32</span><span class="s3">, </span><span class="s4">0</span><span class="s1">)</span>
  <span class="s3">for </span><span class="s1">i</span><span class="s3">, </span><span class="s1">(start</span><span class="s3">, </span><span class="s1">size) </span><span class="s3">in </span><span class="s1">enumerate(zip(start_batch</span><span class="s3">, </span><span class="s1">size_batch)):</span>
    <span class="s1">data_is_broadcast = mat.data.shape[i] != mat.shape[i]</span>
    <span class="s1">indices_is_broadcast = mat.indices.shape[i] != mat.shape[i]</span>
    <span class="s1">data_start.append(zero </span><span class="s3">if </span><span class="s1">data_is_broadcast </span><span class="s3">else </span><span class="s1">start)</span>
    <span class="s1">data_sizes.append(</span><span class="s4">1 </span><span class="s3">if </span><span class="s1">data_is_broadcast </span><span class="s3">else </span><span class="s1">size)</span>
    <span class="s1">indices_start.append(zero </span><span class="s3">if </span><span class="s1">indices_is_broadcast </span><span class="s3">else </span><span class="s1">start)</span>
    <span class="s1">indices_sizes.append(</span><span class="s4">1 </span><span class="s3">if </span><span class="s1">indices_is_broadcast </span><span class="s3">else </span><span class="s1">size)</span>
  <span class="s1">data_start.append(zero)</span>
  <span class="s1">data_sizes.append(mat.nse)</span>
  <span class="s1">indices_start.extend([zero</span><span class="s3">, </span><span class="s1">zero])</span>
  <span class="s1">indices_sizes.extend([mat.nse</span><span class="s3">, </span><span class="s1">mat.n_sparse])</span>
  <span class="s1">data_start.extend(start_dense)</span>
  <span class="s1">data_sizes.extend(size_dense)</span>

  <span class="s1">new_data = lax.dynamic_slice(mat.data</span><span class="s3">, </span><span class="s1">data_start</span><span class="s3">, </span><span class="s1">data_sizes)</span>
  <span class="s1">new_indices = lax.dynamic_slice(mat.indices</span><span class="s3">, </span><span class="s1">indices_start</span><span class="s3">, </span><span class="s1">indices_sizes)</span>
  <span class="s1">new_shape = slice_sizes</span>

  <span class="s3">if </span><span class="s1">mat.n_sparse:</span>
    <span class="s1">starts = jnp.array(start_sparse</span><span class="s3">, </span><span class="s1">dtype=new_indices.dtype)</span>
    <span class="s1">sizes = jnp.array(size_sparse</span><span class="s3">, </span><span class="s1">dtype=new_indices.dtype)</span>
    <span class="s1">sparse_shape = jnp.array(mat.shape[mat.n_batch: mat.n_batch + mat.n_sparse]</span><span class="s3">, </span><span class="s1">dtype=new_indices.dtype)</span>
    <span class="s1">starts = jnp.where(starts &lt; </span><span class="s4">0</span><span class="s3">, </span><span class="s1">starts + sparse_shape</span><span class="s3">, </span><span class="s1">starts)</span>
    <span class="s1">starts = jnp.clip(starts</span><span class="s3">, </span><span class="s4">0</span><span class="s3">, </span><span class="s1">sparse_shape - sizes)</span>

    <span class="s1">starts = jnp.expand_dims(starts</span><span class="s3">, </span><span class="s1">range(mat.n_batch + </span><span class="s4">1</span><span class="s1">))</span>
    <span class="s1">sizes = jnp.expand_dims(sizes</span><span class="s3">, </span><span class="s1">range(mat.n_batch + </span><span class="s4">1</span><span class="s1">))</span>
    <span class="s1">sparse_shape = jnp.expand_dims(sparse_shape</span><span class="s3">, </span><span class="s1">range(mat.n_batch + </span><span class="s4">1</span><span class="s1">))</span>

    <span class="s1">keep = jnp.all((new_indices &gt;= starts) &amp; (new_indices &lt; starts + sizes)</span><span class="s3">, </span><span class="s1">-</span><span class="s4">1</span><span class="s3">, </span><span class="s1">keepdims=</span><span class="s3">True</span><span class="s1">)</span>
    <span class="s1">new_indices = jnp.where(keep</span><span class="s3">, </span><span class="s1">new_indices - starts</span><span class="s3">, </span><span class="s1">sizes)</span>

    <span class="s1">keep_data = lax.expand_dims(keep[...</span><span class="s3">, </span><span class="s4">0</span><span class="s1">]</span><span class="s3">, </span><span class="s1">range(mat.n_batch + </span><span class="s4">1</span><span class="s3">, </span><span class="s1">mat.n_batch + </span><span class="s4">1 </span><span class="s1">+ mat.n_dense))</span>
    <span class="s1">new_data = jnp.where(keep_data</span><span class="s3">, </span><span class="s1">new_data</span><span class="s3">, </span><span class="s4">0</span><span class="s1">)</span>

    <span class="s3">if </span><span class="s1">mat.nse &gt; np.prod(size_sparse):</span>
      <span class="s1">new_nse = int(np.prod(size_sparse))</span>
      <span class="s1">new_data</span><span class="s3">, </span><span class="s1">new_indices = _bcoo_sum_duplicates(</span>
        <span class="s1">new_data</span><span class="s3">, </span><span class="s1">new_indices</span><span class="s3">, </span><span class="s1">spinfo=SparseInfo(shape=new_shape)</span><span class="s3">, </span><span class="s1">nse=new_nse)</span>

  <span class="s3">return </span><span class="s1">BCOO((new_data</span><span class="s3">, </span><span class="s1">new_indices)</span><span class="s3">, </span><span class="s1">shape=new_shape)</span>


<span class="s3">def </span><span class="s1">_tuple_replace(tup</span><span class="s3">, </span><span class="s1">ind</span><span class="s3">, </span><span class="s1">val):</span>
  <span class="s3">return </span><span class="s1">tuple(val </span><span class="s3">if </span><span class="s1">i == ind </span><span class="s3">else </span><span class="s1">t </span><span class="s3">for </span><span class="s1">i</span><span class="s3">, </span><span class="s1">t </span><span class="s3">in </span><span class="s1">enumerate(tup))</span>

<span class="s3">def </span><span class="s1">bcoo_reduce_sum(mat: BCOO</span><span class="s3">, </span><span class="s1">*</span><span class="s3">, </span><span class="s1">axes: Sequence[int]) -&gt; BCOO:</span>
  <span class="s2">&quot;&quot;&quot;Sum array element over given axes. 
 
  Args: 
    mat: A BCOO-format array. 
    shape: The shape of the target array. 
    axes:  A tuple or list or ndarray which contains axes of ``mat`` over which 
      sum is performed. 
 
  Returns: 
    A BCOO-format array containing the result. 
  &quot;&quot;&quot;</span>
  <span class="s1">out_data</span><span class="s3">, </span><span class="s1">out_indices</span><span class="s3">, </span><span class="s1">out_shape = _bcoo_reduce_sum(</span>
      <span class="s1">mat.data</span><span class="s3">, </span><span class="s1">mat.indices</span><span class="s3">, </span><span class="s1">spinfo=mat._info</span><span class="s3">, </span><span class="s1">axes=axes)</span>
  <span class="s3">return </span><span class="s1">BCOO((out_data</span><span class="s3">, </span><span class="s1">out_indices)</span><span class="s3">, </span><span class="s1">shape=out_shape)</span>

<span class="s3">def </span><span class="s1">_bcoo_reduce_sum(data: Array</span><span class="s3">, </span><span class="s1">indices: Array</span><span class="s3">, </span><span class="s1">*</span><span class="s3">, </span><span class="s1">spinfo: SparseInfo</span><span class="s3">, </span><span class="s1">axes: Sequence[int]) -&gt; Tuple[Array</span><span class="s3">, </span><span class="s1">Array</span><span class="s3">, </span><span class="s1">Shape]:</span>
  <span class="s1">shape = spinfo.shape</span>
  <span class="s3">assert </span><span class="s1">all(</span><span class="s4">0 </span><span class="s1">&lt;= a &lt; len(shape) </span><span class="s3">for </span><span class="s1">a </span><span class="s3">in </span><span class="s1">axes)</span>
  <span class="s1">n_batch</span><span class="s3">, </span><span class="s1">n_sparse</span><span class="s3">, </span><span class="s1">_</span><span class="s3">, </span><span class="s1">nse = _validate_bcoo(data</span><span class="s3">, </span><span class="s1">indices</span><span class="s3">, </span><span class="s1">shape)</span>
  <span class="s1">axes = sorted(set(axes))</span>

  <span class="s0"># Sum over dense dimensions -&gt; sum over data</span>
  <span class="s1">dense_axes = tuple(ax - n_sparse + </span><span class="s4">1 </span><span class="s3">for </span><span class="s1">ax </span><span class="s3">in </span><span class="s1">axes </span><span class="s3">if </span><span class="s1">ax &gt;= n_batch + n_sparse)</span>
  <span class="s1">data = data.sum(dense_axes)</span>
  <span class="s3">if </span><span class="s1">n_sparse:</span>
    <span class="s0"># zero-out data corresponding to invalid indices.</span>
    <span class="s1">fill_value = jnp.expand_dims(</span>
      <span class="s1">jnp.array(shape[n_batch: n_batch + n_sparse]</span><span class="s3">, </span><span class="s1">dtype=indices.dtype)</span><span class="s3">,</span>
      <span class="s1">range(indices.ndim - </span><span class="s4">1</span><span class="s1">))</span>
    <span class="s1">mask = jnp.all(indices &lt; fill_value</span><span class="s3">, </span><span class="s1">-</span><span class="s4">1</span><span class="s1">)</span>
    <span class="s3">if </span><span class="s1">data.ndim &gt; mask.ndim:</span>
      <span class="s1">mask = lax.expand_dims(mask</span><span class="s3">, </span><span class="s1">tuple(range(mask.ndim</span><span class="s3">, </span><span class="s1">data.ndim)))</span>
    <span class="s1">data = jnp.where(mask</span><span class="s3">, </span><span class="s1">data</span><span class="s3">, </span><span class="s4">0</span><span class="s1">)</span>

  <span class="s0"># Sum over sparse dimensions -&gt; drop index; sum is implicit</span>
  <span class="s1">sparse_idx = [i </span><span class="s3">for </span><span class="s1">i </span><span class="s3">in </span><span class="s1">range(n_sparse) </span><span class="s3">if </span><span class="s1">i + n_batch </span><span class="s3">not in </span><span class="s1">axes]</span>
  <span class="s3">if not </span><span class="s1">sparse_idx:</span>
    <span class="s1">indices = jnp.zeros(_tuple_replace(indices.shape</span><span class="s3">, </span><span class="s1">n_batch + </span><span class="s4">1</span><span class="s3">, </span><span class="s4">0</span><span class="s1">)</span><span class="s3">, </span><span class="s1">indices.dtype)</span>
  <span class="s3">else</span><span class="s1">:</span>
    <span class="s1">indices = indices[...</span><span class="s3">, </span><span class="s1">np.array(sparse_idx)]</span>

  <span class="s0"># Sum over batch dimensions -&gt; reshape into nse</span>
  <span class="s1">batch_axes = {ax </span><span class="s3">for </span><span class="s1">ax </span><span class="s3">in </span><span class="s1">axes </span><span class="s3">if </span><span class="s1">ax &lt; n_batch}</span>

  <span class="s0"># First handle broadcasted batch dimensions</span>
  <span class="s3">for </span><span class="s1">ax </span><span class="s3">in </span><span class="s1">batch_axes:</span>
    <span class="s3">if </span><span class="s1">data.shape[ax] == </span><span class="s4">1</span><span class="s1">:</span>
      <span class="s3">if </span><span class="s1">indices.shape[ax] == </span><span class="s4">1</span><span class="s1">:</span>
        <span class="s1">data = data * shape[ax]</span>
      <span class="s3">else</span><span class="s1">:</span>
        <span class="s1">data = lax.broadcast_in_dim(data</span><span class="s3">, </span><span class="s1">_tuple_replace(data.shape</span><span class="s3">, </span><span class="s1">ax</span><span class="s3">, </span><span class="s1">shape[ax])</span><span class="s3">, </span><span class="s1">tuple(range(data.ndim)))</span>
    <span class="s3">else</span><span class="s1">:</span>
      <span class="s3">if </span><span class="s1">indices.shape[ax] == </span><span class="s4">1</span><span class="s1">:</span>
        <span class="s1">data = data.sum(ax)</span>
    <span class="s3">assert </span><span class="s1">data.shape[ax] == indices.shape[ax]</span>

  <span class="s1">new_batch_dims = tuple(sorted(set(range(n_batch)) - batch_axes))</span>
  <span class="s1">new_batch_shape = tuple(data.shape[i] </span><span class="s3">for </span><span class="s1">i </span><span class="s3">in </span><span class="s1">new_batch_dims)</span>
  <span class="s1">new_nse = int(nse * np.prod([data.shape[i] </span><span class="s3">for </span><span class="s1">i </span><span class="s3">in </span><span class="s1">batch_axes]))</span>

  <span class="s1">data = lax.reshape(data</span><span class="s3">,</span>
                     <span class="s1">(*new_batch_shape</span><span class="s3">, </span><span class="s1">new_nse</span><span class="s3">, </span><span class="s1">*data.shape[n_batch + </span><span class="s4">1</span><span class="s1">:])</span><span class="s3">,</span>
                     <span class="s1">(*new_batch_dims</span><span class="s3">, </span><span class="s1">*batch_axes</span><span class="s3">, </span><span class="s1">*range(n_batch</span><span class="s3">, </span><span class="s1">data.ndim)))</span>
  <span class="s1">indices = lax.reshape(indices</span><span class="s3">,</span>
                        <span class="s1">(*new_batch_shape</span><span class="s3">, </span><span class="s1">new_nse</span><span class="s3">, </span><span class="s1">*indices.shape[n_batch + </span><span class="s4">1</span><span class="s1">:])</span><span class="s3">,</span>
                        <span class="s1">(*new_batch_dims</span><span class="s3">, </span><span class="s1">*batch_axes</span><span class="s3">, </span><span class="s1">*range(n_batch</span><span class="s3">, </span><span class="s1">indices.ndim)))</span>

  <span class="s1">out_shape = tuple(shape[i] </span><span class="s3">for </span><span class="s1">i </span><span class="s3">in </span><span class="s1">range(len(shape)) </span><span class="s3">if </span><span class="s1">i </span><span class="s3">not in </span><span class="s1">axes)</span>
  <span class="s3">return </span><span class="s1">data</span><span class="s3">, </span><span class="s1">indices</span><span class="s3">, </span><span class="s1">out_shape</span>

<span class="s3">def </span><span class="s1">bcoo_multiply_sparse(lhs: BCOO</span><span class="s3">, </span><span class="s1">rhs: BCOO) -&gt; BCOO:</span>
  <span class="s2">&quot;&quot;&quot;An element-wise multiplication of two sparse arrays. 
 
  Args: 
    lhs: A BCOO-format array. 
    rhs: A BCOO-format array. 
 
  Returns: 
    An BCOO-format array containing the result. 
  &quot;&quot;&quot;</span>
  <span class="s1">out_data</span><span class="s3">, </span><span class="s1">out_indices</span><span class="s3">, </span><span class="s1">out_shape = _bcoo_multiply_sparse(</span>
      <span class="s1">lhs.data</span><span class="s3">, </span><span class="s1">lhs.indices</span><span class="s3">, </span><span class="s1">rhs.data</span><span class="s3">, </span><span class="s1">rhs.indices</span><span class="s3">, </span><span class="s1">lhs_spinfo=lhs._info</span><span class="s3">,</span>
      <span class="s1">rhs_spinfo=rhs._info)</span>
  <span class="s3">return </span><span class="s1">BCOO((out_data</span><span class="s3">, </span><span class="s1">out_indices)</span><span class="s3">, </span><span class="s1">shape=out_shape)</span>

<span class="s3">def </span><span class="s1">_bcoo_multiply_sparse(lhs_data: Array</span><span class="s3">, </span><span class="s1">lhs_indices: Array</span><span class="s3">, </span><span class="s1">rhs_data: Array</span><span class="s3">, </span><span class="s1">rhs_indices: Array</span><span class="s3">, </span><span class="s1">*</span><span class="s3">,</span>
                          <span class="s1">lhs_spinfo: SparseInfo</span><span class="s3">, </span><span class="s1">rhs_spinfo: SparseInfo) -&gt; Tuple[Array</span><span class="s3">, </span><span class="s1">Array</span><span class="s3">, </span><span class="s1">Shape]:</span>
  <span class="s1">lhs_shape = lhs_spinfo.shape</span>
  <span class="s1">rhs_shape = rhs_spinfo.shape</span>

  <span class="s1">lhs = _validate_bcoo(lhs_data</span><span class="s3">, </span><span class="s1">lhs_indices</span><span class="s3">, </span><span class="s1">lhs_shape)</span>
  <span class="s1">rhs = _validate_bcoo(rhs_data</span><span class="s3">, </span><span class="s1">rhs_indices</span><span class="s3">, </span><span class="s1">rhs_shape)</span>
  <span class="s3">if </span><span class="s1">len(lhs_shape) != len(rhs_shape):</span>
    <span class="s0"># Similar requirement as lax.mul:</span>
    <span class="s3">raise </span><span class="s1">TypeError(</span><span class="s5">&quot;bcoo_multiply_sparse: arrays must have same number of dimensions, &quot;</span>
                    <span class="s5">f&quot;got </span><span class="s3">{</span><span class="s1">lhs_shape</span><span class="s3">}</span><span class="s5">, </span><span class="s3">{</span><span class="s1">rhs_shape</span><span class="s3">}</span><span class="s5">&quot;</span><span class="s1">)</span>
  <span class="s3">if </span><span class="s1">lhs.n_dense != rhs.n_dense:</span>
    <span class="s3">raise </span><span class="s1">NotImplementedError(</span><span class="s5">&quot;bcoo_multiply_sparse: arrays with differing numbers of &quot;</span>
                              <span class="s5">f&quot;dense dimensions: </span><span class="s3">{</span><span class="s1">lhs</span><span class="s3">}</span><span class="s5">, </span><span class="s3">{</span><span class="s1">rhs</span><span class="s3">}</span><span class="s5">&quot;</span><span class="s1">)</span>
  <span class="s1">n_batch = min(lhs.n_batch</span><span class="s3">, </span><span class="s1">rhs.n_batch)</span>
  <span class="s1">_mul = functools.partial(_bcoo_multiply_sparse_unbatched</span><span class="s3">,</span>
                           <span class="s1">lhs_shape=lhs_shape[n_batch:]</span><span class="s3">,</span>
                           <span class="s1">rhs_shape=rhs_shape[n_batch:])</span>
  <span class="s1">_mul = nfold_vmap(_mul</span><span class="s3">, </span><span class="s1">n_batch)</span>
  <span class="s1">data</span><span class="s3">, </span><span class="s1">indices = _mul(lhs_data</span><span class="s3">, </span><span class="s1">lhs_indices</span><span class="s3">, </span><span class="s1">rhs_data</span><span class="s3">, </span><span class="s1">rhs_indices)</span>
  <span class="s3">return </span><span class="s1">data</span><span class="s3">, </span><span class="s1">indices</span><span class="s3">, </span><span class="s1">jnp.broadcast_shapes(lhs_shape</span><span class="s3">, </span><span class="s1">rhs_shape)</span>

<span class="s3">def </span><span class="s1">_bcoo_multiply_sparse_unbatched(lhs_data</span><span class="s3">, </span><span class="s1">lhs_indices</span><span class="s3">, </span><span class="s1">rhs_data</span><span class="s3">, </span><span class="s1">rhs_indices</span><span class="s3">, </span><span class="s1">*</span><span class="s3">, </span><span class="s1">lhs_shape</span><span class="s3">, </span><span class="s1">rhs_shape):</span>
  <span class="s1">lhs = _validate_bcoo(lhs_data</span><span class="s3">, </span><span class="s1">lhs_indices</span><span class="s3">, </span><span class="s1">lhs_shape)</span>
  <span class="s1">rhs = _validate_bcoo(rhs_data</span><span class="s3">, </span><span class="s1">rhs_indices</span><span class="s3">, </span><span class="s1">rhs_shape)</span>
  <span class="s3">assert </span><span class="s1">(lhs.n_batch == </span><span class="s4">0</span><span class="s1">) </span><span class="s3">or </span><span class="s1">(rhs.n_batch == </span><span class="s4">0</span><span class="s1">)  </span><span class="s0"># Ensured at call site above</span>

  <span class="s0"># TODO(jakevdp): this can be made more efficient by utilizing batch structure.</span>
  <span class="s3">if </span><span class="s1">lhs.n_batch:</span>
    <span class="s1">lhs_data</span><span class="s3">, </span><span class="s1">lhs_indices = bcoo_update_layout(BCOO((lhs_data</span><span class="s3">, </span><span class="s1">lhs_indices)</span><span class="s3">, </span><span class="s1">shape=lhs_shape)</span><span class="s3">, </span><span class="s1">n_batch=</span><span class="s4">0</span><span class="s1">)._bufs</span>
    <span class="s1">lhs = _validate_bcoo(lhs_data</span><span class="s3">, </span><span class="s1">lhs_indices</span><span class="s3">, </span><span class="s1">lhs_shape)</span>
  <span class="s3">elif </span><span class="s1">rhs.n_batch:</span>
    <span class="s1">rhs_data</span><span class="s3">, </span><span class="s1">rhs_indices = bcoo_update_layout(BCOO((rhs_data</span><span class="s3">, </span><span class="s1">rhs_indices)</span><span class="s3">, </span><span class="s1">shape=rhs_shape)</span><span class="s3">, </span><span class="s1">n_batch=</span><span class="s4">0</span><span class="s1">)._bufs</span>
    <span class="s1">rhs = _validate_bcoo(rhs_data</span><span class="s3">, </span><span class="s1">rhs_indices</span><span class="s3">, </span><span class="s1">rhs_shape)</span>
  <span class="s1">dims = jnp.array([i </span><span class="s3">for </span><span class="s1">i</span><span class="s3">, </span><span class="s1">(s1</span><span class="s3">, </span><span class="s1">s2) </span><span class="s3">in </span><span class="s1">enumerate(safe_zip(lhs_shape[:lhs.n_sparse]</span><span class="s3">, </span><span class="s1">rhs_shape[:rhs.n_sparse]))</span>
                    <span class="s3">if </span><span class="s1">s1 != </span><span class="s4">1 </span><span class="s3">and </span><span class="s1">s2 != </span><span class="s4">1</span><span class="s1">]</span><span class="s3">, </span><span class="s1">dtype=int)</span>

  <span class="s0"># TODO(jakevdp): this nse can be tightened to min(lhs.nse, rhs.nse) if there</span>
  <span class="s0"># is no broadcasting and indices are unique.</span>
  <span class="s1">nse = lhs.nse * rhs.nse</span>

  <span class="s0"># TODO(jakevdp): this is pretty inefficient. Can we do this membership check</span>
  <span class="s0"># without constructing the full (lhs.nse, rhs.nse) masking matrix?</span>
  <span class="s1">mask = jnp.all(lhs_indices[:</span><span class="s3">, None, </span><span class="s1">dims] == rhs_indices[</span><span class="s3">None, </span><span class="s1">:</span><span class="s3">, </span><span class="s1">dims]</span><span class="s3">, </span><span class="s1">-</span><span class="s4">1</span><span class="s1">)</span>
  <span class="s1">i_lhs</span><span class="s3">, </span><span class="s1">i_rhs = jnp.nonzero(mask</span><span class="s3">, </span><span class="s1">size=nse</span><span class="s3">, </span><span class="s1">fill_value=(lhs.nse</span><span class="s3">, </span><span class="s1">rhs.nse))</span>
  <span class="s1">data = (lhs_data.at[i_lhs].get(mode=</span><span class="s5">'fill'</span><span class="s3">, </span><span class="s1">fill_value=</span><span class="s4">0</span><span class="s1">) *</span>
          <span class="s1">rhs_data.at[i_rhs].get(mode=</span><span class="s5">'fill'</span><span class="s3">, </span><span class="s1">fill_value=</span><span class="s4">0</span><span class="s1">))</span>
  <span class="s1">indices = jnp.maximum(</span>
      <span class="s1">lhs_indices.at[i_lhs].get(mode=</span><span class="s5">'fill'</span><span class="s3">, </span><span class="s1">fill_value=max(lhs_shape</span><span class="s3">, </span><span class="s1">default=</span><span class="s4">0</span><span class="s1">))</span><span class="s3">,</span>
      <span class="s1">rhs_indices.at[i_rhs].get(mode=</span><span class="s5">'fill'</span><span class="s3">, </span><span class="s1">fill_value=max(rhs_shape</span><span class="s3">, </span><span class="s1">default=</span><span class="s4">0</span><span class="s1">)))</span>
  <span class="s3">return </span><span class="s1">data</span><span class="s3">, </span><span class="s1">indices</span>

<span class="s3">def </span><span class="s1">bcoo_multiply_dense(sp_mat: BCOO</span><span class="s3">, </span><span class="s1">v: Array) -&gt; Array:</span>
  <span class="s2">&quot;&quot;&quot;An element-wise multiplication between a sparse and a dense array. 
 
  Args: 
    lhs: A BCOO-format array. 
    rhs: An ndarray. 
 
  Returns: 
    An ndarray containing the result. 
  &quot;&quot;&quot;</span>
  <span class="s3">return </span><span class="s1">_bcoo_multiply_dense(sp_mat.data</span><span class="s3">, </span><span class="s1">sp_mat.indices</span><span class="s3">, </span><span class="s1">v</span><span class="s3">, </span><span class="s1">spinfo=sp_mat._info)</span>

<span class="s3">def </span><span class="s1">_bcoo_multiply_dense(data: Array</span><span class="s3">, </span><span class="s1">indices: Array</span><span class="s3">, </span><span class="s1">v: Array</span><span class="s3">, </span><span class="s1">*</span><span class="s3">, </span><span class="s1">spinfo: SparseInfo) -&gt; Array:</span>
  <span class="s2">&quot;&quot;&quot;Broadcasted elementwise multiplication between a BCOO array and a dense array.&quot;&quot;&quot;</span>
  <span class="s0"># TODO(jakevdp): the logic here is similar to bcoo_extract... can we reuse that?</span>
  <span class="s1">shape = spinfo.shape</span>
  <span class="s3">if </span><span class="s1">v.ndim == </span><span class="s4">0</span><span class="s1">:</span>
    <span class="s3">return </span><span class="s1">lax.mul(data</span><span class="s3">, </span><span class="s1">v)</span>
  <span class="s3">if </span><span class="s1">shape == v.shape:</span>
    <span class="s0"># Note: due to distributive property, no deduplication necessary!</span>
    <span class="s3">return </span><span class="s1">lax.mul(data</span><span class="s3">, </span><span class="s1">_bcoo_extract(indices</span><span class="s3">, </span><span class="s1">v))</span>

  <span class="s3">if </span><span class="s1">lax.broadcast_shapes(v.shape</span><span class="s3">, </span><span class="s1">shape) != shape:</span>
    <span class="s3">raise </span><span class="s1">NotImplementedError(</span>
      <span class="s5">&quot;multiplication between sparse and dense is only implemented for cases &quot;</span>
      <span class="s5">&quot;where the output shape matches the sparse matrix shape. Got &quot;</span>
      <span class="s5">f&quot;</span><span class="s3">{</span><span class="s1">shape=</span><span class="s3">}</span><span class="s5">, </span><span class="s3">{</span><span class="s1">v.shape=</span><span class="s3">}</span><span class="s5">&quot;</span><span class="s1">)</span>
  <span class="s1">v = lax.expand_dims(v</span><span class="s3">, </span><span class="s1">range(len(shape) - v.ndim))</span>

  <span class="s1">props = _validate_bcoo(data</span><span class="s3">, </span><span class="s1">indices</span><span class="s3">, </span><span class="s1">shape)</span>

  <span class="s1">@partial(nfold_vmap</span><span class="s3">, </span><span class="s1">N=props.n_batch)</span>
  <span class="s3">def </span><span class="s1">_mul(data</span><span class="s3">, </span><span class="s1">indices</span><span class="s3">, </span><span class="s1">v):</span>
    <span class="s3">assert </span><span class="s1">indices.shape[</span><span class="s4">1</span><span class="s1">] == v.ndim - props.n_dense</span>
    <span class="s1">ind = tuple(indices[:</span><span class="s3">, </span><span class="s1">i] </span><span class="s3">for </span><span class="s1">i </span><span class="s3">in </span><span class="s1">range(indices.shape[</span><span class="s4">1</span><span class="s1">]))</span>
    <span class="s1">ind = tuple(i </span><span class="s3">if </span><span class="s1">s != </span><span class="s4">1 </span><span class="s3">else </span><span class="s4">0 </span><span class="s3">for </span><span class="s1">i</span><span class="s3">, </span><span class="s1">s </span><span class="s3">in </span><span class="s1">zip(ind</span><span class="s3">, </span><span class="s1">v.shape))</span>
    <span class="s3">return </span><span class="s1">data * v[ind]</span>
  <span class="s3">return </span><span class="s1">_mul(data</span><span class="s3">, </span><span class="s1">indices</span><span class="s3">, </span><span class="s1">v)</span>

<span class="s3">def </span><span class="s1">bcoo_gather(operand: BCOO</span><span class="s3">, </span><span class="s1">start_indices: Array</span><span class="s3">,</span>
                <span class="s1">dimension_numbers: GatherDimensionNumbers</span><span class="s3">,</span>
                <span class="s1">slice_sizes: Shape</span><span class="s3">, </span><span class="s1">*</span><span class="s3">,</span>
                <span class="s1">unique_indices: bool = </span><span class="s3">False,</span>
                <span class="s1">indices_are_sorted: bool = </span><span class="s3">False,</span>
                <span class="s1">mode: Optional[Union[str</span><span class="s3">, </span><span class="s1">GatherScatterMode]] = </span><span class="s3">None,</span>
                <span class="s1">fill_value = </span><span class="s3">None</span><span class="s1">) -&gt; BCOO:</span>
  <span class="s2">&quot;&quot;&quot;BCOO version of lax.gather.&quot;&quot;&quot;</span>
  <span class="s1">_validate_bcoo(operand.data</span><span class="s3">, </span><span class="s1">operand.indices</span><span class="s3">, </span><span class="s1">operand.shape)</span>

  <span class="s0"># TODO(jakevdp) make use of unique_indices and indices_are_sorted?</span>
  <span class="s3">if </span><span class="s1">mode </span><span class="s3">is None</span><span class="s1">:</span>
    <span class="s1">mode = GatherScatterMode.PROMISE_IN_BOUNDS</span>
  <span class="s1">parsed_mode = GatherScatterMode.from_any(mode)</span>
  <span class="s3">if </span><span class="s1">parsed_mode != GatherScatterMode.PROMISE_IN_BOUNDS:</span>
    <span class="s3">raise </span><span class="s1">NotImplementedError(</span><span class="s5">f&quot;bcoo_gather: </span><span class="s3">{</span><span class="s1">mode=</span><span class="s3">} </span><span class="s5">not yet supported.&quot;</span><span class="s1">)</span>

  <span class="s1">kwds = dict(dimension_numbers=dimension_numbers</span><span class="s3">, </span><span class="s1">slice_sizes=slice_sizes</span><span class="s3">,</span>
              <span class="s1">unique_indices=unique_indices</span><span class="s3">, </span><span class="s1">indices_are_sorted=indices_are_sorted</span><span class="s3">,</span>
              <span class="s1">mode=mode</span><span class="s3">, </span><span class="s1">fill_value=fill_value)</span>

  <span class="s0"># Abstract eval lax.gather to validate arguments &amp; determine output shape.</span>
  <span class="s1">out_aval = jax.eval_shape(partial(lax.gather</span><span class="s3">, </span><span class="s1">**kwds)</span><span class="s3">,</span>
    <span class="s1">jax.ShapeDtypeStruct(operand.shape</span><span class="s3">, </span><span class="s1">operand.dtype)</span><span class="s3">,</span>
    <span class="s1">jax.ShapeDtypeStruct(start_indices.shape</span><span class="s3">, </span><span class="s1">start_indices.dtype))</span>
  <span class="s1">offset_dims = dimension_numbers.offset_dims</span>
  <span class="s1">collapsed_slice_dims = dimension_numbers.collapsed_slice_dims</span>
  <span class="s1">start_index_map = dimension_numbers.start_index_map</span>

  <span class="s0"># Expand start_indices &amp; slice_sizes to full rank &amp; use bcoo_dynamic_slice</span>
  <span class="s1">full_start_indices: List[ArrayLike] = [_const(start_indices</span><span class="s3">, </span><span class="s4">0</span><span class="s1">)] * operand.ndim</span>
  <span class="s1">in_axes: List[Optional[int]] = [</span><span class="s3">None for </span><span class="s1">i </span><span class="s3">in </span><span class="s1">range(operand.ndim)]</span>
  <span class="s1">full_slice_sizes = list(operand.shape)</span>
  <span class="s3">for </span><span class="s1">i</span><span class="s3">, </span><span class="s1">j </span><span class="s3">in </span><span class="s1">enumerate(start_index_map):</span>
    <span class="s1">full_start_indices[j] = start_indices[...</span><span class="s3">, </span><span class="s1">i].ravel()</span>
    <span class="s1">full_slice_sizes[j] = slice_sizes[j]</span>
    <span class="s1">in_axes[j] = </span><span class="s4">0</span>
  <span class="s3">def </span><span class="s1">slice_func(indices):</span>
    <span class="s1">slc = bcoo_dynamic_slice(operand</span><span class="s3">, </span><span class="s1">indices</span><span class="s3">, </span><span class="s1">slice_sizes=full_slice_sizes)</span>
    <span class="s3">return </span><span class="s1">bcoo_squeeze(slc</span><span class="s3">, </span><span class="s1">dimensions=collapsed_slice_dims)</span>
  <span class="s1">result = vmap(slice_func</span><span class="s3">, </span><span class="s1">in_axes=(in_axes</span><span class="s3">,</span><span class="s1">))(full_start_indices)</span>
  <span class="s1">result = bcoo_reshape(result</span><span class="s3">,</span>
    <span class="s1">new_sizes=(*start_indices.shape[:-</span><span class="s4">1</span><span class="s1">]</span><span class="s3">, </span><span class="s1">*result.shape[</span><span class="s4">1</span><span class="s1">:])</span><span class="s3">,</span>
    <span class="s1">dimensions=tuple(range(result.ndim)))</span>

  <span class="s0"># Use offset_dims to permute result dimensions</span>
  <span class="s3">if </span><span class="s1">result.shape:</span>
    <span class="s1">batch_dims = tuple(dim </span><span class="s3">for </span><span class="s1">dim </span><span class="s3">in </span><span class="s1">range(len(out_aval.shape))</span>
                      <span class="s3">if </span><span class="s1">dim </span><span class="s3">not in </span><span class="s1">offset_dims)</span>
    <span class="s1">permutation = np.zeros(result.ndim</span><span class="s3">, </span><span class="s1">dtype=int)</span>
    <span class="s1">permutation[np.array(batch_dims + offset_dims)] = np.arange(result.ndim)</span>
    <span class="s3">if </span><span class="s1">set(permutation[:len(batch_dims)]) != set(range(len(batch_dims))):</span>
      <span class="s0"># TODO: jakevdp more granular approach here. Can we do this in a</span>
      <span class="s0"># way that preserves the original batch dimensions?</span>
      <span class="s1">result = bcoo_update_layout(result</span><span class="s3">, </span><span class="s1">n_batch=</span><span class="s4">0</span><span class="s1">)</span>
    <span class="s1">result = bcoo_transpose(result</span><span class="s3">, </span><span class="s1">permutation=tuple(permutation))</span>

  <span class="s3">return </span><span class="s1">result.reshape(out_aval.shape).astype(out_aval.dtype)</span>

<span class="s3">def </span><span class="s1">bcoo_conv_general_dilated(lhs</span><span class="s3">, </span><span class="s1">rhs</span><span class="s3">, </span><span class="s1">*</span><span class="s3">, </span><span class="s1">window_strides</span><span class="s3">, </span><span class="s1">padding</span><span class="s3">,</span>
                              <span class="s1">lhs_dilation=</span><span class="s3">None, </span><span class="s1">rhs_dilation=</span><span class="s3">None, </span><span class="s1">dimension_numbers=</span><span class="s3">None,</span>
                              <span class="s1">feature_group_count=</span><span class="s4">1</span><span class="s3">, </span><span class="s1">batch_group_count=</span><span class="s4">1</span><span class="s3">, </span><span class="s1">precision=</span><span class="s3">None,</span>
                              <span class="s1">preferred_element_type=</span><span class="s3">None</span><span class="s1">) -&gt; BCOO:</span>
  <span class="s0"># Validate and process parameters using lax.conv_general_dilated abstract evaluation.</span>
  <span class="s1">func = functools.partial(</span>
      <span class="s1">lax.conv_general_dilated</span><span class="s3">,</span>
      <span class="s1">window_strides=window_strides</span><span class="s3">, </span><span class="s1">padding=padding</span><span class="s3">,</span>
      <span class="s1">lhs_dilation=lhs_dilation</span><span class="s3">, </span><span class="s1">rhs_dilation=rhs_dilation</span><span class="s3">, </span><span class="s1">dimension_numbers=dimension_numbers</span><span class="s3">,</span>
      <span class="s1">feature_group_count=feature_group_count</span><span class="s3">, </span><span class="s1">batch_group_count=batch_group_count</span><span class="s3">,</span>
      <span class="s1">precision=precision</span><span class="s3">, </span><span class="s1">preferred_element_type=preferred_element_type)</span>
  <span class="s1">jaxpr = jax.make_jaxpr(func)(jax.ShapeDtypeStruct(lhs.shape</span><span class="s3">, </span><span class="s1">lhs.dtype)</span><span class="s3">,</span>
                               <span class="s1">jax.ShapeDtypeStruct(rhs.shape</span><span class="s3">, </span><span class="s1">rhs.dtype))</span>
  <span class="s3">assert </span><span class="s1">isinstance(jaxpr</span><span class="s3">, </span><span class="s1">core.ClosedJaxpr) </span><span class="s3">and </span><span class="s1">len(jaxpr.eqns) == </span><span class="s4">1</span>
  <span class="s1">params = jaxpr.eqns[</span><span class="s4">0</span><span class="s1">].params</span>

  <span class="s3">if </span><span class="s1">params[</span><span class="s5">'lhs_dilation'</span><span class="s1">] !=  (</span><span class="s4">1</span><span class="s3">,</span><span class="s1">) * (lhs.ndim - </span><span class="s4">2</span><span class="s1">):</span>
    <span class="s3">raise </span><span class="s1">NotImplementedError(</span><span class="s5">&quot;bcoo convolution with lhs_dilation.&quot;</span><span class="s1">)</span>
  <span class="s3">if </span><span class="s1">params[</span><span class="s5">'rhs_dilation'</span><span class="s1">] != (</span><span class="s4">1</span><span class="s3">,</span><span class="s1">) * (rhs.ndim - </span><span class="s4">2</span><span class="s1">):</span>
    <span class="s3">raise </span><span class="s1">NotImplementedError(</span><span class="s5">&quot;bcoo convolution with lhs_dilation.&quot;</span><span class="s1">)</span>
  <span class="s3">if </span><span class="s1">params[</span><span class="s5">'window_strides'</span><span class="s1">] != (</span><span class="s4">1</span><span class="s3">,</span><span class="s1">) * (lhs.ndim - </span><span class="s4">2</span><span class="s1">):</span>
    <span class="s3">raise </span><span class="s1">NotImplementedError(</span><span class="s5">&quot;bcoo convolution with non-unit window_strides.&quot;</span><span class="s1">)</span>
  <span class="s3">if </span><span class="s1">params[</span><span class="s5">'batch_group_count'</span><span class="s1">] != params[</span><span class="s5">'feature_group_count'</span><span class="s1">] != </span><span class="s4">1</span><span class="s1">:</span>
    <span class="s3">raise </span><span class="s1">NotImplementedError(</span><span class="s5">&quot;bcoo convolution with non-unit group counts.&quot;</span><span class="s1">)</span>

  <span class="s3">if </span><span class="s1">lhs.shape[:</span><span class="s4">2</span><span class="s1">] != rhs.shape[:</span><span class="s4">2</span><span class="s1">] != (</span><span class="s4">1</span><span class="s3">, </span><span class="s4">1</span><span class="s1">):</span>
    <span class="s3">raise </span><span class="s1">NotImplementedError(</span><span class="s5">&quot;bcoo convolution with leading dimensions other than (1, 1)&quot;</span><span class="s1">)</span>

  <span class="s1">index_dtype = (lhs.indices.dtype </span><span class="s3">if </span><span class="s1">hasattr(lhs</span><span class="s3">, </span><span class="s5">'indices'</span><span class="s1">)</span>
                 <span class="s3">else </span><span class="s1">rhs.indices.dtype </span><span class="s3">if </span><span class="s1">hasattr(rhs</span><span class="s3">, </span><span class="s5">'indices'</span><span class="s1">)</span>
                 <span class="s3">else </span><span class="s5">'int32'</span><span class="s1">)</span>

  <span class="s1">padding</span><span class="s3">, </span><span class="s1">= params[</span><span class="s5">'padding'</span><span class="s1">]</span>
  <span class="s3">return </span><span class="s1">_bcoo_conv_1d(_convert_to_1d_for_conv(lhs</span><span class="s3">, </span><span class="s1">index_dtype)</span><span class="s3">,</span>
                       <span class="s1">_convert_to_1d_for_conv(rhs</span><span class="s3">, </span><span class="s1">index_dtype)</span><span class="s3">,</span>
                       <span class="s1">padding=padding)</span>

<span class="s3">def </span><span class="s1">_convert_to_1d_for_conv(mat</span><span class="s3">, </span><span class="s1">index_dtype):</span>
  <span class="s3">if </span><span class="s1">isinstance(mat</span><span class="s3">, </span><span class="s1">(jax.Array</span><span class="s3">, </span><span class="s1">np.ndarray)):</span>
    <span class="s1">data = lax.squeeze(mat</span><span class="s3">, </span><span class="s1">(</span><span class="s4">0</span><span class="s3">, </span><span class="s4">1</span><span class="s1">))</span>
    <span class="s1">indices = lax.broadcasted_iota(index_dtype</span><span class="s3">, </span><span class="s1">(len(data)</span><span class="s3">, </span><span class="s4">1</span><span class="s1">)</span><span class="s3">, </span><span class="s4">0</span><span class="s1">)</span>
  <span class="s3">elif </span><span class="s1">isinstance(mat</span><span class="s3">, </span><span class="s1">BCOO):</span>
    <span class="s1">mat = mat.update_layout(n_batch=</span><span class="s4">2</span><span class="s3">, </span><span class="s1">n_dense=</span><span class="s4">0</span><span class="s1">)</span>
    <span class="s1">data = lax.squeeze(mat.data</span><span class="s3">, </span><span class="s1">(</span><span class="s4">0</span><span class="s3">, </span><span class="s4">1</span><span class="s1">))</span>
    <span class="s1">indices = lax.squeeze(mat.indices</span><span class="s3">, </span><span class="s1">(</span><span class="s4">0</span><span class="s3">, </span><span class="s4">1</span><span class="s1">))</span>
    <span class="s0"># zero-out data at OOB indices, otherwise strange things happen.</span>
    <span class="s1">data = jnp.where(lax.squeeze(indices</span><span class="s3">, </span><span class="s1">(</span><span class="s4">1</span><span class="s3">,</span><span class="s1">)) &lt; mat.shape[-</span><span class="s4">1</span><span class="s1">]</span><span class="s3">, </span><span class="s1">data</span><span class="s3">, </span><span class="s4">0</span><span class="s1">)</span>
  <span class="s3">else</span><span class="s1">:</span>
    <span class="s3">raise </span><span class="s1">ValueError(</span><span class="s5">f&quot;bcoo_conv_general_dilated: input of type </span><span class="s3">{</span><span class="s1">type(mat)</span><span class="s3">} </span><span class="s5">not recognized.&quot;</span><span class="s1">)</span>
  <span class="s3">return </span><span class="s1">BCOO((data</span><span class="s3">, </span><span class="s1">indices)</span><span class="s3">, </span><span class="s1">shape=mat.shape[</span><span class="s4">2</span><span class="s1">:])</span>

<span class="s3">def </span><span class="s1">_bcoo_conv_1d(lhs: BCOO</span><span class="s3">, </span><span class="s1">rhs: BCOO</span><span class="s3">, </span><span class="s1">padding: Sequence[int]) -&gt; BCOO:</span>
  <span class="s3">assert </span><span class="s1">lhs.ndim == lhs.n_sparse == rhs.ndim == rhs.n_sparse == </span><span class="s4">1</span>
  <span class="s3">assert </span><span class="s1">lhs.dtype == rhs.dtype</span>
  <span class="s1">padding = tuple(map(int</span><span class="s3">, </span><span class="s1">padding))</span>
  <span class="s3">assert </span><span class="s1">len(padding) == </span><span class="s4">2</span>

  <span class="s1">new_data = (lhs.data[:</span><span class="s3">, None</span><span class="s1">] * rhs.data[</span><span class="s3">None, </span><span class="s1">:]).ravel()</span>

  <span class="s1">offset = padding[</span><span class="s4">0</span><span class="s1">] - rhs.indices</span>
  <span class="s1">new_indices = (lhs.indices[:</span><span class="s3">, None</span><span class="s1">] + offset[</span><span class="s3">None, </span><span class="s1">:]).ravel()</span>

  <span class="s1">mask = (new_indices &lt; </span><span class="s4">0</span><span class="s1">)</span>
  <span class="s1">new_indices = jnp.where(mask</span><span class="s3">, </span><span class="s4">0</span><span class="s3">, </span><span class="s1">new_indices)</span>
  <span class="s1">new_data = jnp.where(mask</span><span class="s3">, </span><span class="s4">0</span><span class="s3">, </span><span class="s1">new_data)</span>
  <span class="s1">dimsize = max(</span><span class="s4">0</span><span class="s3">, </span><span class="s1">lhs.shape[</span><span class="s4">0</span><span class="s1">] + padding[</span><span class="s4">0</span><span class="s1">] + padding[</span><span class="s4">1</span><span class="s1">] - rhs.shape[</span><span class="s4">0</span><span class="s1">] + </span><span class="s4">1</span><span class="s1">)</span>

  <span class="s1">new_data = lax.expand_dims(new_data</span><span class="s3">, </span><span class="s1">(</span><span class="s4">0</span><span class="s3">, </span><span class="s4">1</span><span class="s1">))</span>
  <span class="s1">new_indices = lax.expand_dims(new_indices</span><span class="s3">, </span><span class="s1">(</span><span class="s4">0</span><span class="s3">, </span><span class="s4">1</span><span class="s3">, </span><span class="s4">3</span><span class="s1">))</span>
  <span class="s3">return </span><span class="s1">BCOO((new_data</span><span class="s3">, </span><span class="s1">new_indices)</span><span class="s3">, </span><span class="s1">shape=(</span><span class="s4">1</span><span class="s3">, </span><span class="s4">1</span><span class="s3">, </span><span class="s1">dimsize))</span>


<span class="s1">@tree_util.register_pytree_node_class</span>
<span class="s3">class </span><span class="s1">BCOO(JAXSparse):</span>
  <span class="s2">&quot;&quot;&quot;Experimental batched COO matrix implemented in JAX 
 
  Args: 
    (data, indices) : data and indices in batched COO format. 
    shape : shape of sparse array. 
 
  Attributes: 
    data : ndarray of shape ``[*batch_dims, nse, *dense_dims]`` containing the 
      explicitly stored data within the sparse matrix. 
    indices : ndarray of shape ``[*batch_dims, nse, n_sparse]`` containing the 
      indices of the explicitly stored data. Duplicate entries will be summed. 
 
  Examples: 
    Create a sparse array from a dense array: 
 
    &gt;&gt;&gt; M = jnp.array([[0., 2., 0.], [1., 0., 4.]]) 
    &gt;&gt;&gt; M_sp = BCOO.fromdense(M) 
    &gt;&gt;&gt; M_sp 
    BCOO(float32[2, 3], nse=3) 
 
    Examine the internal representation: 
 
    &gt;&gt;&gt; M_sp.data 
    Array([2., 1., 4.], dtype=float32) 
    &gt;&gt;&gt; M_sp.indices 
    Array([[0, 1], 
           [1, 0], 
           [1, 2]], dtype=int32) 
 
    Create a dense array from a sparse array: 
 
    &gt;&gt;&gt; M_sp.todense() 
    Array([[0., 2., 0.], 
           [1., 0., 4.]], dtype=float32) 
 
    Create a sparse array from COO data &amp; indices: 
 
    &gt;&gt;&gt; data = jnp.array([1., 3., 5.]) 
    &gt;&gt;&gt; indices = jnp.array([[0, 0], 
    ...                      [1, 1], 
    ...                      [2, 2]]) 
    &gt;&gt;&gt; mat = BCOO((data, indices), shape=(3, 3)) 
    &gt;&gt;&gt; mat 
    BCOO(float32[3, 3], nse=3) 
    &gt;&gt;&gt; mat.todense() 
    Array([[1., 0., 0.], 
           [0., 3., 0.], 
           [0., 0., 5.]], dtype=float32) 
  &quot;&quot;&quot;</span>
  <span class="s0"># Note: additional BCOO methods are defined in transform.py</span>

  <span class="s1">data: Array</span>
  <span class="s1">indices: Array</span>
  <span class="s1">shape: Shape</span>
  <span class="s1">nse = property(</span><span class="s3">lambda </span><span class="s1">self: self.indices.shape[-</span><span class="s4">2</span><span class="s1">])</span>
  <span class="s1">dtype = property(</span><span class="s3">lambda </span><span class="s1">self: self.data.dtype)</span>
  <span class="s1">n_batch = property(</span><span class="s3">lambda </span><span class="s1">self: self.indices.ndim - </span><span class="s4">2</span><span class="s1">)</span>
  <span class="s1">n_sparse = property(</span><span class="s3">lambda </span><span class="s1">self: self.indices.shape[-</span><span class="s4">1</span><span class="s1">])</span>
  <span class="s1">n_dense = property(</span><span class="s3">lambda </span><span class="s1">self: self.data.ndim - </span><span class="s4">1 </span><span class="s1">- self.n_batch)</span>
  <span class="s1">indices_sorted: bool</span>
  <span class="s1">unique_indices: bool</span>
  <span class="s1">_info = property(</span><span class="s3">lambda </span><span class="s1">self: SparseInfo(self.shape</span><span class="s3">, </span><span class="s1">self.indices_sorted</span><span class="s3">,</span>
                                           <span class="s1">self.unique_indices))</span>
  <span class="s1">_bufs = property(</span><span class="s3">lambda </span><span class="s1">self: (self.data</span><span class="s3">, </span><span class="s1">self.indices))</span>

  <span class="s3">def </span><span class="s1">__init__(self</span><span class="s3">, </span><span class="s1">args: Tuple[Array</span><span class="s3">, </span><span class="s1">Array]</span><span class="s3">, </span><span class="s1">*</span><span class="s3">, </span><span class="s1">shape: Sequence[int]</span><span class="s3">,</span>
               <span class="s1">indices_sorted: bool = </span><span class="s3">False, </span><span class="s1">unique_indices: bool = </span><span class="s3">False</span><span class="s1">):</span>
    <span class="s1">self.data</span><span class="s3">, </span><span class="s1">self.indices = map(jnp.asarray</span><span class="s3">, </span><span class="s1">args)</span>
    <span class="s1">self.indices_sorted = indices_sorted</span>
    <span class="s1">self.unique_indices = unique_indices</span>
    <span class="s1">super().__init__(args</span><span class="s3">, </span><span class="s1">shape=tuple(shape))</span>
    <span class="s1">_validate_bcoo(self.data</span><span class="s3">, </span><span class="s1">self.indices</span><span class="s3">, </span><span class="s1">self.shape)</span>

  <span class="s3">def </span><span class="s1">__repr__(self):</span>
    <span class="s1">name = self.__class__.__name__</span>
    <span class="s3">try</span><span class="s1">:</span>
      <span class="s1">nse = self.nse</span>
      <span class="s1">n_batch = self.n_batch</span>
      <span class="s1">n_dense = self.n_dense</span>
      <span class="s1">dtype = self.dtype</span>
      <span class="s1">shape = list(self.shape)</span>
    <span class="s3">except</span><span class="s1">:</span>
      <span class="s1">repr_ = </span><span class="s5">f&quot;</span><span class="s3">{</span><span class="s1">name</span><span class="s3">}</span><span class="s5">(&lt;invalid&gt;)&quot;</span>
    <span class="s3">else</span><span class="s1">:</span>
      <span class="s1">extra = </span><span class="s5">f&quot;, </span><span class="s3">{</span><span class="s1">nse=</span><span class="s3">}</span><span class="s5">&quot;</span>
      <span class="s3">if </span><span class="s1">n_batch: extra += </span><span class="s5">f&quot;, </span><span class="s3">{</span><span class="s1">n_batch=</span><span class="s3">}</span><span class="s5">&quot;</span>
      <span class="s3">if </span><span class="s1">n_dense: extra += </span><span class="s5">f&quot;, </span><span class="s3">{</span><span class="s1">n_dense=</span><span class="s3">}</span><span class="s5">&quot;</span>
      <span class="s1">repr_ = </span><span class="s5">f&quot;</span><span class="s3">{</span><span class="s1">name</span><span class="s3">}</span><span class="s5">(</span><span class="s3">{</span><span class="s1">dtype</span><span class="s3">}{</span><span class="s1">shape</span><span class="s3">}{</span><span class="s1">extra</span><span class="s3">}</span><span class="s5">)&quot;</span>
    <span class="s3">if </span><span class="s1">isinstance(self.data</span><span class="s3">, </span><span class="s1">core.Tracer):</span>
      <span class="s1">repr_ = </span><span class="s5">f&quot;</span><span class="s3">{</span><span class="s1">type(self.data).__name__</span><span class="s3">}</span><span class="s5">[</span><span class="s3">{</span><span class="s1">repr_</span><span class="s3">}</span><span class="s5">]&quot;</span>
    <span class="s3">return </span><span class="s1">repr_</span>

  <span class="s0"># Stub methods: these are defined in transform.py</span>
  <span class="s3">def </span><span class="s1">reshape(self</span><span class="s3">, </span><span class="s1">*args</span><span class="s3">, </span><span class="s1">**kwargs) -&gt; BCOO:</span>
    <span class="s3">raise </span><span class="s1">NotImplementedError(</span><span class="s5">&quot;BCOO.reshape&quot;</span><span class="s1">)</span>

  <span class="s3">def </span><span class="s1">astype(self</span><span class="s3">, </span><span class="s1">*args</span><span class="s3">, </span><span class="s1">**kwargs) -&gt; BCOO:</span>
    <span class="s3">raise </span><span class="s1">NotImplementedError(</span><span class="s5">&quot;BCOO.astype&quot;</span><span class="s1">)</span>

  <span class="s3">def </span><span class="s1">sum(self) -&gt; BCOO:</span>
    <span class="s3">raise </span><span class="s1">NotImplementedError(</span><span class="s5">&quot;BCOO.sum&quot;</span><span class="s1">)</span>

  <span class="s1">@classmethod</span>
  <span class="s3">def </span><span class="s1">fromdense(cls</span><span class="s3">, </span><span class="s1">mat: Array</span><span class="s3">, </span><span class="s1">*</span><span class="s3">, </span><span class="s1">nse: Optional[int] = </span><span class="s3">None, </span><span class="s1">index_dtype: DTypeLike = np.int32</span><span class="s3">,</span>
                <span class="s1">n_dense: int = </span><span class="s4">0</span><span class="s3">, </span><span class="s1">n_batch: int = </span><span class="s4">0</span><span class="s1">) -&gt; BCOO:</span>
    <span class="s2">&quot;&quot;&quot;Create a BCOO array from a (dense) :class:`DeviceArray`.&quot;&quot;&quot;</span>
    <span class="s3">return </span><span class="s1">bcoo_fromdense(</span>
      <span class="s1">mat</span><span class="s3">, </span><span class="s1">nse=nse</span><span class="s3">, </span><span class="s1">index_dtype=index_dtype</span><span class="s3">, </span><span class="s1">n_dense=n_dense</span><span class="s3">, </span><span class="s1">n_batch=n_batch)</span>

  <span class="s1">@classmethod</span>
  <span class="s3">def </span><span class="s1">from_scipy_sparse(cls</span><span class="s3">, </span><span class="s1">mat</span><span class="s3">, </span><span class="s1">*</span><span class="s3">, </span><span class="s1">index_dtype: Optional[DTypeLike]=</span><span class="s3">None,</span>
                        <span class="s1">n_dense: int = </span><span class="s4">0</span><span class="s3">, </span><span class="s1">n_batch: int = </span><span class="s4">0</span><span class="s1">) -&gt; BCOO:</span>
    <span class="s2">&quot;&quot;&quot;Create a BCOO array from a :mod:`scipy.sparse` array.&quot;&quot;&quot;</span>
    <span class="s3">if </span><span class="s1">n_dense != </span><span class="s4">0 </span><span class="s3">or </span><span class="s1">n_batch != </span><span class="s4">0</span><span class="s1">:</span>
      <span class="s3">raise </span><span class="s1">NotImplementedError(</span><span class="s5">&quot;BCOO.fromscipy with nonzero n_dense/n_batch&quot;</span><span class="s1">)</span>

    <span class="s1">mat = mat.tocoo()</span>
    <span class="s1">data = jnp.asarray(mat.data)</span>
    <span class="s1">indices = jnp.column_stack((mat.row</span><span class="s3">, </span><span class="s1">mat.col)).astype(</span>
        <span class="s1">index_dtype </span><span class="s3">or </span><span class="s1">jnp.int32)</span>
    <span class="s0"># TODO: determines sorted and unique indices for scipy conversion.</span>
    <span class="s3">return </span><span class="s1">cls((data</span><span class="s3">, </span><span class="s1">indices)</span><span class="s3">, </span><span class="s1">shape=mat.shape</span><span class="s3">, </span><span class="s1">indices_sorted=</span><span class="s3">False,</span>
               <span class="s1">unique_indices=</span><span class="s3">False</span><span class="s1">)</span>

  <span class="s1">@classmethod</span>
  <span class="s3">def </span><span class="s1">_empty(cls</span><span class="s3">, </span><span class="s1">shape: Shape</span><span class="s3">, </span><span class="s1">*</span><span class="s3">, </span><span class="s1">dtype: Optional[DTypeLike] = </span><span class="s3">None, </span><span class="s1">index_dtype: DTypeLike = </span><span class="s5">'int32'</span><span class="s3">,</span>
             <span class="s1">n_dense: int = </span><span class="s4">0</span><span class="s3">, </span><span class="s1">n_batch: int = </span><span class="s4">0</span><span class="s3">, </span><span class="s1">nse: int = </span><span class="s4">0</span><span class="s1">) -&gt; BCOO:</span>
    <span class="s2">&quot;&quot;&quot;Create an empty BCOO instance. Public method is sparse.empty().&quot;&quot;&quot;</span>
    <span class="s1">shape = tuple(shape)</span>
    <span class="s1">n_sparse = len(shape) - n_dense - n_batch</span>
    <span class="s3">if </span><span class="s1">n_sparse &lt; </span><span class="s4">0 </span><span class="s3">or </span><span class="s1">n_dense &lt; </span><span class="s4">0 </span><span class="s3">or </span><span class="s1">n_batch &lt; </span><span class="s4">0 </span><span class="s3">or </span><span class="s1">nse &lt; </span><span class="s4">0</span><span class="s1">:</span>
      <span class="s3">raise </span><span class="s1">ValueError(</span><span class="s5">f&quot;Invalid inputs: </span><span class="s3">{</span><span class="s1">shape=</span><span class="s3">}</span><span class="s5">, </span><span class="s3">{</span><span class="s1">n_dense=</span><span class="s3">}</span><span class="s5">, </span><span class="s3">{</span><span class="s1">n_batch=</span><span class="s3">}</span><span class="s5">, </span><span class="s3">{</span><span class="s1">nse=</span><span class="s3">}</span><span class="s5">&quot;</span><span class="s1">)</span>
    <span class="s1">batch_shape</span><span class="s3">, </span><span class="s1">sparse_shape</span><span class="s3">, </span><span class="s1">dense_shape = split_list(shape</span><span class="s3">, </span><span class="s1">[n_batch</span><span class="s3">, </span><span class="s1">n_sparse])</span>
    <span class="s1">data = jnp.zeros((*batch_shape</span><span class="s3">, </span><span class="s1">nse</span><span class="s3">, </span><span class="s1">*dense_shape)</span><span class="s3">, </span><span class="s1">dtype)</span>
    <span class="s1">indices = jnp.full((*batch_shape</span><span class="s3">, </span><span class="s1">nse</span><span class="s3">, </span><span class="s1">n_sparse)</span><span class="s3">, </span><span class="s1">jnp.array(sparse_shape)</span><span class="s3">, </span><span class="s1">index_dtype)</span>
    <span class="s3">return </span><span class="s1">cls((data</span><span class="s3">, </span><span class="s1">indices)</span><span class="s3">, </span><span class="s1">shape=shape</span><span class="s3">, </span><span class="s1">indices_sorted=</span><span class="s3">True,</span>
               <span class="s1">unique_indices=</span><span class="s3">True</span><span class="s1">)</span>

  <span class="s1">@classmethod</span>
  <span class="s3">def </span><span class="s1">_eye(cls</span><span class="s3">, </span><span class="s1">N: int</span><span class="s3">, </span><span class="s1">M: int</span><span class="s3">, </span><span class="s1">k: int</span><span class="s3">, </span><span class="s1">*</span><span class="s3">, </span><span class="s1">dtype: Optional[DTypeLike] = </span><span class="s3">None,</span>
           <span class="s1">index_dtype: DTypeLike = </span><span class="s5">'int32'</span><span class="s3">, </span><span class="s1">n_batch: int = </span><span class="s4">0</span><span class="s3">, </span><span class="s1">n_dense: int = </span><span class="s4">0</span><span class="s1">) -&gt; BCOO:</span>
    <span class="s1">n_sparse = </span><span class="s4">2 </span><span class="s1">- n_batch - n_dense</span>
    <span class="s3">if </span><span class="s1">n_sparse &lt; </span><span class="s4">0 </span><span class="s3">or </span><span class="s1">n_dense &lt; </span><span class="s4">0 </span><span class="s3">or </span><span class="s1">n_batch &lt; </span><span class="s4">0</span><span class="s1">:</span>
      <span class="s3">raise </span><span class="s1">ValueError(</span><span class="s5">f&quot;Invalid inputs: shape=</span><span class="s3">{</span><span class="s1">(N</span><span class="s3">, </span><span class="s1">M)</span><span class="s3">}</span><span class="s5">, </span><span class="s3">{</span><span class="s1">n_dense=</span><span class="s3">}</span><span class="s5">, </span><span class="s3">{</span><span class="s1">n_batch=</span><span class="s3">}</span><span class="s5">&quot;</span><span class="s1">)</span>

    <span class="s3">if </span><span class="s1">k &gt; </span><span class="s4">0</span><span class="s1">:</span>
      <span class="s1">diag_size = min(N</span><span class="s3">, </span><span class="s1">M - k)</span>
    <span class="s3">else</span><span class="s1">:</span>
      <span class="s1">diag_size = min(N + k</span><span class="s3">, </span><span class="s1">M)</span>

    <span class="s3">if </span><span class="s1">diag_size &lt;= </span><span class="s4">0</span><span class="s1">:</span>
      <span class="s0"># if k is out of range, return an empty matrix.</span>
      <span class="s3">return </span><span class="s1">cls._empty((N</span><span class="s3">, </span><span class="s1">M)</span><span class="s3">, </span><span class="s1">dtype=dtype</span><span class="s3">, </span><span class="s1">index_dtype=index_dtype</span><span class="s3">,</span>
                        <span class="s1">n_batch=n_batch</span><span class="s3">, </span><span class="s1">n_dense=n_dense)</span>

    <span class="s3">if </span><span class="s1">n_dense &gt; </span><span class="s4">0 </span><span class="s3">or </span><span class="s1">n_batch &gt; </span><span class="s4">1</span><span class="s1">:</span>
      <span class="s0"># These cases explicitly store all the zeros, so fall back to fromdense.</span>
      <span class="s3">return </span><span class="s1">cls.fromdense(jnp.eye(N</span><span class="s3">, </span><span class="s1">M</span><span class="s3">, </span><span class="s1">k</span><span class="s3">, </span><span class="s1">dtype=dtype)</span><span class="s3">,</span>
                           <span class="s1">n_batch=n_batch</span><span class="s3">, </span><span class="s1">n_dense=n_dense</span><span class="s3">,</span>
                           <span class="s1">index_dtype=index_dtype)</span>

    <span class="s3">if </span><span class="s1">n_batch == </span><span class="s4">0</span><span class="s1">:</span>
      <span class="s1">data = jnp.ones(diag_size</span><span class="s3">, </span><span class="s1">dtype=dtype)</span>
      <span class="s1">idx = jnp.arange(diag_size</span><span class="s3">, </span><span class="s1">dtype=index_dtype)</span>
      <span class="s1">zero = _const(idx</span><span class="s3">, </span><span class="s4">0</span><span class="s1">)</span>
      <span class="s1">k = _const(idx</span><span class="s3">, </span><span class="s1">k)</span>
      <span class="s1">indices = jnp.column_stack([</span>
        <span class="s1">lax.sub(idx</span><span class="s3">, </span><span class="s1">lax.cond(k &gt;= </span><span class="s4">0</span><span class="s3">, lambda</span><span class="s1">: zero</span><span class="s3">, lambda</span><span class="s1">: k))</span><span class="s3">,</span>
        <span class="s1">lax.add(idx</span><span class="s3">, </span><span class="s1">lax.cond(k &lt;= </span><span class="s4">0</span><span class="s3">, lambda</span><span class="s1">: zero</span><span class="s3">, lambda</span><span class="s1">: k))])</span>
    <span class="s3">else</span><span class="s1">:</span>
      <span class="s1">data = jnp.ones(N</span><span class="s3">, </span><span class="s1">dtype=dtype)</span>
      <span class="s1">indices = jnp.arange(N</span><span class="s3">, </span><span class="s1">dtype=index_dtype)</span>
      <span class="s1">indices = indices + _const(indices</span><span class="s3">, </span><span class="s1">k)</span>
      <span class="s3">if </span><span class="s1">k &lt; </span><span class="s4">0</span><span class="s1">:</span>
        <span class="s1">data = data.at[:abs(k)].set(</span><span class="s4">0</span><span class="s1">)</span>
        <span class="s1">indices = indices.at[:abs(k)].set(M)</span>
      <span class="s3">elif </span><span class="s1">k &gt; </span><span class="s4">0</span><span class="s1">:</span>
        <span class="s1">data = data.at[M - abs(k):].set(</span><span class="s4">0</span><span class="s1">)</span>
        <span class="s1">indices = indices.at[M - abs(k)].set(M)</span>
      <span class="s1">data = data[:</span><span class="s3">, None</span><span class="s1">]</span>
      <span class="s1">indices = indices[:</span><span class="s3">, None, None</span><span class="s1">]</span>
    <span class="s3">return </span><span class="s1">cls((data</span><span class="s3">, </span><span class="s1">indices)</span><span class="s3">, </span><span class="s1">shape=(N</span><span class="s3">, </span><span class="s1">M)</span><span class="s3">, </span><span class="s1">indices_sorted=</span><span class="s3">True,</span>
               <span class="s1">unique_indices=</span><span class="s3">True</span><span class="s1">)</span>

  <span class="s3">def </span><span class="s1">update_layout(self</span><span class="s3">, </span><span class="s1">*</span><span class="s3">, </span><span class="s1">n_batch: Optional[int] = </span><span class="s3">None, </span><span class="s1">n_dense: Optional[int] = </span><span class="s3">None,</span>
                    <span class="s1">on_inefficient: str = </span><span class="s5">'error'</span><span class="s1">) -&gt; BCOO:</span>
    <span class="s2">&quot;&quot;&quot;Update the storage layout (i.e. n_batch &amp; n_dense) of a BCOO matrix. 
 
    In many cases this can be done without introducing undue storage overhead. However, 
    increasing ``mat.n_batch`` or ``mat.n_dense`` will lead to very inefficient storage, 
    with many explicitly-stored zeros, unless the new batch or dense dimensions have size 
    0 or 1. In such cases, ``update_layout`` will raise a :class:`SparseEfficiencyError`. 
    This can be silenced by specifying the ``on_inefficient`` argument. 
 
    Args: 
      n_batch : optional(int) the number of batch dimensions in the output matrix. If None, 
        then n_batch = mat.n_batch. 
      n_dense : optional(int) the number of dense dimensions in the output matrix. If None, 
        then n_dense = mat.n_dense. 
      on_inefficient : optional(string), one of ``['error', 'warn', None]``. Specify the 
        behavior in case of an inefficient reconfiguration. This is defined as a reconfiguration 
        where the size of the resulting representation is much larger than the size of the 
        input representation. 
 
    Returns: 
      mat_out : BCOO array 
        A BCOO array representing the same sparse array as the input, with the specified 
        layout. ``mat_out.todense()`` will match ``mat.todense()`` up to appropriate precision. 
    &quot;&quot;&quot;</span>
    <span class="s3">return </span><span class="s1">bcoo_update_layout(self</span><span class="s3">, </span><span class="s1">n_batch=n_batch</span><span class="s3">, </span><span class="s1">n_dense=n_dense</span><span class="s3">, </span><span class="s1">on_inefficient=on_inefficient)</span>

  <span class="s3">def </span><span class="s1">sum_duplicates(self</span><span class="s3">, </span><span class="s1">nse: Optional[int] = </span><span class="s3">None, </span><span class="s1">remove_zeros: bool = </span><span class="s3">True</span><span class="s1">) -&gt; BCOO:</span>
    <span class="s2">&quot;&quot;&quot;Return a copy of the array with duplicate indices summed. 
 
    Additionally, this operation will result in explicit zero entries removed, and 
    indices being sorted in lexicographic order. 
 
    Because the size of the resulting representation depends on the values in the 
    arrays, this operation is not compatible with JIT or other transforms. To use 
    ``sum_duplicates`` in such cases, you may pass a value to `nse` to specify the 
    desired size of the output representation. 
 
    Args: 
      nse : integer (optional), if specified, gives the number of specified elements in 
        the output sparse representation; if it is larger than the number required, data 
        will be padded with zeros and indices will be padded with out-of-bounds values. 
        If it is smaller than the number required, data will be silently discarded. 
      remove_zeros : bool (default=True). If True, remove explicit zeros from the data 
        as part of summing duplicates. If False, then explicit zeros at unique indices 
        will remain among the specified elements. Note: remove_zeros=True is incompatible 
        with autodiff. 
    &quot;&quot;&quot;</span>
    <span class="s3">if </span><span class="s1">remove_zeros:</span>
      <span class="s3">return </span><span class="s1">bcoo_eliminate_zeros(self</span><span class="s3">, </span><span class="s1">nse=nse)</span>
    <span class="s3">else</span><span class="s1">:</span>
      <span class="s3">return </span><span class="s1">bcoo_sum_duplicates(self</span><span class="s3">, </span><span class="s1">nse=nse)</span>

  <span class="s3">def </span><span class="s1">sort_indices(self) -&gt; BCOO:</span>
    <span class="s2">&quot;&quot;&quot;Return a copy of the matrix with indices sorted.&quot;&quot;&quot;</span>
    <span class="s3">return </span><span class="s1">bcoo_sort_indices(self)</span>

  <span class="s3">def </span><span class="s1">todense(self) -&gt; Array:</span>
    <span class="s2">&quot;&quot;&quot;Create a dense version of the array.&quot;&quot;&quot;</span>
    <span class="s3">return </span><span class="s1">bcoo_todense(self)</span>

  <span class="s3">def </span><span class="s1">transpose(self</span><span class="s3">, </span><span class="s1">axes: Optional[Sequence[int]] = </span><span class="s3">None</span><span class="s1">) -&gt; BCOO:</span>
    <span class="s2">&quot;&quot;&quot;Create a new array containing the transpose.&quot;&quot;&quot;</span>
    <span class="s1">perm: List[int] = list(range(self.ndim)[::-</span><span class="s4">1</span><span class="s1">] </span><span class="s3">if </span><span class="s1">axes </span><span class="s3">is None else </span><span class="s1">axes)</span>
    <span class="s1">mat_T = bcoo_transpose(self</span><span class="s3">, </span><span class="s1">permutation=perm)</span>
    <span class="s1">shape_T = tuple(self.shape[i] </span><span class="s3">for </span><span class="s1">i </span><span class="s3">in </span><span class="s1">perm)</span>
    <span class="s1">sparse_perm = [p - self.n_batch</span>
                   <span class="s3">for </span><span class="s1">p </span><span class="s3">in </span><span class="s1">perm[self.n_batch: self.n_batch + self.n_sparse]]</span>
    <span class="s3">if </span><span class="s1">tuple(sparse_perm) == tuple(range(self.n_sparse)):</span>
      <span class="s1">is_sorted = self.indices_sorted</span>
    <span class="s3">else</span><span class="s1">:</span>
      <span class="s0"># TODO: address the corner cases that the transposed indices are sorted.</span>
      <span class="s0"># possibly use permutation?</span>
      <span class="s1">is_sorted = </span><span class="s3">False</span>
    <span class="s3">return </span><span class="s1">BCOO((mat_T.data</span><span class="s3">, </span><span class="s1">mat_T.indices)</span><span class="s3">, </span><span class="s1">shape=shape_T</span><span class="s3">,</span>
                <span class="s1">indices_sorted=is_sorted</span><span class="s3">, </span><span class="s1">unique_indices=self.unique_indices)</span>

  <span class="s3">def </span><span class="s1">tree_flatten(self):</span>
    <span class="s3">return </span><span class="s1">(self.data</span><span class="s3">, </span><span class="s1">self.indices)</span><span class="s3">, </span><span class="s1">self._info._asdict()</span>

  <span class="s1">@classmethod</span>
  <span class="s3">def </span><span class="s1">tree_unflatten(cls</span><span class="s3">, </span><span class="s1">aux_data</span><span class="s3">, </span><span class="s1">children):</span>
    <span class="s1">obj = object.__new__(cls)</span>
    <span class="s1">obj.data</span><span class="s3">, </span><span class="s1">obj.indices = children</span>
    <span class="s3">if </span><span class="s1">aux_data.keys() != {</span><span class="s5">'shape'</span><span class="s3">, </span><span class="s5">'indices_sorted'</span><span class="s3">, </span><span class="s5">'unique_indices'</span><span class="s1">}:</span>
      <span class="s3">raise </span><span class="s1">ValueError(</span><span class="s5">f&quot;BCOO.tree_unflatten: invalid </span><span class="s3">{</span><span class="s1">aux_data=</span><span class="s3">}</span><span class="s5">&quot;</span><span class="s1">)</span>
    <span class="s1">obj.__dict__.update(**aux_data)</span>
    <span class="s3">return </span><span class="s1">obj</span>


<span class="s0"># vmappable handlers</span>
<span class="s3">def </span><span class="s1">_bcoo_to_elt(cont</span><span class="s3">, </span><span class="s1">_</span><span class="s3">, </span><span class="s1">val</span><span class="s3">, </span><span class="s1">axis):</span>
  <span class="s3">if </span><span class="s1">axis </span><span class="s3">is None</span><span class="s1">:</span>
    <span class="s3">return </span><span class="s1">val</span>
  <span class="s3">if </span><span class="s1">axis &gt;= val.n_batch:</span>
    <span class="s3">raise </span><span class="s1">ValueError(</span><span class="s5">f&quot;Cannot map in_axis=</span><span class="s3">{</span><span class="s1">axis</span><span class="s3">} </span><span class="s5">for BCOO array with n_batch=</span><span class="s3">{</span><span class="s1">val.n_batch</span><span class="s3">}</span><span class="s5">. &quot;</span>
                     <span class="s5">&quot;in_axes for batched BCOO operations must correspond to a batch dimension.&quot;</span><span class="s1">)</span>
  <span class="s3">return </span><span class="s1">BCOO((cont(val.data</span><span class="s3">, </span><span class="s1">axis)</span><span class="s3">, </span><span class="s1">cont(val.indices</span><span class="s3">, </span><span class="s1">axis))</span><span class="s3">,</span>
              <span class="s1">shape=val.shape[:axis] + val.shape[axis + </span><span class="s4">1</span><span class="s1">:]</span><span class="s3">,</span>
              <span class="s1">indices_sorted=val.indices_sorted</span><span class="s3">, </span><span class="s1">unique_indices=val.unique_indices)</span>

<span class="s3">def </span><span class="s1">_bcoo_from_elt(cont</span><span class="s3">, </span><span class="s1">axis_size</span><span class="s3">, </span><span class="s1">elt</span><span class="s3">, </span><span class="s1">axis):</span>
  <span class="s3">if </span><span class="s1">axis </span><span class="s3">is None</span><span class="s1">:</span>
    <span class="s3">return </span><span class="s1">elt</span>
  <span class="s3">if </span><span class="s1">axis &gt; elt.n_batch:</span>
    <span class="s3">raise </span><span class="s1">ValueError(</span><span class="s5">f&quot;BCOO: cannot add out_axis=</span><span class="s3">{</span><span class="s1">axis</span><span class="s3">} </span><span class="s5">for BCOO array with n_batch=</span><span class="s3">{</span><span class="s1">elt.n_batch</span><span class="s3">}</span><span class="s5">. &quot;</span>
                     <span class="s5">&quot;BCOO batch axes must be a contiguous block of leading dimensions.&quot;</span><span class="s1">)</span>
  <span class="s3">return </span><span class="s1">BCOO((cont(axis_size</span><span class="s3">, </span><span class="s1">elt.data</span><span class="s3">, </span><span class="s1">axis)</span><span class="s3">, </span><span class="s1">cont(axis_size</span><span class="s3">, </span><span class="s1">elt.indices</span><span class="s3">, </span><span class="s1">axis))</span><span class="s3">,</span>
              <span class="s1">shape=elt.shape[:axis] + (axis_size</span><span class="s3">,</span><span class="s1">) + elt.shape[axis:]</span><span class="s3">,</span>
              <span class="s1">indices_sorted=elt.indices_sorted</span><span class="s3">, </span><span class="s1">unique_indices=elt.unique_indices)</span>

<span class="s1">batching.register_vmappable(BCOO</span><span class="s3">, </span><span class="s1">int</span><span class="s3">, </span><span class="s1">int</span><span class="s3">, </span><span class="s1">_bcoo_to_elt</span><span class="s3">, </span><span class="s1">_bcoo_from_elt</span><span class="s3">, None</span><span class="s1">)</span>
</pre>
</body>
</html>