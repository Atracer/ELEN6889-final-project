<html>
<head>
<title>public_test_util.py</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<style type="text/css">
.s0 { color: #808080;}
.s1 { color: #a9b7c6;}
.s2 { color: #cc7832;}
.s3 { color: #6a8759;}
.s4 { color: #6897bb;}
.s5 { color: #629755; font-style: italic;}
</style>
</head>
<body bgcolor="#2b2b2b">
<table CELLSPACING=0 CELLPADDING=5 COLS=1 WIDTH="100%" BGCOLOR="#606060" >
<tr><td><center>
<font face="Arial, Helvetica" color="#000000">
public_test_util.py</font>
</center></td></tr></table>
<pre><span class="s0"># Copyright 2022 The JAX Authors.</span>
<span class="s0">#</span>
<span class="s0"># Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);</span>
<span class="s0"># you may not use this file except in compliance with the License.</span>
<span class="s0"># You may obtain a copy of the License at</span>
<span class="s0">#</span>
<span class="s0">#     https://www.apache.org/licenses/LICENSE-2.0</span>
<span class="s0">#</span>
<span class="s0"># Unless required by applicable law or agreed to in writing, software</span>
<span class="s0"># distributed under the License is distributed on an &quot;AS IS&quot; BASIS,</span>
<span class="s0"># WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span>
<span class="s0"># See the License for the specific language governing permissions and</span>
<span class="s0"># limitations under the License.</span>

<span class="s2">from </span><span class="s1">functools </span><span class="s2">import </span><span class="s1">partial</span>
<span class="s2">import </span><span class="s1">operator</span>

<span class="s2">from </span><span class="s1">jax </span><span class="s2">import </span><span class="s1">config</span>
<span class="s2">from </span><span class="s1">jax.tree_util </span><span class="s2">import </span><span class="s1">tree_map</span><span class="s2">, </span><span class="s1">tree_reduce</span>
<span class="s2">from </span><span class="s1">jax._src </span><span class="s2">import </span><span class="s1">api</span>
<span class="s2">from </span><span class="s1">jax._src </span><span class="s2">import </span><span class="s1">dtypes </span><span class="s2">as </span><span class="s1">_dtypes</span>
<span class="s2">from </span><span class="s1">jax._src </span><span class="s2">import </span><span class="s1">xla_bridge</span>
<span class="s2">from </span><span class="s1">jax._src.config </span><span class="s2">import </span><span class="s1">flags</span>
<span class="s2">from </span><span class="s1">jax._src.lib </span><span class="s2">import </span><span class="s1">xla_client</span>
<span class="s2">import </span><span class="s1">numpy </span><span class="s2">as </span><span class="s1">np</span>


<span class="s0"># The only functions intended to be exported are these; they should be used via</span>
<span class="s0"># jax.test_util. All other functionality appearing here is for internal use only,</span>
<span class="s0"># and may be changed or removed at any time and without any deprecation cycle.</span>
<span class="s1">__all__ = [</span><span class="s3">'check_grads'</span><span class="s2">, </span><span class="s3">'check_jvp'</span><span class="s2">, </span><span class="s3">'check_vjp'</span><span class="s1">]</span>


<span class="s1">FLAGS = flags.FLAGS</span>

<span class="s1">EPS = </span><span class="s4">1e-4</span>
<span class="s1">_fp8_enabled = xla_client._version &gt;= </span><span class="s4">117</span>


<span class="s2">def </span><span class="s1">_dtype(x):</span>
  <span class="s2">if </span><span class="s1">hasattr(x</span><span class="s2">, </span><span class="s3">'dtype'</span><span class="s1">):</span>
    <span class="s2">return </span><span class="s1">x.dtype</span>
  <span class="s2">elif </span><span class="s1">type(x) </span><span class="s2">in </span><span class="s1">_dtypes.python_scalar_dtypes:</span>
    <span class="s2">return </span><span class="s1">np.dtype(_dtypes.python_scalar_dtypes[type(x)])</span>
  <span class="s2">else</span><span class="s1">:</span>
    <span class="s2">return </span><span class="s1">np.asarray(x).dtype</span>


<span class="s1">_default_tolerance = {</span>
  <span class="s1">_dtypes.float0: </span><span class="s4">0</span><span class="s2">,</span>
  <span class="s1">np.dtype(np.bool_): </span><span class="s4">0</span><span class="s2">,</span>
  <span class="s1">np.dtype(np.int8): </span><span class="s4">0</span><span class="s2">,</span>
  <span class="s1">np.dtype(np.int16): </span><span class="s4">0</span><span class="s2">,</span>
  <span class="s1">np.dtype(np.int32): </span><span class="s4">0</span><span class="s2">,</span>
  <span class="s1">np.dtype(np.int64): </span><span class="s4">0</span><span class="s2">,</span>
  <span class="s1">np.dtype(np.uint8): </span><span class="s4">0</span><span class="s2">,</span>
  <span class="s1">np.dtype(np.uint16): </span><span class="s4">0</span><span class="s2">,</span>
  <span class="s1">np.dtype(np.uint32): </span><span class="s4">0</span><span class="s2">,</span>
  <span class="s1">np.dtype(np.uint64): </span><span class="s4">0</span><span class="s2">,</span>
  <span class="s1">np.dtype(_dtypes.bfloat16): </span><span class="s4">1e-2</span><span class="s2">,</span>
  <span class="s1">np.dtype(np.float16): </span><span class="s4">1e-3</span><span class="s2">,</span>
  <span class="s1">np.dtype(np.float32): </span><span class="s4">1e-6</span><span class="s2">,</span>
  <span class="s1">np.dtype(np.float64): </span><span class="s4">1e-15</span><span class="s2">,</span>
  <span class="s1">np.dtype(np.complex64): </span><span class="s4">1e-6</span><span class="s2">,</span>
  <span class="s1">np.dtype(np.complex128): </span><span class="s4">1e-15</span><span class="s2">,</span>
<span class="s1">}</span>
<span class="s2">if </span><span class="s1">_fp8_enabled:</span>
  <span class="s1">_default_tolerance.update({</span>
    <span class="s1">np.dtype(_dtypes.float8_e4m3fn): </span><span class="s4">1e-1</span><span class="s2">,</span>
    <span class="s1">np.dtype(_dtypes.float8_e5m2): </span><span class="s4">1e-1</span><span class="s2">,</span>
  <span class="s1">})</span>


<span class="s2">def </span><span class="s1">default_tolerance():</span>
  <span class="s2">if </span><span class="s1">device_under_test() != </span><span class="s3">&quot;tpu&quot;</span><span class="s1">:</span>
    <span class="s2">return </span><span class="s1">_default_tolerance</span>
  <span class="s1">tol = _default_tolerance.copy()</span>
  <span class="s1">tol[np.dtype(np.float32)] = </span><span class="s4">1e-3</span>
  <span class="s1">tol[np.dtype(np.complex64)] = </span><span class="s4">1e-3</span>
  <span class="s2">return </span><span class="s1">tol</span>


<span class="s1">default_gradient_tolerance = {</span>
  <span class="s1">np.dtype(_dtypes.bfloat16): </span><span class="s4">1e-1</span><span class="s2">,</span>
  <span class="s1">np.dtype(np.float16): </span><span class="s4">1e-2</span><span class="s2">,</span>
  <span class="s1">np.dtype(np.float32): </span><span class="s4">2e-3</span><span class="s2">,</span>
  <span class="s1">np.dtype(np.float64): </span><span class="s4">1e-5</span><span class="s2">,</span>
  <span class="s1">np.dtype(np.complex64): </span><span class="s4">1e-3</span><span class="s2">,</span>
  <span class="s1">np.dtype(np.complex128): </span><span class="s4">1e-5</span><span class="s2">,</span>
<span class="s1">}</span>
<span class="s2">if </span><span class="s1">_fp8_enabled:</span>
  <span class="s1">default_gradient_tolerance.update({</span>
    <span class="s1">np.dtype(_dtypes.float8_e4m3fn): </span><span class="s4">1e-1</span><span class="s2">,</span>
    <span class="s1">np.dtype(_dtypes.float8_e5m2): </span><span class="s4">1e-1</span><span class="s2">,</span>
  <span class="s1">})</span>

<span class="s2">def </span><span class="s1">is_python_scalar(val):</span>
  <span class="s2">return not </span><span class="s1">isinstance(val</span><span class="s2">, </span><span class="s1">np.generic) </span><span class="s2">and </span><span class="s1">isinstance(val</span><span class="s2">, </span><span class="s1">(bool</span><span class="s2">, </span><span class="s1">int</span><span class="s2">, </span><span class="s1">float</span><span class="s2">, </span><span class="s1">complex))</span>

<span class="s2">def </span><span class="s1">_assert_numpy_allclose(a</span><span class="s2">, </span><span class="s1">b</span><span class="s2">, </span><span class="s1">atol=</span><span class="s2">None, </span><span class="s1">rtol=</span><span class="s2">None, </span><span class="s1">err_msg=</span><span class="s3">''</span><span class="s1">):</span>
  <span class="s2">if </span><span class="s1">a.dtype == b.dtype == _dtypes.float0:</span>
    <span class="s1">np.testing.assert_array_equal(a</span><span class="s2">, </span><span class="s1">b</span><span class="s2">, </span><span class="s1">err_msg=err_msg)</span>
    <span class="s2">return</span>
  <span class="s2">if </span><span class="s1">_fp8_enabled:</span>
    <span class="s1">custom_dtypes = [_dtypes.float8_e4m3fn</span><span class="s2">, </span><span class="s1">_dtypes.float8_e5m2</span><span class="s2">, </span><span class="s1">_dtypes.bfloat16]</span>
  <span class="s2">else</span><span class="s1">:</span>
    <span class="s1">custom_dtypes = [_dtypes.bfloat16]</span>
  <span class="s1">a = a.astype(np.float32) </span><span class="s2">if </span><span class="s1">a.dtype </span><span class="s2">in </span><span class="s1">custom_dtypes </span><span class="s2">else </span><span class="s1">a</span>
  <span class="s1">b = b.astype(np.float32) </span><span class="s2">if </span><span class="s1">b.dtype </span><span class="s2">in </span><span class="s1">custom_dtypes </span><span class="s2">else </span><span class="s1">b</span>
  <span class="s1">kw = {}</span>
  <span class="s2">if </span><span class="s1">atol: kw[</span><span class="s3">&quot;atol&quot;</span><span class="s1">] = atol</span>
  <span class="s2">if </span><span class="s1">rtol: kw[</span><span class="s3">&quot;rtol&quot;</span><span class="s1">] = rtol</span>
  <span class="s2">with </span><span class="s1">np.errstate(invalid=</span><span class="s3">'ignore'</span><span class="s1">):</span>
    <span class="s0"># TODO(phawkins): surprisingly, assert_allclose sometimes reports invalid</span>
    <span class="s0"># value errors. It should not do that.</span>
    <span class="s1">np.testing.assert_allclose(a</span><span class="s2">, </span><span class="s1">b</span><span class="s2">, </span><span class="s1">**kw</span><span class="s2">, </span><span class="s1">err_msg=err_msg)</span>

<span class="s2">def </span><span class="s1">tolerance(dtype</span><span class="s2">, </span><span class="s1">tol=</span><span class="s2">None</span><span class="s1">):</span>
  <span class="s1">tol = {} </span><span class="s2">if </span><span class="s1">tol </span><span class="s2">is None else </span><span class="s1">tol</span>
  <span class="s2">if not </span><span class="s1">isinstance(tol</span><span class="s2">, </span><span class="s1">dict):</span>
    <span class="s2">return </span><span class="s1">tol</span>
  <span class="s1">tol = {np.dtype(key): value </span><span class="s2">for </span><span class="s1">key</span><span class="s2">, </span><span class="s1">value </span><span class="s2">in </span><span class="s1">tol.items()}</span>
  <span class="s1">dtype = _dtypes.canonicalize_dtype(np.dtype(dtype))</span>
  <span class="s2">return </span><span class="s1">tol.get(dtype</span><span class="s2">, </span><span class="s1">default_tolerance()[dtype])</span>


<span class="s2">def </span><span class="s1">_assert_numpy_close(a</span><span class="s2">, </span><span class="s1">b</span><span class="s2">, </span><span class="s1">atol=</span><span class="s2">None, </span><span class="s1">rtol=</span><span class="s2">None, </span><span class="s1">err_msg=</span><span class="s3">''</span><span class="s1">):</span>
  <span class="s1">a</span><span class="s2">, </span><span class="s1">b = np.asarray(a)</span><span class="s2">, </span><span class="s1">np.asarray(b)</span>
  <span class="s2">assert </span><span class="s1">a.shape == b.shape</span>
  <span class="s1">atol = max(tolerance(a.dtype</span><span class="s2">, </span><span class="s1">atol)</span><span class="s2">, </span><span class="s1">tolerance(b.dtype</span><span class="s2">, </span><span class="s1">atol))</span>
  <span class="s1">rtol = max(tolerance(a.dtype</span><span class="s2">, </span><span class="s1">rtol)</span><span class="s2">, </span><span class="s1">tolerance(b.dtype</span><span class="s2">, </span><span class="s1">rtol))</span>
  <span class="s1">_assert_numpy_allclose(a</span><span class="s2">, </span><span class="s1">b</span><span class="s2">, </span><span class="s1">atol=atol * a.size</span><span class="s2">, </span><span class="s1">rtol=rtol * b.size</span><span class="s2">,</span>
                         <span class="s1">err_msg=err_msg)</span>


<span class="s2">def </span><span class="s1">check_close(xs</span><span class="s2">, </span><span class="s1">ys</span><span class="s2">, </span><span class="s1">atol=</span><span class="s2">None, </span><span class="s1">rtol=</span><span class="s2">None, </span><span class="s1">err_msg=</span><span class="s3">''</span><span class="s1">):</span>
  <span class="s1">assert_close = partial(_assert_numpy_close</span><span class="s2">, </span><span class="s1">atol=atol</span><span class="s2">, </span><span class="s1">rtol=rtol</span><span class="s2">,</span>
                         <span class="s1">err_msg=err_msg)</span>
  <span class="s1">tree_map(assert_close</span><span class="s2">, </span><span class="s1">xs</span><span class="s2">, </span><span class="s1">ys)</span>


<span class="s2">def </span><span class="s1">_check_dtypes_match(xs</span><span class="s2">, </span><span class="s1">ys):</span>
  <span class="s2">def </span><span class="s1">_assert_dtypes_match(x</span><span class="s2">, </span><span class="s1">y):</span>
    <span class="s2">if </span><span class="s1">config.x64_enabled:</span>
      <span class="s2">assert </span><span class="s1">_dtype(x) == _dtype(y)</span>
    <span class="s2">else</span><span class="s1">:</span>
      <span class="s2">assert </span><span class="s1">(_dtypes.canonicalize_dtype(_dtype(x)) ==</span>
              <span class="s1">_dtypes.canonicalize_dtype(_dtype(y)))</span>
  <span class="s1">tree_map(_assert_dtypes_match</span><span class="s2">, </span><span class="s1">xs</span><span class="s2">, </span><span class="s1">ys)</span>


<span class="s2">def </span><span class="s1">inner_prod(xs</span><span class="s2">, </span><span class="s1">ys):</span>
  <span class="s2">def </span><span class="s1">contract(x</span><span class="s2">, </span><span class="s1">y):</span>
    <span class="s2">return </span><span class="s1">np.real(np.dot(np.conj(x).reshape(-</span><span class="s4">1</span><span class="s1">)</span><span class="s2">, </span><span class="s1">y.reshape(-</span><span class="s4">1</span><span class="s1">)))</span>
  <span class="s2">return </span><span class="s1">tree_reduce(np.add</span><span class="s2">, </span><span class="s1">tree_map(contract</span><span class="s2">, </span><span class="s1">xs</span><span class="s2">, </span><span class="s1">ys))</span>


<span class="s2">def </span><span class="s1">_safe_subtract(x</span><span class="s2">, </span><span class="s1">y</span><span class="s2">, </span><span class="s1">*</span><span class="s2">, </span><span class="s1">dtype):</span>
  <span class="s5">&quot;&quot;&quot;Subtraction that with `inf - inf == 0` semantics.&quot;&quot;&quot;</span>
  <span class="s2">with </span><span class="s1">np.errstate(invalid=</span><span class="s3">'ignore'</span><span class="s1">):</span>
    <span class="s2">return </span><span class="s1">np.where(np.equal(x</span><span class="s2">, </span><span class="s1">y)</span><span class="s2">, </span><span class="s1">np.array(</span><span class="s4">0</span><span class="s2">, </span><span class="s1">dtype)</span><span class="s2">,</span>
                    <span class="s1">np.subtract(x</span><span class="s2">, </span><span class="s1">y</span><span class="s2">, </span><span class="s1">dtype=dtype))</span>

<span class="s2">def </span><span class="s1">_preserve_input_types(f):</span>
  <span class="s2">def </span><span class="s1">wrapped(*args):</span>
    <span class="s1">dtype = _dtype(args[</span><span class="s4">0</span><span class="s1">])</span>
    <span class="s1">result = np.array(f(*args)</span><span class="s2">, </span><span class="s1">dtype=dtype)</span>
    <span class="s2">if </span><span class="s1">all(is_python_scalar(arg) </span><span class="s2">for </span><span class="s1">arg </span><span class="s2">in </span><span class="s1">args):</span>
      <span class="s1">result = result.item()</span>
    <span class="s2">return </span><span class="s1">result</span>
  <span class="s2">return </span><span class="s1">wrapped</span>


<span class="s1">add = partial(tree_map</span><span class="s2">, </span><span class="s1">_preserve_input_types(operator.add))</span>
<span class="s1">sub = partial(tree_map</span><span class="s2">, </span><span class="s1">_preserve_input_types(operator.sub))</span>
<span class="s1">safe_sub = partial(tree_map</span><span class="s2">,</span>
                   <span class="s2">lambda </span><span class="s1">x</span><span class="s2">, </span><span class="s1">y: _safe_subtract(x</span><span class="s2">, </span><span class="s1">y</span><span class="s2">, </span><span class="s1">dtype=_dtype(x)))</span>
<span class="s1">conj = partial(tree_map</span><span class="s2">, </span><span class="s1">_preserve_input_types(np.conj))</span>


<span class="s2">def </span><span class="s1">scalar_mul(xs</span><span class="s2">, </span><span class="s1">a):</span>
  <span class="s2">def </span><span class="s1">mul(x):</span>
    <span class="s1">dtype = _dtype(x)</span>
    <span class="s1">result = np.multiply(x</span><span class="s2">, </span><span class="s1">np.array(a</span><span class="s2">, </span><span class="s1">dtype=dtype)</span><span class="s2">, </span><span class="s1">dtype=dtype)</span>
    <span class="s2">return </span><span class="s1">result.item() </span><span class="s2">if </span><span class="s1">is_python_scalar(x) </span><span class="s2">else </span><span class="s1">result</span>
  <span class="s2">return </span><span class="s1">tree_map(mul</span><span class="s2">, </span><span class="s1">xs)</span>


<span class="s2">def </span><span class="s1">rand_like(rng</span><span class="s2">, </span><span class="s1">x):</span>
  <span class="s1">shape = np.shape(x)</span>
  <span class="s1">dtype = _dtype(x)</span>
  <span class="s1">randn = </span><span class="s2">lambda</span><span class="s1">: np.asarray(rng.randn(*shape)</span><span class="s2">, </span><span class="s1">dtype=dtype)</span>
  <span class="s2">if </span><span class="s1">_dtypes.issubdtype(dtype</span><span class="s2">, </span><span class="s1">np.complexfloating):</span>
    <span class="s1">result = randn() + dtype.type(</span><span class="s4">1.0j</span><span class="s1">) * randn()</span>
  <span class="s2">else</span><span class="s1">:</span>
    <span class="s1">result = randn()</span>
  <span class="s2">return </span><span class="s1">result.item() </span><span class="s2">if </span><span class="s1">is_python_scalar(x) </span><span class="s2">else </span><span class="s1">result</span>


<span class="s2">def </span><span class="s1">numerical_jvp(f</span><span class="s2">, </span><span class="s1">primals</span><span class="s2">, </span><span class="s1">tangents</span><span class="s2">, </span><span class="s1">eps=EPS):</span>
  <span class="s1">delta = scalar_mul(tangents</span><span class="s2">, </span><span class="s1">eps)</span>
  <span class="s1">f_pos = f(*add(primals</span><span class="s2">, </span><span class="s1">delta))</span>
  <span class="s1">f_neg = f(*sub(primals</span><span class="s2">, </span><span class="s1">delta))</span>
  <span class="s2">return </span><span class="s1">scalar_mul(safe_sub(f_pos</span><span class="s2">, </span><span class="s1">f_neg)</span><span class="s2">, </span><span class="s4">0.5 </span><span class="s1">/ eps)</span>


<span class="s2">def </span><span class="s1">_merge_tolerance(tol</span><span class="s2">, </span><span class="s1">default):</span>
  <span class="s2">if </span><span class="s1">tol </span><span class="s2">is None</span><span class="s1">:</span>
    <span class="s2">return </span><span class="s1">default</span>
  <span class="s2">if not </span><span class="s1">isinstance(tol</span><span class="s2">, </span><span class="s1">dict):</span>
    <span class="s2">return </span><span class="s1">tol</span>
  <span class="s1">out = default.copy()</span>
  <span class="s2">for </span><span class="s1">k</span><span class="s2">, </span><span class="s1">v </span><span class="s2">in </span><span class="s1">tol.items():</span>
    <span class="s1">out[np.dtype(k)] = v</span>
  <span class="s2">return </span><span class="s1">out</span>


<span class="s2">def </span><span class="s1">check_jvp(f</span><span class="s2">, </span><span class="s1">f_jvp</span><span class="s2">, </span><span class="s1">args</span><span class="s2">, </span><span class="s1">atol=</span><span class="s2">None, </span><span class="s1">rtol=</span><span class="s2">None, </span><span class="s1">eps=EPS</span><span class="s2">, </span><span class="s1">err_msg=</span><span class="s3">''</span><span class="s1">):</span>
  <span class="s1">atol = _merge_tolerance(atol</span><span class="s2">, </span><span class="s1">default_gradient_tolerance)</span>
  <span class="s1">rtol = _merge_tolerance(rtol</span><span class="s2">, </span><span class="s1">default_gradient_tolerance)</span>
  <span class="s1">rng = np.random.RandomState(</span><span class="s4">0</span><span class="s1">)</span>
  <span class="s1">tangent = tree_map(partial(rand_like</span><span class="s2">, </span><span class="s1">rng)</span><span class="s2">, </span><span class="s1">args)</span>
  <span class="s1">v_out</span><span class="s2">, </span><span class="s1">t_out = f_jvp(args</span><span class="s2">, </span><span class="s1">tangent)</span>
  <span class="s1">_check_dtypes_match(v_out</span><span class="s2">, </span><span class="s1">t_out)</span>
  <span class="s1">v_out_expected = f(*args)</span>
  <span class="s1">_check_dtypes_match(v_out</span><span class="s2">, </span><span class="s1">v_out_expected)</span>
  <span class="s1">t_out_expected = numerical_jvp(f</span><span class="s2">, </span><span class="s1">args</span><span class="s2">, </span><span class="s1">tangent</span><span class="s2">, </span><span class="s1">eps=eps)</span>
  <span class="s0"># In principle we should expect exact equality of v_out and v_out_expected,</span>
  <span class="s0"># but due to nondeterminism especially on GPU (e.g., due to convolution</span>
  <span class="s0"># autotuning) we only require &quot;close&quot;.</span>
  <span class="s1">check_close(v_out</span><span class="s2">, </span><span class="s1">v_out_expected</span><span class="s2">, </span><span class="s1">atol=atol</span><span class="s2">, </span><span class="s1">rtol=rtol</span><span class="s2">,</span>
              <span class="s1">err_msg=</span><span class="s3">f'</span><span class="s2">{</span><span class="s1">err_msg</span><span class="s2">} </span><span class="s3">primal' </span><span class="s2">if </span><span class="s1">err_msg </span><span class="s2">else </span><span class="s3">'primal'</span><span class="s1">)</span>
  <span class="s1">check_close(t_out</span><span class="s2">, </span><span class="s1">t_out_expected</span><span class="s2">, </span><span class="s1">atol=atol</span><span class="s2">, </span><span class="s1">rtol=rtol</span><span class="s2">,</span>
              <span class="s1">err_msg=</span><span class="s3">f'</span><span class="s2">{</span><span class="s1">err_msg</span><span class="s2">} </span><span class="s3">tangent' </span><span class="s2">if </span><span class="s1">err_msg </span><span class="s2">else </span><span class="s3">'tangent'</span><span class="s1">)</span>


<span class="s2">def </span><span class="s1">check_vjp(f</span><span class="s2">, </span><span class="s1">f_vjp</span><span class="s2">, </span><span class="s1">args</span><span class="s2">, </span><span class="s1">atol=</span><span class="s2">None, </span><span class="s1">rtol=</span><span class="s2">None, </span><span class="s1">eps=EPS</span><span class="s2">, </span><span class="s1">err_msg=</span><span class="s3">''</span><span class="s1">):</span>
  <span class="s1">atol = _merge_tolerance(atol</span><span class="s2">, </span><span class="s1">default_gradient_tolerance)</span>
  <span class="s1">rtol = _merge_tolerance(rtol</span><span class="s2">, </span><span class="s1">default_gradient_tolerance)</span>
  <span class="s1">_rand_like = partial(rand_like</span><span class="s2">, </span><span class="s1">np.random.RandomState(</span><span class="s4">0</span><span class="s1">))</span>
  <span class="s1">v_out</span><span class="s2">, </span><span class="s1">vjpfun = f_vjp(*args)</span>
  <span class="s1">v_out_expected = f(*args)</span>
  <span class="s1">check_close(v_out</span><span class="s2">, </span><span class="s1">v_out_expected</span><span class="s2">, </span><span class="s1">atol=atol</span><span class="s2">, </span><span class="s1">rtol=rtol</span><span class="s2">,</span>
              <span class="s1">err_msg=</span><span class="s3">f'</span><span class="s2">{</span><span class="s1">err_msg</span><span class="s2">} </span><span class="s3">primal' </span><span class="s2">if </span><span class="s1">err_msg </span><span class="s2">else </span><span class="s3">'primal'</span><span class="s1">)</span>
  <span class="s1">tangent = tree_map(_rand_like</span><span class="s2">, </span><span class="s1">args)</span>
  <span class="s1">tangent_out = numerical_jvp(f</span><span class="s2">, </span><span class="s1">args</span><span class="s2">, </span><span class="s1">tangent</span><span class="s2">, </span><span class="s1">eps=eps)</span>
  <span class="s1">cotangent = tree_map(_rand_like</span><span class="s2">, </span><span class="s1">v_out)</span>
  <span class="s1">cotangent_out = conj(vjpfun(conj(cotangent)))</span>
  <span class="s1">ip = inner_prod(tangent</span><span class="s2">, </span><span class="s1">cotangent_out)</span>
  <span class="s1">ip_expected = inner_prod(tangent_out</span><span class="s2">, </span><span class="s1">cotangent)</span>
  <span class="s1">check_close(ip</span><span class="s2">, </span><span class="s1">ip_expected</span><span class="s2">, </span><span class="s1">atol=atol</span><span class="s2">, </span><span class="s1">rtol=rtol</span><span class="s2">,</span>
              <span class="s1">err_msg=(</span><span class="s3">f'</span><span class="s2">{</span><span class="s1">err_msg</span><span class="s2">} </span><span class="s3">cotangent projection'</span>
                       <span class="s2">if </span><span class="s1">err_msg </span><span class="s2">else </span><span class="s3">'cotangent projection'</span><span class="s1">))</span>


<span class="s2">def </span><span class="s1">check_grads(f</span><span class="s2">, </span><span class="s1">args</span><span class="s2">, </span><span class="s1">order</span><span class="s2">,</span>
                <span class="s1">modes=(</span><span class="s3">&quot;fwd&quot;</span><span class="s2">, </span><span class="s3">&quot;rev&quot;</span><span class="s1">)</span><span class="s2">, </span><span class="s1">atol=</span><span class="s2">None, </span><span class="s1">rtol=</span><span class="s2">None, </span><span class="s1">eps=</span><span class="s2">None</span><span class="s1">):</span>
  <span class="s5">&quot;&quot;&quot;Check gradients from automatic differentiation against finite differences. 
 
  Gradients are only checked in a single randomly chosen direction, which 
  ensures that the finite difference calculation does not become prohibitively 
  expensive even for large input/output spaces. 
 
  Args: 
    f: function to check at ``f(*args)``. 
    args: tuple of argument values. 
    order: forward and backwards gradients up to this order are checked. 
    modes: lists of gradient modes to check ('fwd' and/or 'rev'). 
    atol: absolute tolerance for gradient equality. 
    rtol: relative tolerance for gradient equality. 
    eps: step size used for finite differences. 
 
  Raises: 
    AssertionError: if gradients do not match. 
  &quot;&quot;&quot;</span>
  <span class="s1">args = tuple(args)</span>
  <span class="s1">eps = eps </span><span class="s2">or </span><span class="s1">EPS</span>

  <span class="s1">_check_jvp = partial(check_jvp</span><span class="s2">, </span><span class="s1">atol=atol</span><span class="s2">, </span><span class="s1">rtol=rtol</span><span class="s2">, </span><span class="s1">eps=eps)</span>
  <span class="s1">_check_vjp = partial(check_vjp</span><span class="s2">, </span><span class="s1">atol=atol</span><span class="s2">, </span><span class="s1">rtol=rtol</span><span class="s2">, </span><span class="s1">eps=eps)</span>

  <span class="s2">def </span><span class="s1">_check_grads(f</span><span class="s2">, </span><span class="s1">args</span><span class="s2">, </span><span class="s1">order</span><span class="s2">, </span><span class="s1">err_msg=</span><span class="s3">''</span><span class="s1">):</span>
    <span class="s2">if </span><span class="s3">&quot;fwd&quot; </span><span class="s2">in </span><span class="s1">modes:</span>
      <span class="s1">fwd_msg = </span><span class="s3">f'JVP of </span><span class="s2">{</span><span class="s1">err_msg</span><span class="s2">}</span><span class="s3">' </span><span class="s2">if </span><span class="s1">err_msg </span><span class="s2">else </span><span class="s3">'JVP'</span>
      <span class="s1">_check_jvp(f</span><span class="s2">, </span><span class="s1">partial(api.jvp</span><span class="s2">, </span><span class="s1">f)</span><span class="s2">, </span><span class="s1">args</span><span class="s2">, </span><span class="s1">err_msg=fwd_msg)</span>
      <span class="s2">if </span><span class="s1">order &gt; </span><span class="s4">1</span><span class="s1">:</span>
        <span class="s1">_check_grads(partial(api.jvp</span><span class="s2">, </span><span class="s1">f)</span><span class="s2">, </span><span class="s1">(args</span><span class="s2">, </span><span class="s1">args)</span><span class="s2">, </span><span class="s1">order - </span><span class="s4">1</span><span class="s2">, </span><span class="s1">fwd_msg)</span>

    <span class="s2">if </span><span class="s3">&quot;rev&quot; </span><span class="s2">in </span><span class="s1">modes:</span>
      <span class="s1">rev_msg = </span><span class="s3">f'VJP of </span><span class="s2">{</span><span class="s1">err_msg</span><span class="s2">}</span><span class="s3">' </span><span class="s2">if </span><span class="s1">err_msg </span><span class="s2">else </span><span class="s3">'VJP'</span>
      <span class="s1">_check_vjp(f</span><span class="s2">, </span><span class="s1">partial(api.vjp</span><span class="s2">, </span><span class="s1">f)</span><span class="s2">, </span><span class="s1">args</span><span class="s2">, </span><span class="s1">err_msg=rev_msg)</span>
      <span class="s2">if </span><span class="s1">order &gt; </span><span class="s4">1</span><span class="s1">:</span>
        <span class="s2">def </span><span class="s1">f_vjp(*args):</span>
          <span class="s1">out_primal_py</span><span class="s2">, </span><span class="s1">vjp_py = api.vjp(f</span><span class="s2">, </span><span class="s1">*args)</span>
          <span class="s2">return </span><span class="s1">vjp_py(out_primal_py)</span>
        <span class="s1">_check_grads(f_vjp</span><span class="s2">, </span><span class="s1">args</span><span class="s2">, </span><span class="s1">order - </span><span class="s4">1</span><span class="s2">, </span><span class="s1">rev_msg)</span>

  <span class="s1">_check_grads(f</span><span class="s2">, </span><span class="s1">args</span><span class="s2">, </span><span class="s1">order)</span>


<span class="s2">def </span><span class="s1">device_under_test():</span>
  <span class="s2">return </span><span class="s1">getattr(FLAGS</span><span class="s2">, </span><span class="s3">'jax_test_dut'</span><span class="s2">, None</span><span class="s1">) </span><span class="s2">or </span><span class="s1">xla_bridge.get_backend().platform</span>
</pre>
</body>
</html>