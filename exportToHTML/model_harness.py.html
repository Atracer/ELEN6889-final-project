<html>
<head>
<title>model_harness.py</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<style type="text/css">
.s0 { color: #808080;}
.s1 { color: #a9b7c6;}
.s2 { color: #629755; font-style: italic;}
.s3 { color: #cc7832;}
.s4 { color: #6897bb;}
.s5 { color: #6a8759;}
</style>
</head>
<body bgcolor="#2b2b2b">
<table CELLSPACING=0 CELLPADDING=5 COLS=1 WIDTH="100%" BGCOLOR="#606060" >
<tr><td><center>
<font face="Arial, Helvetica" color="#000000">
model_harness.py</font>
</center></td></tr></table>
<pre><span class="s0"># Copyright 2022 The JAX Authors.</span>
<span class="s0">#</span>
<span class="s0"># Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);</span>
<span class="s0"># you may not use this file except in compliance with the License.</span>
<span class="s0"># You may obtain a copy of the License at</span>
<span class="s0">#</span>
<span class="s0">#     https://www.apache.org/licenses/LICENSE-2.0</span>
<span class="s0">#</span>
<span class="s0"># Unless required by applicable law or agreed to in writing, software</span>
<span class="s0"># distributed under the License is distributed on an &quot;AS IS&quot; BASIS,</span>
<span class="s0"># WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span>
<span class="s0"># See the License for the specific language governing permissions and</span>
<span class="s0"># limitations under the License.</span>
<span class="s2">&quot;&quot;&quot;All the models to convert.&quot;&quot;&quot;</span>
<span class="s3">import </span><span class="s1">dataclasses</span>
<span class="s3">import </span><span class="s1">functools</span>
<span class="s3">from </span><span class="s1">typing </span><span class="s3">import </span><span class="s1">Any</span><span class="s3">, </span><span class="s1">Callable</span><span class="s3">, </span><span class="s1">Dict</span><span class="s3">, </span><span class="s1">Optional</span><span class="s3">, </span><span class="s1">Sequence</span><span class="s3">, </span><span class="s1">Union</span>
<span class="s3">import </span><span class="s1">re</span>

<span class="s3">import </span><span class="s1">numpy </span><span class="s3">as </span><span class="s1">np</span>
<span class="s3">import </span><span class="s1">jraph</span>

<span class="s3">from </span><span class="s1">jax.experimental.jax2tf.tests.flax_models </span><span class="s3">import </span><span class="s1">actor_critic</span>
<span class="s3">from </span><span class="s1">jax.experimental.jax2tf.tests.flax_models </span><span class="s3">import </span><span class="s1">bilstm_classifier</span>
<span class="s3">from </span><span class="s1">jax.experimental.jax2tf.tests.flax_models </span><span class="s3">import </span><span class="s1">cnn</span>
<span class="s3">from </span><span class="s1">jax.experimental.jax2tf.tests.flax_models </span><span class="s3">import </span><span class="s1">gnn</span>
<span class="s3">from </span><span class="s1">jax.experimental.jax2tf.tests.flax_models </span><span class="s3">import </span><span class="s1">resnet</span>
<span class="s3">from </span><span class="s1">jax.experimental.jax2tf.tests.flax_models </span><span class="s3">import </span><span class="s1">seq2seq_lstm</span>
<span class="s3">from </span><span class="s1">jax.experimental.jax2tf.tests.flax_models </span><span class="s3">import </span><span class="s1">transformer_lm1b </span><span class="s3">as </span><span class="s1">lm1b</span>
<span class="s3">from </span><span class="s1">jax.experimental.jax2tf.tests.flax_models </span><span class="s3">import </span><span class="s1">transformer_nlp_seq </span><span class="s3">as </span><span class="s1">nlp_seq</span>
<span class="s3">from </span><span class="s1">jax.experimental.jax2tf.tests.flax_models </span><span class="s3">import </span><span class="s1">transformer_wmt </span><span class="s3">as </span><span class="s1">wmt</span>
<span class="s3">from </span><span class="s1">jax.experimental.jax2tf.tests.flax_models </span><span class="s3">import </span><span class="s1">vae</span>

<span class="s3">import </span><span class="s1">jax</span>
<span class="s3">from </span><span class="s1">jax </span><span class="s3">import </span><span class="s1">random</span>

<span class="s3">import </span><span class="s1">tensorflow </span><span class="s3">as </span><span class="s1">tf</span>


<span class="s1">@dataclasses.dataclass</span>
<span class="s3">class </span><span class="s1">ModelHarness:</span>
  <span class="s1">name: str</span>
  <span class="s1">apply: Callable[...</span><span class="s3">, </span><span class="s1">Any]</span>
  <span class="s1">variables: Dict[str</span><span class="s3">, </span><span class="s1">Any]</span>
  <span class="s1">inputs: Sequence[np.ndarray]</span>
  <span class="s1">rtol: float = </span><span class="s4">1e-4</span>
  <span class="s1">polymorphic_shapes: Optional[Sequence[Union[str</span><span class="s3">, None</span><span class="s1">]]] = </span><span class="s3">None</span>
  <span class="s1">tensor_spec: Optional[Sequence[tf.TensorSpec]] = </span><span class="s3">None</span>

  <span class="s3">def </span><span class="s1">__post_init__(self):</span>
    <span class="s0"># When providing polymorphic shapes, tensor_spec should be provided as well.</span>
    <span class="s3">assert </span><span class="s1">bool(self.polymorphic_shapes) == bool(self.tensor_spec)</span>

  <span class="s1">@property</span>
  <span class="s3">def </span><span class="s1">tf_input_signature(self):</span>
    <span class="s3">def </span><span class="s1">_to_tensorspec(x):</span>
      <span class="s3">return </span><span class="s1">tf.TensorSpec(x.shape</span><span class="s3">, </span><span class="s1">tf.dtypes.as_dtype(x.dtype))</span>

    <span class="s3">if </span><span class="s1">self.tensor_spec:</span>
      <span class="s3">return </span><span class="s1">self.tensor_spec</span>
    <span class="s3">else</span><span class="s1">:</span>
      <span class="s3">return </span><span class="s1">jax.tree_util.tree_map(_to_tensorspec</span><span class="s3">, </span><span class="s1">self.inputs)</span>

  <span class="s3">def </span><span class="s1">apply_with_vars(self</span><span class="s3">, </span><span class="s1">*args</span><span class="s3">, </span><span class="s1">**kwargs):</span>
    <span class="s3">return </span><span class="s1">self.apply(self.variables</span><span class="s3">, </span><span class="s1">*args</span><span class="s3">, </span><span class="s1">**kwargs)</span>


<span class="s0">##### All harnesses in this file.</span>
<span class="s1">ALL_HARNESSES: Dict[str</span><span class="s3">, </span><span class="s1">Callable[[str]</span><span class="s3">, </span><span class="s1">ModelHarness]] = {}</span>


<span class="s3">def </span><span class="s1">_make_harness(harness_fn</span><span class="s3">, </span><span class="s1">name</span><span class="s3">, </span><span class="s1">poly_shapes=</span><span class="s3">None, </span><span class="s1">tensor_specs=</span><span class="s3">None</span><span class="s1">):</span>
  <span class="s2">&quot;&quot;&quot;Partially apply harness in order to create variables lazily. 
 
  Note: quotes and commas are stripped from `name` to ensure they can be passed 
        through the command-line. 
  &quot;&quot;&quot;</span>
  <span class="s3">if </span><span class="s1">poly_shapes:</span>
    <span class="s1">name += </span><span class="s5">&quot;_&quot; </span><span class="s1">+ re.sub(</span><span class="s5">r&quot;(?:'|\&quot;|,)&quot;</span><span class="s3">, </span><span class="s5">&quot;&quot;</span><span class="s3">, </span><span class="s1">str(poly_shapes))</span>
  <span class="s3">if </span><span class="s1">tensor_specs:</span>
    <span class="s1">tensor_specs = [tf.TensorSpec(spec</span><span class="s3">, </span><span class="s1">dtype) </span><span class="s3">for </span><span class="s1">spec</span><span class="s3">, </span><span class="s1">dtype </span><span class="s3">in </span><span class="s1">tensor_specs]</span>
  <span class="s1">partial_fn = functools.partial(</span>
      <span class="s1">harness_fn</span><span class="s3">,</span>
      <span class="s1">name=name</span><span class="s3">,</span>
      <span class="s1">polymorphic_shapes=poly_shapes</span><span class="s3">,</span>
      <span class="s1">tensor_spec=tensor_specs)</span>
  <span class="s3">if </span><span class="s1">name </span><span class="s3">in </span><span class="s1">ALL_HARNESSES:</span>
    <span class="s3">raise </span><span class="s1">ValueError(</span><span class="s5">f&quot;Harness </span><span class="s3">{</span><span class="s1">name</span><span class="s3">} </span><span class="s5">exists already&quot;</span><span class="s1">)</span>
  <span class="s1">ALL_HARNESSES[name] = partial_fn</span>


<span class="s0">######################## Model Harness Definitions #############################</span>


<span class="s3">def </span><span class="s1">_actor_critic_harness(name</span><span class="s3">, </span><span class="s1">**kwargs):</span>
  <span class="s1">model = actor_critic.ActorCritic(num_outputs=</span><span class="s4">8</span><span class="s1">)</span>
  <span class="s1">x = np.zeros((</span><span class="s4">1</span><span class="s3">, </span><span class="s4">84</span><span class="s3">, </span><span class="s4">84</span><span class="s3">, </span><span class="s4">4</span><span class="s1">)</span><span class="s3">, </span><span class="s1">np.float32)</span>
  <span class="s1">variables = model.init(random.PRNGKey(</span><span class="s4">0</span><span class="s1">)</span><span class="s3">, </span><span class="s1">x)</span>
  <span class="s3">return </span><span class="s1">ModelHarness(name</span><span class="s3">, </span><span class="s1">model.apply</span><span class="s3">, </span><span class="s1">variables</span><span class="s3">, </span><span class="s1">[x]</span><span class="s3">, </span><span class="s1">**kwargs)</span>


<span class="s3">def </span><span class="s1">_bilstm_harness(name</span><span class="s3">, </span><span class="s1">**kwargs):</span>
  <span class="s1">model = bilstm_classifier.TextClassifier(</span>
      <span class="s0"># TODO(marcvanzee): This fails when</span>
      <span class="s0"># `embedding_size != hidden_size`. I suppose some arrays are</span>
      <span class="s0"># concatenated with incompatible shapes, which could mean</span>
      <span class="s0"># something is going wrong in the translation.</span>
      <span class="s1">embedding_size=</span><span class="s4">3</span><span class="s3">,</span>
      <span class="s1">hidden_size=</span><span class="s4">1</span><span class="s3">,</span>
      <span class="s1">vocab_size=</span><span class="s4">13</span><span class="s3">,</span>
      <span class="s1">output_size=</span><span class="s4">1</span><span class="s3">,</span>
      <span class="s1">dropout_rate=</span><span class="s4">0.</span><span class="s3">,</span>
      <span class="s1">word_dropout_rate=</span><span class="s4">0.</span><span class="s1">)</span>
  <span class="s1">x = np.array([[</span><span class="s4">2</span><span class="s3">, </span><span class="s4">4</span><span class="s3">, </span><span class="s4">3</span><span class="s1">]</span><span class="s3">, </span><span class="s1">[</span><span class="s4">2</span><span class="s3">, </span><span class="s4">6</span><span class="s3">, </span><span class="s4">3</span><span class="s1">]]</span><span class="s3">, </span><span class="s1">np.int32)</span>
  <span class="s1">lengths = np.array([</span><span class="s4">2</span><span class="s3">, </span><span class="s4">3</span><span class="s1">]</span><span class="s3">, </span><span class="s1">np.int32)</span>
  <span class="s1">variables = model.init(random.PRNGKey(</span><span class="s4">0</span><span class="s1">)</span><span class="s3">, </span><span class="s1">x</span><span class="s3">, </span><span class="s1">lengths</span><span class="s3">, </span><span class="s1">deterministic=</span><span class="s3">True</span><span class="s1">)</span>
  <span class="s1">apply = functools.partial(model.apply</span><span class="s3">, </span><span class="s1">deterministic=</span><span class="s3">True</span><span class="s1">)</span>
  <span class="s3">return </span><span class="s1">ModelHarness(name</span><span class="s3">, </span><span class="s1">apply</span><span class="s3">, </span><span class="s1">variables</span><span class="s3">, </span><span class="s1">[x</span><span class="s3">, </span><span class="s1">lengths]</span><span class="s3">, </span><span class="s1">**kwargs)</span>


<span class="s3">def </span><span class="s1">_cnn_harness(name</span><span class="s3">, </span><span class="s1">**kwargs):</span>
  <span class="s1">model = cnn.CNN()</span>
  <span class="s1">x = np.zeros((</span><span class="s4">1</span><span class="s3">, </span><span class="s4">28</span><span class="s3">, </span><span class="s4">28</span><span class="s3">, </span><span class="s4">1</span><span class="s1">)</span><span class="s3">, </span><span class="s1">np.float32)</span>
  <span class="s1">variables = model.init(random.PRNGKey(</span><span class="s4">0</span><span class="s1">)</span><span class="s3">, </span><span class="s1">x)</span>
  <span class="s3">return </span><span class="s1">ModelHarness(name</span><span class="s3">, </span><span class="s1">model.apply</span><span class="s3">, </span><span class="s1">variables</span><span class="s3">, </span><span class="s1">[x]</span><span class="s3">, </span><span class="s1">**kwargs)</span>


<span class="s3">def </span><span class="s1">_get_gnn_graphs():</span>
  <span class="s1">n_node = np.arange(</span><span class="s4">3</span><span class="s3">, </span><span class="s4">11</span><span class="s1">)</span>
  <span class="s1">n_edge = np.arange(</span><span class="s4">4</span><span class="s3">, </span><span class="s4">12</span><span class="s1">)</span>
  <span class="s1">total_n_node = np.sum(n_node)</span>
  <span class="s1">total_n_edge = np.sum(n_edge)</span>
  <span class="s1">n_graph = n_node.shape[</span><span class="s4">0</span><span class="s1">]</span>
  <span class="s1">feature_dim = </span><span class="s4">10</span>
  <span class="s1">graphs = jraph.GraphsTuple(</span>
      <span class="s1">n_node=n_node</span><span class="s3">,</span>
      <span class="s1">n_edge=n_edge</span><span class="s3">,</span>
      <span class="s1">senders=np.zeros(total_n_edge</span><span class="s3">, </span><span class="s1">dtype=np.int32)</span><span class="s3">,</span>
      <span class="s1">receivers=np.ones(total_n_edge</span><span class="s3">, </span><span class="s1">dtype=np.int32)</span><span class="s3">,</span>
      <span class="s1">nodes=np.ones((total_n_node</span><span class="s3">, </span><span class="s1">feature_dim))</span><span class="s3">,</span>
      <span class="s1">edges=np.zeros((total_n_edge</span><span class="s3">, </span><span class="s1">feature_dim))</span><span class="s3">,</span>
      <span class="s1">globals=np.zeros((n_graph</span><span class="s3">, </span><span class="s1">feature_dim))</span><span class="s3">,</span>
  <span class="s1">)</span>
  <span class="s3">return </span><span class="s1">graphs</span>


<span class="s3">def </span><span class="s1">_gnn_harness(name</span><span class="s3">, </span><span class="s1">**kwargs):</span>
  <span class="s0"># Setting taken from flax/examples/ogbg_molpcba/models_test.py.</span>
  <span class="s1">rngs = {</span>
      <span class="s5">'params'</span><span class="s1">: random.PRNGKey(</span><span class="s4">0</span><span class="s1">)</span><span class="s3">,</span>
      <span class="s5">'dropout'</span><span class="s1">: random.PRNGKey(</span><span class="s4">1</span><span class="s1">)</span><span class="s3">,</span>
  <span class="s1">}</span>
  <span class="s1">graphs = _get_gnn_graphs()</span>
  <span class="s1">model = gnn.GraphNet(</span>
      <span class="s1">latent_size=</span><span class="s4">5</span><span class="s3">,</span>
      <span class="s1">num_mlp_layers=</span><span class="s4">2</span><span class="s3">,</span>
      <span class="s1">message_passing_steps=</span><span class="s4">2</span><span class="s3">,</span>
      <span class="s1">output_globals_size=</span><span class="s4">15</span><span class="s3">,</span>
      <span class="s1">use_edge_model=</span><span class="s3">True</span><span class="s1">)</span>
  <span class="s1">variables = model.init(rngs</span><span class="s3">, </span><span class="s1">graphs)</span>
  <span class="s3">return </span><span class="s1">ModelHarness(name</span><span class="s3">, </span><span class="s1">model.apply</span><span class="s3">, </span><span class="s1">variables</span><span class="s3">, </span><span class="s1">[graphs]</span><span class="s3">, </span><span class="s1">rtol=</span><span class="s4">2e-4</span><span class="s3">,</span>
                      <span class="s1">**kwargs)</span>


<span class="s3">def </span><span class="s1">_gnn_conv_harness(name</span><span class="s3">, </span><span class="s1">**kwargs):</span>
  <span class="s0"># Setting taken from flax/examples/ogbg_molpcba/models_test.py.</span>
  <span class="s1">rngs = {</span>
      <span class="s5">'params'</span><span class="s1">: random.PRNGKey(</span><span class="s4">0</span><span class="s1">)</span><span class="s3">,</span>
      <span class="s5">'dropout'</span><span class="s1">: random.PRNGKey(</span><span class="s4">1</span><span class="s1">)</span><span class="s3">,</span>
  <span class="s1">}</span>
  <span class="s1">graphs = _get_gnn_graphs()</span>
  <span class="s1">model = gnn.GraphConvNet(</span>
      <span class="s1">latent_size=</span><span class="s4">5</span><span class="s3">,</span>
      <span class="s1">num_mlp_layers=</span><span class="s4">2</span><span class="s3">,</span>
      <span class="s1">message_passing_steps=</span><span class="s4">2</span><span class="s3">,</span>
      <span class="s1">output_globals_size=</span><span class="s4">5</span><span class="s1">)</span>
  <span class="s1">variables = model.init(rngs</span><span class="s3">, </span><span class="s1">graphs)</span>
  <span class="s3">return </span><span class="s1">ModelHarness(name</span><span class="s3">, </span><span class="s1">model.apply</span><span class="s3">, </span><span class="s1">variables</span><span class="s3">, </span><span class="s1">[graphs]</span><span class="s3">, </span><span class="s1">**kwargs)</span>


<span class="s3">def </span><span class="s1">_resnet50_harness(name</span><span class="s3">, </span><span class="s1">**kwargs):</span>
  <span class="s1">model = resnet.ResNet50(num_classes=</span><span class="s4">2</span><span class="s3">, </span><span class="s1">dtype=np.float32)</span>
  <span class="s1">x = np.zeros((</span><span class="s4">8</span><span class="s3">, </span><span class="s4">16</span><span class="s3">, </span><span class="s4">16</span><span class="s3">, </span><span class="s4">3</span><span class="s1">)</span><span class="s3">, </span><span class="s1">np.float32)</span>
  <span class="s1">variables = model.init(random.PRNGKey(</span><span class="s4">0</span><span class="s1">)</span><span class="s3">, </span><span class="s1">x)</span>
  <span class="s1">apply = functools.partial(model.apply</span><span class="s3">, </span><span class="s1">train=</span><span class="s3">False, </span><span class="s1">mutable=</span><span class="s3">False</span><span class="s1">)</span>
  <span class="s3">return </span><span class="s1">ModelHarness(name</span><span class="s3">, </span><span class="s1">apply</span><span class="s3">, </span><span class="s1">variables</span><span class="s3">, </span><span class="s1">[x]</span><span class="s3">, </span><span class="s1">**kwargs)</span>


<span class="s3">def </span><span class="s1">_seq2seq_lstm_harness(name</span><span class="s3">, </span><span class="s1">**kwargs):</span>
  <span class="s1">model = seq2seq_lstm.Seq2seq(teacher_force=</span><span class="s3">True, </span><span class="s1">hidden_size=</span><span class="s4">2</span><span class="s3">, </span><span class="s1">vocab_size=</span><span class="s4">4</span><span class="s1">)</span>
  <span class="s1">encoder_inputs = np.zeros((</span><span class="s4">1</span><span class="s3">, </span><span class="s4">2</span><span class="s3">, </span><span class="s4">4</span><span class="s1">)</span><span class="s3">, </span><span class="s1">np.float32)  </span><span class="s0"># [batch, inp_len, vocab]</span>
  <span class="s1">decoder_inputs = np.zeros((</span><span class="s4">1</span><span class="s3">, </span><span class="s4">3</span><span class="s3">, </span><span class="s4">4</span><span class="s1">)</span><span class="s3">, </span><span class="s1">np.float32)  </span><span class="s0"># [batch, outp_len, vocab]</span>
  <span class="s1">rngs = {</span>
      <span class="s5">'params'</span><span class="s1">: random.PRNGKey(</span><span class="s4">0</span><span class="s1">)</span><span class="s3">,</span>
      <span class="s5">'lstm'</span><span class="s1">: random.PRNGKey(</span><span class="s4">1</span><span class="s1">)</span><span class="s3">,</span>
  <span class="s1">}</span>
  <span class="s1">xs = [encoder_inputs</span><span class="s3">, </span><span class="s1">decoder_inputs]</span>
  <span class="s1">variables = model.init(rngs</span><span class="s3">, </span><span class="s1">*xs)</span>
  <span class="s1">apply = functools.partial(model.apply</span><span class="s3">, </span><span class="s1">rngs={</span><span class="s5">'lstm'</span><span class="s1">: random.PRNGKey(</span><span class="s4">2</span><span class="s1">)})</span>
  <span class="s3">return </span><span class="s1">ModelHarness(name</span><span class="s3">, </span><span class="s1">apply</span><span class="s3">, </span><span class="s1">variables</span><span class="s3">, </span><span class="s1">xs</span><span class="s3">, </span><span class="s1">**kwargs)</span>


<span class="s3">def </span><span class="s1">_min_transformer_kwargs():</span>
  <span class="s3">return </span><span class="s1">dict(</span>
      <span class="s1">vocab_size=</span><span class="s4">8</span><span class="s3">,</span>
      <span class="s1">output_vocab_size=</span><span class="s4">8</span><span class="s3">,</span>
      <span class="s1">emb_dim = </span><span class="s4">4</span><span class="s3">,</span>
      <span class="s1">num_heads= </span><span class="s4">1</span><span class="s3">,</span>
      <span class="s1">num_layers = </span><span class="s4">1</span><span class="s3">,</span>
      <span class="s1">qkv_dim= </span><span class="s4">2</span><span class="s3">,</span>
      <span class="s1">mlp_dim = </span><span class="s4">2</span><span class="s3">,</span>
      <span class="s1">max_len = </span><span class="s4">2</span><span class="s3">,</span>
      <span class="s1">dropout_rate = </span><span class="s4">0.</span><span class="s3">,</span>
      <span class="s1">attention_dropout_rate = </span><span class="s4">0.</span><span class="s1">)</span>


<span class="s3">def </span><span class="s1">_full_transformer_kwargs():</span>
  <span class="s1">kwargs = dict(</span>
      <span class="s1">decode = </span><span class="s3">True,</span>
      <span class="s1">deterministic = </span><span class="s3">True,</span>
      <span class="s1">logits_via_embedding=</span><span class="s3">False,</span>
      <span class="s1">share_embeddings=</span><span class="s3">False</span><span class="s1">)</span>
  <span class="s3">return </span><span class="s1">{**kwargs</span><span class="s3">, </span><span class="s1">**_min_transformer_kwargs()}</span>


<span class="s3">def </span><span class="s1">_transformer_lm1b_harness(name</span><span class="s3">, </span><span class="s1">**kwargs):</span>
  <span class="s1">config = lm1b.TransformerConfig(**_full_transformer_kwargs())</span>
  <span class="s1">model = lm1b.TransformerLM(config=config)</span>
  <span class="s1">x = np.zeros((</span><span class="s4">2</span><span class="s3">, </span><span class="s4">1</span><span class="s1">)</span><span class="s3">, </span><span class="s1">np.float32)</span>
  <span class="s1">rng1</span><span class="s3">, </span><span class="s1">rng2 = random.split(random.PRNGKey(</span><span class="s4">0</span><span class="s1">))</span>
  <span class="s1">variables = model.init(rng1</span><span class="s3">, </span><span class="s1">x)</span>

  <span class="s3">def </span><span class="s1">apply(*args):</span>
    <span class="s0"># Don't return the new state (containing the cache).</span>
    <span class="s1">output</span><span class="s3">, </span><span class="s1">_ = model.apply(*args</span><span class="s3">, </span><span class="s1">rngs={</span><span class="s5">'cache'</span><span class="s1">: rng2}</span><span class="s3">, </span><span class="s1">mutable=[</span><span class="s5">'cache'</span><span class="s1">])</span>
    <span class="s3">return </span><span class="s1">output</span>

  <span class="s3">return </span><span class="s1">ModelHarness(name</span><span class="s3">, </span><span class="s1">apply</span><span class="s3">, </span><span class="s1">variables</span><span class="s3">, </span><span class="s1">[x]</span><span class="s3">, </span><span class="s1">**kwargs)</span>


<span class="s3">def </span><span class="s1">_transformer_nlp_seq_harness(name</span><span class="s3">, </span><span class="s1">**kwargs):</span>
  <span class="s1">config = nlp_seq.TransformerConfig(**_min_transformer_kwargs())</span>
  <span class="s1">model = nlp_seq.Transformer(config=config)</span>
  <span class="s1">x = np.zeros((</span><span class="s4">2</span><span class="s3">, </span><span class="s4">1</span><span class="s1">)</span><span class="s3">, </span><span class="s1">np.float32)</span>
  <span class="s1">variables = model.init(random.PRNGKey(</span><span class="s4">0</span><span class="s1">)</span><span class="s3">, </span><span class="s1">x</span><span class="s3">, </span><span class="s1">train=</span><span class="s3">False</span><span class="s1">)</span>
  <span class="s1">apply = functools.partial(model.apply</span><span class="s3">, </span><span class="s1">train=</span><span class="s3">False</span><span class="s1">)</span>
  <span class="s3">return </span><span class="s1">ModelHarness(name</span><span class="s3">, </span><span class="s1">apply</span><span class="s3">, </span><span class="s1">variables</span><span class="s3">, </span><span class="s1">[x]</span><span class="s3">, </span><span class="s1">**kwargs)</span>


<span class="s3">def </span><span class="s1">_transformer_wmt_harness(name</span><span class="s3">, </span><span class="s1">**kwargs):</span>
  <span class="s1">config = wmt.TransformerConfig(**_full_transformer_kwargs())</span>
  <span class="s1">model = wmt.Transformer(config=config)</span>
  <span class="s1">x = np.zeros((</span><span class="s4">2</span><span class="s3">, </span><span class="s4">1</span><span class="s1">)</span><span class="s3">, </span><span class="s1">np.float32)</span>
  <span class="s1">variables = model.init(random.PRNGKey(</span><span class="s4">0</span><span class="s1">)</span><span class="s3">, </span><span class="s1">x</span><span class="s3">, </span><span class="s1">x)</span>

  <span class="s3">def </span><span class="s1">apply(*args):</span>
    <span class="s0"># Don't return the new state (containing the cache).</span>
    <span class="s1">output</span><span class="s3">, </span><span class="s1">_ = model.apply(*args</span><span class="s3">, </span><span class="s1">mutable=[</span><span class="s5">'cache'</span><span class="s1">])</span>
    <span class="s3">return </span><span class="s1">output</span>

  <span class="s3">return </span><span class="s1">ModelHarness(name</span><span class="s3">, </span><span class="s1">apply</span><span class="s3">, </span><span class="s1">variables</span><span class="s3">, </span><span class="s1">[x</span><span class="s3">, </span><span class="s1">x]</span><span class="s3">, </span><span class="s1">**kwargs)</span>


<span class="s3">def </span><span class="s1">_vae_harness(name</span><span class="s3">, </span><span class="s1">**kwargs):</span>
  <span class="s1">model = vae.VAE(latents=</span><span class="s4">3</span><span class="s1">)</span>
  <span class="s1">x = np.zeros((</span><span class="s4">1</span><span class="s3">, </span><span class="s4">8</span><span class="s3">, </span><span class="s4">8</span><span class="s3">, </span><span class="s4">3</span><span class="s1">)</span><span class="s3">, </span><span class="s1">np.float32)</span>
  <span class="s1">rng1</span><span class="s3">, </span><span class="s1">rng2 = random.split(random.PRNGKey(</span><span class="s4">0</span><span class="s1">))</span>
  <span class="s1">variables = model.init(rng1</span><span class="s3">, </span><span class="s1">x</span><span class="s3">, </span><span class="s1">rng2)</span>
  <span class="s1">generate = </span><span class="s3">lambda </span><span class="s1">v</span><span class="s3">, </span><span class="s1">x: model.apply(v</span><span class="s3">, </span><span class="s1">x</span><span class="s3">, </span><span class="s1">method=model.generate)</span>
  <span class="s3">return </span><span class="s1">ModelHarness(name</span><span class="s3">, </span><span class="s1">generate</span><span class="s3">, </span><span class="s1">variables</span><span class="s3">, </span><span class="s1">[x]</span><span class="s3">, </span><span class="s1">**kwargs)</span>


<span class="s0">####################### Model Harness Construction #############################</span>


<span class="s0"># actor_critic input spec: [((1, 84, 84, 4), np.float32)].</span>
<span class="s3">for </span><span class="s1">poly_shapes</span><span class="s3">, </span><span class="s1">tensor_specs </span><span class="s3">in </span><span class="s1">[</span>
    <span class="s1">(</span><span class="s3">None, None</span><span class="s1">)</span><span class="s3">,  </span><span class="s0"># No polymorphism.</span>
    <span class="s0"># batch polymorphism.</span>
    <span class="s1">([</span><span class="s5">&quot;(b, ...)&quot;</span><span class="s1">]</span><span class="s3">, </span><span class="s1">[((</span><span class="s3">None, </span><span class="s4">84</span><span class="s3">, </span><span class="s4">84</span><span class="s3">, </span><span class="s4">4</span><span class="s1">)</span><span class="s3">, </span><span class="s1">tf.float32)])</span><span class="s3">,</span>
    <span class="s0"># Dependent shapes for spatial dims.</span>
    <span class="s0"># TODO(marcvanzee): Figure out the right multiple for these dimensions.</span>
    <span class="s1">([</span><span class="s5">&quot;(_, 4*b, 4*b, _)&quot;</span><span class="s1">]</span><span class="s3">, </span><span class="s1">[((</span><span class="s4">1</span><span class="s3">, None, None, </span><span class="s4">4</span><span class="s1">)</span><span class="s3">, </span><span class="s1">tf.float32)])</span><span class="s3">,</span>
<span class="s1">]:</span>
  <span class="s1">_make_harness(</span>
      <span class="s1">harness_fn=_actor_critic_harness</span><span class="s3">,</span>
      <span class="s1">name=</span><span class="s5">&quot;flax/actor_critic&quot;</span><span class="s3">,</span>
      <span class="s1">poly_shapes=poly_shapes</span><span class="s3">,</span>
      <span class="s1">tensor_specs=tensor_specs)</span>

<span class="s0"># bilstm input specs: [((2, 3), np.int32), ((2,), np.int32)] = [inputs, lengths]</span>
<span class="s3">for </span><span class="s1">poly_shapes</span><span class="s3">, </span><span class="s1">tensor_specs </span><span class="s3">in </span><span class="s1">[  </span><span class="s0"># type: ignore</span>
    <span class="s1">(</span><span class="s3">None, None</span><span class="s1">)</span><span class="s3">,</span>
    <span class="s0"># batch polymorphism</span>
    <span class="s1">([</span><span class="s5">&quot;(b, _)&quot;</span><span class="s3">, </span><span class="s5">&quot;(_,)&quot;</span><span class="s1">]</span><span class="s3">, </span><span class="s1">[((</span><span class="s3">None, </span><span class="s4">3</span><span class="s1">)</span><span class="s3">, </span><span class="s1">tf.int32)</span><span class="s3">, </span><span class="s1">((</span><span class="s4">2</span><span class="s3">,</span><span class="s1">)</span><span class="s3">, </span><span class="s1">tf.int32)])</span><span class="s3">,</span>
    <span class="s0"># dynamic input lengths</span>
    <span class="s1">([</span><span class="s5">&quot;(_, _)&quot;</span><span class="s3">, </span><span class="s5">&quot;(b,)&quot;</span><span class="s1">]</span><span class="s3">, </span><span class="s1">[((</span><span class="s4">2</span><span class="s3">, </span><span class="s4">3</span><span class="s1">)</span><span class="s3">, </span><span class="s1">tf.int32)</span><span class="s3">, </span><span class="s1">((</span><span class="s3">None,</span><span class="s1">)</span><span class="s3">, </span><span class="s1">tf.int32)])</span><span class="s3">,</span>
<span class="s1">]:</span>
  <span class="s1">_make_harness(</span>
      <span class="s1">harness_fn=_bilstm_harness</span><span class="s3">,</span>
      <span class="s1">name=</span><span class="s5">&quot;flax/bilstm&quot;</span><span class="s3">,</span>
      <span class="s1">poly_shapes=poly_shapes</span><span class="s3">,</span>
      <span class="s1">tensor_specs=tensor_specs)</span>

<span class="s0"># cnn input spec: [((1, 28, 28, 1), np.float32)].</span>
<span class="s3">for </span><span class="s1">poly_shapes</span><span class="s3">, </span><span class="s1">tensor_specs </span><span class="s3">in </span><span class="s1">[</span>
    <span class="s1">(</span><span class="s3">None, None</span><span class="s1">)</span><span class="s3">,  </span><span class="s0"># No polymorphism.</span>
    <span class="s0"># batch polymorphism.</span>
    <span class="s1">([</span><span class="s5">&quot;(b, ...)&quot;</span><span class="s1">]</span><span class="s3">, </span><span class="s1">[((</span><span class="s3">None, </span><span class="s4">28</span><span class="s3">, </span><span class="s4">28</span><span class="s3">, </span><span class="s4">1</span><span class="s1">)</span><span class="s3">, </span><span class="s1">tf.float32)])</span><span class="s3">,</span>
    <span class="s0"># Dependent shapes for spatial dims.</span>
    <span class="s0"># TODO(marcvanzee): Figure out the right multiple for these dimensions.</span>
    <span class="s1">([</span><span class="s5">&quot;(_, b, b, _)&quot;</span><span class="s1">]</span><span class="s3">, </span><span class="s1">[((</span><span class="s4">1</span><span class="s3">, None, None, </span><span class="s4">1</span><span class="s1">)</span><span class="s3">, </span><span class="s1">tf.float32)])</span><span class="s3">,</span>
<span class="s1">]:</span>
  <span class="s1">_make_harness(</span>
      <span class="s1">harness_fn=_cnn_harness</span><span class="s3">,</span>
      <span class="s1">name=</span><span class="s5">&quot;flax/cnn&quot;</span><span class="s3">,</span>
      <span class="s1">poly_shapes=poly_shapes</span><span class="s3">,</span>
      <span class="s1">tensor_specs=tensor_specs)</span>

<span class="s0"># We do not support polymorphism for the GNN examples since they use GraphTuples</span>
<span class="s0"># as input rather than regular arrays.</span>
<span class="s1">_make_harness(harness_fn=_gnn_harness</span><span class="s3">, </span><span class="s1">name=</span><span class="s5">&quot;flax/gnn&quot;</span><span class="s1">)</span>
<span class="s1">_make_harness(harness_fn=_gnn_conv_harness</span><span class="s3">, </span><span class="s1">name=</span><span class="s5">&quot;flax/gnn_conv&quot;</span><span class="s1">)</span>

<span class="s0"># resnet50 input spec: [((8, 16, 16, 3), np.float32)]</span>
<span class="s3">for </span><span class="s1">poly_shapes</span><span class="s3">, </span><span class="s1">tensor_specs </span><span class="s3">in </span><span class="s1">[</span>
    <span class="s1">(</span><span class="s3">None, None</span><span class="s1">)</span><span class="s3">,  </span><span class="s0"># No polymorphism.</span>
    <span class="s0"># batch polymorphism.</span>
    <span class="s1">([</span><span class="s5">&quot;(b, ...)&quot;</span><span class="s1">]</span><span class="s3">, </span><span class="s1">[((</span><span class="s3">None, </span><span class="s4">16</span><span class="s3">, </span><span class="s4">16</span><span class="s3">, </span><span class="s4">3</span><span class="s1">)</span><span class="s3">, </span><span class="s1">tf.float32)])</span><span class="s3">,</span>
    <span class="s0"># Dependent shapes for spatial dims.</span>
    <span class="s0"># TODO(marcvanzee): Figure out the right multiple for these dimensions.</span>
    <span class="s1">([</span><span class="s5">&quot;(_, 4*b, 4*b, _)&quot;</span><span class="s1">]</span><span class="s3">, </span><span class="s1">[((</span><span class="s4">8</span><span class="s3">, None, None, </span><span class="s4">3</span><span class="s1">)</span><span class="s3">, </span><span class="s1">tf.float32)])</span><span class="s3">,</span>
<span class="s1">]:</span>
  <span class="s1">_make_harness(</span>
      <span class="s1">harness_fn=_resnet50_harness</span><span class="s3">,</span>
      <span class="s1">name=</span><span class="s5">&quot;flax/resnet50&quot;</span><span class="s3">,</span>
      <span class="s1">poly_shapes=poly_shapes</span><span class="s3">,</span>
      <span class="s1">tensor_specs=tensor_specs)</span>


<span class="s0"># seq2seq input specs (we use the same input and output lengths for now):</span>
<span class="s0"># [</span>
<span class="s0">#   ((1, 2, 4), np.float32),  # encoder inp: [batch, max_input_len, vocab_size]</span>
<span class="s0">#   ((1, 3, 4), np.float32),  # decoder_inp: [batch, max_output_len, vocab_size]</span>
<span class="s0"># ]</span>
<span class="s3">for </span><span class="s1">poly_shapes</span><span class="s3">, </span><span class="s1">tensor_specs </span><span class="s3">in </span><span class="s1">[  </span><span class="s0"># type: ignore</span>
    <span class="s1">(</span><span class="s3">None, None</span><span class="s1">)</span><span class="s3">,</span>
    <span class="s0"># batch polymorphism</span>
    <span class="s1">(</span>
        <span class="s1">[</span><span class="s5">&quot;(b, _, _)&quot;</span><span class="s3">,                </span><span class="s5">&quot;(b, _, _)&quot;</span><span class="s1">]</span><span class="s3">,</span>
        <span class="s1">[((</span><span class="s3">None, </span><span class="s4">2</span><span class="s3">, </span><span class="s4">4</span><span class="s1">)</span><span class="s3">, </span><span class="s1">tf.float32)</span><span class="s3">, </span><span class="s1">((</span><span class="s3">None, </span><span class="s4">3</span><span class="s3">, </span><span class="s4">4</span><span class="s1">)</span><span class="s3">, </span><span class="s1">tf.float32)]</span><span class="s3">,</span>
    <span class="s1">)</span><span class="s3">,</span>
    <span class="s0"># dynamic input lengths</span>
    <span class="s1">(</span>
        <span class="s1">[</span><span class="s5">&quot;(_, b, _)&quot;</span><span class="s3">,                </span><span class="s5">&quot;(_, _, _)&quot;</span><span class="s1">]</span><span class="s3">,</span>
        <span class="s1">[((</span><span class="s4">1</span><span class="s3">, None, </span><span class="s4">4</span><span class="s1">)</span><span class="s3">, </span><span class="s1">tf.float32)</span><span class="s3">, </span><span class="s1">((</span><span class="s4">1</span><span class="s3">, </span><span class="s4">3</span><span class="s3">, </span><span class="s4">4</span><span class="s1">)</span><span class="s3">, </span><span class="s1">tf.float32)]</span><span class="s3">,</span>
    <span class="s1">)</span><span class="s3">,</span>
    <span class="s0"># dynamic output lengths</span>
    <span class="s1">(</span>
        <span class="s1">[</span><span class="s5">&quot;(_, _, _)&quot;</span><span class="s3">,                 </span><span class="s5">&quot;(_, b, _)&quot;</span><span class="s1">]</span><span class="s3">,</span>
        <span class="s1">[((</span><span class="s4">1</span><span class="s3">, </span><span class="s4">2</span><span class="s3">, </span><span class="s4">4</span><span class="s1">)</span><span class="s3">, </span><span class="s1">tf.float32)</span><span class="s3">,     </span><span class="s1">((</span><span class="s4">1</span><span class="s3">, None, </span><span class="s4">4</span><span class="s1">)</span><span class="s3">, </span><span class="s1">tf.float32)]</span><span class="s3">,</span>
    <span class="s1">)</span><span class="s3">,</span>
<span class="s1">]:</span>
  <span class="s1">_make_harness(</span>
      <span class="s1">harness_fn=_seq2seq_lstm_harness</span><span class="s3">,</span>
      <span class="s1">name=</span><span class="s5">&quot;flax/seq2seq_lstm&quot;</span><span class="s3">,</span>
      <span class="s1">poly_shapes=poly_shapes</span><span class="s3">,</span>
      <span class="s1">tensor_specs=tensor_specs)</span>

<span class="s0"># lm1b/nlp_seq input spec: [((2, 1), np.float32)]  [batch, seq_len]</span>
<span class="s3">for </span><span class="s1">poly_shapes</span><span class="s3">, </span><span class="s1">tensor_specs </span><span class="s3">in </span><span class="s1">[  </span><span class="s0"># type: ignore</span>
    <span class="s1">(</span><span class="s3">None, None</span><span class="s1">)</span><span class="s3">,</span>
    <span class="s0"># batch polymorphism.</span>
    <span class="s1">([</span><span class="s5">&quot;(b, _)&quot;</span><span class="s1">]</span><span class="s3">, </span><span class="s1">[((</span><span class="s3">None, </span><span class="s4">1</span><span class="s1">)</span><span class="s3">, </span><span class="s1">tf.float32)])</span><span class="s3">,</span>
<span class="s1">]:</span>
  <span class="s3">for </span><span class="s1">name</span><span class="s3">, </span><span class="s1">harness_fn </span><span class="s3">in </span><span class="s1">[</span>
      <span class="s1">(</span><span class="s5">&quot;flax/lm1b&quot;</span><span class="s3">, </span><span class="s1">_transformer_lm1b_harness)</span><span class="s3">,</span>
      <span class="s1">(</span><span class="s5">&quot;flax/nlp_seq&quot;</span><span class="s3">, </span><span class="s1">_transformer_nlp_seq_harness)</span>
  <span class="s1">]:</span>
    <span class="s1">_make_harness(</span>
        <span class="s1">harness_fn=harness_fn</span><span class="s3">,</span>
        <span class="s1">name=name</span><span class="s3">,</span>
        <span class="s1">poly_shapes=poly_shapes</span><span class="s3">,</span>
        <span class="s1">tensor_specs=tensor_specs)</span>

<span class="s0"># wmt input spec (both inputs have the same shape):</span>
<span class="s0"># [</span>
<span class="s0">#   ((1, 2), np.float32),  # inputs:  [batch, max_target_len]</span>
<span class="s0">#   ((1, 2), np.float32),  # targets: [batch, max_target_len]</span>
<span class="s0"># ]</span>
<span class="s3">for </span><span class="s1">poly_shapes</span><span class="s3">, </span><span class="s1">tensor_specs </span><span class="s3">in </span><span class="s1">[  </span><span class="s0"># type: ignore</span>
    <span class="s1">(</span><span class="s3">None, None</span><span class="s1">)</span><span class="s3">,</span>
    <span class="s0"># batch polymorphism.</span>
    <span class="s1">([</span><span class="s5">&quot;(b, _)&quot;</span><span class="s1">] * </span><span class="s4">2</span><span class="s3">, </span><span class="s1">[((</span><span class="s3">None, </span><span class="s4">1</span><span class="s1">)</span><span class="s3">, </span><span class="s1">tf.float32)] * </span><span class="s4">2</span><span class="s1">)</span><span class="s3">,</span>
    <span class="s0"># dynamic lengths.</span>
    <span class="s1">([</span><span class="s5">&quot;(_, b)&quot;</span><span class="s1">] * </span><span class="s4">2</span><span class="s3">, </span><span class="s1">[((</span><span class="s4">1</span><span class="s3">, None</span><span class="s1">)</span><span class="s3">, </span><span class="s1">tf.float32)] * </span><span class="s4">2</span><span class="s1">)</span><span class="s3">,</span>
<span class="s1">]:</span>
  <span class="s1">_make_harness(</span>
      <span class="s1">harness_fn=_transformer_wmt_harness</span><span class="s3">,</span>
      <span class="s1">name=</span><span class="s5">&quot;flax/wmt&quot;</span><span class="s3">,</span>
      <span class="s1">poly_shapes=poly_shapes</span><span class="s3">,</span>
      <span class="s1">tensor_specs=tensor_specs)</span>

<span class="s0"># vae input spec: [((1, 8, 8, 3), np.float32)].</span>
<span class="s3">for </span><span class="s1">poly_shapes</span><span class="s3">, </span><span class="s1">tensor_specs </span><span class="s3">in </span><span class="s1">[</span>
    <span class="s1">(</span><span class="s3">None, None</span><span class="s1">)</span><span class="s3">,  </span><span class="s0"># No polymorphism.</span>
    <span class="s0"># batch polymorphism.</span>
    <span class="s1">([</span><span class="s5">&quot;(b, ...)&quot;</span><span class="s1">]</span><span class="s3">, </span><span class="s1">[((</span><span class="s3">None, </span><span class="s4">8</span><span class="s3">, </span><span class="s4">8</span><span class="s3">, </span><span class="s4">3</span><span class="s1">)</span><span class="s3">, </span><span class="s1">tf.float32)])</span><span class="s3">,</span>
    <span class="s0"># Dependent shapes for spatial dims.</span>
    <span class="s0"># TODO(marcvanzee): Figure out the right multiple for these dimensions.</span>
    <span class="s1">([</span><span class="s5">&quot;(_, b, b, _)&quot;</span><span class="s1">]</span><span class="s3">, </span><span class="s1">[((</span><span class="s4">1</span><span class="s3">, None, None, </span><span class="s4">3</span><span class="s1">)</span><span class="s3">, </span><span class="s1">tf.float32)])</span><span class="s3">,</span>
<span class="s1">]:</span>
  <span class="s1">_make_harness(</span>
      <span class="s1">harness_fn=_vae_harness</span><span class="s3">,</span>
      <span class="s1">name=</span><span class="s5">&quot;flax/vae&quot;</span><span class="s3">,</span>
      <span class="s1">poly_shapes=poly_shapes</span><span class="s3">,</span>
      <span class="s1">tensor_specs=tensor_specs)</span>
</pre>
</body>
</html>