<html>
<head>
<title>_lbfgs.py</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<style type="text/css">
.s0 { color: #808080;}
.s1 { color: #a9b7c6;}
.s2 { color: #629755; font-style: italic;}
.s3 { color: #cc7832;}
.s4 { color: #6897bb;}
</style>
</head>
<body bgcolor="#2b2b2b">
<table CELLSPACING=0 CELLPADDING=5 COLS=1 WIDTH="100%" BGCOLOR="#606060" >
<tr><td><center>
<font face="Arial, Helvetica" color="#000000">
_lbfgs.py</font>
</center></td></tr></table>
<pre><span class="s0"># Copyright 2020 The JAX Authors.</span>
<span class="s0">#</span>
<span class="s0"># Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);</span>
<span class="s0"># you may not use this file except in compliance with the License.</span>
<span class="s0"># You may obtain a copy of the License at</span>
<span class="s0">#</span>
<span class="s0">#     https://www.apache.org/licenses/LICENSE-2.0</span>
<span class="s0">#</span>
<span class="s0"># Unless required by applicable law or agreed to in writing, software</span>
<span class="s0"># distributed under the License is distributed on an &quot;AS IS&quot; BASIS,</span>
<span class="s0"># WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span>
<span class="s0"># See the License for the specific language governing permissions and</span>
<span class="s0"># limitations under the License.</span>
<span class="s2">&quot;&quot;&quot;The Limited-Memory Broyden-Fletcher-Goldfarb-Shanno minimization algorithm.&quot;&quot;&quot;</span>
<span class="s3">from </span><span class="s1">typing </span><span class="s3">import </span><span class="s1">Any</span><span class="s3">, </span><span class="s1">Callable</span><span class="s3">, </span><span class="s1">NamedTuple</span><span class="s3">, </span><span class="s1">Optional</span><span class="s3">, </span><span class="s1">Union</span>
<span class="s3">from </span><span class="s1">functools </span><span class="s3">import </span><span class="s1">partial</span>

<span class="s3">import </span><span class="s1">jax</span>
<span class="s3">import </span><span class="s1">jax.numpy </span><span class="s3">as </span><span class="s1">jnp</span>
<span class="s3">from </span><span class="s1">jax </span><span class="s3">import </span><span class="s1">lax</span>
<span class="s3">from </span><span class="s1">jax._src.scipy.optimize.line_search </span><span class="s3">import </span><span class="s1">line_search</span>

<span class="s1">_dot = partial(jnp.dot</span><span class="s3">, </span><span class="s1">precision=lax.Precision.HIGHEST)</span>


<span class="s1">Array = Any</span>

<span class="s3">class </span><span class="s1">LBFGSResults(NamedTuple):</span>
  <span class="s2">&quot;&quot;&quot;Results from L-BFGS optimization 
 
  Parameters: 
    converged: True if minimization converged 
    failed: True if non-zero status and not converged 
    k: integer number of iterations of the main loop (optimisation steps) 
    nfev: integer total number of objective evaluations performed. 
    ngev: integer total number of jacobian evaluations 
    x_k: array containing the last argument value found during the search. If 
      the search converged, then this value is the argmin of the objective 
      function. 
    f_k: array containing the value of the objective function at `x_k`. If the 
      search converged, then this is the (local) minimum of the objective 
      function. 
    g_k: array containing the gradient of the objective function at `x_k`. If 
      the search converged the l2-norm of this tensor should be below the 
      tolerance. 
    status: integer describing the status: 
      0 = nominal  ,  1 = max iters reached  ,  2 = max fun evals reached 
      3 = max grad evals reached  ,  4 = insufficient progress (ftol) 
      5 = line search failed 
    ls_status: integer describing the end status of the last line search 
  &quot;&quot;&quot;</span>
  <span class="s1">converged: Union[bool</span><span class="s3">, </span><span class="s1">Array]</span>
  <span class="s1">failed: Union[bool</span><span class="s3">, </span><span class="s1">Array]</span>
  <span class="s1">k: Union[int</span><span class="s3">, </span><span class="s1">Array]</span>
  <span class="s1">nfev: Union[int</span><span class="s3">, </span><span class="s1">Array]</span>
  <span class="s1">ngev: Union[int</span><span class="s3">, </span><span class="s1">Array]</span>
  <span class="s1">x_k: Array</span>
  <span class="s1">f_k: Array</span>
  <span class="s1">g_k: Array</span>
  <span class="s1">s_history: Array</span>
  <span class="s1">y_history: Array</span>
  <span class="s1">rho_history: Array</span>
  <span class="s1">gamma: Union[float</span><span class="s3">, </span><span class="s1">Array]</span>
  <span class="s1">status: Union[int</span><span class="s3">, </span><span class="s1">Array]</span>
  <span class="s1">ls_status: Union[int</span><span class="s3">, </span><span class="s1">Array]</span>


<span class="s3">def </span><span class="s1">_minimize_lbfgs(</span>
    <span class="s1">fun: Callable</span><span class="s3">,</span>
    <span class="s1">x0: Array</span><span class="s3">,</span>
    <span class="s1">maxiter: Optional[float] = </span><span class="s3">None,</span>
    <span class="s1">norm=jnp.inf</span><span class="s3">,</span>
    <span class="s1">maxcor: int = </span><span class="s4">10</span><span class="s3">,</span>
    <span class="s1">ftol: float = </span><span class="s4">2.220446049250313e-09</span><span class="s3">,</span>
    <span class="s1">gtol: float = </span><span class="s4">1e-05</span><span class="s3">,</span>
    <span class="s1">maxfun: Optional[float] = </span><span class="s3">None,</span>
    <span class="s1">maxgrad: Optional[float] = </span><span class="s3">None,</span>
    <span class="s1">maxls: int = </span><span class="s4">20</span><span class="s3">,</span>
<span class="s1">):</span>
  <span class="s2">&quot;&quot;&quot; 
  Minimize a function using L-BFGS 
 
  Implements the L-BFGS algorithm from 
    Algorithm 7.5 from Wright and Nocedal, 'Numerical Optimization', 1999, pg. 176-185 
  And generalizes to complex variables from 
     Sorber, L., Barel, M.V. and Lathauwer, L.D., 2012. 
     &quot;Unconstrained optimization of real functions in complex variables&quot; 
     SIAM Journal on Optimization, 22(3), pp.879-898. 
 
  Args: 
    fun: function of the form f(x) where x is a flat ndarray and returns a real scalar. 
      The function should be composed of operations with vjp defined. 
    x0: initial guess 
    maxiter: maximum number of iterations 
    norm: order of norm for convergence check. Default inf. 
    maxcor: maximum number of metric corrections (&quot;history size&quot;) 
    ftol: terminates the minimization when `(f_k - f_{k+1}) &lt; ftol` 
    gtol: terminates the minimization when `|g_k|_norm &lt; gtol` 
    maxfun: maximum number of function evaluations 
    maxgrad: maximum number of gradient evaluations 
    maxls: maximum number of line search steps (per iteration) 
 
  Returns: 
    Optimization results. 
  &quot;&quot;&quot;</span>
  <span class="s1">d = len(x0)</span>
  <span class="s1">dtype = jnp.dtype(x0)</span>

  <span class="s0"># ensure there is at least one termination condition</span>
  <span class="s3">if </span><span class="s1">(maxiter </span><span class="s3">is None</span><span class="s1">) </span><span class="s3">and </span><span class="s1">(maxfun </span><span class="s3">is None</span><span class="s1">) </span><span class="s3">and </span><span class="s1">(maxgrad </span><span class="s3">is None</span><span class="s1">):</span>
    <span class="s1">maxiter = d * </span><span class="s4">200</span>

  <span class="s0"># set others to inf, such that &gt;= is supported</span>
  <span class="s3">if </span><span class="s1">maxiter </span><span class="s3">is None</span><span class="s1">:</span>
    <span class="s1">maxiter = jnp.inf</span>
  <span class="s3">if </span><span class="s1">maxfun </span><span class="s3">is None</span><span class="s1">:</span>
    <span class="s1">maxfun = jnp.inf</span>
  <span class="s3">if </span><span class="s1">maxgrad </span><span class="s3">is None</span><span class="s1">:</span>
    <span class="s1">maxgrad = jnp.inf</span>

  <span class="s0"># initial evaluation</span>
  <span class="s1">f_0</span><span class="s3">, </span><span class="s1">g_0 = jax.value_and_grad(fun)(x0)</span>
  <span class="s1">state_initial = LBFGSResults(</span>
    <span class="s1">converged=</span><span class="s3">False,</span>
    <span class="s1">failed=</span><span class="s3">False,</span>
    <span class="s1">k=</span><span class="s4">0</span><span class="s3">,</span>
    <span class="s1">nfev=</span><span class="s4">1</span><span class="s3">,</span>
    <span class="s1">ngev=</span><span class="s4">1</span><span class="s3">,</span>
    <span class="s1">x_k=x0</span><span class="s3">,</span>
    <span class="s1">f_k=f_0</span><span class="s3">,</span>
    <span class="s1">g_k=g_0</span><span class="s3">,</span>
    <span class="s1">s_history=jnp.zeros((maxcor</span><span class="s3">, </span><span class="s1">d)</span><span class="s3">, </span><span class="s1">dtype=dtype)</span><span class="s3">,</span>
    <span class="s1">y_history=jnp.zeros((maxcor</span><span class="s3">, </span><span class="s1">d)</span><span class="s3">, </span><span class="s1">dtype=dtype)</span><span class="s3">,</span>
    <span class="s1">rho_history=jnp.zeros((maxcor</span><span class="s3">,</span><span class="s1">)</span><span class="s3">, </span><span class="s1">dtype=dtype)</span><span class="s3">,</span>
    <span class="s1">gamma=</span><span class="s4">1.</span><span class="s3">,</span>
    <span class="s1">status=</span><span class="s4">0</span><span class="s3">,</span>
    <span class="s1">ls_status=</span><span class="s4">0</span><span class="s3">,</span>
  <span class="s1">)</span>

  <span class="s3">def </span><span class="s1">cond_fun(state: LBFGSResults):</span>
    <span class="s3">return </span><span class="s1">(~state.converged) &amp; (~state.failed)</span>

  <span class="s3">def </span><span class="s1">body_fun(state: LBFGSResults):</span>
    <span class="s0"># find search direction</span>
    <span class="s1">p_k = _two_loop_recursion(state)</span>

    <span class="s0"># line search</span>
    <span class="s1">ls_results = line_search(</span>
      <span class="s1">f=fun</span><span class="s3">,</span>
      <span class="s1">xk=state.x_k</span><span class="s3">,</span>
      <span class="s1">pk=p_k</span><span class="s3">,</span>
      <span class="s1">old_fval=state.f_k</span><span class="s3">,</span>
      <span class="s1">gfk=state.g_k</span><span class="s3">,</span>
      <span class="s1">maxiter=maxls</span><span class="s3">,</span>
    <span class="s1">)</span>

    <span class="s0"># evaluate at next iterate</span>
    <span class="s1">s_k = ls_results.a_k.astype(p_k.dtype) * p_k</span>
    <span class="s1">x_kp1 = state.x_k + s_k</span>
    <span class="s1">f_kp1 = ls_results.f_k</span>
    <span class="s1">g_kp1 = ls_results.g_k</span>
    <span class="s1">y_k = g_kp1 - state.g_k</span>
    <span class="s1">rho_k_inv = jnp.real(_dot(y_k</span><span class="s3">, </span><span class="s1">s_k))</span>
    <span class="s1">rho_k = jnp.reciprocal(rho_k_inv).astype(y_k.dtype)</span>
    <span class="s1">gamma = rho_k_inv / jnp.real(_dot(jnp.conj(y_k)</span><span class="s3">, </span><span class="s1">y_k))</span>

    <span class="s0"># replacements for next iteration</span>
    <span class="s1">status = jnp.array(</span><span class="s4">0</span><span class="s1">)</span>
    <span class="s1">status = jnp.where(state.f_k - f_kp1 &lt; ftol</span><span class="s3">, </span><span class="s4">4</span><span class="s3">, </span><span class="s1">status)</span>
    <span class="s1">status = jnp.where(state.ngev &gt;= maxgrad</span><span class="s3">, </span><span class="s4">3</span><span class="s3">, </span><span class="s1">status)  </span><span class="s0"># type: ignore</span>
    <span class="s1">status = jnp.where(state.nfev &gt;= maxfun</span><span class="s3">, </span><span class="s4">2</span><span class="s3">, </span><span class="s1">status)  </span><span class="s0"># type: ignore</span>
    <span class="s1">status = jnp.where(state.k &gt;= maxiter</span><span class="s3">, </span><span class="s4">1</span><span class="s3">, </span><span class="s1">status)  </span><span class="s0"># type: ignore</span>
    <span class="s1">status = jnp.where(ls_results.failed</span><span class="s3">, </span><span class="s4">5</span><span class="s3">, </span><span class="s1">status)</span>

    <span class="s1">converged = jnp.linalg.norm(g_kp1</span><span class="s3">, </span><span class="s1">ord=norm) &lt; gtol</span>

    <span class="s0"># TODO(jakevdp): use a fixed-point procedure rather than type-casting?</span>
    <span class="s1">state = state._replace(</span>
      <span class="s1">converged=converged</span><span class="s3">,</span>
      <span class="s1">failed=(status &gt; </span><span class="s4">0</span><span class="s1">) &amp; (~converged)</span><span class="s3">,</span>
      <span class="s1">k=state.k + </span><span class="s4">1</span><span class="s3">,</span>
      <span class="s1">nfev=state.nfev + ls_results.nfev</span><span class="s3">,</span>
      <span class="s1">ngev=state.ngev + ls_results.ngev</span><span class="s3">,</span>
      <span class="s1">x_k=x_kp1.astype(state.x_k.dtype)</span><span class="s3">,</span>
      <span class="s1">f_k=f_kp1.astype(state.f_k.dtype)</span><span class="s3">,</span>
      <span class="s1">g_k=g_kp1.astype(state.g_k.dtype)</span><span class="s3">,</span>
      <span class="s1">s_history=_update_history_vectors(history=state.s_history</span><span class="s3">, </span><span class="s1">new=s_k)</span><span class="s3">,</span>
      <span class="s1">y_history=_update_history_vectors(history=state.y_history</span><span class="s3">, </span><span class="s1">new=y_k)</span><span class="s3">,</span>
      <span class="s1">rho_history=_update_history_scalars(history=state.rho_history</span><span class="s3">, </span><span class="s1">new=rho_k)</span><span class="s3">,</span>
      <span class="s1">gamma=gamma.astype(state.g_k.dtype)</span><span class="s3">,</span>
      <span class="s1">status=jnp.where(converged</span><span class="s3">, </span><span class="s4">0</span><span class="s3">, </span><span class="s1">status)</span><span class="s3">,</span>
      <span class="s1">ls_status=ls_results.status</span><span class="s3">,</span>
    <span class="s1">)</span>

    <span class="s3">return </span><span class="s1">state</span>

  <span class="s3">return </span><span class="s1">lax.while_loop(cond_fun</span><span class="s3">, </span><span class="s1">body_fun</span><span class="s3">, </span><span class="s1">state_initial)</span>


<span class="s3">def </span><span class="s1">_two_loop_recursion(state: LBFGSResults):</span>
  <span class="s1">dtype = state.rho_history.dtype</span>
  <span class="s1">his_size = len(state.rho_history)</span>
  <span class="s1">curr_size = jnp.where(state.k &lt; his_size</span><span class="s3">, </span><span class="s1">state.k</span><span class="s3">, </span><span class="s1">his_size)</span>
  <span class="s1">q = -jnp.conj(state.g_k)</span>
  <span class="s1">a_his = jnp.zeros_like(state.rho_history)</span>

  <span class="s3">def </span><span class="s1">body_fun1(j</span><span class="s3">, </span><span class="s1">carry):</span>
    <span class="s1">i = his_size - </span><span class="s4">1 </span><span class="s1">- j</span>
    <span class="s1">_q</span><span class="s3">, </span><span class="s1">_a_his = carry</span>
    <span class="s1">a_i = state.rho_history[i] * _dot(jnp.conj(state.s_history[i])</span><span class="s3">, </span><span class="s1">_q).real.astype(dtype)</span>
    <span class="s1">_a_his = _a_his.at[i].set(a_i)</span>
    <span class="s1">_q = _q - a_i * jnp.conj(state.y_history[i])</span>
    <span class="s3">return </span><span class="s1">_q</span><span class="s3">, </span><span class="s1">_a_his</span>

  <span class="s1">q</span><span class="s3">, </span><span class="s1">a_his = lax.fori_loop(</span><span class="s4">0</span><span class="s3">, </span><span class="s1">curr_size</span><span class="s3">, </span><span class="s1">body_fun1</span><span class="s3">, </span><span class="s1">(q</span><span class="s3">, </span><span class="s1">a_his))</span>
  <span class="s1">q = state.gamma * q</span>

  <span class="s3">def </span><span class="s1">body_fun2(j</span><span class="s3">, </span><span class="s1">_q):</span>
    <span class="s1">i = his_size - curr_size + j</span>
    <span class="s1">b_i = state.rho_history[i] * _dot(state.y_history[i]</span><span class="s3">, </span><span class="s1">_q).real.astype(dtype)</span>
    <span class="s1">_q = _q + (a_his[i] - b_i) * state.s_history[i]</span>
    <span class="s3">return </span><span class="s1">_q</span>

  <span class="s1">q = lax.fori_loop(</span><span class="s4">0</span><span class="s3">, </span><span class="s1">curr_size</span><span class="s3">, </span><span class="s1">body_fun2</span><span class="s3">, </span><span class="s1">q)</span>
  <span class="s3">return </span><span class="s1">q</span>


<span class="s3">def </span><span class="s1">_update_history_vectors(history</span><span class="s3">, </span><span class="s1">new):</span>
  <span class="s0"># TODO(Jakob-Unfried) use rolling buffer instead? See #6053</span>
  <span class="s3">return </span><span class="s1">jnp.roll(history</span><span class="s3">, </span><span class="s1">-</span><span class="s4">1</span><span class="s3">, </span><span class="s1">axis=</span><span class="s4">0</span><span class="s1">).at[-</span><span class="s4">1</span><span class="s3">, </span><span class="s1">:].set(new)</span>


<span class="s3">def </span><span class="s1">_update_history_scalars(history</span><span class="s3">, </span><span class="s1">new):</span>
  <span class="s0"># TODO(Jakob-Unfried) use rolling buffer instead? See #6053</span>
  <span class="s3">return </span><span class="s1">jnp.roll(history</span><span class="s3">, </span><span class="s1">-</span><span class="s4">1</span><span class="s3">, </span><span class="s1">axis=</span><span class="s4">0</span><span class="s1">).at[-</span><span class="s4">1</span><span class="s1">].set(new)</span>
</pre>
</body>
</html>