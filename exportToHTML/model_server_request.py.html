<html>
<head>
<title>model_server_request.py</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<style type="text/css">
.s0 { color: #808080;}
.s1 { color: #a9b7c6;}
.s2 { color: #629755; font-style: italic;}
.s3 { color: #cc7832;}
.s4 { color: #6a8759;}
.s5 { color: #6897bb;}
</style>
</head>
<body bgcolor="#2b2b2b">
<table CELLSPACING=0 CELLPADDING=5 COLS=1 WIDTH="100%" BGCOLOR="#606060" >
<tr><td><center>
<font face="Arial, Helvetica" color="#000000">
model_server_request.py</font>
</center></td></tr></table>
<pre><span class="s0"># Copyright 2021 The JAX Authors.</span>
<span class="s0">#</span>
<span class="s0"># Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);</span>
<span class="s0"># you may not use this file except in compliance with the License.</span>
<span class="s0"># You may obtain a copy of the License at</span>
<span class="s0">#</span>
<span class="s0">#     https://www.apache.org/licenses/LICENSE-2.0</span>
<span class="s0">#</span>
<span class="s0"># Unless required by applicable law or agreed to in writing, software</span>
<span class="s0"># distributed under the License is distributed on an &quot;AS IS&quot; BASIS,</span>
<span class="s0"># WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span>
<span class="s0"># See the License for the specific language governing permissions and</span>
<span class="s0"># limitations under the License.</span>
<span class="s2">&quot;&quot;&quot;Demonstrates using jax2tf with TensorFlow model server. 
 
See README.md for instructions. 
&quot;&quot;&quot;</span>
<span class="s3">import </span><span class="s1">grpc  </span><span class="s0"># type: ignore[import]</span>
<span class="s3">import </span><span class="s1">json</span>
<span class="s3">import </span><span class="s1">logging</span>
<span class="s3">import </span><span class="s1">requests  </span><span class="s0"># type: ignore[import]</span>

<span class="s3">from </span><span class="s1">absl </span><span class="s3">import </span><span class="s1">app</span>
<span class="s3">from </span><span class="s1">absl </span><span class="s3">import </span><span class="s1">flags</span>

<span class="s3">from </span><span class="s1">jax.experimental.jax2tf.examples </span><span class="s3">import </span><span class="s1">mnist_lib  </span><span class="s0"># type: ignore</span>

<span class="s3">import </span><span class="s1">numpy </span><span class="s3">as </span><span class="s1">np</span>
<span class="s3">import </span><span class="s1">tensorflow </span><span class="s3">as </span><span class="s1">tf  </span><span class="s0"># type: ignore[import]</span>
<span class="s3">import </span><span class="s1">tensorflow_datasets </span><span class="s3">as </span><span class="s1">tfds  </span><span class="s0"># type: ignore[import]</span>
<span class="s3">from </span><span class="s1">tensorflow_serving.apis </span><span class="s3">import </span><span class="s1">predict_pb2  </span><span class="s0"># type: ignore[import]</span>
<span class="s3">from </span><span class="s1">tensorflow_serving.apis </span><span class="s3">import </span><span class="s1">prediction_service_pb2_grpc</span>


<span class="s1">FLAGS = flags.FLAGS</span>

<span class="s1">flags.DEFINE_boolean(</span>
    <span class="s4">&quot;use_grpc&quot;</span><span class="s3">, True,</span>
    <span class="s4">&quot;Use the gRPC API (default), or the HTTP REST API.&quot;</span><span class="s1">)</span>

<span class="s1">flags.DEFINE_string(</span>
    <span class="s4">&quot;model_spec_name&quot;</span><span class="s3">, </span><span class="s4">&quot;&quot;</span><span class="s3">,</span>
    <span class="s4">&quot;The name you used to export your model to model server (e.g., mnist_flax).&quot;</span><span class="s1">)</span>

<span class="s1">flags.DEFINE_string(</span>
    <span class="s4">&quot;prediction_service_addr&quot;</span><span class="s3">,</span>
    <span class="s4">&quot;localhost:8500&quot;</span><span class="s3">,</span>
    <span class="s4">&quot;Stubby endpoint for the prediction service. If you serve your model &quot;</span>
    <span class="s4">&quot;locally using TensorFlow model server, then you can use </span><span class="s3">\&quot;</span><span class="s4">localhost:8500</span><span class="s3">\&quot;</span><span class="s4">&quot;</span>
    <span class="s4">&quot;for the gRPC server and </span><span class="s3">\&quot;</span><span class="s4">localhost:8501</span><span class="s3">\&quot; </span><span class="s4">for the HTTP REST server.&quot;</span><span class="s1">)</span>

<span class="s1">flags.DEFINE_integer(</span><span class="s4">&quot;serving_batch_size&quot;</span><span class="s3">, </span><span class="s5">1</span><span class="s3">,</span>
                     <span class="s4">&quot;Batch size for the serving request. Must match the &quot;</span>
                     <span class="s4">&quot;batch size at which the model was saved. Must divide &quot;</span>
                     <span class="s4">&quot;--count_images&quot;</span><span class="s3">,</span>
                     <span class="s1">lower_bound=</span><span class="s5">1</span><span class="s1">)</span>
<span class="s1">flags.DEFINE_integer(</span><span class="s4">&quot;count_images&quot;</span><span class="s3">, </span><span class="s5">16</span><span class="s3">,</span>
                     <span class="s4">&quot;How many images to test.&quot;</span><span class="s3">,</span>
                     <span class="s1">lower_bound=</span><span class="s5">1</span><span class="s1">)</span>


<span class="s3">def </span><span class="s1">serving_call_mnist(images):</span>
  <span class="s2">&quot;&quot;&quot;Send an RPC or REST request to the model server. 
 
  Args: 
    images: A numpy.ndarray of shape [B, 28, 28, 1] with the batch of images to 
      perform inference on. 
 
  Returns: 
    A numpy.ndarray of shape [B, 10] with the one-hot inference response. 
  &quot;&quot;&quot;</span>
  <span class="s3">if </span><span class="s1">FLAGS.use_grpc:</span>
    <span class="s1">channel = grpc.insecure_channel(FLAGS.prediction_service_addr)</span>
    <span class="s1">stub = prediction_service_pb2_grpc.PredictionServiceStub(channel)</span>

    <span class="s1">request = predict_pb2.PredictRequest()</span>
    <span class="s1">request.model_spec.name = FLAGS.model_spec_name</span>
    <span class="s1">request.model_spec.signature_name = tf.saved_model.DEFAULT_SERVING_SIGNATURE_DEF_KEY</span>
    <span class="s0"># You can see the name of the input (&quot;inputs&quot;) in the SavedModel dump.</span>
    <span class="s1">request.inputs[</span><span class="s4">&quot;inputs&quot;</span><span class="s1">].CopyFrom(</span>
        <span class="s1">tf.make_tensor_proto(images</span><span class="s3">, </span><span class="s1">dtype=images.dtype</span><span class="s3">, </span><span class="s1">shape=images.shape))</span>
    <span class="s1">response = stub.Predict(request)</span>
    <span class="s0"># We could also use response.outputs[&quot;output_0&quot;], where &quot;output_0&quot; is the</span>
    <span class="s0"># name of the output (which you can see in the SavedModel dump.)</span>
    <span class="s0"># Alternatively, we just get the first output.</span>
    <span class="s1">outputs</span><span class="s3">, </span><span class="s1">= response.outputs.values()</span>
    <span class="s3">return </span><span class="s1">tf.make_ndarray(outputs)</span>
  <span class="s3">else</span><span class="s1">:</span>
    <span class="s0"># Use the HTTP REST api</span>
    <span class="s1">images_json = json.dumps(images.tolist())</span>
    <span class="s0"># You can see the name of the input (&quot;inputs&quot;) in the SavedModel dump.</span>
    <span class="s1">data = </span><span class="s4">f'</span><span class="s3">{{</span><span class="s4">&quot;inputs&quot;: </span><span class="s3">{</span><span class="s1">images_json</span><span class="s3">}}}</span><span class="s4">'</span>
    <span class="s1">predict_url = </span><span class="s4">f&quot;http://</span><span class="s3">{</span><span class="s1">FLAGS.prediction_service_addr</span><span class="s3">}</span><span class="s4">/v1/models/</span><span class="s3">{</span><span class="s1">FLAGS.model_spec_name</span><span class="s3">}</span><span class="s4">:predict&quot;</span>
    <span class="s1">response = requests.post(predict_url</span><span class="s3">, </span><span class="s1">data=data)</span>
    <span class="s3">if </span><span class="s1">response.status_code != </span><span class="s5">200</span><span class="s1">:</span>
      <span class="s1">msg = (</span><span class="s4">f&quot;Received error response </span><span class="s3">{</span><span class="s1">response.status_code</span><span class="s3">} </span><span class="s4">from model &quot;</span>
             <span class="s4">f&quot;server: </span><span class="s3">{</span><span class="s1">response.text</span><span class="s3">}</span><span class="s4">&quot;</span><span class="s1">)</span>
      <span class="s3">raise </span><span class="s1">ValueError(msg)</span>
    <span class="s1">outputs = response.json()[</span><span class="s4">&quot;outputs&quot;</span><span class="s1">]</span>
    <span class="s3">return </span><span class="s1">np.array(outputs)</span>


<span class="s3">def </span><span class="s1">main(_):</span>
  <span class="s3">if </span><span class="s1">FLAGS.count_images % FLAGS.serving_batch_size != </span><span class="s5">0</span><span class="s1">:</span>
    <span class="s3">raise </span><span class="s1">ValueError(</span><span class="s4">f&quot;The count_images (</span><span class="s3">{</span><span class="s1">FLAGS.count_images</span><span class="s3">}</span><span class="s4">) must be a &quot;</span>
                     <span class="s4">&quot;multiple of &quot;</span>
                     <span class="s4">f&quot;serving_batch_size (</span><span class="s3">{</span><span class="s1">FLAGS.serving_batch_size</span><span class="s3">}</span><span class="s4">)&quot;</span><span class="s1">)</span>
  <span class="s1">test_ds = mnist_lib.load_mnist(tfds.Split.TEST</span><span class="s3">,</span>
                                 <span class="s1">batch_size=FLAGS.serving_batch_size)</span>
  <span class="s1">images_and_labels = tfds.as_numpy(test_ds.take(</span>
      <span class="s1">FLAGS.count_images // FLAGS.serving_batch_size))</span>

  <span class="s1">accurate_count = </span><span class="s5">0</span>
  <span class="s3">for </span><span class="s1">batch_idx</span><span class="s3">, </span><span class="s1">(images</span><span class="s3">, </span><span class="s1">labels) </span><span class="s3">in </span><span class="s1">enumerate(images_and_labels):</span>
    <span class="s1">predictions_one_hot = serving_call_mnist(images)</span>
    <span class="s1">predictions_digit = np.argmax(predictions_one_hot</span><span class="s3">, </span><span class="s1">axis=</span><span class="s5">1</span><span class="s1">)</span>
    <span class="s1">labels_digit = np.argmax(labels</span><span class="s3">, </span><span class="s1">axis=</span><span class="s5">1</span><span class="s1">)</span>
    <span class="s1">accurate_count += np.sum(labels_digit == predictions_digit)</span>
    <span class="s1">running_accuracy = (</span>
        <span class="s5">100. </span><span class="s1">* accurate_count / (</span><span class="s5">1 </span><span class="s1">+ batch_idx) / FLAGS.serving_batch_size)</span>
    <span class="s1">logging.info(</span>
        <span class="s4">&quot; predicted digits = %s labels %s. Running accuracy %.3f%%&quot;</span><span class="s3">,</span>
        <span class="s1">predictions_digit</span><span class="s3">, </span><span class="s1">labels_digit</span><span class="s3">, </span><span class="s1">running_accuracy)</span>


<span class="s3">if </span><span class="s1">__name__ == </span><span class="s4">&quot;__main__&quot;</span><span class="s1">:</span>
  <span class="s1">app.run(main)</span>
</pre>
</body>
</html>