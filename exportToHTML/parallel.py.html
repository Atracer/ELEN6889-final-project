<html>
<head>
<title>parallel.py</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<style type="text/css">
.s0 { color: #808080;}
.s1 { color: #a9b7c6;}
.s2 { color: #629755; font-style: italic;}
.s3 { color: #cc7832;}
.s4 { color: #6a8759;}
.s5 { color: #6897bb;}
</style>
</head>
<body bgcolor="#2b2b2b">
<table CELLSPACING=0 CELLPADDING=5 COLS=1 WIDTH="100%" BGCOLOR="#606060" >
<tr><td><center>
<font face="Arial, Helvetica" color="#000000">
parallel.py</font>
</center></td></tr></table>
<pre><span class="s0"># Copyright 2019 The JAX Authors.</span>
<span class="s0">#</span>
<span class="s0"># Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);</span>
<span class="s0"># you may not use this file except in compliance with the License.</span>
<span class="s0"># You may obtain a copy of the License at</span>
<span class="s0">#</span>
<span class="s0">#     https://www.apache.org/licenses/LICENSE-2.0</span>
<span class="s0">#</span>
<span class="s0"># Unless required by applicable law or agreed to in writing, software</span>
<span class="s0"># distributed under the License is distributed on an &quot;AS IS&quot; BASIS,</span>
<span class="s0"># WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span>
<span class="s0"># See the License for the specific language governing permissions and</span>
<span class="s0"># limitations under the License.</span>
<span class="s2">&quot;&quot;&quot; 
Parallelization primitives. 
&quot;&quot;&quot;</span>

<span class="s3">from </span><span class="s1">functools </span><span class="s3">import </span><span class="s1">partial</span>
<span class="s3">import </span><span class="s1">itertools</span>
<span class="s3">import </span><span class="s1">math</span>
<span class="s3">import </span><span class="s1">string</span>
<span class="s3">from </span><span class="s1">typing </span><span class="s3">import </span><span class="s1">Sequence</span><span class="s3">, </span><span class="s1">Union</span>
<span class="s3">import </span><span class="s1">warnings</span>

<span class="s3">import </span><span class="s1">numpy </span><span class="s3">as </span><span class="s1">np</span>

<span class="s3">from </span><span class="s1">jax </span><span class="s3">import </span><span class="s1">tree_util</span>
<span class="s3">from </span><span class="s1">jax.interpreters </span><span class="s3">import </span><span class="s1">mlir</span>
<span class="s3">from </span><span class="s1">jax.interpreters </span><span class="s3">import </span><span class="s1">pxla</span>
<span class="s3">from </span><span class="s1">jax.interpreters </span><span class="s3">import </span><span class="s1">xla</span>

<span class="s3">from </span><span class="s1">jax._src </span><span class="s3">import </span><span class="s1">core</span>
<span class="s3">from </span><span class="s1">jax._src </span><span class="s3">import </span><span class="s1">dtypes</span>
<span class="s3">from </span><span class="s1">jax._src </span><span class="s3">import </span><span class="s1">util</span>
<span class="s3">from </span><span class="s1">jax._src.core </span><span class="s3">import </span><span class="s1">ShapedArray</span><span class="s3">, </span><span class="s1">AxisName</span><span class="s3">, </span><span class="s1">raise_to_shaped</span>
<span class="s3">from </span><span class="s1">jax._src.interpreters </span><span class="s3">import </span><span class="s1">ad</span>
<span class="s3">from </span><span class="s1">jax._src.interpreters </span><span class="s3">import </span><span class="s1">batching</span>
<span class="s3">from </span><span class="s1">jax._src.lax </span><span class="s3">import </span><span class="s1">lax</span>
<span class="s3">from </span><span class="s1">jax._src.lax </span><span class="s3">import </span><span class="s1">slicing</span>
<span class="s3">from </span><span class="s1">jax._src.lib.mlir </span><span class="s3">import </span><span class="s1">ir</span>
<span class="s3">from </span><span class="s1">jax._src.lib.mlir.dialects </span><span class="s3">import </span><span class="s1">hlo</span>
<span class="s3">from </span><span class="s1">jax._src.numpy </span><span class="s3">import </span><span class="s1">lax_numpy</span>
<span class="s3">from </span><span class="s1">jax._src.util </span><span class="s3">import </span><span class="s1">(</span>
    <span class="s1">unzip2</span><span class="s3">, </span><span class="s1">canonicalize_axis</span><span class="s3">, </span><span class="s1">safe_map</span><span class="s3">, </span><span class="s1">safe_zip</span><span class="s3">, </span><span class="s1">moveaxis)</span>

<span class="s1">unsafe_map</span><span class="s3">, </span><span class="s1">map = map</span><span class="s3">, </span><span class="s1">safe_map  </span><span class="s0"># type: ignore</span>


<span class="s0">### parallel traceables</span>

<span class="s3">def </span><span class="s1">psum(x</span><span class="s3">, </span><span class="s1">axis_name</span><span class="s3">, </span><span class="s1">*</span><span class="s3">, </span><span class="s1">axis_index_groups=</span><span class="s3">None</span><span class="s1">):</span>
  <span class="s2">&quot;&quot;&quot;Compute an all-reduce sum on ``x`` over the pmapped axis ``axis_name``. 
 
  If ``x`` is a pytree then the result is equivalent to mapping this function to 
  each leaf in the tree. 
 
  Inputs of boolean dtype are converted to integers before the reduction. 
 
  Args: 
    x: array(s) with a mapped axis named ``axis_name``. 
    axis_name: hashable Python object used to name a pmapped axis (see the 
      :func:`jax.pmap` documentation for more details). 
    axis_index_groups: optional list of lists containing axis indices (e.g. for 
      an axis of size 4, [[0, 1], [2, 3]] would perform psums over the first 
      two and last two replicas). Groups must cover all axis indices exactly 
      once. 
 
  Returns: 
    Array(s) with the same shape as ``x`` representing the result of an 
    all-reduce sum along the axis ``axis_name``. 
 
  Examples: 
    For example, with 4 XLA devices available: 
 
    &gt;&gt;&gt; x = np.arange(4) 
    &gt;&gt;&gt; y = jax.pmap(lambda x: jax.lax.psum(x, 'i'), axis_name='i')(x) 
    &gt;&gt;&gt; print(y) 
    [6 6 6 6] 
    &gt;&gt;&gt; y = jax.pmap(lambda x: x / jax.lax.psum(x, 'i'), axis_name='i')(x) 
    &gt;&gt;&gt; print(y) 
    [0.         0.16666667 0.33333334 0.5       ] 
 
    Suppose we want to perform ``psum`` among two groups, one with ``device0`` and ``device1``, the other with `device2` and `device3`, 
 
    &gt;&gt;&gt; y = jax.pmap(lambda x: jax.lax.psum(x, 'i', axis_index_groups=[[0, 1], [2, 3]]), axis_name='i')(x) 
    &gt;&gt;&gt; print(y) 
    [1 1 5 5] 
 
    An example using 2D-shaped x. Each row is data from one device. 
 
    &gt;&gt;&gt; x = np.arange(16).reshape(4, 4) 
    &gt;&gt;&gt; print(x) 
    [[ 0  1  2  3] 
     [ 4  5  6  7] 
     [ 8  9 10 11] 
     [12 13 14 15]] 
 
    Full ``psum`` across all devices: 
 
    &gt;&gt;&gt; y = jax.pmap(lambda x: jax.lax.psum(x, 'i'), axis_name='i')(x) 
    &gt;&gt;&gt; print(y) 
    [[24 28 32 36] 
     [24 28 32 36] 
     [24 28 32 36] 
     [24 28 32 36]] 
 
    Perform ``psum`` among two groups: 
 
    &gt;&gt;&gt; y = jax.pmap(lambda x: jax.lax.psum(x, 'i', axis_index_groups=[[0, 1], [2, 3]]), axis_name='i')(x) 
    &gt;&gt;&gt; print(y) 
    [[ 4  6  8 10] 
     [ 4  6  8 10] 
     [20 22 24 26] 
     [20 22 24 26]] 
  &quot;&quot;&quot;</span>
  <span class="s3">if not </span><span class="s1">isinstance(axis_name</span><span class="s3">, </span><span class="s1">(tuple</span><span class="s3">, </span><span class="s1">list)):</span>
    <span class="s1">axis_name = (axis_name</span><span class="s3">,</span><span class="s1">)</span>
  <span class="s3">if </span><span class="s1">any(isinstance(axis</span><span class="s3">, </span><span class="s1">int) </span><span class="s3">for </span><span class="s1">axis </span><span class="s3">in </span><span class="s1">axis_name) </span><span class="s3">and </span><span class="s1">axis_index_groups </span><span class="s3">is not None</span><span class="s1">:</span>
    <span class="s3">raise </span><span class="s1">ValueError(</span><span class="s4">&quot;axis_index_groups only supported for sums over just named axes&quot;</span><span class="s1">)</span>
  <span class="s1">_validate_reduce_axis_index_groups(axis_index_groups)</span>
  <span class="s1">leaves</span><span class="s3">, </span><span class="s1">treedef = tree_util.tree_flatten(x)</span>
  <span class="s1">leaves = [lax.convert_element_type(l</span><span class="s3">, </span><span class="s1">np.int32)</span>
            <span class="s3">if </span><span class="s1">dtypes.dtype(l) == np.bool_ </span><span class="s3">else </span><span class="s1">l </span><span class="s3">for </span><span class="s1">l </span><span class="s3">in </span><span class="s1">leaves]</span>
  <span class="s1">axis_index_groups = _canonicalize_axis_index_groups(axis_index_groups)</span>
  <span class="s1">out_flat = psum_p.bind(</span>
      <span class="s1">*leaves</span><span class="s3">, </span><span class="s1">axes=tuple(axis_name)</span><span class="s3">, </span><span class="s1">axis_index_groups=axis_index_groups)</span>
  <span class="s3">return </span><span class="s1">tree_util.tree_unflatten(treedef</span><span class="s3">, </span><span class="s1">out_flat)</span>

<span class="s3">def </span><span class="s1">pmean(x</span><span class="s3">, </span><span class="s1">axis_name</span><span class="s3">, </span><span class="s1">*</span><span class="s3">, </span><span class="s1">axis_index_groups=</span><span class="s3">None</span><span class="s1">):</span>
  <span class="s2">&quot;&quot;&quot;Compute an all-reduce mean on ``x`` over the pmapped axis ``axis_name``. 
 
  If ``x`` is a pytree then the result is equivalent to mapping this function to 
  each leaf in the tree. 
 
  Args: 
    x: array(s) with a mapped axis named ``axis_name``. 
    axis_name: hashable Python object used to name a pmapped axis (see the 
      :func:`jax.pmap` documentation for more details). 
    axis_index_groups: optional list of lists containing axis indices (e.g. for 
      an axis of size 4, [[0, 1], [2, 3]] would perform pmeans over the first 
      two and last two replicas). Groups must cover all axis indices exactly 
      once, and on TPUs all groups must be the same size. 
 
  Returns: 
    Array(s) with the same shape as ``x`` representing the result of an 
    all-reduce mean along the axis ``axis_name``. 
 
  For example, with 4 XLA devices available: 
 
  &gt;&gt;&gt; x = np.arange(4) 
  &gt;&gt;&gt; y = jax.pmap(lambda x: jax.lax.pmean(x, 'i'), axis_name='i')(x) 
  &gt;&gt;&gt; print(y) 
  [1.5 1.5 1.5 1.5] 
  &gt;&gt;&gt; y = jax.pmap(lambda x: x / jax.lax.pmean(x, 'i'), axis_name='i')(x) 
  &gt;&gt;&gt; print(y) 
  [0.        0.6666667 1.3333334 2.       ] 
  &quot;&quot;&quot;</span>
  <span class="s1">x = psum(x</span><span class="s3">, </span><span class="s1">axis_name=axis_name</span><span class="s3">, </span><span class="s1">axis_index_groups=axis_index_groups)</span>
  <span class="s1">n = psum(</span><span class="s5">1</span><span class="s3">, </span><span class="s1">axis_name=axis_name</span><span class="s3">, </span><span class="s1">axis_index_groups=axis_index_groups)</span>
  <span class="s3">return </span><span class="s1">tree_util.tree_map(</span><span class="s3">lambda </span><span class="s1">v: v / n</span><span class="s3">, </span><span class="s1">x)</span>

<span class="s3">def </span><span class="s1">pmax(x</span><span class="s3">, </span><span class="s1">axis_name</span><span class="s3">, </span><span class="s1">*</span><span class="s3">, </span><span class="s1">axis_index_groups=</span><span class="s3">None</span><span class="s1">):</span>
  <span class="s2">&quot;&quot;&quot;Compute an all-reduce max on ``x`` over the pmapped axis ``axis_name``. 
 
  If ``x`` is a pytree then the result is equivalent to mapping this function to 
  each leaf in the tree. 
 
  Args: 
    x: array(s) with a mapped axis named ``axis_name``. 
    axis_name: hashable Python object used to name a pmapped axis (see the 
      :func:`jax.pmap` documentation for more details). 
    axis_index_groups: optional list of lists containing axis indices (e.g. for 
      an axis of size 4, [[0, 1], [2, 3]] would perform pmaxes over the first 
      two and last two replicas). Groups must cover all axis indices exactly 
      once, and on TPUs all groups must be the same size. 
 
  Returns: 
    Array(s) with the same shape as ``x`` representing the result of an 
    all-reduce max along the axis ``axis_name``. 
  &quot;&quot;&quot;</span>
  <span class="s3">if not </span><span class="s1">isinstance(axis_name</span><span class="s3">, </span><span class="s1">(tuple</span><span class="s3">, </span><span class="s1">list)):</span>
    <span class="s1">axis_name = (axis_name</span><span class="s3">,</span><span class="s1">)</span>
  <span class="s3">if </span><span class="s1">any(isinstance(axis</span><span class="s3">, </span><span class="s1">int) </span><span class="s3">for </span><span class="s1">axis </span><span class="s3">in </span><span class="s1">axis_name) </span><span class="s3">and </span><span class="s1">axis_index_groups </span><span class="s3">is not None</span><span class="s1">:</span>
    <span class="s3">raise </span><span class="s1">ValueError(</span><span class="s4">&quot;axis_index_groups only supported for sums over just named axes&quot;</span><span class="s1">)</span>
  <span class="s1">_validate_reduce_axis_index_groups(axis_index_groups)</span>
  <span class="s1">leaves</span><span class="s3">, </span><span class="s1">treedef = tree_util.tree_flatten(x)</span>
  <span class="s1">axis_index_groups = _canonicalize_axis_index_groups(axis_index_groups)</span>
  <span class="s1">out_flat = pmax_p.bind(*leaves</span><span class="s3">, </span><span class="s1">axes=axis_name</span><span class="s3">,</span>
                         <span class="s1">axis_index_groups=axis_index_groups)</span>
  <span class="s3">return </span><span class="s1">tree_util.tree_unflatten(treedef</span><span class="s3">, </span><span class="s1">out_flat)</span>

<span class="s3">def </span><span class="s1">pmin(x</span><span class="s3">, </span><span class="s1">axis_name</span><span class="s3">, </span><span class="s1">*</span><span class="s3">, </span><span class="s1">axis_index_groups=</span><span class="s3">None</span><span class="s1">):</span>
  <span class="s2">&quot;&quot;&quot;Compute an all-reduce min on ``x`` over the pmapped axis ``axis_name``. 
 
  If ``x`` is a pytree then the result is equivalent to mapping this function to 
  each leaf in the tree. 
 
  Args: 
    x: array(s) with a mapped axis named ``axis_name``. 
    axis_name: hashable Python object used to name a pmapped axis (see the 
      :func:`jax.pmap` documentation for more details). 
    axis_index_groups: optional list of lists containing axis indices (e.g. for 
      an axis of size 4, [[0, 1], [2, 3]] would perform pmins over the first 
      two and last two replicas). Groups must cover all axis indices exactly 
      once, and on TPUs all groups must be the same size. 
 
  Returns: 
    Array(s) with the same shape as ``x`` representing the result of an 
    all-reduce min along the axis ``axis_name``. 
  &quot;&quot;&quot;</span>
  <span class="s3">if not </span><span class="s1">isinstance(axis_name</span><span class="s3">, </span><span class="s1">(tuple</span><span class="s3">, </span><span class="s1">list)):</span>
    <span class="s1">axis_name = (axis_name</span><span class="s3">,</span><span class="s1">)</span>
  <span class="s3">if </span><span class="s1">any(isinstance(axis</span><span class="s3">, </span><span class="s1">int) </span><span class="s3">for </span><span class="s1">axis </span><span class="s3">in </span><span class="s1">axis_name) </span><span class="s3">and </span><span class="s1">axis_index_groups </span><span class="s3">is not None</span><span class="s1">:</span>
    <span class="s3">raise </span><span class="s1">ValueError(</span><span class="s4">&quot;axis_index_groups only supported for sums over just named axes&quot;</span><span class="s1">)</span>
  <span class="s1">_validate_reduce_axis_index_groups(axis_index_groups)</span>
  <span class="s1">leaves</span><span class="s3">, </span><span class="s1">treedef = tree_util.tree_flatten(x)</span>
  <span class="s1">axis_index_groups = _canonicalize_axis_index_groups(axis_index_groups)</span>
  <span class="s1">out_flat = pmin_p.bind(*leaves</span><span class="s3">, </span><span class="s1">axes=axis_name</span><span class="s3">,</span>
                         <span class="s1">axis_index_groups=axis_index_groups)</span>
  <span class="s3">return </span><span class="s1">tree_util.tree_unflatten(treedef</span><span class="s3">, </span><span class="s1">out_flat)</span>

<span class="s0"># TODO(mattjj): add a pargmin_p, or add named axis support to lax.argmin_p</span>
<span class="s3">def </span><span class="s1">pargmin(x</span><span class="s3">, </span><span class="s1">axis_name):</span>
  <span class="s3">if </span><span class="s1">isinstance(axis_name</span><span class="s3">, </span><span class="s1">(tuple</span><span class="s3">, </span><span class="s1">list)):</span>
    <span class="s3">raise </span><span class="s1">TypeError(</span><span class="s4">f&quot;pargmin only accepts a single axis, got </span><span class="s3">{</span><span class="s1">axis_name</span><span class="s3">}</span><span class="s4">&quot;</span><span class="s1">)</span>
  <span class="s3">return </span><span class="s1">_axis_index_of_val(x</span><span class="s3">, </span><span class="s1">pmin(x</span><span class="s3">, </span><span class="s1">axis_name)</span><span class="s3">, </span><span class="s1">axis_name)</span>

<span class="s0"># TODO(mattjj): add a pargmax_p, or add named axis support to lax.argmax_p</span>
<span class="s3">def </span><span class="s1">pargmax(x</span><span class="s3">, </span><span class="s1">axis_name):</span>
  <span class="s3">if </span><span class="s1">isinstance(axis_name</span><span class="s3">, </span><span class="s1">(tuple</span><span class="s3">, </span><span class="s1">list)):</span>
    <span class="s3">raise </span><span class="s1">TypeError(</span><span class="s4">f&quot;pargmin only accepts a single axis, got </span><span class="s3">{</span><span class="s1">axis_name</span><span class="s3">}</span><span class="s4">&quot;</span><span class="s1">)</span>
  <span class="s3">return </span><span class="s1">_axis_index_of_val(x</span><span class="s3">, </span><span class="s1">pmax(x</span><span class="s3">, </span><span class="s1">axis_name)</span><span class="s3">, </span><span class="s1">axis_name)</span>

<span class="s3">def </span><span class="s1">_axis_index_of_val(x</span><span class="s3">, </span><span class="s1">val</span><span class="s3">, </span><span class="s1">axis_name):</span>
  <span class="s1">idx = axis_index(axis_name)</span>
  <span class="s1">validx = lax_numpy.where(val == x</span><span class="s3">, </span><span class="s1">idx</span><span class="s3">, </span><span class="s1">dtypes.iinfo(dtypes.dtype(idx)).max)</span>
  <span class="s3">return </span><span class="s1">pmin(validx</span><span class="s3">, </span><span class="s1">axis_name)</span>

<span class="s3">def </span><span class="s1">_validate_reduce_axis_index_groups(axis_index_groups):</span>
  <span class="s3">if </span><span class="s1">axis_index_groups </span><span class="s3">is None</span><span class="s1">:</span>
    <span class="s3">return</span>
  <span class="s1">axis_space = range(sum(len(group) </span><span class="s3">for </span><span class="s1">group </span><span class="s3">in </span><span class="s1">axis_index_groups))</span>
  <span class="s3">if </span><span class="s1">{i </span><span class="s3">for </span><span class="s1">g </span><span class="s3">in </span><span class="s1">axis_index_groups </span><span class="s3">for </span><span class="s1">i </span><span class="s3">in </span><span class="s1">g} != set(axis_space):</span>
    <span class="s3">raise </span><span class="s1">ValueError(</span><span class="s4">&quot;axis_index_groups must cover all indices exactly once&quot;</span><span class="s1">)</span>

<span class="s3">def </span><span class="s1">_canonicalize_axis_index_groups(axis_index_groups):</span>
  <span class="s3">if </span><span class="s1">axis_index_groups </span><span class="s3">is None</span><span class="s1">:</span>
    <span class="s3">return</span>
  <span class="s3">return </span><span class="s1">tuple(map(tuple</span><span class="s3">, </span><span class="s1">axis_index_groups))</span>

<span class="s3">def </span><span class="s1">ppermute(x</span><span class="s3">, </span><span class="s1">axis_name</span><span class="s3">, </span><span class="s1">perm):</span>
  <span class="s2">&quot;&quot;&quot;Perform a collective permutation according to the permutation ``perm``. 
 
  If ``x`` is a pytree then the result is equivalent to mapping this function to 
  each leaf in the tree. 
 
  This function is an analog of the CollectivePermute HLO. 
 
  Args: 
    x: array(s) with a mapped axis named ``axis_name``. 
    axis_name: hashable Python object used to name a pmapped axis (see the 
      :func:`jax.pmap` documentation for more details). 
    perm: list of pairs of ints, representing 
      ``(source_index, destination_index)`` 
      pairs that encode how the mapped axis named ``axis_name`` should be 
      shuffled. The integer values are treated as indices into the mapped axis 
      ``axis_name``. Any two pairs should not have the same source index or the 
      same destination index. For each index of the axis ``axis_name`` that does 
      not correspond to a destination index in ``perm``, the corresponding 
      values in the result are filled with zeros of the appropriate type. 
 
  Returns: 
    Array(s) with the same shape as ``x`` with slices along the axis 
    ``axis_name`` gathered from ``x`` according to the permutation ``perm``. 
  &quot;&quot;&quot;</span>
  <span class="s3">return </span><span class="s1">tree_util.tree_map(</span>
      <span class="s1">partial(ppermute_p.bind</span><span class="s3">, </span><span class="s1">axis_name=axis_name</span><span class="s3">,</span>
              <span class="s1">perm=tuple(map(tuple</span><span class="s3">, </span><span class="s1">perm)))</span><span class="s3">, </span><span class="s1">x)</span>

<span class="s3">def </span><span class="s1">pshuffle(x</span><span class="s3">, </span><span class="s1">axis_name</span><span class="s3">, </span><span class="s1">perm):</span>
  <span class="s2">&quot;&quot;&quot;Convenience wrapper of jax.lax.ppermute with alternate permutation encoding 
 
  If ``x`` is a pytree then the result is equivalent to mapping this function to 
  each leaf in the tree. 
 
  Args: 
    x: array(s) with a mapped axis named ``axis_name``. 
    axis_name: hashable Python object used to name a pmapped axis (see the 
      :func:`jax.pmap` documentation for more details). 
    perm: list of of ints encoding sources for the permutation to be applied to 
      the axis named ``axis_name``, so that the output at axis index i 
      comes from the input at axis index perm[i]. Every integer in [0, N) should 
      be included exactly once for axis size N. 
 
  Returns: 
    Array(s) with the same shape as ``x`` with slices along the axis 
    ``axis_name`` gathered from ``x`` according to the permutation ``perm``. 
  &quot;&quot;&quot;</span>
  <span class="s3">if </span><span class="s1">set(perm) != set(range(len(perm))):</span>
    <span class="s3">raise </span><span class="s1">ValueError(</span><span class="s4">f&quot;`perm` does not represent a permutation: </span><span class="s3">{</span><span class="s1">perm</span><span class="s3">}</span><span class="s4">&quot;</span><span class="s1">)</span>
  <span class="s3">return </span><span class="s1">ppermute(x</span><span class="s3">, </span><span class="s1">axis_name</span><span class="s3">, </span><span class="s1">list(zip(perm</span><span class="s3">, </span><span class="s1">range(len(perm)))))</span>


<span class="s3">def </span><span class="s1">pswapaxes(x</span><span class="s3">, </span><span class="s1">axis_name</span><span class="s3">, </span><span class="s1">axis</span><span class="s3">, </span><span class="s1">*</span><span class="s3">, </span><span class="s1">axis_index_groups=</span><span class="s3">None</span><span class="s1">):</span>
  <span class="s2">&quot;&quot;&quot;Swap the pmapped axis ``axis_name`` with the unmapped axis ``axis``. 
 
  If ``x`` is a pytree then the result is equivalent to mapping this function to 
  each leaf in the tree. 
 
  The group size of the mapped axis size must be equal to the size of the 
  unmapped axis; that is, we must have 
  ``lax.psum(1, axis_name, axis_index_groups=axis_index_groups) == x.shape[axis]``. 
  By default, when ``axis_index_groups=None``, this encompasses all the devices. 
 
  This function is a special case of ``all_to_all`` where the pmapped axis of 
  the input is placed at the position ``axis`` in the output. That is, it is 
  equivalent to ``all_to_all(x, axis_name, axis, axis)``. 
 
  Args: 
    x: array(s) with a mapped axis named ``axis_name``. 
    axis_name: hashable Python object used to name a pmapped axis (see the 
      :func:`jax.pmap` documentation for more details). 
    axis: int indicating the unmapped axis of ``x`` to map with the name 
      ``axis_name``. 
    axis_index_groups: optional list of lists containing axis indices (e.g. for 
      an axis of size 4, [[0, 1], [2, 3]] would run pswapaxes over the first 
      two and last two replicas). Groups must cover all axis indices exactly 
      once, and all groups must be the same size. 
 
  Returns: 
    Array(s) with the same shape as ``x``. 
  &quot;&quot;&quot;</span>
  <span class="s3">return </span><span class="s1">all_to_all(x</span><span class="s3">, </span><span class="s1">axis_name</span><span class="s3">, </span><span class="s1">axis</span><span class="s3">, </span><span class="s1">axis</span><span class="s3">, </span><span class="s1">axis_index_groups=axis_index_groups)</span>

<span class="s3">def </span><span class="s1">all_to_all(x</span><span class="s3">, </span><span class="s1">axis_name</span><span class="s3">, </span><span class="s1">split_axis</span><span class="s3">, </span><span class="s1">concat_axis</span><span class="s3">, </span><span class="s1">*</span><span class="s3">, </span><span class="s1">axis_index_groups=</span><span class="s3">None, </span><span class="s1">tiled=</span><span class="s3">False</span><span class="s1">):</span>
  <span class="s2">&quot;&quot;&quot;Materialize the mapped axis and map a different axis. 
 
  If ``x`` is a pytree then the result is equivalent to mapping this function to 
  each leaf in the tree. 
 
  In the output, the input mapped axis ``axis_name`` is materialized at the 
  logical axis position ``concat_axis``, and the input unmapped axis at position 
  ``split_axis`` is mapped with the name ``axis_name``. 
 
  The group size of the mapped axis size must be equal to the size of the 
  unmapped axis; that is, we must have 
  ``lax.psum(1, axis_name, axis_index_groups=axis_index_groups) == x.shape[axis]``. 
  By default, when ``axis_index_groups=None``, this encompasses all the devices. 
 
  Args: 
    x: array(s) with a mapped axis named ``axis_name``. 
    axis_name: hashable Python object used to name a pmapped axis (see the 
      :func:`jax.pmap` documentation for more details). 
    split_axis: int indicating the unmapped axis of ``x`` to map with the name 
      ``axis_name``. 
    concat_axis: int indicating the position in the output to materialize the 
      mapped axis of the input with the name ``axis_name``. 
    axis_index_groups: optional list of lists containing axis indices (e.g. for 
      an axis of size 4, [[0, 1], [2, 3]] would run all_to_all over the first 
      two and last two replicas). Groups must cover all axis indices exactly 
      once, and all groups must be the same size. 
    tiled: when True, all_to_all will divide split_axis into chunks and concatenate 
      them along concat_axis. In particular, no dimensions are added or removed. 
      False by default. 
 
  Returns: 
    When tiled is False, array(s) with shape given by the expression:: 
 
      np.insert(np.delete(x.shape, split_axis), concat_axis, axis_size) 
 
    where ``axis_size`` is the size of the mapped axis named ``axis_name`` in 
    the input ``x``, i.e. ``axis_size = lax.psum(1, axis_name)``. 
 
    Otherwise array with shape similar to the input shape, except with split_axis 
    divided by axis size and concat_axis multiplied by axis size. 
  &quot;&quot;&quot;</span>
  <span class="s1">axis_index_groups = _canonicalize_axis_index_groups(axis_index_groups)</span>
  <span class="s3">def </span><span class="s1">bind(x</span><span class="s3">, </span><span class="s1">split_axis=split_axis</span><span class="s3">, </span><span class="s1">concat_axis=concat_axis):</span>
    <span class="s1">group_size = psum(</span><span class="s5">1</span><span class="s3">, </span><span class="s1">axis_name</span><span class="s3">, </span><span class="s1">axis_index_groups=axis_index_groups)</span>
    <span class="s3">if </span><span class="s1">tiled:</span>
      <span class="s3">if </span><span class="s1">x.shape[split_axis] % group_size != </span><span class="s5">0</span><span class="s1">:</span>
        <span class="s3">raise </span><span class="s1">ValueError(</span><span class="s4">f&quot;The size of all_to_all split_axis (</span><span class="s3">{</span><span class="s1">x.shape[split_axis]</span><span class="s3">}</span><span class="s4">) &quot;</span>
                         <span class="s4">f&quot;has to be divisible by the size of the named axis &quot;</span>
                         <span class="s4">f&quot;</span><span class="s3">{</span><span class="s1">axis_name</span><span class="s3">} </span><span class="s4">(</span><span class="s3">{</span><span class="s1">group_size</span><span class="s3">}</span><span class="s4">)&quot;</span><span class="s1">)</span>
    <span class="s3">else</span><span class="s1">:</span>
      <span class="s3">if </span><span class="s1">group_size != x.shape[split_axis]:</span>
        <span class="s1">msg = (</span><span class="s4">&quot;all_to_all requires the size of the mapped axis axis_name to &quot;</span>
               <span class="s4">&quot;equal x.shape[split_axis], but they are {} and {} respectively.&quot;</span><span class="s1">)</span>
        <span class="s3">raise </span><span class="s1">ValueError(msg.format(group_size</span><span class="s3">, </span><span class="s1">x.shape[split_axis]))</span>
      <span class="s3">if </span><span class="s1">split_axis &lt; concat_axis:</span>
        <span class="s1">concat_axis += </span><span class="s5">1  </span><span class="s0"># concat_axis gives a position _after_ split_axis is removed</span>
        <span class="s1">x = lax.expand_dims(x</span><span class="s3">, </span><span class="s1">(concat_axis</span><span class="s3">,</span><span class="s1">))  </span><span class="s0"># insert the new axis</span>
      <span class="s3">elif </span><span class="s1">split_axis == concat_axis:</span>
        <span class="s3">pass</span>
      <span class="s3">else</span><span class="s1">:  </span><span class="s0"># concat_axis &lt; split_axis</span>
        <span class="s1">x = lax.expand_dims(x</span><span class="s3">, </span><span class="s1">(concat_axis</span><span class="s3">,</span><span class="s1">))  </span><span class="s0"># insert the new axis</span>
        <span class="s1">split_axis += </span><span class="s5">1   </span><span class="s0"># we have a new axis before split_axis now</span>
    <span class="s1">result = all_to_all_p.bind(x</span><span class="s3">, </span><span class="s1">split_axis=split_axis</span><span class="s3">, </span><span class="s1">concat_axis=concat_axis</span><span class="s3">,</span>
                               <span class="s1">axis_name=axis_name</span><span class="s3">,</span>
                               <span class="s1">axis_index_groups=axis_index_groups)</span>
    <span class="s3">if not </span><span class="s1">tiled </span><span class="s3">and </span><span class="s1">split_axis != concat_axis:</span>
      <span class="s1">result = lax.squeeze(result</span><span class="s3">, </span><span class="s1">(split_axis</span><span class="s3">,</span><span class="s1">))</span>
    <span class="s3">return </span><span class="s1">result</span>

  <span class="s3">return </span><span class="s1">tree_util.tree_map(bind</span><span class="s3">, </span><span class="s1">x)</span>

<span class="s3">def </span><span class="s1">axis_index(axis_name):</span>
  <span class="s2">&quot;&quot;&quot;Return the index along the mapped axis ``axis_name``. 
 
  Args: 
    axis_name: hashable Python object used to name the mapped axis. 
 
  Returns: 
    An integer representing the index. 
 
  For example, with 8 XLA devices available: 
 
  &gt;&gt;&gt; from functools import partial 
  &gt;&gt;&gt; @partial(jax.pmap, axis_name='i') 
  ... def f(_): 
  ...   return lax.axis_index('i') 
  ... 
  &gt;&gt;&gt; f(np.zeros(4)) 
  Array([0, 1, 2, 3], dtype=int32) 
  &gt;&gt;&gt; f(np.zeros(8)) 
  Array([0, 1, 2, 3, 4, 5, 6, 7], dtype=int32) 
  &gt;&gt;&gt; @partial(jax.pmap, axis_name='i') 
  ... @partial(jax.pmap, axis_name='j') 
  ... def f(_): 
  ...   return lax.axis_index('i'), lax.axis_index('j') 
  ... 
  &gt;&gt;&gt; x, y = f(np.zeros((4, 2))) 
  &gt;&gt;&gt; print(x) 
  [[0 0] 
  [1 1] 
  [2 2] 
  [3 3]] 
  &gt;&gt;&gt; print(y) 
  [[0 1] 
  [0 1] 
  [0 1] 
  [0 1]] 
  &quot;&quot;&quot;</span>
  <span class="s3">return </span><span class="s1">axis_index_p.bind(axis_name=axis_name)</span>


<span class="s3">def </span><span class="s1">pdot(x</span><span class="s3">, </span><span class="s1">y</span><span class="s3">, </span><span class="s1">axis_name</span><span class="s3">, </span><span class="s1">pos_contract=(()</span><span class="s3">, </span><span class="s1">())</span><span class="s3">, </span><span class="s1">pos_batch=(()</span><span class="s3">, </span><span class="s1">())</span><span class="s3">,</span>
         <span class="s1">precision=</span><span class="s3">None</span><span class="s1">):</span>
  <span class="s3">if not </span><span class="s1">isinstance(axis_name</span><span class="s3">, </span><span class="s1">(list</span><span class="s3">, </span><span class="s1">tuple)):</span>
    <span class="s1">axis_name = (axis_name</span><span class="s3">,</span><span class="s1">)</span>
  <span class="s1">pos_contract = tuple(map(tuple</span><span class="s3">, </span><span class="s1">pos_contract))</span>
  <span class="s1">pos_batch = tuple(map(tuple</span><span class="s3">, </span><span class="s1">pos_batch))</span>
  <span class="s3">return </span><span class="s1">pdot_p.bind(x</span><span class="s3">, </span><span class="s1">y</span><span class="s3">, </span><span class="s1">axis_name=tuple(axis_name)</span><span class="s3">,</span>
                     <span class="s1">pos_contract=pos_contract</span><span class="s3">, </span><span class="s1">pos_batch=pos_batch</span><span class="s3">,</span>
                     <span class="s1">precision=lax.canonicalize_precision(precision))</span>


<span class="s3">def </span><span class="s1">xeinsum(spec: str</span><span class="s3">, </span><span class="s1">*operands):</span>
  <span class="s1">in_spec</span><span class="s3">, </span><span class="s1">out_spec = spec.split(</span><span class="s4">'-&gt;'</span><span class="s1">)</span>
  <span class="s1">all_in_subs</span><span class="s3">, </span><span class="s1">all_in_named = unzip2(XeinsumSpecParser(in_spec).parse_args())</span>
  <span class="s1">(out_subs</span><span class="s3">, </span><span class="s1">out_named)</span><span class="s3">, </span><span class="s1">= XeinsumSpecParser(out_spec).parse_args()</span>

  <span class="s3">if </span><span class="s1">len(operands) != len(all_in_named):</span>
    <span class="s3">raise </span><span class="s1">ValueError(</span><span class="s4">&quot;Expecting the same number of argument specs in the &quot;</span>
                     <span class="s4">&quot;subscript ({in_spec}) as the number of operands. But got &quot;</span>
                     <span class="s4">&quot;{len(all_in_named)} argument specs for &quot;</span>
                     <span class="s4">&quot;{len(operands)} operands&quot;</span><span class="s1">)</span>

  <span class="s3">if </span><span class="s1">len(operands) &gt; </span><span class="s5">2</span><span class="s1">:</span>
    <span class="s3">raise </span><span class="s1">NotImplementedError(</span><span class="s4">&quot;Only one or two operands are supported. &quot;</span>
                              <span class="s4">f&quot;But got </span><span class="s3">{</span><span class="s1">len(operands)</span><span class="s3">} </span><span class="s4">operands&quot;</span><span class="s1">)</span>

  <span class="s0"># output subs and named axes must appear in at least one of the inputs.</span>
  <span class="s3">if not </span><span class="s1">set(out_named).issubset(set().union(*all_in_named)):</span>
    <span class="s3">raise </span><span class="s1">ValueError(</span><span class="s4">&quot;Found named axes &quot;</span>
                     <span class="s4">f&quot;</span><span class="s3">{</span><span class="s1">set(out_named) - set().union(*all_in_named)</span><span class="s3">} </span><span class="s4">&quot;</span>
                     <span class="s4">&quot;appearing in the output spec but not in the input&quot;</span><span class="s1">)</span>
  <span class="s3">if not </span><span class="s1">set(out_subs).issubset(set().union(*all_in_subs)):</span>
    <span class="s3">raise </span><span class="s1">ValueError(</span><span class="s4">&quot;Found subscript(s) &quot;</span>
                     <span class="s4">f&quot;</span><span class="s3">{</span><span class="s1">set(out_subs) - set().union(*all_in_subs)</span><span class="s3">} </span><span class="s4">&quot;</span>
                     <span class="s4">&quot;appearing in the output spec but not in the input&quot;</span><span class="s1">)</span>

  <span class="s1">xs = list(operands)</span>
  <span class="s3">for </span><span class="s1">idx</span><span class="s3">, </span><span class="s1">(in_subs</span><span class="s3">, </span><span class="s1">in_named) </span><span class="s3">in </span><span class="s1">enumerate(safe_zip(all_in_subs</span><span class="s3">, </span><span class="s1">all_in_named)):</span>
    <span class="s0"># if a subscript axis appears only in one input and not the output, reduce!</span>
    <span class="s1">other_named = set().union(  </span><span class="s0"># type: ignore</span>
        <span class="s1">*[named </span><span class="s3">for </span><span class="s1">i</span><span class="s3">, </span><span class="s1">named </span><span class="s3">in </span><span class="s1">enumerate(all_in_named) </span><span class="s3">if </span><span class="s1">i != idx])</span>
    <span class="s1">other_subs = set().union(  </span><span class="s0"># type: ignore</span>
        <span class="s1">*[subs </span><span class="s3">for </span><span class="s1">i</span><span class="s3">, </span><span class="s1">subs </span><span class="s3">in </span><span class="s1">enumerate(all_in_subs) </span><span class="s3">if </span><span class="s1">i != idx])</span>

    <span class="s1">subs_reduce = list(set(in_subs) - {*out_subs</span><span class="s3">, </span><span class="s1">*other_subs})</span>
    <span class="s1">subs_reduce_axes = [in_subs.index(n) </span><span class="s3">for </span><span class="s1">n </span><span class="s3">in </span><span class="s1">subs_reduce]</span>
    <span class="s1">named_reduce_axes = list(set(in_named) - {*out_named</span><span class="s3">, </span><span class="s1">*other_named})</span>

    <span class="s3">if </span><span class="s1">subs_reduce_axes </span><span class="s3">or </span><span class="s1">named_reduce_axes:</span>
      <span class="s1">xs[idx] = psum(xs[idx]</span><span class="s3">, </span><span class="s1">axis_name=subs_reduce_axes + named_reduce_axes)</span>
      <span class="s3">for </span><span class="s1">i </span><span class="s3">in </span><span class="s1">sorted(subs_reduce_axes</span><span class="s3">, </span><span class="s1">reverse=</span><span class="s3">True</span><span class="s1">):</span>
        <span class="s3">del </span><span class="s1">all_in_subs[idx][i]</span>
      <span class="s3">for </span><span class="s1">named_axis </span><span class="s3">in </span><span class="s1">named_reduce_axes:</span>
        <span class="s1">all_in_named[idx].remove(named_axis)</span>

  <span class="s3">if </span><span class="s1">len(operands) == </span><span class="s5">1</span><span class="s1">:</span>
    <span class="s3">return </span><span class="s1">xs[</span><span class="s5">0</span><span class="s1">]</span>

  <span class="s3">if </span><span class="s1">len(operands) == </span><span class="s5">2</span><span class="s1">:</span>
    <span class="s1">x</span><span class="s3">, </span><span class="s1">y = xs</span>
    <span class="s1">lhs_subs</span><span class="s3">, </span><span class="s1">rhs_subs = all_in_subs</span>
    <span class="s1">lhs_named</span><span class="s3">, </span><span class="s1">rhs_named = all_in_named</span>

    <span class="s0"># if a named axis appears in both inputs and not the output, contract!</span>
    <span class="s1">named_contract = list((set(lhs_named) &amp; set(rhs_named)) - set(out_named))</span>

    <span class="s0"># if a subscript appears in both inputs and not the outputs, contract!</span>
    <span class="s1">subs_contract = (set(lhs_subs) &amp; set(rhs_subs)) - set(out_subs)</span>

    <span class="s1">pos_contract = unzip2((lhs_subs.index(n)</span><span class="s3">, </span><span class="s1">rhs_subs.index(n))</span>
                          <span class="s3">for </span><span class="s1">n </span><span class="s3">in </span><span class="s1">subs_contract)</span>

    <span class="s0"># if a subscript appears in both inputs _and_ the outputs, batch!</span>
    <span class="s1">subs_batch = (set(lhs_subs) &amp; set(rhs_subs)) - subs_contract</span>
    <span class="s1">pos_batch = unzip2((lhs_subs.index(n)</span><span class="s3">, </span><span class="s1">rhs_subs.index(n)) </span><span class="s3">for </span><span class="s1">n </span><span class="s3">in </span><span class="s1">subs_batch)</span>

    <span class="s3">return </span><span class="s1">pdot(x</span><span class="s3">, </span><span class="s1">y</span><span class="s3">, </span><span class="s1">axis_name=named_contract</span><span class="s3">,</span>
                <span class="s1">pos_contract=pos_contract</span><span class="s3">, </span><span class="s1">pos_batch=pos_batch)</span>


<span class="s3">class </span><span class="s1">XeinsumSpecParser:</span>
  <span class="s1">spec: str</span>
  <span class="s1">pos: int</span>

  <span class="s3">def </span><span class="s1">__init__(self</span><span class="s3">, </span><span class="s1">spec: str):</span>
    <span class="s1">self.spec = spec</span>
    <span class="s1">self.pos = </span><span class="s5">0</span>

  <span class="s1">@property</span>
  <span class="s3">def </span><span class="s1">eof(self):</span>
    <span class="s3">return </span><span class="s1">self.pos == len(self.spec)</span>

  <span class="s1">@property</span>
  <span class="s3">def </span><span class="s1">cur(self):</span>
    <span class="s3">return </span><span class="s1">self.spec[self.pos]</span>

  <span class="s3">def </span><span class="s1">parse_subscript(self):</span>
    <span class="s3">if </span><span class="s1">self.cur </span><span class="s3">in </span><span class="s1">string.ascii_lowercase:</span>
      <span class="s1">out = self.cur</span>
      <span class="s1">self.pos += </span><span class="s5">1</span>
      <span class="s3">return </span><span class="s1">out</span><span class="s3">, True</span>
    <span class="s3">else</span><span class="s1">:</span>
      <span class="s3">return None, False</span>

  <span class="s3">def </span><span class="s1">parse_axis_name(self):</span>
    <span class="s3">try</span><span class="s1">:</span>
      <span class="s1">end = self.spec.index(</span><span class="s4">'}'</span><span class="s3">, </span><span class="s1">self.pos)</span>
    <span class="s3">except </span><span class="s1">ValueError:</span>
      <span class="s3">assert False</span>

    <span class="s3">try</span><span class="s1">:</span>
      <span class="s1">end = self.spec.index(</span><span class="s4">','</span><span class="s3">, </span><span class="s1">self.pos</span><span class="s3">, </span><span class="s1">end)</span>
    <span class="s3">except </span><span class="s1">ValueError:</span>
      <span class="s3">pass</span>

    <span class="s1">axis_name = self.spec[self.pos:end]</span>
    <span class="s3">assert </span><span class="s1">axis_name</span>
    <span class="s1">self.pos = end</span>
    <span class="s3">return </span><span class="s1">axis_name</span>

  <span class="s3">def </span><span class="s1">maybe_take(self</span><span class="s3">, </span><span class="s1">char: str</span><span class="s3">, </span><span class="s1">on_eof: bool = </span><span class="s3">False</span><span class="s1">):</span>
    <span class="s3">if </span><span class="s1">self.eof:</span>
      <span class="s3">return </span><span class="s1">on_eof</span>
    <span class="s3">if </span><span class="s1">self.cur == char:</span>
      <span class="s1">self.pos += </span><span class="s5">1</span>
      <span class="s3">return True</span>

  <span class="s3">def </span><span class="s1">parse_arg(self):</span>
    <span class="s1">subscripts = []</span>
    <span class="s1">names = []</span>
    <span class="s3">while not </span><span class="s1">self.eof:</span>
      <span class="s1">subscript</span><span class="s3">, </span><span class="s1">cont = self.parse_subscript()</span>
      <span class="s3">if not </span><span class="s1">cont: </span><span class="s3">break</span>
      <span class="s1">subscripts.append(subscript)</span>
    <span class="s3">if </span><span class="s1">self.eof:</span>
      <span class="s3">return False, </span><span class="s1">(subscripts</span><span class="s3">, </span><span class="s1">names)</span>
    <span class="s3">if </span><span class="s1">self.maybe_take(</span><span class="s4">','</span><span class="s1">):</span>
      <span class="s3">return True, </span><span class="s1">(subscripts</span><span class="s3">, </span><span class="s1">names)</span>
    <span class="s3">else</span><span class="s1">:</span>
      <span class="s3">assert </span><span class="s1">self.maybe_take(</span><span class="s4">'{'</span><span class="s1">)</span>
      <span class="s1">first = </span><span class="s3">True</span>
      <span class="s3">while not </span><span class="s1">self.maybe_take(</span><span class="s4">'}'</span><span class="s1">):</span>
        <span class="s3">if not </span><span class="s1">first:</span>
          <span class="s3">assert </span><span class="s1">self.maybe_take(</span><span class="s4">','</span><span class="s1">)</span>
        <span class="s1">first = </span><span class="s3">False</span>
        <span class="s3">if </span><span class="s1">self.eof:</span>
          <span class="s3">raise </span><span class="s1">ValueError(</span><span class="s4">&quot;Unterminated named axis brace&quot;</span><span class="s1">)</span>
        <span class="s1">axis_name = self.parse_axis_name()</span>
        <span class="s1">names.append(axis_name)</span>
      <span class="s3">return </span><span class="s1">self.maybe_take(</span><span class="s4">','</span><span class="s3">, False</span><span class="s1">)</span><span class="s3">, </span><span class="s1">(subscripts</span><span class="s3">, </span><span class="s1">names)</span>

  <span class="s3">def </span><span class="s1">parse_args(self):</span>
    <span class="s1">arg_specs = []</span>
    <span class="s1">cont = </span><span class="s3">True</span>
    <span class="s3">while not </span><span class="s1">self.eof:</span>
      <span class="s1">cont</span><span class="s3">, </span><span class="s1">result = self.parse_arg()</span>
      <span class="s1">arg_specs.append(result)</span>
    <span class="s3">if </span><span class="s1">cont:</span>
      <span class="s1">arg_specs.append(([]</span><span class="s3">, </span><span class="s1">[]))</span>
    <span class="s3">return </span><span class="s1">arg_specs</span>


<span class="s3">def </span><span class="s1">pgather(src</span><span class="s3">, </span><span class="s1">idx</span><span class="s3">, </span><span class="s1">axes: Union[int</span><span class="s3">, </span><span class="s1">AxisName]):</span>
  <span class="s2">&quot;&quot;&quot;Uses the last positional axis of idx to index into src's axes.&quot;&quot;&quot;</span>
  <span class="s3">if not </span><span class="s1">isinstance(axes</span><span class="s3">, </span><span class="s1">(tuple</span><span class="s3">, </span><span class="s1">list)):</span>
    <span class="s1">axes = (axes</span><span class="s3">,</span><span class="s1">)</span>
  <span class="s0"># TODO: Canonicalize exes!</span>
  <span class="s3">return </span><span class="s1">pgather_p.bind(src</span><span class="s3">, </span><span class="s1">idx</span><span class="s3">, </span><span class="s1">axes=tuple(axes))</span>


<span class="s0">### parallel primitives</span>

<span class="s3">def </span><span class="s1">_subst_all_names_in_param(</span>
    <span class="s1">pname: str</span><span class="s3">, </span><span class="s1">params: core.ParamDict</span><span class="s3">, </span><span class="s1">subst: core.AxisSubst</span><span class="s3">, </span><span class="s1">traverse: bool) -&gt; core.ParamDict:</span>
  <span class="s1">axis_name = params[pname]</span>
  <span class="s3">if not </span><span class="s1">isinstance(axis_name</span><span class="s3">, </span><span class="s1">(tuple</span><span class="s3">, </span><span class="s1">list)):</span>
    <span class="s1">axis_name = (axis_name</span><span class="s3">,</span><span class="s1">)</span>
  <span class="s1">result = dict(params)</span>
  <span class="s1">result[pname] = sum(((name</span><span class="s3">,</span><span class="s1">) </span><span class="s3">if </span><span class="s1">isinstance(name</span><span class="s3">, </span><span class="s1">int) </span><span class="s3">else </span><span class="s1">subst(name)</span>
                       <span class="s3">for </span><span class="s1">name </span><span class="s3">in </span><span class="s1">axis_name)</span><span class="s3">,</span>
                      <span class="s1">())</span>
  <span class="s3">return </span><span class="s1">result</span>

<span class="s3">def </span><span class="s1">_reduction_with_positional_batcher(prim</span><span class="s3">, </span><span class="s1">vals_in</span><span class="s3">, </span><span class="s1">dims_in</span><span class="s3">, </span><span class="s1">axis_index_groups</span><span class="s3">,</span>
    <span class="s1">transform_unmapped</span><span class="s3">, </span><span class="s1">transform_mapped):</span>
  <span class="s3">if </span><span class="s1">axis_index_groups </span><span class="s3">is not None</span><span class="s1">:</span>
    <span class="s3">raise </span><span class="s1">NotImplementedError(</span><span class="s4">&quot;axis_index_groups not supported in vmap collectives. &quot;</span>
                              <span class="s4">&quot;Please open a feature request!&quot;</span><span class="s1">)</span>
  <span class="s1">vals_in = [val </span><span class="s3">if </span><span class="s1">d </span><span class="s3">is </span><span class="s1">batching.not_mapped </span><span class="s3">or </span><span class="s1">d == </span><span class="s5">0 </span><span class="s3">else </span><span class="s1">_moveaxis(d</span><span class="s3">, </span><span class="s5">0</span><span class="s3">, </span><span class="s1">val)</span>
             <span class="s3">for </span><span class="s1">val</span><span class="s3">, </span><span class="s1">d </span><span class="s3">in </span><span class="s1">zip(vals_in</span><span class="s3">, </span><span class="s1">dims_in)]</span>
  <span class="s1">mapped_vals_in</span><span class="s3">, </span><span class="s1">unmapped_vals_in = partitioned_vals_in = []</span><span class="s3">, </span><span class="s1">[]</span>
  <span class="s1">mapped_idxs</span><span class="s3">, </span><span class="s1">unmapped_idxs = partitioned_idxs = []</span><span class="s3">, </span><span class="s1">[]</span>
  <span class="s3">for </span><span class="s1">i</span><span class="s3">, </span><span class="s1">(val</span><span class="s3">, </span><span class="s1">d) </span><span class="s3">in </span><span class="s1">enumerate(zip(vals_in</span><span class="s3">, </span><span class="s1">dims_in)):</span>
    <span class="s1">partitioned_vals_in[d </span><span class="s3">is </span><span class="s1">batching.not_mapped].append(val)</span>
    <span class="s1">partitioned_idxs[d </span><span class="s3">is </span><span class="s1">batching.not_mapped].append(i)</span>
  <span class="s1">vals_out = [</span><span class="s3">None</span><span class="s1">] * len(vals_in)</span>
  <span class="s3">if </span><span class="s1">unmapped_vals_in:</span>
    <span class="s1">unmapped_axes</span><span class="s3">, </span><span class="s1">unmapped_vals_in = transform_unmapped(</span><span class="s5">0</span><span class="s3">, </span><span class="s1">unmapped_vals_in)</span>
    <span class="s1">unmapped_vals_out = prim.bind(*unmapped_vals_in</span><span class="s3">, </span><span class="s1">axes=unmapped_axes</span><span class="s3">, </span><span class="s1">axis_index_groups=</span><span class="s3">None</span><span class="s1">)</span>
    <span class="s3">for </span><span class="s1">i</span><span class="s3">, </span><span class="s1">val </span><span class="s3">in </span><span class="s1">zip(unmapped_idxs</span><span class="s3">, </span><span class="s1">unmapped_vals_out):</span>
      <span class="s1">vals_out[i] = val</span>
  <span class="s3">if </span><span class="s1">mapped_vals_in:</span>
    <span class="s1">mapped_axes</span><span class="s3">, </span><span class="s1">mapped_vals_in = transform_mapped(</span><span class="s5">0</span><span class="s3">, </span><span class="s1">mapped_vals_in)</span>
    <span class="s1">mapped_vals_out = prim.bind(*mapped_vals_in</span><span class="s3">, </span><span class="s1">axes=mapped_axes</span><span class="s3">, </span><span class="s1">axis_index_groups=</span><span class="s3">None</span><span class="s1">)</span>
    <span class="s3">for </span><span class="s1">i</span><span class="s3">, </span><span class="s1">val </span><span class="s3">in </span><span class="s1">zip(mapped_idxs</span><span class="s3">, </span><span class="s1">mapped_vals_out):</span>
      <span class="s1">vals_out[i] = val</span>
  <span class="s3">assert </span><span class="s1">all(v </span><span class="s3">is not None for </span><span class="s1">v </span><span class="s3">in </span><span class="s1">vals_out)</span>
  <span class="s3">return </span><span class="s1">vals_out</span>

<span class="s3">def </span><span class="s1">_reduction_batcher(prim</span><span class="s3">, </span><span class="s1">vals_in</span><span class="s3">, </span><span class="s1">dims_in</span><span class="s3">, </span><span class="s1">*</span><span class="s3">, </span><span class="s1">axes</span><span class="s3">, </span><span class="s1">axis_index_groups):</span>
  <span class="s3">assert </span><span class="s1">prim.multiple_results</span>
  <span class="s3">if not </span><span class="s1">any(isinstance(axis</span><span class="s3">, </span><span class="s1">int) </span><span class="s3">for </span><span class="s1">axis </span><span class="s3">in </span><span class="s1">axes):</span>
    <span class="s3">return </span><span class="s1">prim.bind(*vals_in</span><span class="s3">, </span><span class="s1">axes=axes</span><span class="s3">, </span><span class="s1">axis_index_groups=axis_index_groups)</span><span class="s3">, </span><span class="s1">dims_in</span>
  <span class="s1">vals_out = _reduction_with_positional_batcher(</span>
      <span class="s1">prim</span><span class="s3">, </span><span class="s1">vals_in</span><span class="s3">, </span><span class="s1">dims_in</span><span class="s3">, </span><span class="s1">axis_index_groups</span><span class="s3">,</span>
      <span class="s3">lambda </span><span class="s1">d</span><span class="s3">, </span><span class="s1">d_vals_in: (axes</span><span class="s3">, </span><span class="s1">d_vals_in)</span><span class="s3">,</span>
      <span class="s3">lambda </span><span class="s1">d</span><span class="s3">, </span><span class="s1">d_vals_in: (tuple(axis + (axis &gt;= d) </span><span class="s3">if </span><span class="s1">isinstance(axis</span><span class="s3">, </span><span class="s1">int) </span><span class="s3">else </span><span class="s1">axis</span>
                                  <span class="s3">for </span><span class="s1">axis </span><span class="s3">in </span><span class="s1">axes)</span><span class="s3">,</span>
                            <span class="s1">d_vals_in))</span>
  <span class="s0"># _reduction_with_positional_batcher moves all map dims to 0</span>
  <span class="s3">return </span><span class="s1">vals_out</span><span class="s3">, </span><span class="s1">[d </span><span class="s3">if </span><span class="s1">d </span><span class="s3">is </span><span class="s1">batching.not_mapped </span><span class="s3">else </span><span class="s5">0 </span><span class="s3">for </span><span class="s1">d </span><span class="s3">in </span><span class="s1">dims_in]</span>

<span class="s3">def </span><span class="s1">_batched_reduction_collective(</span>
    <span class="s1">prim</span><span class="s3">, </span><span class="s1">if_unmapped</span><span class="s3">, </span><span class="s1">axis_size</span><span class="s3">, </span><span class="s1">frame_name</span><span class="s3">, </span><span class="s1">_</span><span class="s3">, </span><span class="s1">vals_in</span><span class="s3">, </span><span class="s1">dims_in</span><span class="s3">, </span><span class="s1">axes</span><span class="s3">,</span>
    <span class="s1">axis_index_groups):</span>
  <span class="s3">assert </span><span class="s1">prim.multiple_results</span>
  <span class="s3">assert </span><span class="s1">frame_name </span><span class="s3">in </span><span class="s1">axes</span>
  <span class="s0"># Note that we have a choice here. We can either unfuse the reduction into one</span>
  <span class="s0"># that handles the batched dims and then another one that handles the rest.</span>
  <span class="s0"># Alternatively, we can keep the dimension reduction fused with the rest, but</span>
  <span class="s0"># we have to split the primitive into one for unmapped inputs and another</span>
  <span class="s0"># one for mapped, because they differ in their `axes` parameter.</span>
  <span class="s0"># We choose the second strategy here.</span>
  <span class="s1">vals_out = _reduction_with_positional_batcher(</span>
      <span class="s1">prim</span><span class="s3">, </span><span class="s1">vals_in</span><span class="s3">, </span><span class="s1">dims_in</span><span class="s3">, </span><span class="s1">axis_index_groups</span><span class="s3">,</span>
      <span class="s3">lambda </span><span class="s1">d</span><span class="s3">, </span><span class="s1">d_vals_in: (tuple(axis </span><span class="s3">for </span><span class="s1">axis </span><span class="s3">in </span><span class="s1">axes </span><span class="s3">if </span><span class="s1">axis != frame_name)</span><span class="s3">,</span>
                            <span class="s1">[if_unmapped(v</span><span class="s3">, </span><span class="s1">axis_size) </span><span class="s3">for </span><span class="s1">v </span><span class="s3">in </span><span class="s1">d_vals_in])</span><span class="s3">,</span>
      <span class="s3">lambda </span><span class="s1">d</span><span class="s3">, </span><span class="s1">d_vals_in: (tuple(axis + (axis &gt;= d) </span><span class="s3">if </span><span class="s1">isinstance(axis</span><span class="s3">, </span><span class="s1">int) </span><span class="s3">else</span>
                                  <span class="s1">axis </span><span class="s3">if </span><span class="s1">axis != frame_name </span><span class="s3">else</span>
                                  <span class="s1">d</span>
                                  <span class="s3">for </span><span class="s1">axis </span><span class="s3">in </span><span class="s1">axes)</span><span class="s3">,</span>
                            <span class="s1">d_vals_in))</span>
  <span class="s3">return </span><span class="s1">vals_out</span><span class="s3">, </span><span class="s1">[batching.not_mapped] * len(vals_out)</span>

<span class="s3">def </span><span class="s1">_replica_groups(axis_env</span><span class="s3">, </span><span class="s1">axis_name</span><span class="s3">, </span><span class="s1">axis_index_groups):</span>
  <span class="s1">replica_groups = xla.axis_groups(axis_env</span><span class="s3">, </span><span class="s1">axis_name)</span>
  <span class="s3">if </span><span class="s1">axis_index_groups </span><span class="s3">is not None</span><span class="s1">:</span>
    <span class="s1">replica_groups = [[axis_group[i] </span><span class="s3">for </span><span class="s1">i </span><span class="s3">in </span><span class="s1">axis_index_group]</span>
                      <span class="s3">for </span><span class="s1">axis_group </span><span class="s3">in </span><span class="s1">replica_groups</span>
                      <span class="s3">for </span><span class="s1">axis_index_group </span><span class="s3">in </span><span class="s1">axis_index_groups]</span>
  <span class="s3">return </span><span class="s1">replica_groups</span>

<span class="s3">def </span><span class="s1">_replica_groups_hlo(replica_groups: Sequence[Sequence[int]]</span>
                        <span class="s1">) -&gt; ir.DenseIntElementsAttr:</span>
  <span class="s0"># Uneven replica groups are padded with -1.</span>
  <span class="s1">groups = np.array(list(itertools.zip_longest(*replica_groups</span><span class="s3">, </span><span class="s1">fillvalue=-</span><span class="s5">1</span><span class="s1">))</span><span class="s3">,</span>
                    <span class="s1">dtype=np.int64).T</span>
  <span class="s3">return </span><span class="s1">ir.DenseIntElementsAttr.get(np.ascontiguousarray(groups))</span>

<span class="s3">def </span><span class="s1">_allreduce_impl(pos_reducer</span><span class="s3">, </span><span class="s1">*args</span><span class="s3">, </span><span class="s1">axes</span><span class="s3">, </span><span class="s1">axis_index_groups):</span>
  <span class="s3">assert </span><span class="s1">axis_index_groups </span><span class="s3">is None</span>
  <span class="s3">assert </span><span class="s1">all(isinstance(axis</span><span class="s3">, </span><span class="s1">int) </span><span class="s3">for </span><span class="s1">axis </span><span class="s3">in </span><span class="s1">axes)</span>
  <span class="s3">return </span><span class="s1">[pos_reducer(arg</span><span class="s3">, </span><span class="s1">axes) </span><span class="s3">for </span><span class="s1">arg </span><span class="s3">in </span><span class="s1">args]</span>

<span class="s3">def </span><span class="s1">_allreduce_abstract_eval(*args</span><span class="s3">, </span><span class="s1">axes</span><span class="s3">, </span><span class="s1">axis_index_groups):</span>
  <span class="s0"># TODO(frostig,mattjj,jekbradbury): maybe check aval names here</span>
  <span class="s1">pos_axes = tuple(axis </span><span class="s3">for </span><span class="s1">axis </span><span class="s3">in </span><span class="s1">axes </span><span class="s3">if </span><span class="s1">isinstance(axis</span><span class="s3">, </span><span class="s1">int))</span>
  <span class="s1">named_shapes = [arg.named_shape </span><span class="s3">for </span><span class="s1">arg </span><span class="s3">in </span><span class="s1">args]</span>
  <span class="s3">if </span><span class="s1">axis_index_groups </span><span class="s3">is None</span><span class="s1">:</span>
    <span class="s1">named_axes = {axis </span><span class="s3">for </span><span class="s1">axis </span><span class="s3">in </span><span class="s1">axes </span><span class="s3">if not </span><span class="s1">isinstance(axis</span><span class="s3">, </span><span class="s1">int)}</span>
    <span class="s1">named_shapes = [{name: size </span><span class="s3">for </span><span class="s1">name</span><span class="s3">, </span><span class="s1">size </span><span class="s3">in </span><span class="s1">arg.named_shape.items()</span>
                     <span class="s3">if </span><span class="s1">name </span><span class="s3">not in </span><span class="s1">named_axes} </span><span class="s3">for </span><span class="s1">arg </span><span class="s3">in </span><span class="s1">args]</span>
  <span class="s3">else</span><span class="s1">:</span>
    <span class="s3">if </span><span class="s1">len(pos_axes) != </span><span class="s5">0</span><span class="s1">:</span>
      <span class="s3">raise </span><span class="s1">ValueError(</span><span class="s4">f&quot;axis_index_groups can only be used with reductions over &quot;</span>
                       <span class="s4">f&quot;named axes, but got: </span><span class="s3">{</span><span class="s1">axes</span><span class="s3">}</span><span class="s4">&quot;</span><span class="s1">)</span>
  <span class="s3">return </span><span class="s1">[ShapedArray(lax._reduce_op_shape_rule(raise_to_shaped(arg)</span><span class="s3">, </span><span class="s1">axes=pos_axes)</span><span class="s3">,</span>
                      <span class="s1">arg.dtype</span><span class="s3">, </span><span class="s1">named_shape=named_shape)</span>
          <span class="s3">for </span><span class="s1">arg</span><span class="s3">, </span><span class="s1">named_shape </span><span class="s3">in </span><span class="s1">zip(args</span><span class="s3">, </span><span class="s1">named_shapes)]</span>

<span class="s3">def </span><span class="s1">_allreduce_lowering(prim</span><span class="s3">, </span><span class="s1">pos_fn</span><span class="s3">, </span><span class="s1">ctx</span><span class="s3">, </span><span class="s1">*args</span><span class="s3">, </span><span class="s1">axes</span><span class="s3">, </span><span class="s1">axis_index_groups):</span>
  <span class="s3">if </span><span class="s1">axis_index_groups </span><span class="s3">is not None and </span><span class="s1">ctx.module_context.platform == </span><span class="s4">&quot;tpu&quot;</span><span class="s1">:</span>
    <span class="s1">len_0 = len(axis_index_groups[</span><span class="s5">0</span><span class="s1">])</span>
    <span class="s3">if </span><span class="s1">any(len(g) != len_0 </span><span class="s3">for </span><span class="s1">g </span><span class="s3">in </span><span class="s1">axis_index_groups):</span>
      <span class="s3">raise </span><span class="s1">ValueError(</span><span class="s4">&quot;axis_index_groups must all be the same size&quot;</span><span class="s1">)</span>
  <span class="s1">named_axes</span><span class="s3">, </span><span class="s1">positional_axes = axes_partition = []</span><span class="s3">, </span><span class="s1">[]</span>
  <span class="s3">for </span><span class="s1">axis </span><span class="s3">in </span><span class="s1">axes:</span>
    <span class="s1">axes_partition[isinstance(axis</span><span class="s3">, </span><span class="s1">int)].append(axis)</span>

  <span class="s3">if </span><span class="s1">positional_axes:</span>
    <span class="s1">reducer = mlir.lower_fun(pos_fn</span><span class="s3">, </span><span class="s1">multiple_results=</span><span class="s3">False</span><span class="s1">)</span>
    <span class="s3">def </span><span class="s1">_positional_reduce(aval</span><span class="s3">, </span><span class="s1">arg):</span>
      <span class="s1">aval_out = aval.update(</span>
          <span class="s1">shape=np.delete(np.array(aval.shape</span><span class="s3">, </span><span class="s1">dtype=np.int64)</span><span class="s3">,</span>
                          <span class="s1">positional_axes))</span>
      <span class="s1">reducer_ctx = ctx.replace(primitive=</span><span class="s3">None, </span><span class="s1">avals_in=[aval]</span><span class="s3">, </span><span class="s1">avals_out=[aval_out])</span>
      <span class="s1">out</span><span class="s3">, </span><span class="s1">= reducer(reducer_ctx</span><span class="s3">, </span><span class="s1">arg</span><span class="s3">, </span><span class="s1">axes=tuple(positional_axes))[</span><span class="s5">0</span><span class="s1">]</span>
      <span class="s3">return </span><span class="s1">out</span>
    <span class="s1">args = map(_positional_reduce</span><span class="s3">, </span><span class="s1">ctx.avals_in</span><span class="s3">, </span><span class="s1">args)</span>
  <span class="s3">if not </span><span class="s1">named_axes:</span>
    <span class="s3">return </span><span class="s1">args</span>

  <span class="s1">replica_groups = _replica_groups_hlo(</span>
      <span class="s1">_replica_groups(ctx.module_context.axis_env</span><span class="s3">, </span><span class="s1">named_axes</span><span class="s3">,</span>
                      <span class="s1">axis_index_groups))</span>
  <span class="s1">axis_context = ctx.module_context.axis_context</span>
  <span class="s1">is_spmd = isinstance(axis_context</span><span class="s3">,</span>
                       <span class="s1">(mlir.SPMDAxisContext</span><span class="s3">, </span><span class="s1">mlir.ShardingContext))</span>

  <span class="s3">def </span><span class="s1">all_reduce(aval</span><span class="s3">, </span><span class="s1">x):</span>
    <span class="s3">if </span><span class="s1">is_spmd:</span>
      <span class="s1">channel = ctx.module_context.new_channel()</span>
      <span class="s1">other_args = dict(</span>
          <span class="s1">channel_handle=hlo.ChannelHandle.get(</span>
              <span class="s1">channel</span><span class="s3">, </span><span class="s1">mlir.DEVICE_TO_DEVICE_TYPE)</span><span class="s3">,</span>
          <span class="s1">use_global_device_ids=ir.BoolAttr.get(</span><span class="s3">True</span><span class="s1">))</span>
    <span class="s3">else</span><span class="s1">:</span>
      <span class="s1">other_args = {}</span>
    <span class="s1">op = hlo.AllReduceOp(</span>
        <span class="s1">x.type</span><span class="s3">, </span><span class="s1">x</span><span class="s3">, </span><span class="s1">replica_groups=replica_groups</span><span class="s3">, </span><span class="s1">**other_args)</span>
    <span class="s1">scalar_aval = core.ShapedArray(()</span><span class="s3">, </span><span class="s1">aval.dtype)</span>
    <span class="s1">scalar_type = mlir.aval_to_ir_type(scalar_aval)</span>
    <span class="s1">reducer_block = op.regions[</span><span class="s5">0</span><span class="s1">].blocks.append(scalar_type</span><span class="s3">, </span><span class="s1">scalar_type)</span>
    <span class="s3">with </span><span class="s1">ir.InsertionPoint(reducer_block):</span>
      <span class="s1">lower_reducer = mlir.lower_fun(prim.bind</span><span class="s3">, </span><span class="s1">multiple_results=</span><span class="s3">False</span><span class="s1">)</span>
      <span class="s1">reducer_ctx = ctx.replace(primitive=</span><span class="s3">None,</span>
                                <span class="s1">avals_in=[scalar_aval] * </span><span class="s5">2</span><span class="s3">, </span><span class="s1">avals_out=[scalar_aval])</span>
      <span class="s1">out_nodes = lower_reducer(</span>
          <span class="s1">reducer_ctx</span><span class="s3">, </span><span class="s1">*([a] </span><span class="s3">for </span><span class="s1">a </span><span class="s3">in </span><span class="s1">reducer_block.arguments))</span>
      <span class="s1">hlo.ReturnOp(util.flatten(out_nodes))</span>
    <span class="s3">return </span><span class="s1">op.result</span>

  <span class="s3">return </span><span class="s1">[all_reduce(aval</span><span class="s3">, </span><span class="s1">x) </span><span class="s3">for </span><span class="s1">aval</span><span class="s3">, </span><span class="s1">x </span><span class="s3">in </span><span class="s1">zip(ctx.avals_in</span><span class="s3">, </span><span class="s1">args)]</span>


<span class="s3">def </span><span class="s1">_psum_transpose_rule(cts</span><span class="s3">, </span><span class="s1">*args</span><span class="s3">, </span><span class="s1">axes</span><span class="s3">, </span><span class="s1">axis_index_groups):</span>
  <span class="s1">named_axes</span><span class="s3">, </span><span class="s1">pos_axes = axes_partition = []</span><span class="s3">, </span><span class="s1">[]</span>
  <span class="s3">for </span><span class="s1">axis </span><span class="s3">in </span><span class="s1">axes:</span>
    <span class="s1">axes_partition[isinstance(axis</span><span class="s3">, </span><span class="s1">int)].append(axis)</span>

  <span class="s3">if </span><span class="s1">pos_axes:</span>
    <span class="s3">def </span><span class="s1">broadcast_positional(ct</span><span class="s3">, </span><span class="s1">arg):</span>
      <span class="s3">assert </span><span class="s1">ad.is_undefined_primal(arg)</span>
      <span class="s3">if </span><span class="s1">type(ct) </span><span class="s3">is </span><span class="s1">ad.Zero: </span><span class="s3">return </span><span class="s1">ad.Zero(arg.aval)</span>
      <span class="s3">return </span><span class="s1">lax._reduce_sum_transpose_rule(ct</span><span class="s3">, </span><span class="s1">arg</span><span class="s3">, </span><span class="s1">axes=pos_axes)[</span><span class="s5">0</span><span class="s1">]</span>
    <span class="s1">cts = map(broadcast_positional</span><span class="s3">, </span><span class="s1">cts</span><span class="s3">, </span><span class="s1">args)</span>

  <span class="s0"># We treat psum as psum + pbroadcast, which is why the transpose reduces</span>
  <span class="s0"># over the named axes again (unlike for positional axes).</span>
  <span class="s1">nonzero_out_cts</span><span class="s3">, </span><span class="s1">treedef = tree_util.tree_flatten(cts)</span>
  <span class="s1">nonzero_in_cts = psum_p.bind(*nonzero_out_cts</span><span class="s3">, </span><span class="s1">axes=tuple(named_axes)</span><span class="s3">,</span>
                               <span class="s1">axis_index_groups=axis_index_groups)</span>
  <span class="s3">return </span><span class="s1">tree_util.tree_unflatten(treedef</span><span class="s3">, </span><span class="s1">nonzero_in_cts)</span>

<span class="s1">psum_p = core.AxisPrimitive(</span><span class="s4">'psum'</span><span class="s1">)</span>
<span class="s1">psum_p.multiple_results = </span><span class="s3">True</span>
<span class="s1">psum_p.def_impl(partial(_allreduce_impl</span><span class="s3">, </span><span class="s1">lax._reduce_sum))</span>
<span class="s1">psum_p.def_abstract_eval(_allreduce_abstract_eval)</span>
<span class="s1">xla.register_collective_primitive(psum_p)</span>
<span class="s1">mlir.register_lowering(</span>
    <span class="s1">psum_p</span><span class="s3">, </span><span class="s1">partial(_allreduce_lowering</span><span class="s3">, </span><span class="s1">lax.add_p</span><span class="s3">, </span><span class="s1">lax._reduce_sum))</span>
<span class="s1">ad.deflinear2(psum_p</span><span class="s3">, </span><span class="s1">_psum_transpose_rule)</span>
<span class="s1">pxla.multi_host_supported_collectives.add(psum_p)</span>
<span class="s1">batching.primitive_batchers[psum_p] = partial(_reduction_batcher</span><span class="s3">, </span><span class="s1">psum_p)</span>
<span class="s1">batching.axis_primitive_batchers[psum_p] = \</span>
  <span class="s1">partial(_batched_reduction_collective</span><span class="s3">, </span><span class="s1">psum_p</span><span class="s3">, lambda </span><span class="s1">v</span><span class="s3">, </span><span class="s1">axis_size: axis_size * v)</span>
<span class="s1">core.axis_substitution_rules[psum_p] = partial(_subst_all_names_in_param</span><span class="s3">, </span><span class="s4">'axes'</span><span class="s1">)</span>

<span class="s0"># We set a special bind rule for psum so that psum(1, 'i') can be evaluated at</span>
<span class="s0"># tracing time.</span>
<span class="s1">@psum_p.def_custom_bind</span>
<span class="s3">def </span><span class="s1">psum_bind(*args</span><span class="s3">, </span><span class="s1">axes</span><span class="s3">, </span><span class="s1">axis_index_groups):</span>
  <span class="s3">if </span><span class="s1">all(</span><span class="s3">not </span><span class="s1">isinstance(x</span><span class="s3">, </span><span class="s1">core.Tracer) </span><span class="s3">for </span><span class="s1">x </span><span class="s3">in </span><span class="s1">args):</span>
    <span class="s1">named_axes</span><span class="s3">, </span><span class="s1">pos_axes = axes_partition = []</span><span class="s3">, </span><span class="s1">[]</span>
    <span class="s3">for </span><span class="s1">axis </span><span class="s3">in </span><span class="s1">axes:</span>
      <span class="s1">axes_partition[isinstance(axis</span><span class="s3">, </span><span class="s1">int)].append(axis)</span>
    <span class="s3">def </span><span class="s1">pos_reduce(x):</span>
      <span class="s3">if not </span><span class="s1">pos_axes:</span>
        <span class="s3">return </span><span class="s1">x</span>
      <span class="s3">return </span><span class="s1">lax._reduce_sum(x</span><span class="s3">, </span><span class="s1">[canonicalize_axis(axis</span><span class="s3">, </span><span class="s1">getattr(x</span><span class="s3">, </span><span class="s4">'ndim'</span><span class="s3">, </span><span class="s5">0</span><span class="s1">))</span>
                                 <span class="s3">for </span><span class="s1">axis </span><span class="s3">in </span><span class="s1">pos_axes])</span>
    <span class="s3">if </span><span class="s1">axis_index_groups </span><span class="s3">is not None</span><span class="s1">:</span>
      <span class="s3">assert not </span><span class="s1">pos_axes</span>
      <span class="s1">size = len(axis_index_groups[</span><span class="s5">0</span><span class="s1">])</span>
    <span class="s3">else</span><span class="s1">:</span>
      <span class="s1">size = math.prod([core.axis_frame(name).size </span><span class="s3">for </span><span class="s1">name </span><span class="s3">in </span><span class="s1">named_axes])  </span><span class="s0"># type: ignore</span>
    <span class="s3">return </span><span class="s1">tuple(lax._const(x</span><span class="s3">, </span><span class="s1">size) * pos_reduce(x) </span><span class="s3">for </span><span class="s1">x </span><span class="s3">in </span><span class="s1">args)</span>
  <span class="s3">return </span><span class="s1">core.AxisPrimitive.bind(</span>
      <span class="s1">psum_p</span><span class="s3">, </span><span class="s1">*args</span><span class="s3">, </span><span class="s1">axes=axes</span><span class="s3">, </span><span class="s1">axis_index_groups=axis_index_groups)</span>


<span class="s1">pmax_p = core.AxisPrimitive(</span><span class="s4">'pmax'</span><span class="s1">)</span>
<span class="s1">pmax_p.multiple_results = </span><span class="s3">True</span>
<span class="s1">pmax_p.def_impl(partial(_allreduce_impl</span><span class="s3">, </span><span class="s1">lax._reduce_max))</span>
<span class="s1">pmax_p.def_abstract_eval(_allreduce_abstract_eval)</span>
<span class="s1">xla.register_collective_primitive(pmax_p)</span>
<span class="s1">mlir.register_lowering(</span>
    <span class="s1">pmax_p</span><span class="s3">, </span><span class="s1">partial(_allreduce_lowering</span><span class="s3">, </span><span class="s1">lax.max_p</span><span class="s3">, </span><span class="s1">lax._reduce_max))</span>
<span class="s1">pxla.multi_host_supported_collectives.add(pmax_p)</span>
<span class="s1">batching.primitive_batchers[pmax_p] = partial(_reduction_batcher</span><span class="s3">, </span><span class="s1">pmax_p)</span>
<span class="s1">batching.axis_primitive_batchers[pmax_p] = \</span>
  <span class="s1">partial(_batched_reduction_collective</span><span class="s3">, </span><span class="s1">pmax_p</span><span class="s3">, lambda </span><span class="s1">v</span><span class="s3">, </span><span class="s1">axis_size: v)</span>
<span class="s1">core.axis_substitution_rules[pmax_p] = partial(_subst_all_names_in_param</span><span class="s3">, </span><span class="s4">'axes'</span><span class="s1">)</span>


<span class="s1">pmin_p = core.AxisPrimitive(</span><span class="s4">'pmin'</span><span class="s1">)</span>
<span class="s1">pmin_p.multiple_results = </span><span class="s3">True</span>
<span class="s1">pmin_p.def_impl(partial(_allreduce_impl</span><span class="s3">, </span><span class="s1">lax._reduce_min))</span>
<span class="s1">pmin_p.def_abstract_eval(_allreduce_abstract_eval)</span>
<span class="s1">xla.register_collective_primitive(pmin_p)</span>
<span class="s1">mlir.register_lowering(</span>
    <span class="s1">pmin_p</span><span class="s3">, </span><span class="s1">partial(_allreduce_lowering</span><span class="s3">, </span><span class="s1">lax.min_p</span><span class="s3">, </span><span class="s1">lax._reduce_min))</span>
<span class="s1">pxla.multi_host_supported_collectives.add(pmin_p)</span>
<span class="s1">batching.primitive_batchers[pmin_p] = partial(_reduction_batcher</span><span class="s3">, </span><span class="s1">pmin_p)</span>
<span class="s1">batching.axis_primitive_batchers[pmin_p] = \</span>
  <span class="s1">partial(_batched_reduction_collective</span><span class="s3">, </span><span class="s1">pmin_p</span><span class="s3">, lambda </span><span class="s1">v</span><span class="s3">, </span><span class="s1">axis_size: v)</span>
<span class="s1">core.axis_substitution_rules[pmin_p] = partial(_subst_all_names_in_param</span><span class="s3">, </span><span class="s4">'axes'</span><span class="s1">)</span>


<span class="s3">def </span><span class="s1">_ppermute_lowering(ctx</span><span class="s3">, </span><span class="s1">x</span><span class="s3">, </span><span class="s1">*</span><span class="s3">, </span><span class="s1">axis_name</span><span class="s3">, </span><span class="s1">perm):</span>
  <span class="s1">replica_groups = _replica_groups(ctx.module_context.axis_env</span><span class="s3">, </span><span class="s1">axis_name</span><span class="s3">, None</span><span class="s1">)</span>
  <span class="s1">group_size = len(replica_groups[</span><span class="s5">0</span><span class="s1">])</span>
  <span class="s1">srcs</span><span class="s3">, </span><span class="s1">dsts = unzip2((src % group_size</span><span class="s3">, </span><span class="s1">dst % group_size) </span><span class="s3">for </span><span class="s1">src</span><span class="s3">, </span><span class="s1">dst </span><span class="s3">in </span><span class="s1">perm)</span>
  <span class="s3">if not </span><span class="s1">(len(srcs) == len(set(srcs)) </span><span class="s3">and </span><span class="s1">len(dsts) == len(set(dsts))):</span>
    <span class="s1">msg = </span><span class="s4">&quot;ppermute sources and destinations must be unique, got {}.&quot;</span>
    <span class="s3">raise </span><span class="s1">ValueError(msg.format(perm))</span>

  <span class="s1">full_perm = np.zeros((len(replica_groups)</span><span class="s3">, </span><span class="s1">len(perm)</span><span class="s3">, </span><span class="s5">2</span><span class="s1">)</span><span class="s3">, </span><span class="s1">np.int64)</span>
  <span class="s3">for </span><span class="s1">i</span><span class="s3">, </span><span class="s1">grp </span><span class="s3">in </span><span class="s1">enumerate(replica_groups):</span>
    <span class="s1">grp = list(sorted(grp))</span>
    <span class="s3">for </span><span class="s1">j</span><span class="s3">, </span><span class="s1">(src</span><span class="s3">, </span><span class="s1">dst) </span><span class="s3">in </span><span class="s1">enumerate(perm):</span>
      <span class="s1">full_perm[i</span><span class="s3">, </span><span class="s1">j</span><span class="s3">, </span><span class="s5">0</span><span class="s1">] = grp[src]</span>
      <span class="s1">full_perm[i</span><span class="s3">, </span><span class="s1">j</span><span class="s3">, </span><span class="s5">1</span><span class="s1">] = grp[dst]</span>
  <span class="s1">full_perm = full_perm.reshape((-</span><span class="s5">1</span><span class="s3">, </span><span class="s5">2</span><span class="s1">))</span>

  <span class="s1">axis_context = ctx.module_context.axis_context</span>
  <span class="s1">is_manual = isinstance(axis_context</span><span class="s3">, </span><span class="s1">mlir.SPMDAxisContext) </span><span class="s3">and </span><span class="s1">axis_context.manual_axes</span>
  <span class="s3">if </span><span class="s1">is_manual:</span>
    <span class="s1">channel = ctx.module_context.new_channel()</span>
    <span class="s1">other_args = dict(</span>
        <span class="s1">channel_handle=hlo.ChannelHandle.get(channel</span><span class="s3">, </span><span class="s1">mlir.DEVICE_TO_DEVICE_TYPE))</span>
  <span class="s3">else</span><span class="s1">:</span>
    <span class="s1">other_args = {}</span>

  <span class="s3">return </span><span class="s1">hlo.CollectivePermuteOp(</span>
      <span class="s1">x</span><span class="s3">, </span><span class="s1">mlir.dense_int_elements(full_perm)</span><span class="s3">, </span><span class="s1">**other_args).results</span>

<span class="s3">def </span><span class="s1">_ppermute_transpose_rule(t</span><span class="s3">, </span><span class="s1">x</span><span class="s3">, </span><span class="s1">perm</span><span class="s3">, </span><span class="s1">axis_name):</span>
  <span class="s1">srcs</span><span class="s3">, </span><span class="s1">dsts = unzip2(perm)</span>
  <span class="s1">inverse_perm = list(zip(dsts</span><span class="s3">, </span><span class="s1">srcs))</span>
  <span class="s3">return </span><span class="s1">[ppermute(t</span><span class="s3">, </span><span class="s1">axis_name=axis_name</span><span class="s3">, </span><span class="s1">perm=inverse_perm)]</span>

<span class="s3">def </span><span class="s1">_ppermute_batcher(axis_size</span><span class="s3">, </span><span class="s1">frame_name</span><span class="s3">, </span><span class="s1">_</span><span class="s3">, </span><span class="s1">vals_in</span><span class="s3">, </span><span class="s1">dims_in</span><span class="s3">, </span><span class="s1">axis_name</span><span class="s3">, </span><span class="s1">perm):</span>
  <span class="s1">(v</span><span class="s3">,</span><span class="s1">)</span><span class="s3">, </span><span class="s1">(d</span><span class="s3">,</span><span class="s1">) = vals_in</span><span class="s3">, </span><span class="s1">dims_in</span>
  <span class="s3">if not </span><span class="s1">isinstance(axis_name</span><span class="s3">, </span><span class="s1">(tuple</span><span class="s3">, </span><span class="s1">list)):</span>
    <span class="s1">axis_name = (axis_name</span><span class="s3">,</span><span class="s1">)</span>
  <span class="s1">remaining_axes = tuple(axis </span><span class="s3">for </span><span class="s1">axis </span><span class="s3">in </span><span class="s1">axis_name </span><span class="s3">if </span><span class="s1">axis != frame_name)</span>
  <span class="s3">if </span><span class="s1">axis_size == </span><span class="s5">1 </span><span class="s3">and </span><span class="s1">remaining_axes:</span>
    <span class="s3">return </span><span class="s1">ppermute_p.bind(v</span><span class="s3">, </span><span class="s1">perm=perm</span><span class="s3">, </span><span class="s1">axis_name=remaining_axes)</span><span class="s3">, </span><span class="s1">d</span>
  <span class="s3">if </span><span class="s1">remaining_axes:</span>
    <span class="s3">raise </span><span class="s1">NotImplementedError(</span><span class="s4">&quot;ppermute batcher only supports a single axis&quot;</span><span class="s1">)</span>
  <span class="s3">assert </span><span class="s1">axis_name[</span><span class="s5">0</span><span class="s1">] == frame_name</span><span class="s3">, </span><span class="s4">&quot;ppermute batcher called with a wrong axis!&quot;</span>
  <span class="s3">assert </span><span class="s1">len(perm) == axis_size</span><span class="s3">, </span><span class="s4">&quot;Permutation doesn't match the axis size!&quot;</span>
  <span class="s3">if </span><span class="s1">d </span><span class="s3">is </span><span class="s1">batching.not_mapped:</span>
    <span class="s3">return </span><span class="s1">v</span><span class="s3">, </span><span class="s1">d</span>
  <span class="s1">perm_indices = np.zeros(axis_size</span><span class="s3">, </span><span class="s1">dtype=int)</span>
  <span class="s3">for </span><span class="s1">src</span><span class="s3">, </span><span class="s1">dst </span><span class="s3">in </span><span class="s1">perm:</span>
    <span class="s1">perm_indices[dst] = src</span>
  <span class="s3">return </span><span class="s1">lax_numpy.take(v</span><span class="s3">, </span><span class="s1">perm_indices</span><span class="s3">, </span><span class="s1">d)</span><span class="s3">, </span><span class="s1">d</span>

<span class="s3">def </span><span class="s1">_collective_batcher(prim</span><span class="s3">, </span><span class="s1">args</span><span class="s3">, </span><span class="s1">dims</span><span class="s3">, </span><span class="s1">**params):</span>
  <span class="s3">return </span><span class="s1">prim.bind(*args</span><span class="s3">, </span><span class="s1">**params)</span><span class="s3">, </span><span class="s1">dims </span><span class="s3">if </span><span class="s1">prim.multiple_results </span><span class="s3">else </span><span class="s1">dims[</span><span class="s5">0</span><span class="s1">]</span>

<span class="s1">ppermute_p = core.AxisPrimitive(</span><span class="s4">'ppermute'</span><span class="s1">)</span>
<span class="s1">ppermute_p.def_abstract_eval(</span><span class="s3">lambda </span><span class="s1">x</span><span class="s3">, </span><span class="s1">**params: raise_to_shaped(x))</span>
<span class="s1">ad.deflinear2(ppermute_p</span><span class="s3">, </span><span class="s1">_ppermute_transpose_rule)</span>
<span class="s1">xla.register_collective_primitive(ppermute_p)</span>
<span class="s1">mlir.register_lowering(ppermute_p</span><span class="s3">, </span><span class="s1">_ppermute_lowering)</span>
<span class="s1">pxla.multi_host_supported_collectives.add(ppermute_p)</span>
<span class="s1">batching.primitive_batchers[ppermute_p] = partial(_collective_batcher</span><span class="s3">, </span><span class="s1">ppermute_p)</span>
<span class="s1">batching.axis_primitive_batchers[ppermute_p] = _ppermute_batcher</span>
<span class="s1">core.axis_substitution_rules[ppermute_p] = partial(_subst_all_names_in_param</span><span class="s3">, </span><span class="s4">'axis_name'</span><span class="s1">)</span>


<span class="s3">def </span><span class="s1">_moveaxis(src</span><span class="s3">, </span><span class="s1">dst</span><span class="s3">, </span><span class="s1">x):</span>
  <span class="s1">perm = [i </span><span class="s3">for </span><span class="s1">i </span><span class="s3">in </span><span class="s1">range(x.ndim) </span><span class="s3">if </span><span class="s1">i != src]</span>
  <span class="s1">perm.insert(dst</span><span class="s3">, </span><span class="s1">src)</span>
  <span class="s3">return </span><span class="s1">lax.transpose(x</span><span class="s3">, </span><span class="s1">perm)</span>

<span class="s3">def </span><span class="s1">_splitaxis(axis</span><span class="s3">, </span><span class="s1">factor</span><span class="s3">, </span><span class="s1">x):</span>
  <span class="s1">new_shape = list(x.shape)</span>
  <span class="s3">assert </span><span class="s1">new_shape[axis] % factor == </span><span class="s5">0</span><span class="s3">, </span><span class="s1">(new_shape[axis]</span><span class="s3">, </span><span class="s1">factor)</span>
  <span class="s1">new_shape[axis:axis+</span><span class="s5">1</span><span class="s1">] = [factor</span><span class="s3">, </span><span class="s1">new_shape[axis] // factor]</span>
  <span class="s3">return </span><span class="s1">x.reshape(new_shape)</span>

<span class="s3">def </span><span class="s1">_foldaxis(axis</span><span class="s3">, </span><span class="s1">x):</span>
  <span class="s1">new_shape = list(x.shape)</span>
  <span class="s1">new_shape[axis:axis+</span><span class="s5">2</span><span class="s1">] = [x.shape[axis] * x.shape[axis + </span><span class="s5">1</span><span class="s1">]]</span>
  <span class="s3">return </span><span class="s1">x.reshape(new_shape)</span>

<span class="s3">def </span><span class="s1">_index_in_group(axis_name</span><span class="s3">, </span><span class="s1">axis_index_groups):</span>
  <span class="s1">cur_device_id = axis_index(axis_name)</span>
  <span class="s3">if </span><span class="s1">axis_index_groups </span><span class="s3">is None</span><span class="s1">:</span>
    <span class="s3">return </span><span class="s1">cur_device_id</span>
  <span class="s0"># We use argsort to invert the axis_index_groups permutation</span>
  <span class="s1">flat_groups = np.array(axis_index_groups).flatten()</span>
  <span class="s1">device_id_to_idx = flat_groups.argsort() % len(axis_index_groups[</span><span class="s5">0</span><span class="s1">])</span>
  <span class="s3">return </span><span class="s1">lax.squeeze(</span>
      <span class="s1">slicing.dynamic_slice_in_dim(device_id_to_idx</span><span class="s3">, </span><span class="s1">cur_device_id</span><span class="s3">, </span><span class="s5">1</span><span class="s1">)</span><span class="s3">, </span><span class="s1">[</span><span class="s5">0</span><span class="s1">])</span>

<span class="s3">def </span><span class="s1">_all_to_all_via_all_gather(x</span><span class="s3">, </span><span class="s1">*</span><span class="s3">, </span><span class="s1">axis_name</span><span class="s3">, </span><span class="s1">split_axis</span><span class="s3">, </span><span class="s1">concat_axis</span><span class="s3">, </span><span class="s1">axis_index_groups):</span>
  <span class="s1">idx = _index_in_group(axis_name</span><span class="s3">, </span><span class="s1">axis_index_groups)</span>
  <span class="s1">full = all_gather(x</span><span class="s3">, </span><span class="s1">axis_name</span><span class="s3">, </span><span class="s1">axis_index_groups=axis_index_groups)</span>
  <span class="s1">axis_size = full.shape[</span><span class="s5">0</span><span class="s1">]</span>
  <span class="s1">tile_size = x.shape[split_axis] // axis_size</span>
  <span class="s1">tile_base_idx = idx * tile_size</span>
  <span class="s1">sliced = slicing.dynamic_slice_in_dim(full</span><span class="s3">, </span><span class="s1">tile_base_idx</span><span class="s3">, </span><span class="s1">tile_size</span><span class="s3">,</span>
                                        <span class="s1">split_axis + </span><span class="s5">1</span><span class="s1">)</span>
  <span class="s3">return </span><span class="s1">_foldaxis(concat_axis</span><span class="s3">, </span><span class="s1">_moveaxis(</span><span class="s5">0</span><span class="s3">, </span><span class="s1">concat_axis</span><span class="s3">, </span><span class="s1">sliced))</span>


<span class="s3">def </span><span class="s1">_all_to_all_lowering(ctx</span><span class="s3">, </span><span class="s1">x</span><span class="s3">, </span><span class="s1">*</span><span class="s3">,</span>
                         <span class="s1">split_axis</span><span class="s3">, </span><span class="s1">concat_axis</span><span class="s3">, </span><span class="s1">axis_name</span><span class="s3">, </span><span class="s1">axis_index_groups):</span>
  <span class="s0"># Workaround for AllToAll not being implemented on CPU.</span>
  <span class="s1">replica_groups = _replica_groups(ctx.module_context.axis_env</span><span class="s3">, </span><span class="s1">axis_name</span><span class="s3">,</span>
                                   <span class="s1">axis_index_groups)</span>
  <span class="s3">if </span><span class="s1">len(replica_groups[</span><span class="s5">0</span><span class="s1">]) == </span><span class="s5">1</span><span class="s1">:</span>
    <span class="s3">return </span><span class="s1">[x]</span>
  <span class="s3">elif </span><span class="s1">((ctx.module_context.platform == </span><span class="s4">&quot;tpu&quot;</span><span class="s1">) </span><span class="s3">or</span>
        <span class="s1">((ctx.module_context.platform </span><span class="s3">in </span><span class="s1">(</span><span class="s4">&quot;cuda&quot;</span><span class="s3">, </span><span class="s4">&quot;rocm&quot;</span><span class="s1">))</span>
         <span class="s3">and </span><span class="s1">(split_axis == </span><span class="s5">0</span><span class="s1">) </span><span class="s3">and </span><span class="s1">(concat_axis == </span><span class="s5">0</span><span class="s1">))):</span>
    <span class="s1">split_count = len(replica_groups[</span><span class="s5">0</span><span class="s1">])</span>
    <span class="s3">if not </span><span class="s1">all(split_count == len(g) </span><span class="s3">for </span><span class="s1">g </span><span class="s3">in </span><span class="s1">replica_groups):</span>
      <span class="s3">raise </span><span class="s1">ValueError(</span><span class="s4">'Replica groups must be equally sized'</span><span class="s1">)</span>
    <span class="s1">is_spmd = isinstance(ctx.module_context.axis_context</span><span class="s3">,</span>
                         <span class="s1">(mlir.SPMDAxisContext</span><span class="s3">, </span><span class="s1">mlir.ShardingContext))</span>
    <span class="s3">if </span><span class="s1">is_spmd:</span>
      <span class="s0"># We want to emit the all-gather with global device IDs and a unique</span>
      <span class="s0"># channel ID, as otherwise it interprets the devices as replicas instead</span>
      <span class="s0"># of partitions - and XLA is configured with only a single replica.</span>
      <span class="s1">channel = ctx.module_context.new_channel()</span>
      <span class="s1">other_args = dict(</span>
          <span class="s1">channel_handle=hlo.ChannelHandle.get(channel</span><span class="s3">,</span>
                                               <span class="s1">mlir.DEVICE_TO_DEVICE_TYPE))</span>
    <span class="s3">else</span><span class="s1">:</span>
      <span class="s1">other_args = {}</span>
    <span class="s3">return </span><span class="s1">hlo.AllToAllOp(</span>
        <span class="s1">x</span><span class="s3">,</span>
        <span class="s1">split_dimension=mlir.i64_attr(split_axis)</span><span class="s3">,</span>
        <span class="s1">concat_dimension=mlir.i64_attr(concat_axis)</span><span class="s3">,</span>
        <span class="s1">split_count=mlir.i64_attr(split_count)</span><span class="s3">,</span>
        <span class="s1">replica_groups=_replica_groups_hlo(replica_groups)</span><span class="s3">,</span>
        <span class="s1">**other_args).results</span>
  <span class="s3">else</span><span class="s1">:</span>
    <span class="s1">warnings.warn(</span>
        <span class="s4">&quot;all_to_all (and pswapaxes) are only implemented properly for TPUs and GPUs (if &quot;</span>
        <span class="s4">&quot;split_axis and concat_axis are both 0). All other backends emulate it using a &quot;</span>
        <span class="s4">&quot;very slow and memory intensive algorithm, so expect significant slowdowns.&quot;</span>
    <span class="s1">)</span>
    <span class="s1">lowering = mlir.lower_fun(_all_to_all_via_all_gather</span><span class="s3">,</span>
                              <span class="s1">multiple_results=</span><span class="s3">False</span><span class="s1">)</span>
    <span class="s3">return </span><span class="s1">lowering(ctx</span><span class="s3">, </span><span class="s1">x</span><span class="s3">,</span>
                    <span class="s1">axis_name=axis_name</span><span class="s3">,</span>
                    <span class="s1">split_axis=split_axis</span><span class="s3">,</span>
                    <span class="s1">concat_axis=concat_axis</span><span class="s3">,</span>
                    <span class="s1">axis_index_groups=axis_index_groups)</span>

<span class="s3">def </span><span class="s1">_all_to_all_transpose_rule(cts</span><span class="s3">, </span><span class="s1">x</span><span class="s3">, </span><span class="s1">axis_name</span><span class="s3">, </span><span class="s1">split_axis</span><span class="s3">, </span><span class="s1">concat_axis</span><span class="s3">, </span><span class="s1">axis_index_groups):</span>
  <span class="s3">return </span><span class="s1">(all_to_all(</span>
      <span class="s1">cts</span><span class="s3">,</span>
      <span class="s1">axis_name=axis_name</span><span class="s3">,</span>
      <span class="s1">split_axis=concat_axis</span><span class="s3">,</span>
      <span class="s1">concat_axis=split_axis</span><span class="s3">,</span>
      <span class="s1">axis_index_groups=axis_index_groups)</span><span class="s3">,</span><span class="s1">)</span>

<span class="s3">def </span><span class="s1">_all_to_all_batcher(vals_in</span><span class="s3">, </span><span class="s1">dims_in</span><span class="s3">, </span><span class="s1">*</span><span class="s3">, </span><span class="s1">axis_name</span><span class="s3">, </span><span class="s1">split_axis</span><span class="s3">, </span><span class="s1">concat_axis</span><span class="s3">, </span><span class="s1">axis_index_groups):</span>
  <span class="s1">x</span><span class="s3">, </span><span class="s1">= vals_in</span>
  <span class="s1">d</span><span class="s3">, </span><span class="s1">= dims_in</span>
  <span class="s1">result = all_to_all_p.bind(</span>
      <span class="s1">x</span><span class="s3">,</span>
      <span class="s1">axis_name=axis_name</span><span class="s3">,</span>
      <span class="s1">split_axis=split_axis + (d &lt;= split_axis)</span><span class="s3">,</span>
      <span class="s1">concat_axis=concat_axis + (d &lt;= concat_axis)</span><span class="s3">,</span>
      <span class="s1">axis_index_groups=axis_index_groups)</span>
  <span class="s3">return </span><span class="s1">result</span><span class="s3">, </span><span class="s1">d</span>

<span class="s3">def </span><span class="s1">_all_to_all_batched_collective(axis_size</span><span class="s3">, </span><span class="s1">frame_name</span><span class="s3">, </span><span class="s1">_</span><span class="s3">, </span><span class="s1">vals_in</span><span class="s3">, </span><span class="s1">dims_in</span><span class="s3">,</span>
                                   <span class="s1">axis_name</span><span class="s3">, </span><span class="s1">split_axis</span><span class="s3">, </span><span class="s1">concat_axis</span><span class="s3">,</span>
                                   <span class="s1">axis_index_groups):</span>
  <span class="s3">if </span><span class="s1">axis_index_groups </span><span class="s3">is not None</span><span class="s1">:</span>
    <span class="s3">raise </span><span class="s1">NotImplementedError(</span><span class="s4">&quot;Please open a feature request!&quot;</span><span class="s1">)</span>
  <span class="s1">x</span><span class="s3">, </span><span class="s1">= vals_in</span>
  <span class="s1">d</span><span class="s3">, </span><span class="s1">= dims_in</span>
  <span class="s3">if </span><span class="s1">d </span><span class="s3">is </span><span class="s1">batching.not_mapped:</span>
    <span class="s0"># TODO(sharadmv,apaszke): Remove this broadcast that comes from</span>
    <span class="s0"># all_gather_transpose and instead avoid using all_to_all in</span>
    <span class="s0"># all_gather_transpose.</span>
    <span class="s1">x = lax.broadcast(x</span><span class="s3">, </span><span class="s1">(axis_size</span><span class="s3">, </span><span class="s1">*x.shape))</span>
    <span class="s1">d = </span><span class="s5">0</span>
  <span class="s3">if </span><span class="s1">isinstance(axis_name</span><span class="s3">, </span><span class="s1">(list</span><span class="s3">, </span><span class="s1">tuple)):</span>
    <span class="s1">pos = axis_name.index(frame_name)</span>
    <span class="s1">major_axes</span><span class="s3">, </span><span class="s1">minor_axes = axis_name[:pos]</span><span class="s3">, </span><span class="s1">axis_name[pos + </span><span class="s5">1</span><span class="s1">:]</span>
  <span class="s3">else</span><span class="s1">:</span>
    <span class="s1">major_axes</span><span class="s3">, </span><span class="s1">minor_axes = ()</span><span class="s3">, </span><span class="s1">()</span>
  <span class="s0"># Optimized case when no splitting is necessary</span>
  <span class="s3">if not </span><span class="s1">major_axes </span><span class="s3">and not </span><span class="s1">minor_axes:</span>
    <span class="s3">if </span><span class="s1">split_axis == concat_axis:</span>
      <span class="s1">axis = split_axis + (d &lt;= split_axis)</span>
      <span class="s1">d_pre_split = d</span>
      <span class="s1">x = _splitaxis(axis</span><span class="s3">, </span><span class="s1">axis_size</span><span class="s3">, </span><span class="s1">x)</span>
      <span class="s1">d += (axis &lt;= d)</span>
      <span class="s3">return </span><span class="s1">_foldaxis(axis</span><span class="s3">, </span><span class="s1">moveaxis(x</span><span class="s3">, </span><span class="s1">(d</span><span class="s3">, </span><span class="s1">axis)</span><span class="s3">, </span><span class="s1">(axis</span><span class="s3">, </span><span class="s1">d)))</span><span class="s3">, </span><span class="s1">d_pre_split</span>
    <span class="s3">else</span><span class="s1">:</span>
      <span class="s1">x_concat = _foldaxis(concat_axis</span><span class="s3">, </span><span class="s1">_moveaxis(d</span><span class="s3">, </span><span class="s1">concat_axis</span><span class="s3">, </span><span class="s1">x))</span>
      <span class="s3">return </span><span class="s1">_splitaxis(split_axis</span><span class="s3">, </span><span class="s1">axis_size</span><span class="s3">, </span><span class="s1">x_concat)</span><span class="s3">, </span><span class="s1">split_axis</span>
  <span class="s0"># Here we have to handle either the major or the minor dimensions</span>
  <span class="s0"># We will be accumulating chunks into the three leading dims: [Major, Current, Minor, ...]</span>
  <span class="s1">x</span><span class="s3">, </span><span class="s1">d = lax.expand_dims(_moveaxis(d</span><span class="s3">, </span><span class="s5">0</span><span class="s3">, </span><span class="s1">x)</span><span class="s3">, </span><span class="s1">(</span><span class="s5">0</span><span class="s3">, </span><span class="s5">2</span><span class="s1">))</span><span class="s3">, </span><span class="s5">1</span>
  <span class="s1">split_axis += </span><span class="s5">3</span><span class="s1">; concat_axis += </span><span class="s5">3  </span><span class="s0"># Offset by extra three leading dims</span>

  <span class="s3">if </span><span class="s1">major_axes:</span>
    <span class="s1">x = all_to_all_p.bind(x</span><span class="s3">, </span><span class="s1">axis_name=major_axes</span><span class="s3">,</span>
                          <span class="s1">split_axis=split_axis</span><span class="s3">, </span><span class="s1">concat_axis=</span><span class="s5">0</span><span class="s3">,</span>
                          <span class="s1">axis_index_groups=axis_index_groups)</span>
  <span class="s0"># Split out the local part into axis new_d (NOTE: d is already in axis 1)</span>
  <span class="s1">x = _splitaxis(split_axis</span><span class="s3">, </span><span class="s1">axis_size</span><span class="s3">, </span><span class="s1">x)</span>
  <span class="s1">new_d = split_axis</span>
  <span class="s1">concat_axis += (split_axis &lt;= concat_axis)  </span><span class="s0"># Offset the existing axes by the new batch axis</span>
  <span class="s1">split_axis += </span><span class="s5">1</span>
  <span class="s3">if </span><span class="s1">minor_axes:</span>
    <span class="s1">x = all_to_all_p.bind(x</span><span class="s3">, </span><span class="s1">axis_name=minor_axes</span><span class="s3">,</span>
                          <span class="s1">split_axis=split_axis</span><span class="s3">, </span><span class="s1">concat_axis=</span><span class="s5">2</span><span class="s3">,</span>
                          <span class="s1">axis_index_groups=axis_index_groups)</span>

  <span class="s0"># Fold the chunk axes into a single one</span>
  <span class="s1">x = _foldaxis(</span><span class="s5">0</span><span class="s3">, </span><span class="s1">_foldaxis(</span><span class="s5">0</span><span class="s3">, </span><span class="s1">x))</span>
  <span class="s1">split_axis -= </span><span class="s5">2</span><span class="s1">; concat_axis -= </span><span class="s5">2</span><span class="s1">; new_d -= </span><span class="s5">2</span>
  <span class="s0"># Fold gathered axes into concat_axis</span>
  <span class="s1">x = _foldaxis(concat_axis - </span><span class="s5">1</span><span class="s3">, </span><span class="s1">_moveaxis(</span><span class="s5">0</span><span class="s3">, </span><span class="s1">concat_axis - </span><span class="s5">1</span><span class="s3">, </span><span class="s1">x))</span>
  <span class="s1">new_d -= </span><span class="s5">1  </span><span class="s0"># We've removed 0th dimension, so new_d needs to be adjusted</span>
  <span class="s3">return </span><span class="s1">x</span><span class="s3">, </span><span class="s1">new_d</span>

<span class="s3">def </span><span class="s1">_all_to_all_abstract_eval(x</span><span class="s3">, </span><span class="s1">axis_name</span><span class="s3">, </span><span class="s1">split_axis</span><span class="s3">, </span><span class="s1">concat_axis</span><span class="s3">, </span><span class="s1">axis_index_groups):</span>
  <span class="s1">input_aval = raise_to_shaped(x)</span>
  <span class="s1">shape = list(input_aval.shape)</span>
  <span class="s1">axis_size = psum(</span><span class="s5">1</span><span class="s3">, </span><span class="s1">axis_name) </span><span class="s3">if </span><span class="s1">axis_index_groups </span><span class="s3">is None else </span><span class="s1">len(axis_index_groups[</span><span class="s5">0</span><span class="s1">])</span>
  <span class="s3">assert </span><span class="s1">shape[split_axis] % axis_size == </span><span class="s5">0</span><span class="s3">, </span><span class="s1">(shape[split_axis]</span><span class="s3">, </span><span class="s1">axis_size)</span>
  <span class="s1">shape[split_axis] //= axis_size</span>
  <span class="s1">shape[concat_axis] *= axis_size</span>
  <span class="s3">return </span><span class="s1">input_aval.update(shape=tuple(shape)</span><span class="s3">, </span><span class="s1">weak_type=</span><span class="s3">False</span><span class="s1">)</span>

<span class="s1">all_to_all_p = core.AxisPrimitive(</span><span class="s4">'all_to_all'</span><span class="s1">)</span>
<span class="s1">all_to_all_p.def_abstract_eval(_all_to_all_abstract_eval)</span>
<span class="s1">xla.register_collective_primitive(all_to_all_p)</span>
<span class="s1">mlir.register_lowering(all_to_all_p</span><span class="s3">, </span><span class="s1">_all_to_all_lowering)</span>
<span class="s1">ad.deflinear2(all_to_all_p</span><span class="s3">, </span><span class="s1">_all_to_all_transpose_rule)</span>
<span class="s1">pxla.multi_host_supported_collectives.add(all_to_all_p)</span>
<span class="s1">batching.primitive_batchers[all_to_all_p] = _all_to_all_batcher</span>
<span class="s1">batching.axis_primitive_batchers[all_to_all_p] = _all_to_all_batched_collective</span>
<span class="s1">core.axis_substitution_rules[all_to_all_p] = partial(_subst_all_names_in_param</span><span class="s3">, </span><span class="s4">'axis_name'</span><span class="s1">)</span>


<span class="s3">def </span><span class="s1">all_gather(x</span><span class="s3">, </span><span class="s1">axis_name</span><span class="s3">, </span><span class="s1">*</span><span class="s3">, </span><span class="s1">axis_index_groups=</span><span class="s3">None, </span><span class="s1">axis=</span><span class="s5">0</span><span class="s3">, </span><span class="s1">tiled=</span><span class="s3">False</span><span class="s1">):</span>
  <span class="s2">&quot;&quot;&quot;Gather values of x across all replicas. 
 
  If ``x`` is a pytree then the result is equivalent to mapping this function to 
  each leaf in the tree. 
 
  This is equivalent to, but faster than, all_to_all(broadcast(x)). 
 
  Args: 
    x: array(s) with a mapped axis named ``axis_name``. 
    axis_name: hashable Python object used to name a pmapped axis (see the 
      :func:`jax.pmap` documentation for more details). 
    axis_index_groups: optional list of lists containing axis indices (e.g. for 
      an axis of size 4, [[0, 1], [2, 3]] would run all gather over the first 
      two and last two replicas). Groups must cover all axis indices exactly 
      once, and all groups must be the same size. 
    axis: a positional axis into which the chunks along ``axis_name`` will be 
      concatenated. 
    tiled: when ``False``, the chunks will be stacked into a fresh positional 
      axis at index ``axis`` in the output. When ``True``, ``axis`` has to 
      refer to an existing positional dimension and the chunks will be 
      concatenated into that dimension. 
 
  Returns: 
    Array(s) representing the result of an all-gather along the axis 
    ``axis_name``. Shapes are the same as ``x.shape``, but: 
 
    - when ``tiled`` is ``False``, there is a new dimension equal to the 
      size of axis ``axis_name`` in position ``axis``, 
    - when ``tiled`` is ``True``, the size of dimension in position ``axis`` 
      is multiplied by the size of axis ``axis_name``. 
 
  For example, with 4 XLA devices available: 
 
  &gt;&gt;&gt; x = np.arange(4) 
  &gt;&gt;&gt; y = jax.pmap(lambda x: jax.lax.all_gather(x, 'i'), axis_name='i')(x) 
  &gt;&gt;&gt; print(y) 
  [[0 1 2 3] 
   [0 1 2 3] 
   [0 1 2 3] 
   [0 1 2 3]] 
 
  An example of using axis_index_groups, groups split by even &amp; odd device ids: 
 
  &gt;&gt;&gt; x = np.arange(16).reshape(4, 4) 
  &gt;&gt;&gt; print(x) 
    [[ 0  1  2  3] 
     [ 4  5  6  7] 
     [ 8  9 10 11] 
     [12 13 14 15]] 
  &gt;&gt;&gt; def f(x): 
  ...   return jax.lax.all_gather( 
  ...       x, 'i', axis_index_groups=[[0, 2], [3, 1]]) 
  &gt;&gt;&gt; y = jax.pmap(f, axis_name='i')(x) 
  &gt;&gt;&gt; print(y) 
  [[[ 0  1  2  3] 
    [ 8  9 10 11]] 
   [[12 13 14 15] 
    [ 4  5  6  7]] 
   [[ 0  1  2  3] 
    [ 8  9 10 11]] 
   [[12 13 14 15] 
    [ 4  5  6  7]]] 
  &quot;&quot;&quot;</span>
  <span class="s1">axis_index_groups = _canonicalize_axis_index_groups(axis_index_groups)</span>
  <span class="s1">axis_size = psum(</span><span class="s5">1</span><span class="s3">, </span><span class="s1">axis_name</span><span class="s3">, </span><span class="s1">axis_index_groups=axis_index_groups)</span>
  <span class="s3">def </span><span class="s1">bind(leaf):</span>
    <span class="s3">return </span><span class="s1">all_gather_p.bind(</span>
        <span class="s1">leaf</span><span class="s3">,</span>
        <span class="s1">all_gather_dimension=canonicalize_axis(</span>
            <span class="s1">axis</span><span class="s3">, </span><span class="s1">np.ndim(leaf) </span><span class="s3">if </span><span class="s1">tiled </span><span class="s3">else </span><span class="s1">np.ndim(leaf) + </span><span class="s5">1</span><span class="s1">)</span><span class="s3">,</span>
        <span class="s1">axis_name=axis_name</span><span class="s3">, </span><span class="s1">axis_index_groups=axis_index_groups</span><span class="s3">,</span>
        <span class="s1">axis_size=axis_size</span><span class="s3">, </span><span class="s1">tiled=tiled)</span>
  <span class="s3">return </span><span class="s1">tree_util.tree_map(bind</span><span class="s3">, </span><span class="s1">x)</span>

<span class="s3">def </span><span class="s1">_expand(dim</span><span class="s3">, </span><span class="s1">size</span><span class="s3">, </span><span class="s1">index</span><span class="s3">, </span><span class="s1">tiled</span><span class="s3">, </span><span class="s1">x):</span>
  <span class="s1">shape = list(x.shape)</span>
  <span class="s3">if </span><span class="s1">tiled:</span>
    <span class="s1">tile_size = shape[dim]</span>
    <span class="s1">shape[dim] *= size</span>
    <span class="s1">out = lax.full(shape</span><span class="s3">, </span><span class="s1">lax._const(x</span><span class="s3">, </span><span class="s5">0</span><span class="s1">))</span>
    <span class="s3">return </span><span class="s1">slicing.dynamic_update_slice_in_dim(out</span><span class="s3">, </span><span class="s1">x</span><span class="s3">, </span><span class="s1">index * tile_size</span><span class="s3">, </span><span class="s1">dim)</span>
  <span class="s3">else</span><span class="s1">:</span>
    <span class="s1">shape.insert(dim</span><span class="s3">, </span><span class="s1">size)</span>
    <span class="s1">out = lax.full(shape</span><span class="s3">, </span><span class="s1">lax._const(x</span><span class="s3">, </span><span class="s5">0</span><span class="s1">))</span>
    <span class="s3">return </span><span class="s1">slicing.dynamic_update_index_in_dim(out</span><span class="s3">, </span><span class="s1">x</span><span class="s3">, </span><span class="s1">index</span><span class="s3">, </span><span class="s1">dim)</span>

<span class="s3">def </span><span class="s1">_all_gather_via_psum(x</span><span class="s3">, </span><span class="s1">*</span><span class="s3">, </span><span class="s1">all_gather_dimension</span><span class="s3">, </span><span class="s1">axis_name</span><span class="s3">, </span><span class="s1">axis_index_groups</span><span class="s3">, </span><span class="s1">axis_size</span><span class="s3">, </span><span class="s1">tiled):</span>
  <span class="s1">index = _index_in_group(axis_name</span><span class="s3">, </span><span class="s1">axis_index_groups)</span>
  <span class="s1">outs = tree_util.tree_map(partial(_expand</span><span class="s3">, </span><span class="s1">all_gather_dimension</span><span class="s3">, </span><span class="s1">axis_size</span><span class="s3">, </span><span class="s1">index</span><span class="s3">, </span><span class="s1">tiled)</span><span class="s3">, </span><span class="s1">x)</span>
  <span class="s1">sums = psum(outs</span><span class="s3">, </span><span class="s1">axis_name</span><span class="s3">, </span><span class="s1">axis_index_groups=axis_index_groups)</span>
  <span class="s0"># psum casts bool elements to int32; cast back.</span>
  <span class="s3">return </span><span class="s1">tree_util.tree_map(</span><span class="s3">lambda </span><span class="s1">o</span><span class="s3">, </span><span class="s1">s: s.astype(o.dtype)</span><span class="s3">, </span><span class="s1">outs</span><span class="s3">, </span><span class="s1">sums)</span>

<span class="s3">def </span><span class="s1">_all_gather_impl(x</span><span class="s3">, </span><span class="s1">*</span><span class="s3">, </span><span class="s1">all_gather_dimension</span><span class="s3">, </span><span class="s1">axis_name</span><span class="s3">, </span><span class="s1">axis_index_groups</span><span class="s3">, </span><span class="s1">axis_size</span><span class="s3">, </span><span class="s1">tiled):</span>
  <span class="s3">raise </span><span class="s1">AssertionError(</span><span class="s4">&quot;Unexpected call to _all_gather_impl&quot;</span><span class="s1">)</span>

<span class="s3">def </span><span class="s1">_all_gather_lowering(ctx</span><span class="s3">, </span><span class="s1">x</span><span class="s3">, </span><span class="s1">*</span><span class="s3">, </span><span class="s1">all_gather_dimension</span><span class="s3">, </span><span class="s1">axis_name</span><span class="s3">,</span>
                         <span class="s1">axis_index_groups</span><span class="s3">, </span><span class="s1">axis_size</span><span class="s3">, </span><span class="s1">tiled):</span>
  <span class="s0"># TODO(jekbradbury): enable for all_gather_dimension &gt; 0</span>
  <span class="s1">x_aval</span><span class="s3">, </span><span class="s1">= ctx.avals_in</span>
  <span class="s1">out_aval</span><span class="s3">, </span><span class="s1">= ctx.avals_out</span>
  <span class="s1">axis_context = ctx.module_context.axis_context</span>
  <span class="s1">is_spmd = isinstance(axis_context</span><span class="s3">,</span>
                       <span class="s1">(mlir.SPMDAxisContext</span><span class="s3">, </span><span class="s1">mlir.ShardingContext))</span>
  <span class="s3">if </span><span class="s1">(ctx.module_context.platform == </span><span class="s4">'tpu' </span><span class="s3">or</span>
      <span class="s1">ctx.module_context.platform </span><span class="s3">in </span><span class="s1">(</span><span class="s4">'cuda'</span><span class="s3">, </span><span class="s4">'rocm'</span><span class="s1">)</span>
      <span class="s3">and </span><span class="s1">all_gather_dimension == </span><span class="s5">0</span><span class="s1">):</span>
    <span class="s3">if not </span><span class="s1">tiled:</span>
      <span class="s1">new_shape = list(x_aval.shape)</span>
      <span class="s1">new_shape.insert(all_gather_dimension</span><span class="s3">, </span><span class="s5">1</span><span class="s1">)</span>
      <span class="s1">broadcast_dimensions = [i </span><span class="s3">for </span><span class="s1">i </span><span class="s3">in </span><span class="s1">range(len(new_shape)) </span><span class="s3">if </span><span class="s1">i != all_gather_dimension]</span>
      <span class="s1">x = hlo.BroadcastInDimOp(</span>
          <span class="s1">mlir.aval_to_ir_type(x_aval.update(shape=new_shape))</span><span class="s3">, </span><span class="s1">x</span><span class="s3">,</span>
          <span class="s1">mlir.dense_int_elements(broadcast_dimensions))</span>
    <span class="s1">replica_groups = _replica_groups(ctx.module_context.axis_env</span><span class="s3">, </span><span class="s1">axis_name</span><span class="s3">,</span>
                                     <span class="s1">axis_index_groups)</span>
    <span class="s3">if </span><span class="s1">is_spmd:</span>
      <span class="s0"># We want to emit the all-gather with global device IDs and a unique</span>
      <span class="s0"># channel ID, as otherwise it interprets the devices as replicas instead</span>
      <span class="s0"># of partitions - and XLA is configured with only a single replica.</span>
      <span class="s1">channel = ctx.module_context.new_channel()</span>
      <span class="s1">other_args = dict(</span>
          <span class="s1">channel_handle=hlo.ChannelHandle.get(</span>
              <span class="s1">channel</span><span class="s3">, </span><span class="s1">mlir.DEVICE_TO_DEVICE_TYPE)</span><span class="s3">,</span>
          <span class="s1">use_global_device_ids=ir.BoolAttr.get(</span><span class="s3">True</span><span class="s1">))</span>
    <span class="s3">else</span><span class="s1">:</span>
      <span class="s1">other_args = {}</span>
    <span class="s3">return </span><span class="s1">hlo.AllGatherOp(</span>
        <span class="s1">mlir.aval_to_ir_type(out_aval)</span><span class="s3">,</span>
        <span class="s1">x</span><span class="s3">, </span><span class="s1">all_gather_dim=mlir.i64_attr(all_gather_dimension)</span><span class="s3">,</span>
        <span class="s1">replica_groups=_replica_groups_hlo(replica_groups)</span><span class="s3">,</span>
        <span class="s1">**other_args).results</span>
  <span class="s3">else</span><span class="s1">:</span>
    <span class="s1">lowering = mlir.lower_fun(_all_gather_via_psum</span><span class="s3">, </span><span class="s1">multiple_results=</span><span class="s3">False</span><span class="s1">)</span>
    <span class="s3">return </span><span class="s1">lowering(</span>
        <span class="s1">ctx</span><span class="s3">, </span><span class="s1">x</span><span class="s3">, </span><span class="s1">all_gather_dimension=all_gather_dimension</span><span class="s3">,</span>
        <span class="s1">axis_name=axis_name</span><span class="s3">, </span><span class="s1">axis_index_groups=axis_index_groups</span><span class="s3">,</span>
        <span class="s1">axis_size=axis_size</span><span class="s3">, </span><span class="s1">tiled=tiled)</span>

<span class="s3">def </span><span class="s1">_all_gather_abstract_eval(x</span><span class="s3">, </span><span class="s1">*</span><span class="s3">, </span><span class="s1">all_gather_dimension</span><span class="s3">, </span><span class="s1">axis_name</span><span class="s3">, </span><span class="s1">axis_index_groups</span><span class="s3">, </span><span class="s1">axis_size</span><span class="s3">, </span><span class="s1">tiled):</span>
  <span class="s3">if not </span><span class="s1">isinstance(axis_name</span><span class="s3">, </span><span class="s1">(list</span><span class="s3">, </span><span class="s1">tuple)):</span>
    <span class="s1">axis_name = (axis_name</span><span class="s3">,</span><span class="s1">)</span>
  <span class="s1">x_aval = raise_to_shaped(x)</span>
  <span class="s1">new_shape = list(x_aval.shape)</span>
  <span class="s3">if </span><span class="s1">tiled:</span>
    <span class="s1">new_shape[all_gather_dimension] *= axis_size</span>
  <span class="s3">else</span><span class="s1">:</span>
    <span class="s1">new_shape.insert(all_gather_dimension</span><span class="s3">, </span><span class="s1">axis_size)</span>
  <span class="s1">new_named_shape = {name: size </span><span class="s3">for </span><span class="s1">name</span><span class="s3">, </span><span class="s1">size </span><span class="s3">in </span><span class="s1">x_aval.named_shape.items()</span>
                     <span class="s3">if </span><span class="s1">name </span><span class="s3">not in </span><span class="s1">axis_name}</span>
  <span class="s3">return </span><span class="s1">x_aval.update(shape=new_shape</span><span class="s3">, </span><span class="s1">named_shape=new_named_shape)</span>

<span class="s3">def </span><span class="s1">_all_gather_transpose_rule(cts</span><span class="s3">, </span><span class="s1">x</span><span class="s3">, </span><span class="s1">*</span><span class="s3">, </span><span class="s1">all_gather_dimension</span><span class="s3">, </span><span class="s1">axis_name</span><span class="s3">, </span><span class="s1">axis_index_groups</span><span class="s3">, </span><span class="s1">axis_size</span><span class="s3">, </span><span class="s1">tiled):</span>
  <span class="s3">return </span><span class="s1">(psum_scatter(cts</span><span class="s3">, </span><span class="s1">axis_name=axis_name</span><span class="s3">,</span>
                       <span class="s1">scatter_dimension=all_gather_dimension</span><span class="s3">,</span>
                       <span class="s1">axis_index_groups=axis_index_groups</span><span class="s3">,</span>
                       <span class="s1">tiled=tiled)</span><span class="s3">,</span><span class="s1">)</span>
  <span class="s0"># TODO(sharadmv,apaszke): re-enable this when we can properly detect replication.</span>
  <span class="s0"># return (lax.dynamic_index_in_dim(cts, idx, axis=all_gather_dimension, keepdims=False) * axis_size,)</span>

<span class="s3">def </span><span class="s1">_all_gather_batcher(vals_in</span><span class="s3">, </span><span class="s1">dims_in</span><span class="s3">, </span><span class="s1">*</span><span class="s3">, </span><span class="s1">all_gather_dimension</span><span class="s3">, </span><span class="s1">axis_name</span><span class="s3">, </span><span class="s1">axis_index_groups</span><span class="s3">, </span><span class="s1">axis_size</span><span class="s3">, </span><span class="s1">tiled):</span>
  <span class="s1">(x</span><span class="s3">,</span><span class="s1">)</span><span class="s3">, </span><span class="s1">(d</span><span class="s3">,</span><span class="s1">) = vals_in</span><span class="s3">, </span><span class="s1">dims_in</span>
  <span class="s3">if </span><span class="s1">d &lt;= all_gather_dimension:</span>
    <span class="s1">all_gather_dimension += </span><span class="s5">1</span>
  <span class="s3">elif not </span><span class="s1">tiled:  </span><span class="s0"># Tiled all-gather doesn't modify the set of dimensions</span>
    <span class="s1">d += </span><span class="s5">1</span>
  <span class="s1">result = all_gather_p.bind(</span>
      <span class="s1">x</span><span class="s3">,</span>
      <span class="s1">all_gather_dimension=all_gather_dimension</span><span class="s3">,</span>
      <span class="s1">axis_name=axis_name</span><span class="s3">,</span>
      <span class="s1">axis_index_groups=axis_index_groups</span><span class="s3">,</span>
      <span class="s1">axis_size=axis_size</span><span class="s3">,</span>
      <span class="s1">tiled=tiled)</span>
  <span class="s3">return </span><span class="s1">result</span><span class="s3">, </span><span class="s1">d</span>

<span class="s3">def </span><span class="s1">_all_gather_batched_collective(frame_size</span><span class="s3">, </span><span class="s1">frame_name</span><span class="s3">, </span><span class="s1">_</span><span class="s3">, </span><span class="s1">vals_in</span><span class="s3">, </span><span class="s1">dims_in</span><span class="s3">,</span>
                                   <span class="s1">all_gather_dimension</span><span class="s3">, </span><span class="s1">axis_name</span><span class="s3">,</span>
                                   <span class="s1">axis_index_groups</span><span class="s3">, </span><span class="s1">axis_size</span><span class="s3">, </span><span class="s1">tiled):</span>
  <span class="s3">if </span><span class="s1">axis_index_groups </span><span class="s3">is not None</span><span class="s1">:</span>
    <span class="s3">raise </span><span class="s1">NotImplementedError(</span><span class="s4">&quot;axis_index_groups not supported in vmap&quot;</span><span class="s1">)</span>
  <span class="s3">assert </span><span class="s1">axis_size == frame_size</span><span class="s3">, </span><span class="s4">&quot;axis size doesn't match&quot;</span>
  <span class="s3">if not </span><span class="s1">isinstance(axis_name</span><span class="s3">, </span><span class="s1">tuple):</span>
    <span class="s1">axis_name = (axis_name</span><span class="s3">,</span><span class="s1">)</span>
  <span class="s3">if </span><span class="s1">len(axis_name) &gt; </span><span class="s5">1</span><span class="s1">:</span>
    <span class="s3">raise </span><span class="s1">NotImplementedError(</span><span class="s4">&quot;Please open a feature request!&quot;</span><span class="s1">)</span>
  <span class="s3">assert </span><span class="s1">axis_name == (frame_name</span><span class="s3">,</span><span class="s1">)</span><span class="s3">, </span><span class="s4">&quot;batcher called with wrong axis name&quot;</span>
  <span class="s1">(x</span><span class="s3">,</span><span class="s1">)</span><span class="s3">, </span><span class="s1">(d</span><span class="s3">,</span><span class="s1">) = vals_in</span><span class="s3">, </span><span class="s1">dims_in</span>
  <span class="s3">if </span><span class="s1">d </span><span class="s3">is </span><span class="s1">batching.not_mapped:</span>
    <span class="s1">out_shape = list(np.shape(x))</span>
    <span class="s1">out_shape.insert(all_gather_dimension</span><span class="s3">, </span><span class="s1">axis_size)</span>
    <span class="s1">broadcast_dims = [i </span><span class="s3">for </span><span class="s1">i </span><span class="s3">in </span><span class="s1">range(len(out_shape)) </span><span class="s3">if </span><span class="s1">i != all_gather_dimension]</span>
    <span class="s1">y = lax.broadcast_in_dim(x</span><span class="s3">, </span><span class="s1">out_shape</span><span class="s3">, </span><span class="s1">broadcast_dims)</span>
  <span class="s3">else</span><span class="s1">:</span>
    <span class="s1">y = _moveaxis(d</span><span class="s3">, </span><span class="s1">all_gather_dimension</span><span class="s3">, </span><span class="s1">x)</span>
  <span class="s3">if </span><span class="s1">tiled:</span>
    <span class="s1">y = _foldaxis(all_gather_dimension</span><span class="s3">, </span><span class="s1">y)</span>
  <span class="s3">return </span><span class="s1">y</span><span class="s3">, </span><span class="s1">batching.not_mapped</span>

<span class="s1">all_gather_p = core.AxisPrimitive(</span><span class="s4">'all_gather'</span><span class="s1">)</span>
<span class="s1">all_gather_p.def_abstract_eval(_all_gather_abstract_eval)</span>
<span class="s1">all_gather_p.def_impl(_all_gather_impl)</span>
<span class="s1">xla.register_collective_primitive(all_gather_p)</span>
<span class="s1">mlir.register_lowering(all_gather_p</span><span class="s3">, </span><span class="s1">_all_gather_lowering)</span>
<span class="s1">ad.deflinear2(all_gather_p</span><span class="s3">, </span><span class="s1">_all_gather_transpose_rule)</span>
<span class="s1">pxla.multi_host_supported_collectives.add(all_gather_p)</span>
<span class="s1">batching.primitive_batchers[all_gather_p] = _all_gather_batcher</span>
<span class="s1">batching.axis_primitive_batchers[all_gather_p] = _all_gather_batched_collective</span>
<span class="s1">core.axis_substitution_rules[all_gather_p] = partial(_subst_all_names_in_param</span><span class="s3">, </span><span class="s4">'axis_name'</span><span class="s1">)</span>


<span class="s3">def </span><span class="s1">_reduce_scatter_via_reducer(x</span><span class="s3">, </span><span class="s1">*</span><span class="s3">, </span><span class="s1">reducer</span><span class="s3">, </span><span class="s1">scatter_dimension</span><span class="s3">, </span><span class="s1">axis_name</span><span class="s3">,</span>
                                <span class="s1">axis_index_groups</span><span class="s3">, </span><span class="s1">axis_size</span><span class="s3">, </span><span class="s1">tiled):</span>
  <span class="s1">index = _index_in_group(axis_name</span><span class="s3">, </span><span class="s1">axis_index_groups)</span>
  <span class="s1">scatter_dim_input_size = x.shape[scatter_dimension]</span>
  <span class="s3">if </span><span class="s1">tiled </span><span class="s3">and </span><span class="s1">scatter_dim_input_size % axis_size != </span><span class="s5">0</span><span class="s1">:</span>
    <span class="s3">raise </span><span class="s1">ValueError(</span><span class="s4">f&quot;tiled reduce_scatter operand scatter dimension size &quot;</span>
                     <span class="s4">f&quot;</span><span class="s3">{</span><span class="s1">scatter_dim_input_size</span><span class="s3">} </span><span class="s4">must be divisible by &quot;</span>
                     <span class="s4">f&quot;shard count </span><span class="s3">{</span><span class="s1">axis_size</span><span class="s3">}</span><span class="s4">&quot;</span><span class="s1">)</span>
  <span class="s3">elif not </span><span class="s1">tiled </span><span class="s3">and </span><span class="s1">scatter_dim_input_size != axis_size:</span>
    <span class="s3">raise </span><span class="s1">ValueError(</span><span class="s4">f&quot;reduce_scatter operand scatter dimension size &quot;</span>
                     <span class="s4">f&quot;</span><span class="s3">{</span><span class="s1">scatter_dim_input_size</span><span class="s3">} </span><span class="s4">must match shard count&quot;</span>
                     <span class="s4">f&quot;</span><span class="s3">{</span><span class="s1">axis_size</span><span class="s3">}</span><span class="s4">&quot;</span><span class="s1">)</span>
  <span class="s1">scatter_dim_output_size = scatter_dim_input_size // axis_size</span>

  <span class="s1">outs = reducer(x</span><span class="s3">, </span><span class="s1">axis_name=axis_name</span><span class="s3">, </span><span class="s1">axis_index_groups=axis_index_groups)</span>
  <span class="s1">outs = slicing.dynamic_slice_in_dim(</span>
      <span class="s1">outs</span><span class="s3">,</span>
      <span class="s1">start_index=index * scatter_dim_output_size</span><span class="s3">,</span>
      <span class="s1">slice_size=scatter_dim_output_size</span><span class="s3">,</span>
      <span class="s1">axis=scatter_dimension)</span>
  <span class="s3">if not </span><span class="s1">tiled:</span>
    <span class="s1">outs = lax.squeeze(outs</span><span class="s3">, </span><span class="s1">[scatter_dimension])</span>
  <span class="s3">return </span><span class="s1">outs</span>


<span class="s3">def </span><span class="s1">_reduce_scatter_lowering(prim</span><span class="s3">, </span><span class="s1">reducer</span><span class="s3">, </span><span class="s1">ctx</span><span class="s3">, </span><span class="s1">x</span><span class="s3">,</span>
                             <span class="s1">*</span><span class="s3">, </span><span class="s1">scatter_dimension</span><span class="s3">, </span><span class="s1">axis_name</span><span class="s3">,</span>
                             <span class="s1">axis_index_groups</span><span class="s3">, </span><span class="s1">axis_size</span><span class="s3">, </span><span class="s1">tiled):</span>
  <span class="s3">if </span><span class="s1">ctx.module_context.platform </span><span class="s3">in </span><span class="s1">(</span><span class="s4">&quot;tpu&quot;</span><span class="s3">, </span><span class="s4">&quot;cuda&quot;</span><span class="s3">, </span><span class="s4">&quot;rocm&quot;</span><span class="s1">):</span>
    <span class="s1">x_aval</span><span class="s3">, </span><span class="s1">= ctx.avals_in</span>
    <span class="s1">aval_out</span><span class="s3">, </span><span class="s1">= ctx.avals_out</span>
    <span class="s1">scalar_aval = x_aval.update(shape=())</span>
    <span class="s1">replica_groups = _replica_groups(ctx.module_context.axis_env</span><span class="s3">, </span><span class="s1">axis_name</span><span class="s3">,</span>
                                     <span class="s1">axis_index_groups)</span>
    <span class="s1">scatter_out_shape = list(x_aval.shape)</span>
    <span class="s1">scatter_out_shape[scatter_dimension] //= axis_size</span>
    <span class="s1">axis_context = ctx.module_context.axis_context</span>
    <span class="s1">is_spmd = isinstance(axis_context</span><span class="s3">,</span>
                        <span class="s1">(mlir.SPMDAxisContext</span><span class="s3">, </span><span class="s1">mlir.ShardingContext))</span>
    <span class="s3">if </span><span class="s1">is_spmd:</span>
      <span class="s0"># We want to emit the all-gather with global device IDs and a unique</span>
      <span class="s0"># channel ID, as otherwise it interprets the devices as replicas instead</span>
      <span class="s0"># of partitions - and XLA is configured with only a single replica.</span>
      <span class="s1">channel = ctx.module_context.new_channel()</span>
      <span class="s1">other_args = dict(</span>
          <span class="s1">channel_handle=hlo.ChannelHandle.get(</span>
              <span class="s1">channel</span><span class="s3">, </span><span class="s1">mlir.DEVICE_TO_DEVICE_TYPE)</span><span class="s3">,</span>
          <span class="s1">use_global_device_ids=ir.BoolAttr.get(</span><span class="s3">True</span><span class="s1">))</span>
    <span class="s3">else</span><span class="s1">:</span>
      <span class="s1">other_args = {}</span>
    <span class="s1">op = hlo.ReduceScatterOp(</span>
        <span class="s1">mlir.aval_to_ir_type(x_aval.update(shape=scatter_out_shape))</span><span class="s3">,</span>
        <span class="s1">x</span><span class="s3">,</span>
        <span class="s1">scatter_dimension=mlir.i64_attr(scatter_dimension)</span><span class="s3">,</span>
        <span class="s1">replica_groups=_replica_groups_hlo(replica_groups)</span><span class="s3">,</span>
        <span class="s1">**other_args)</span>
    <span class="s1">scalar_type = mlir.aval_to_ir_type(scalar_aval)</span>
    <span class="s1">reducer_block = op.regions[</span><span class="s5">0</span><span class="s1">].blocks.append(scalar_type</span><span class="s3">, </span><span class="s1">scalar_type)</span>
    <span class="s3">with </span><span class="s1">ir.InsertionPoint(reducer_block):</span>
      <span class="s1">lower_reducer = mlir.lower_fun(prim.bind</span><span class="s3">, </span><span class="s1">multiple_results=</span><span class="s3">False</span><span class="s1">)</span>
      <span class="s1">reducer_ctx = ctx.replace(primitive=</span><span class="s3">None,</span>
                                <span class="s1">avals_in=[scalar_aval] * </span><span class="s5">2</span><span class="s3">,</span>
                                <span class="s1">avals_out=[scalar_aval])</span>
      <span class="s1">out_nodes = lower_reducer(</span>
          <span class="s1">reducer_ctx</span><span class="s3">, </span><span class="s1">*([a] </span><span class="s3">for </span><span class="s1">a </span><span class="s3">in </span><span class="s1">reducer_block.arguments))</span>
      <span class="s1">hlo.ReturnOp(util.flatten(out_nodes))</span>

    <span class="s3">if </span><span class="s1">tiled:</span>
      <span class="s3">return </span><span class="s1">op.results</span>
    <span class="s3">else</span><span class="s1">:</span>
      <span class="s3">return </span><span class="s1">hlo.ReshapeOp(mlir.aval_to_ir_type(aval_out)</span><span class="s3">, </span><span class="s1">op.result).results</span>
  <span class="s3">else</span><span class="s1">:</span>
    <span class="s3">return </span><span class="s1">mlir.lower_fun(_reduce_scatter_via_reducer</span><span class="s3">, </span><span class="s1">multiple_results=</span><span class="s3">False</span><span class="s1">)(</span>
        <span class="s1">ctx</span><span class="s3">, </span><span class="s1">x</span><span class="s3">,</span>
        <span class="s1">reducer=reducer</span><span class="s3">,</span>
        <span class="s1">scatter_dimension=scatter_dimension</span><span class="s3">,</span>
        <span class="s1">axis_name=axis_name</span><span class="s3">,</span>
        <span class="s1">axis_index_groups=axis_index_groups</span><span class="s3">,</span>
        <span class="s1">axis_size=axis_size</span><span class="s3">,</span>
        <span class="s1">tiled=tiled)</span>


<span class="s3">def </span><span class="s1">_reduce_scatter_abstract_eval(x</span><span class="s3">, </span><span class="s1">*</span><span class="s3">, </span><span class="s1">axis_name</span><span class="s3">, </span><span class="s1">scatter_dimension</span><span class="s3">,</span>
                                  <span class="s1">axis_index_groups</span><span class="s3">, </span><span class="s1">axis_size</span><span class="s3">, </span><span class="s1">tiled):</span>
  <span class="s3">if not </span><span class="s1">isinstance(axis_name</span><span class="s3">, </span><span class="s1">(list</span><span class="s3">, </span><span class="s1">tuple)):</span>
    <span class="s1">axis_name = (axis_name</span><span class="s3">,</span><span class="s1">)</span>
  <span class="s1">x_aval = core.raise_to_shaped(x)</span>
  <span class="s1">new_shape = list(x_aval.shape)</span>
  <span class="s1">scatter_dim_input_size = x_aval.shape[scatter_dimension]</span>
  <span class="s3">if </span><span class="s1">tiled:</span>
    <span class="s3">if </span><span class="s1">scatter_dim_input_size % axis_size != </span><span class="s5">0</span><span class="s1">:</span>
      <span class="s3">raise </span><span class="s1">ValueError(</span><span class="s4">f&quot;tiled reduce_scatter operand scatter dimension size &quot;</span>
                       <span class="s4">f&quot;</span><span class="s3">{</span><span class="s1">scatter_dim_input_size</span><span class="s3">} </span><span class="s4">must be divisible by &quot;</span>
                       <span class="s4">f&quot;shard_count </span><span class="s3">{</span><span class="s1">axis_size</span><span class="s3">}</span><span class="s4">&quot;</span><span class="s1">)</span>
    <span class="s1">new_shape[scatter_dimension] = scatter_dim_input_size // axis_size</span>
  <span class="s3">else</span><span class="s1">:</span>
    <span class="s3">if </span><span class="s1">scatter_dim_input_size != axis_size:</span>
      <span class="s3">raise </span><span class="s1">ValueError(</span><span class="s4">f&quot;reduce_scatter operand scatter dimension size &quot;</span>
                       <span class="s4">f&quot;</span><span class="s3">{</span><span class="s1">scatter_dim_input_size</span><span class="s3">} </span><span class="s4">must match shard count &quot;</span>
                       <span class="s4">f&quot;</span><span class="s3">{</span><span class="s1">axis_size</span><span class="s3">}</span><span class="s4">&quot;</span><span class="s1">)</span>
    <span class="s3">del </span><span class="s1">new_shape[scatter_dimension]</span>

  <span class="s1">new_named_shape = {</span>
      <span class="s1">name: size</span>
      <span class="s3">for </span><span class="s1">name</span><span class="s3">, </span><span class="s1">size </span><span class="s3">in </span><span class="s1">x_aval.named_shape.items()</span>
      <span class="s3">if </span><span class="s1">name </span><span class="s3">not in </span><span class="s1">axis_name</span>
  <span class="s1">}</span>
  <span class="s3">return </span><span class="s1">x_aval.update(shape=new_shape</span><span class="s3">, </span><span class="s1">named_shape=new_named_shape)</span>


<span class="s3">def </span><span class="s1">_reduce_scatter_transpose_rule(cts</span><span class="s3">, </span><span class="s1">x</span><span class="s3">, </span><span class="s1">*</span><span class="s3">, </span><span class="s1">axis_name</span><span class="s3">, </span><span class="s1">scatter_dimension</span><span class="s3">,</span>
                                   <span class="s1">axis_index_groups</span><span class="s3">, </span><span class="s1">axis_size</span><span class="s3">, </span><span class="s1">tiled):</span>
  <span class="s3">return </span><span class="s1">(all_gather(cts</span><span class="s3">, </span><span class="s1">axis_name=axis_name</span><span class="s3">,</span>
                     <span class="s1">axis_index_groups=axis_index_groups</span><span class="s3">,</span>
                     <span class="s1">axis=scatter_dimension</span><span class="s3">, </span><span class="s1">tiled=tiled)</span><span class="s3">,</span><span class="s1">)</span>


<span class="s3">def </span><span class="s1">_reduce_scatter_batcher(vals_in</span><span class="s3">, </span><span class="s1">dims_in</span><span class="s3">, </span><span class="s1">*</span><span class="s3">, </span><span class="s1">scatter_dimension</span><span class="s3">, </span><span class="s1">axis_name</span><span class="s3">,</span>
                            <span class="s1">axis_index_groups</span><span class="s3">, </span><span class="s1">axis_size</span><span class="s3">, </span><span class="s1">tiled):</span>
  <span class="s1">(x</span><span class="s3">,</span><span class="s1">)</span><span class="s3">, </span><span class="s1">(d</span><span class="s3">,</span><span class="s1">) = vals_in</span><span class="s3">, </span><span class="s1">dims_in</span>
  <span class="s3">if </span><span class="s1">d &lt;= scatter_dimension:</span>
    <span class="s1">scatter_dimension += </span><span class="s5">1</span>
  <span class="s3">elif not </span><span class="s1">tiled:  </span><span class="s0"># Tiled all-scatter doesn't change the rank</span>
    <span class="s1">d += </span><span class="s5">1</span>
  <span class="s1">result = reduce_scatter_p.bind(</span>
      <span class="s1">x</span><span class="s3">,</span>
      <span class="s1">scatter_dimension=scatter_dimension</span><span class="s3">,</span>
      <span class="s1">axis_name=axis_name</span><span class="s3">,</span>
      <span class="s1">axis_index_groups=axis_index_groups</span><span class="s3">,</span>
      <span class="s1">axis_size=axis_size</span><span class="s3">,</span>
      <span class="s1">tiled=tiled)</span>
  <span class="s3">return </span><span class="s1">result</span><span class="s3">, </span><span class="s1">d</span>

<span class="s3">def </span><span class="s1">_reduce_scatter_collective(frame_size</span><span class="s3">, </span><span class="s1">frame_name</span><span class="s3">, </span><span class="s1">_</span><span class="s3">, </span><span class="s1">vals_in</span><span class="s3">, </span><span class="s1">dims_in</span><span class="s3">,</span>
                               <span class="s1">scatter_dimension</span><span class="s3">, </span><span class="s1">axis_name</span><span class="s3">,</span>
                               <span class="s1">axis_index_groups</span><span class="s3">, </span><span class="s1">axis_size</span><span class="s3">, </span><span class="s1">tiled):</span>
  <span class="s3">if </span><span class="s1">axis_index_groups </span><span class="s3">is not None</span><span class="s1">:</span>
    <span class="s3">raise </span><span class="s1">NotImplementedError(</span><span class="s4">&quot;axis_index_groups not supported in vmap&quot;</span><span class="s1">)</span>
  <span class="s3">assert </span><span class="s1">axis_size == frame_size</span><span class="s3">, </span><span class="s4">&quot;axis size doesn't match&quot;</span>
  <span class="s3">if not </span><span class="s1">isinstance(axis_name</span><span class="s3">, </span><span class="s1">tuple):</span>
    <span class="s1">axis_name = (axis_name</span><span class="s3">,</span><span class="s1">)</span>
  <span class="s3">if </span><span class="s1">len(axis_name) &gt; </span><span class="s5">1</span><span class="s1">:</span>
    <span class="s3">raise </span><span class="s1">NotImplementedError(</span><span class="s4">&quot;Please open a feature request!&quot;</span><span class="s1">)</span>
  <span class="s3">assert </span><span class="s1">axis_name == (frame_name</span><span class="s3">,</span><span class="s1">)</span><span class="s3">, </span><span class="s4">&quot;batcher called with wrong axis name&quot;</span>
  <span class="s1">(x</span><span class="s3">,</span><span class="s1">)</span><span class="s3">, </span><span class="s1">(d</span><span class="s3">,</span><span class="s1">) = vals_in</span><span class="s3">, </span><span class="s1">dims_in</span>
  <span class="s3">if </span><span class="s1">d </span><span class="s3">is </span><span class="s1">batching.not_mapped:</span>
    <span class="s1">y</span><span class="s3">, </span><span class="s1">dy = x * axis_size</span><span class="s3">, </span><span class="s1">scatter_dimension</span>
  <span class="s3">else</span><span class="s1">:</span>
    <span class="s1">y</span><span class="s3">, </span><span class="s1">dy = lax.reduce(x</span><span class="s3">, </span><span class="s5">0.</span><span class="s3">, </span><span class="s1">lax.add</span><span class="s3">, </span><span class="s1">(d</span><span class="s3">,</span><span class="s1">))</span><span class="s3">, </span><span class="s1">scatter_dimension</span>
  <span class="s3">if </span><span class="s1">tiled:</span>
    <span class="s1">y = _splitaxis(dy</span><span class="s3">, </span><span class="s1">axis_size</span><span class="s3">, </span><span class="s1">y)</span>
  <span class="s3">return </span><span class="s1">y</span><span class="s3">, </span><span class="s1">dy</span>


<span class="s1">reduce_scatter_p = core.AxisPrimitive(</span><span class="s4">&quot;reduce_scatter&quot;</span><span class="s1">)</span>
<span class="s1">reduce_scatter_p.def_abstract_eval(_reduce_scatter_abstract_eval)</span>
<span class="s1">ad.deflinear2(reduce_scatter_p</span><span class="s3">, </span><span class="s1">_reduce_scatter_transpose_rule)</span>
<span class="s1">batching.primitive_batchers[reduce_scatter_p] = _reduce_scatter_batcher</span>
<span class="s1">batching.axis_primitive_batchers[reduce_scatter_p] = _reduce_scatter_collective</span>
<span class="s1">xla.register_collective_primitive(reduce_scatter_p)</span>
<span class="s1">mlir.register_lowering(</span>
    <span class="s1">reduce_scatter_p</span><span class="s3">,</span>
    <span class="s1">partial(_reduce_scatter_lowering</span><span class="s3">, </span><span class="s1">lax.add_p</span><span class="s3">, </span><span class="s1">psum))</span>
<span class="s1">pxla.multi_host_supported_collectives.add(reduce_scatter_p)</span>
<span class="s1">core.axis_substitution_rules[reduce_scatter_p] = \</span>
    <span class="s1">partial(_subst_all_names_in_param</span><span class="s3">, </span><span class="s4">'axis_name'</span><span class="s1">)</span>


<span class="s3">def </span><span class="s1">psum_scatter(x</span><span class="s3">, </span><span class="s1">axis_name</span><span class="s3">, </span><span class="s1">*</span><span class="s3">, </span><span class="s1">scatter_dimension=</span><span class="s5">0</span><span class="s3">, </span><span class="s1">axis_index_groups=</span><span class="s3">None, </span><span class="s1">tiled=</span><span class="s3">False</span><span class="s1">):</span>
  <span class="s2">&quot;&quot;&quot;Compute an all-reduce sum over the axis ``axis_name``, and scatter the result. 
 
  Args: 
    x: array(s) with a mapped axis named ``axis_name``. 
    axis_name: hashable Python object used to name a pmapped axis (see the 
      :func:`jax.pmap` documentation for more details). 
    scatter_dimension: a positional axis into which the all reduce result along 
      ``axis_name`` will be scattered. 
    axis_index_groups: optional list of lists containing axis indices (e.g. for 
      an axis of size 4, [[0, 1], [2, 3]] would run reduce-scatter over the 
      first two and the last two replicas). Groups must cover all axis indices 
      exactly once, and all groups must be the same size. 
    tiled: when ``False``, the size of dimension in ``scatter_dimension`` must 
      match the size of axis ``axis_name`` (or the group size if 
      ``axis_index_groups`` is given). After scattering the all reduce result 
      along ``scatter_dimension``, the output is sequeezed by removing 
      ``scatter_dimension``. When ``True``, the size of dimension in 
      ``scatter_dimension` must be dividible by the size of axis ``axis_name`` 
      (or the group size if ``axis_index_groups`` is given), 
      and ``scatter_dimension`` is preserved. 
 
  Returns: 
    Array(s) with the similar shape as ``x``, except the size of dimension in 
    position``scatter_dimension`` is divided by the size of axis ``axis_name``. 
 
  For example, with 4 XLA devices available: 
 
  &gt;&gt;&gt; x = np.arange(16).reshape(4, 4) 
  &gt;&gt;&gt; print(x) 
  [[ 0  1  2  3] 
   [ 4  5  6  7] 
   [ 8  9 10 11] 
   [12 13 14 15]] 
  &gt;&gt;&gt; y = jax.pmap(lambda x: jax.lax.psum_scatter(x, 'i'), axis_name='i')(x) 
  &gt;&gt;&gt; print(y) 
  [24 28 32 36] 
 
  if using tiled: 
 
  &gt;&gt;&gt; y = jax.pmap(lambda x: jax.lax.psum_scatter(x, 'i', tiled=True), axis_name='i')(x) 
  &gt;&gt;&gt; print(y) 
  [[24] 
   [28] 
   [32] 
   [36]] 
 
  An example of using axis_index_groups: 
 
  &gt;&gt;&gt; def f(x): 
  ...   return jax.lax.psum_scatter( 
  ...       x, 'i', axis_index_groups=[[0, 2], [3, 1]], tiled=True) 
  &gt;&gt;&gt; y = jax.pmap(f, axis_name='i')(x) 
  &gt;&gt;&gt; print(y) 
  [[ 8 10] 
   [20 22] 
   [12 14] 
   [16 18]] 
  &quot;&quot;&quot;</span>
  <span class="s1">axis_size = psum(</span><span class="s5">1</span><span class="s3">, </span><span class="s1">axis_name</span><span class="s3">, </span><span class="s1">axis_index_groups=axis_index_groups)</span>
  <span class="s1">axis_index_groups = _canonicalize_axis_index_groups(axis_index_groups)</span>
  <span class="s1">bind = partial(</span>
      <span class="s1">reduce_scatter_p.bind</span><span class="s3">,</span>
      <span class="s1">axis_name=axis_name</span><span class="s3">,</span>
      <span class="s1">scatter_dimension=scatter_dimension</span><span class="s3">,</span>
      <span class="s1">axis_index_groups=axis_index_groups</span><span class="s3">,</span>
      <span class="s1">axis_size=axis_size</span><span class="s3">,</span>
      <span class="s1">tiled=tiled)</span>
  <span class="s3">return </span><span class="s1">tree_util.tree_map(bind</span><span class="s3">, </span><span class="s1">x)</span>


<span class="s3">def </span><span class="s1">_build_axis_index_lowering_hlo(ctx</span><span class="s3">, </span><span class="s1">axis_name</span><span class="s3">, </span><span class="s1">axis_env):</span>
  <span class="s3">if </span><span class="s1">isinstance(axis_name</span><span class="s3">, </span><span class="s1">tuple):</span>
    <span class="s3">assert </span><span class="s1">axis_name</span><span class="s3">, </span><span class="s4">'empty axis name'</span>
    <span class="s3">if </span><span class="s1">len(axis_name) &gt; </span><span class="s5">1</span><span class="s1">:</span>
      <span class="s3">raise </span><span class="s1">NotImplementedError(</span>
          <span class="s4">'`axis_index` translation rule does not support multiple axis names.'</span><span class="s1">)</span>
    <span class="s1">axis_name</span><span class="s3">, </span><span class="s1">= axis_name</span>
  <span class="s1">axis_pos = list(axis_env.names).index(axis_name)</span>
  <span class="s1">nreplicas = axis_env.nreps // math.prod(axis_env.sizes)</span>
  <span class="s1">div = mlir.ir_constant(</span>
      <span class="s1">np.array(</span>
          <span class="s1">nreplicas * math.prod(axis_env.sizes[axis_pos + </span><span class="s5">1 </span><span class="s1">:])</span><span class="s3">, </span><span class="s1">dtype=np.uint32</span>
      <span class="s1">)</span>
  <span class="s1">)</span>
  <span class="s1">mod = mlir.ir_constant(np.array(axis_env.sizes[axis_pos]</span><span class="s3">, </span><span class="s1">dtype=np.uint32))</span>
  <span class="s1">axis_context = ctx.module_context.axis_context</span>
  <span class="s1">is_spmd = isinstance(axis_context</span><span class="s3">,</span>
                       <span class="s1">(mlir.SPMDAxisContext</span><span class="s3">, </span><span class="s1">mlir.ShardingContext))</span>
  <span class="s3">if </span><span class="s1">is_spmd:</span>
    <span class="s1">device_id = hlo.PartitionIdOp()</span>
  <span class="s3">else</span><span class="s1">:</span>
    <span class="s1">device_id = hlo.ReplicaIdOp()</span>
  <span class="s1">unsigned_index = hlo.RemOp(hlo.DivOp(device_id</span><span class="s3">, </span><span class="s1">div)</span><span class="s3">, </span><span class="s1">mod)</span>
  <span class="s3">return </span><span class="s1">hlo.ConvertOp(</span>
      <span class="s1">ir.RankedTensorType.get([]</span><span class="s3">, </span><span class="s1">ir.IntegerType.get_signless(</span><span class="s5">32</span><span class="s1">))</span><span class="s3">,</span>
      <span class="s1">unsigned_index).result</span>

<span class="s3">def </span><span class="s1">_axis_index_lowering(ctx</span><span class="s3">, </span><span class="s1">*</span><span class="s3">, </span><span class="s1">axis_name):</span>
  <span class="s3">return </span><span class="s1">[</span>
      <span class="s1">_build_axis_index_lowering_hlo(ctx</span><span class="s3">, </span><span class="s1">axis_name</span><span class="s3">,</span>
                                     <span class="s1">ctx.module_context.axis_env)</span>
  <span class="s1">]</span>


<span class="s3">def </span><span class="s1">_axis_index_abstract_eval(*</span><span class="s3">, </span><span class="s1">axis_name):</span>
  <span class="s1">frame = core.axis_frame(axis_name)</span>
  <span class="s3">return </span><span class="s1">ShapedArray(()</span><span class="s3">, </span><span class="s1">np.int32</span><span class="s3">, </span><span class="s1">named_shape={axis_name: frame.size})</span>

<span class="s1">axis_index_p = core.Primitive(</span><span class="s4">'axis_index'</span><span class="s1">)</span>
<span class="s1">xla.register_collective_primitive(axis_index_p)</span>
<span class="s1">mlir.register_lowering(axis_index_p</span><span class="s3">, </span><span class="s1">_axis_index_lowering)</span>
<span class="s1">axis_index_p.def_abstract_eval(_axis_index_abstract_eval)</span>
<span class="s1">pxla.multi_host_supported_collectives.add(axis_index_p)</span>
<span class="s1">core.axis_substitution_rules[axis_index_p] = partial(_subst_all_names_in_param</span><span class="s3">, </span><span class="s4">'axis_name'</span><span class="s1">)</span>

<span class="s0"># Axis index doesn't get any arguments, so that the default bind would have no</span>
<span class="s0"># way to call into a data-dependency based trace such as vmap. Each trace that</span>
<span class="s0"># wants to bind an axis name has to additionally implement `process_axis_index`</span>
<span class="s0"># and put its main trace on the axis env stack.</span>
<span class="s3">def </span><span class="s1">_axis_index_bind(*</span><span class="s3">, </span><span class="s1">axis_name):</span>
  <span class="s3">def </span><span class="s1">name_idx(name):</span>
    <span class="s1">frame = core.axis_frame(name)</span>
    <span class="s1">dynamic = core.thread_local_state.trace_state.trace_stack.dynamic</span>
    <span class="s3">if </span><span class="s1">(frame.main_trace </span><span class="s3">is None or </span><span class="s1">dynamic.level &gt; frame.main_trace.level):</span>
      <span class="s3">return </span><span class="s1">core.Primitive.bind(axis_index_p</span><span class="s3">, </span><span class="s1">axis_name=name)</span>
    <span class="s3">else</span><span class="s1">:</span>
      <span class="s1">trace = frame.main_trace.with_cur_sublevel()</span>
      <span class="s3">return </span><span class="s1">trace.process_axis_index(frame)</span>

  <span class="s3">if not </span><span class="s1">isinstance(axis_name</span><span class="s3">, </span><span class="s1">(tuple</span><span class="s3">, </span><span class="s1">list)):</span>
    <span class="s3">return </span><span class="s1">name_idx(axis_name)</span>
  <span class="s3">else</span><span class="s1">:</span>
    <span class="s1">inner_size = </span><span class="s5">1</span>
    <span class="s1">index = </span><span class="s5">0</span>
    <span class="s3">for </span><span class="s1">name </span><span class="s3">in </span><span class="s1">reversed(axis_name):</span>
      <span class="s1">index += name_idx(name) * inner_size</span>
      <span class="s1">inner_size *= psum(</span><span class="s5">1</span><span class="s3">, </span><span class="s1">name)</span>
    <span class="s3">return </span><span class="s1">index</span>
<span class="s1">axis_index_p.def_custom_bind(_axis_index_bind)</span>

<span class="s3">def </span><span class="s1">_vmap_process_axis_index(self</span><span class="s3">, </span><span class="s1">frame):</span>
  <span class="s3">assert </span><span class="s1">frame.size </span><span class="s3">is not None</span>
  <span class="s3">return </span><span class="s1">batching.BatchTracer(self</span><span class="s3">, </span><span class="s1">lax.iota(np.int32</span><span class="s3">, </span><span class="s1">frame.size)</span><span class="s3">, </span><span class="s5">0</span><span class="s1">)</span>
<span class="s1">batching.BatchTrace.process_axis_index = _vmap_process_axis_index  </span><span class="s0"># type: ignore</span>


<span class="s1">pdot_p = core.AxisPrimitive(</span><span class="s4">'pdot'</span><span class="s1">)</span>
<span class="s1">core.axis_substitution_rules[pdot_p] = partial(_subst_all_names_in_param</span><span class="s3">, </span><span class="s4">'axis_name'</span><span class="s1">)</span>

<span class="s1">@pdot_p.def_impl</span>
<span class="s3">def </span><span class="s1">_pdot_impl(x</span><span class="s3">, </span><span class="s1">y</span><span class="s3">, </span><span class="s1">*</span><span class="s3">, </span><span class="s1">axis_name</span><span class="s3">, </span><span class="s1">pos_contract</span><span class="s3">, </span><span class="s1">pos_batch</span><span class="s3">, </span><span class="s1">precision):</span>
  <span class="s3">if </span><span class="s1">axis_name: </span><span class="s3">raise </span><span class="s1">NameError(</span><span class="s4">f&quot;unbound axis name: </span><span class="s3">{</span><span class="s1">axis_name[</span><span class="s5">0</span><span class="s1">]</span><span class="s3">}</span><span class="s4">&quot;</span><span class="s1">)</span>
  <span class="s3">return </span><span class="s1">lax.dot_general(x</span><span class="s3">, </span><span class="s1">y</span><span class="s3">, </span><span class="s1">(pos_contract</span><span class="s3">, </span><span class="s1">pos_batch)</span><span class="s3">, </span><span class="s1">precision=precision)</span>

<span class="s1">@pdot_p.def_abstract_eval</span>
<span class="s3">def </span><span class="s1">_pdot_abstract_eval(x</span><span class="s3">, </span><span class="s1">y</span><span class="s3">, </span><span class="s1">*</span><span class="s3">, </span><span class="s1">axis_name</span><span class="s3">, </span><span class="s1">pos_contract</span><span class="s3">, </span><span class="s1">pos_batch</span><span class="s3">, </span><span class="s1">precision):</span>
  <span class="s0"># TODO(frostig,mattjj,jekbradbury): check inputs have given axis names?</span>
  <span class="s3">if not </span><span class="s1">len(set(axis_name)) == len(axis_name): </span><span class="s3">raise </span><span class="s1">ValueError</span>
  <span class="s1">pos_aval = lax.dot_general_p.abstract_eval(</span>
      <span class="s1">x</span><span class="s3">, </span><span class="s1">y</span><span class="s3">, </span><span class="s1">dimension_numbers=[pos_contract</span><span class="s3">, </span><span class="s1">pos_batch]</span><span class="s3">,</span>
      <span class="s1">precision=precision</span><span class="s3">, </span><span class="s1">preferred_element_type=</span><span class="s3">None</span><span class="s1">)[</span><span class="s5">0</span><span class="s1">]</span>
  <span class="s1">common_named_shape = core.join_named_shapes(x.named_shape</span><span class="s3">, </span><span class="s1">y.named_shape)</span>
  <span class="s1">named_shape = {name: size</span>
                 <span class="s3">for </span><span class="s1">name</span><span class="s3">, </span><span class="s1">size </span><span class="s3">in </span><span class="s1">common_named_shape.items()</span>
                 <span class="s3">if </span><span class="s1">name </span><span class="s3">not in </span><span class="s1">axis_name}</span>
  <span class="s3">return </span><span class="s1">pos_aval.update(named_shape=named_shape)</span>

<span class="s3">def </span><span class="s1">_pdot_vmap_collective_rule(axis_size</span><span class="s3">, </span><span class="s1">frame_name</span><span class="s3">, </span><span class="s1">_</span><span class="s3">, </span><span class="s1">vals_in</span><span class="s3">, </span><span class="s1">dims_in</span><span class="s3">, </span><span class="s1">*</span><span class="s3">, </span><span class="s1">axis_name</span><span class="s3">,</span>
                               <span class="s1">pos_contract</span><span class="s3">, </span><span class="s1">pos_batch</span><span class="s3">, </span><span class="s1">precision):</span>
  <span class="s1">x</span><span class="s3">, </span><span class="s1">y = vals_in</span>
  <span class="s1">x_dim</span><span class="s3">, </span><span class="s1">y_dim = dims_in</span>
  <span class="s1">x_pos_contract</span><span class="s3">, </span><span class="s1">y_pos_contract = pos_contract</span>
  <span class="s1">x_pos_contract = [x_dim] + [d + (d &gt;= x_dim) </span><span class="s3">for </span><span class="s1">d </span><span class="s3">in </span><span class="s1">x_pos_contract]</span>
  <span class="s1">y_pos_contract = [y_dim] + [d + (d &gt;= y_dim) </span><span class="s3">for </span><span class="s1">d </span><span class="s3">in </span><span class="s1">y_pos_contract]</span>
  <span class="s1">x_pos_batch</span><span class="s3">, </span><span class="s1">y_pos_batch = pos_batch</span>
  <span class="s1">x_pos_batch = [d + (d &gt;= x_dim) </span><span class="s3">for </span><span class="s1">d </span><span class="s3">in </span><span class="s1">x_pos_batch]</span>
  <span class="s1">y_pos_batch = [d + (d &gt;= y_dim) </span><span class="s3">for </span><span class="s1">d </span><span class="s3">in </span><span class="s1">y_pos_batch]</span>
  <span class="s1">remaining_axis_names = tuple(n </span><span class="s3">for </span><span class="s1">n </span><span class="s3">in </span><span class="s1">axis_name </span><span class="s3">if </span><span class="s1">n != frame_name)</span>
  <span class="s1">out = pdot_p.bind(x</span><span class="s3">, </span><span class="s1">y</span><span class="s3">, </span><span class="s1">axis_name=remaining_axis_names</span><span class="s3">,</span>
                    <span class="s1">pos_contract=(tuple(x_pos_contract)</span><span class="s3">, </span><span class="s1">tuple(y_pos_contract))</span><span class="s3">,</span>
                    <span class="s1">pos_batch=(tuple(x_pos_batch)</span><span class="s3">, </span><span class="s1">tuple(y_pos_batch))</span><span class="s3">,</span>
                    <span class="s1">precision=precision)</span>
  <span class="s3">return </span><span class="s1">out</span><span class="s3">, None</span>
<span class="s1">batching.axis_primitive_batchers[pdot_p] = _pdot_vmap_collective_rule</span>

<span class="s3">def </span><span class="s1">_pdot_vmap_batching_rule(vals_in</span><span class="s3">, </span><span class="s1">dims_in</span><span class="s3">, </span><span class="s1">*</span><span class="s3">, </span><span class="s1">axis_name</span><span class="s3">, </span><span class="s1">pos_contract</span><span class="s3">,</span>
                             <span class="s1">pos_batch</span><span class="s3">, </span><span class="s1">precision):</span>
  <span class="s1">x</span><span class="s3">, </span><span class="s1">y = vals_in</span>
  <span class="s1">(pos_contract</span><span class="s3">, </span><span class="s1">pos_batch)</span><span class="s3">, </span><span class="s1">result_batch_dim = lax._dot_general_batch_dim_nums(</span>
      <span class="s1">(x.ndim</span><span class="s3">, </span><span class="s1">y.ndim)</span><span class="s3">, </span><span class="s1">dims_in</span><span class="s3">, </span><span class="s1">[pos_contract</span><span class="s3">, </span><span class="s1">pos_batch])</span>
  <span class="s1">out = pdot_p.bind(x</span><span class="s3">, </span><span class="s1">y</span><span class="s3">, </span><span class="s1">axis_name=axis_name</span><span class="s3">, </span><span class="s1">pos_contract=pos_contract</span><span class="s3">,</span>
                    <span class="s1">pos_batch=pos_batch</span><span class="s3">, </span><span class="s1">precision=precision)</span>
  <span class="s3">return </span><span class="s1">out</span><span class="s3">, </span><span class="s1">result_batch_dim</span>
<span class="s1">batching.primitive_batchers[pdot_p] = _pdot_vmap_batching_rule</span>


<span class="s3">def </span><span class="s1">_pdot_lowering(x</span><span class="s3">, </span><span class="s1">y</span><span class="s3">, </span><span class="s1">*</span><span class="s3">, </span><span class="s1">axis_name</span><span class="s3">, </span><span class="s1">pos_contract</span><span class="s3">, </span><span class="s1">pos_batch</span><span class="s3">, </span><span class="s1">precision):</span>
  <span class="s1">local_out = lax.dot_general(x</span><span class="s3">, </span><span class="s1">y</span><span class="s3">, </span><span class="s1">dimension_numbers=(pos_contract</span><span class="s3">, </span><span class="s1">pos_batch)</span><span class="s3">,</span>
                              <span class="s1">precision=precision</span><span class="s3">, </span><span class="s1">preferred_element_type=</span><span class="s3">None</span><span class="s1">)</span>
  <span class="s3">return </span><span class="s1">psum(local_out</span><span class="s3">, </span><span class="s1">axis_name) </span><span class="s3">if </span><span class="s1">axis_name </span><span class="s3">is not None else </span><span class="s1">local_out</span>

<span class="s1">xla.register_collective_primitive(pdot_p)</span>
<span class="s1">mlir.register_lowering(</span>
    <span class="s1">pdot_p</span><span class="s3">,</span>
    <span class="s1">mlir.lower_fun(_pdot_lowering</span><span class="s3">, </span><span class="s1">multiple_results=</span><span class="s3">False</span><span class="s1">))</span>

<span class="s3">def </span><span class="s1">_pdot_transpose_lhs(g</span><span class="s3">, </span><span class="s1">x</span><span class="s3">, </span><span class="s1">y</span><span class="s3">, </span><span class="s1">*</span><span class="s3">, </span><span class="s1">axis_name</span><span class="s3">, </span><span class="s1">pos_contract</span><span class="s3">, </span><span class="s1">pos_batch</span><span class="s3">, </span><span class="s1">precision):</span>
  <span class="s0"># TODO: avals with names, call pbroadcast with axis_name</span>
  <span class="s3">return </span><span class="s1">lax._dot_general_transpose_lhs(</span>
      <span class="s1">g</span><span class="s3">, </span><span class="s1">x</span><span class="s3">, </span><span class="s1">y</span><span class="s3">, </span><span class="s1">dimension_numbers=[pos_contract</span><span class="s3">, </span><span class="s1">pos_batch]</span><span class="s3">, </span><span class="s1">precision=precision</span><span class="s3">,</span>
      <span class="s1">preferred_element_type=</span><span class="s3">None</span><span class="s1">)</span>
<span class="s3">def </span><span class="s1">_pdot_transpose_rhs(g</span><span class="s3">, </span><span class="s1">x</span><span class="s3">, </span><span class="s1">y</span><span class="s3">, </span><span class="s1">*</span><span class="s3">, </span><span class="s1">axis_name</span><span class="s3">, </span><span class="s1">pos_contract</span><span class="s3">, </span><span class="s1">pos_batch</span><span class="s3">, </span><span class="s1">precision):</span>
  <span class="s0"># TODO: avals with names, call pbroadcast with axis_name</span>
  <span class="s3">return </span><span class="s1">lax._dot_general_transpose_rhs(</span>
      <span class="s1">g</span><span class="s3">, </span><span class="s1">x</span><span class="s3">, </span><span class="s1">y</span><span class="s3">, </span><span class="s1">dimension_numbers=[pos_contract</span><span class="s3">, </span><span class="s1">pos_batch]</span><span class="s3">, </span><span class="s1">precision=precision</span><span class="s3">,</span>
      <span class="s1">preferred_element_type=</span><span class="s3">None</span><span class="s1">)</span>
<span class="s1">ad.defbilinear(pdot_p</span><span class="s3">, </span><span class="s1">_pdot_transpose_lhs</span><span class="s3">, </span><span class="s1">_pdot_transpose_rhs)</span>

<span class="s1">pxla.multi_host_supported_collectives.add(pdot_p)</span>


<span class="s3">def </span><span class="s1">_pgather_impl(src</span><span class="s3">, </span><span class="s1">idx</span><span class="s3">, </span><span class="s1">*</span><span class="s3">, </span><span class="s1">axes):</span>
  <span class="s3">assert </span><span class="s1">all(isinstance(axis</span><span class="s3">, </span><span class="s1">int) </span><span class="s3">for </span><span class="s1">axis </span><span class="s3">in </span><span class="s1">axes)</span>
  <span class="s1">src_axes_front = moveaxis(src</span><span class="s3">, </span><span class="s1">axes</span><span class="s3">, </span><span class="s1">range(len(axes)))</span>
  <span class="s1">non_axes_shape = src_axes_front.shape[len(axes):]</span>
  <span class="s1">src_one_axis_front = src_axes_front.reshape((-</span><span class="s5">1</span><span class="s3">,</span><span class="s1">) + non_axes_shape)</span>
  <span class="s1">slice_sizes = (</span><span class="s5">1</span><span class="s3">,</span><span class="s1">) + non_axes_shape</span>
  <span class="s1">idx = lax.expand_dims(idx</span><span class="s3">, </span><span class="s1">(-</span><span class="s5">1</span><span class="s3">,</span><span class="s1">))</span>
  <span class="s1">offset_dims = tuple(range(idx.ndim - </span><span class="s5">1</span><span class="s3">, </span><span class="s1">idx.ndim + src_one_axis_front.ndim - </span><span class="s5">2</span><span class="s1">))</span>
  <span class="s1">dnums = slicing.GatherDimensionNumbers(</span>
      <span class="s1">offset_dims=offset_dims</span><span class="s3">,</span>
      <span class="s1">collapsed_slice_dims=(</span><span class="s5">0</span><span class="s3">,</span><span class="s1">)</span><span class="s3">,</span>
      <span class="s1">start_index_map=(</span><span class="s5">0</span><span class="s3">,</span><span class="s1">))</span>
  <span class="s3">return </span><span class="s1">slicing.gather(src_one_axis_front</span><span class="s3">, </span><span class="s1">idx</span><span class="s3">, </span><span class="s1">dimension_numbers=dnums</span><span class="s3">,</span>
                        <span class="s1">slice_sizes=tuple(slice_sizes))</span>

<span class="s3">def </span><span class="s1">_pgather_abstract_eval(src</span><span class="s3">, </span><span class="s1">idx</span><span class="s3">, </span><span class="s1">*</span><span class="s3">, </span><span class="s1">axes):</span>
  <span class="s0"># TODO: Avals with names rule: remove all axes from src, insert those from idx</span>
  <span class="s0">#       The order is important, because it is ok to re-insert one of the deleted axes!</span>
  <span class="s1">shape = list(src.shape)</span>
  <span class="s3">for </span><span class="s1">axis </span><span class="s3">in </span><span class="s1">sorted((a </span><span class="s3">for </span><span class="s1">a </span><span class="s3">in </span><span class="s1">axes </span><span class="s3">if </span><span class="s1">isinstance(a</span><span class="s3">, </span><span class="s1">int))</span><span class="s3">, </span><span class="s1">reverse=</span><span class="s3">True</span><span class="s1">):</span>
    <span class="s3">del </span><span class="s1">shape[axis]</span>
  <span class="s1">shape = idx.shape + tuple(shape)</span>
  <span class="s3">return </span><span class="s1">ShapedArray(shape</span><span class="s3">, </span><span class="s1">src.dtype)</span>

<span class="s3">def </span><span class="s1">_pgather_parallel_lowering(ctx</span><span class="s3">, </span><span class="s1">src</span><span class="s3">, </span><span class="s1">idx</span><span class="s3">, </span><span class="s1">*</span><span class="s3">, </span><span class="s1">axes):</span>
  <span class="s3">if </span><span class="s1">any(</span><span class="s3">not </span><span class="s1">isinstance(axis</span><span class="s3">, </span><span class="s1">int) </span><span class="s3">for </span><span class="s1">axis </span><span class="s3">in </span><span class="s1">axes):</span>
    <span class="s3">raise </span><span class="s1">NotImplementedError(</span><span class="s4">&quot;pgather only supported in the SPMD lowering.&quot;</span>
                              <span class="s4">&quot;Please open a feature request!&quot;</span><span class="s1">)</span>
  <span class="s3">return </span><span class="s1">mlir.lower_fun(_pgather_impl</span><span class="s3">, </span><span class="s1">multiple_results=</span><span class="s3">False</span><span class="s1">)(</span>
      <span class="s1">ctx</span><span class="s3">, </span><span class="s1">src</span><span class="s3">, </span><span class="s1">idx</span><span class="s3">, </span><span class="s1">axes=axes)</span>

<span class="s3">def </span><span class="s1">_pgather_batcher(vals_in</span><span class="s3">, </span><span class="s1">dims_in</span><span class="s3">, </span><span class="s1">*</span><span class="s3">, </span><span class="s1">axes):</span>
  <span class="s1">src</span><span class="s3">, </span><span class="s1">idx = vals_in</span>
  <span class="s1">dsrc</span><span class="s3">, </span><span class="s1">didx = dims_in</span>
  <span class="s3">if </span><span class="s1">didx </span><span class="s3">is not </span><span class="s1">batching.not_mapped </span><span class="s3">and </span><span class="s1">dsrc </span><span class="s3">is not </span><span class="s1">batching.not_mapped:</span>
    <span class="s0"># NB: We could just go forward with it and take the diagonal along the</span>
    <span class="s0">#     two axes we get in the output, but that would be quite inefficient</span>
    <span class="s3">raise </span><span class="s1">NotImplementedError(</span><span class="s4">&quot;Please open a feature request!&quot;</span><span class="s1">)</span>
  <span class="s3">elif </span><span class="s1">didx </span><span class="s3">is not </span><span class="s1">batching.not_mapped:</span>
    <span class="s3">return </span><span class="s1">pgather_p.bind(src</span><span class="s3">, </span><span class="s1">idx</span><span class="s3">, </span><span class="s1">axes=axes)</span><span class="s3">, </span><span class="s1">didx</span>
  <span class="s3">elif </span><span class="s1">dsrc </span><span class="s3">is not </span><span class="s1">batching.not_mapped:</span>
    <span class="s1">src_last_batched = moveaxis(src</span><span class="s3">, </span><span class="s1">dsrc</span><span class="s3">, </span><span class="s1">-</span><span class="s5">1</span><span class="s1">)</span>
    <span class="s1">result = pgather_p.bind(src_last_batched</span><span class="s3">, </span><span class="s1">idx</span><span class="s3">, </span><span class="s1">axes=axes)</span>
    <span class="s3">return </span><span class="s1">result</span><span class="s3">, </span><span class="s1">result.ndim - </span><span class="s5">1</span>
  <span class="s3">else</span><span class="s1">:</span>
    <span class="s3">assert False  </span><span class="s0"># This shouldn't get called anyway</span>

<span class="s3">def </span><span class="s1">_pgather_collective_batcher(axis_size</span><span class="s3">, </span><span class="s1">frame_name</span><span class="s3">, </span><span class="s1">_</span><span class="s3">, </span><span class="s1">vals_in</span><span class="s3">, </span><span class="s1">dims_in</span><span class="s3">, </span><span class="s1">*</span><span class="s3">, </span><span class="s1">axes):</span>
  <span class="s1">src</span><span class="s3">, </span><span class="s1">idx = vals_in</span>
  <span class="s1">dsrc</span><span class="s3">, </span><span class="s1">didx = dims_in</span>
  <span class="s3">if </span><span class="s1">dsrc </span><span class="s3">is </span><span class="s1">batching.not_mapped:</span>
    <span class="s3">raise </span><span class="s1">ValueError(</span><span class="s4">&quot;pgather axis {frame.name} is missing from the indexed value&quot;</span><span class="s1">)</span>
  <span class="s3">if </span><span class="s1">didx </span><span class="s3">is not </span><span class="s1">batching.not_mapped:</span>
    <span class="s0"># NOTE: This is allowed and the output would be mapped along this axis!</span>
    <span class="s3">raise </span><span class="s1">NotImplementedError(</span><span class="s4">&quot;Please open a feature request!&quot;</span><span class="s1">)</span>
  <span class="s0"># Now source is mapped, idx is not</span>
  <span class="s1">new_axes = tuple(dsrc </span><span class="s3">if </span><span class="s1">axis == frame_name </span><span class="s3">else</span>
                   <span class="s1">axis + (dsrc &lt;= axis) </span><span class="s3">if </span><span class="s1">isinstance(axis</span><span class="s3">, </span><span class="s1">int) </span><span class="s3">else</span>
                   <span class="s1">axis</span>
                   <span class="s3">for </span><span class="s1">axis </span><span class="s3">in </span><span class="s1">axes)</span>
  <span class="s0"># The result is not mapped, because we eliminate all axes, and those include</span>
  <span class="s0"># the batched axis.</span>
  <span class="s3">if </span><span class="s1">all(isinstance(axis</span><span class="s3">, </span><span class="s1">int) </span><span class="s3">for </span><span class="s1">axis </span><span class="s3">in </span><span class="s1">axes):</span>
    <span class="s0"># We rewrite a purely positional pgather as a gather, because that one</span>
    <span class="s0"># is more fully featured (e.g. supports AD).</span>
    <span class="s3">return </span><span class="s1">_pgather_impl(src</span><span class="s3">, </span><span class="s1">idx</span><span class="s3">, </span><span class="s1">axes=new_axes)</span><span class="s3">, </span><span class="s1">batching.not_mapped</span>
  <span class="s3">else</span><span class="s1">:</span>
    <span class="s3">return </span><span class="s1">pgather_p.bind(src</span><span class="s3">, </span><span class="s1">idx</span><span class="s3">, </span><span class="s1">axes=new_axes)</span><span class="s3">, </span><span class="s1">batching.not_mapped</span>

<span class="s1">pgather_p = core.AxisPrimitive(</span><span class="s4">'pgather'</span><span class="s1">)</span>
<span class="s1">pgather_p.def_impl(_pgather_impl)</span>
<span class="s1">pgather_p.def_abstract_eval(_pgather_abstract_eval)</span>
<span class="s1">xla.register_collective_primitive(pgather_p)</span>
<span class="s1">mlir.register_lowering(pgather_p</span><span class="s3">, </span><span class="s1">_pgather_parallel_lowering)</span>
<span class="s0"># TODO: Transpose? That requires adding pscatter...</span>
<span class="s1">batching.primitive_batchers[pgather_p] = _pgather_batcher</span>
<span class="s1">batching.axis_primitive_batchers[pgather_p] = _pgather_collective_batcher</span>
<span class="s1">core.axis_substitution_rules[pgather_p] = partial(_subst_all_names_in_param</span><span class="s3">, </span><span class="s4">'axes'</span><span class="s1">)</span>
</pre>
</body>
</html>