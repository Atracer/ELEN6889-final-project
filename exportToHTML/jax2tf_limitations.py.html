<html>
<head>
<title>jax2tf_limitations.py</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<style type="text/css">
.s0 { color: #808080;}
.s1 { color: #a9b7c6;}
.s2 { color: #629755; font-style: italic;}
.s3 { color: #cc7832;}
.s4 { color: #6897bb;}
.s5 { color: #6a8759;}
</style>
</head>
<body bgcolor="#2b2b2b">
<table CELLSPACING=0 CELLPADDING=5 COLS=1 WIDTH="100%" BGCOLOR="#606060" >
<tr><td><center>
<font face="Arial, Helvetica" color="#000000">
jax2tf_limitations.py</font>
</center></td></tr></table>
<pre><span class="s0"># Copyright 2021 The JAX Authors.</span>
<span class="s0">#</span>
<span class="s0"># Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);</span>
<span class="s0"># you may not use this file except in compliance with the License.</span>
<span class="s0"># You may obtain a copy of the License at</span>
<span class="s0">#</span>
<span class="s0">#     https://www.apache.org/licenses/LICENSE-2.0</span>
<span class="s0">#</span>
<span class="s0"># Unless required by applicable law or agreed to in writing, software</span>
<span class="s0"># distributed under the License is distributed on an &quot;AS IS&quot; BASIS,</span>
<span class="s0"># WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span>
<span class="s0"># See the License for the specific language governing permissions and</span>
<span class="s0"># limitations under the License.</span>
<span class="s2">&quot;&quot;&quot;See primitives_test docstring for how the Jax2TfLimitations are used.&quot;&quot;&quot;</span>

<span class="s3">import </span><span class="s1">itertools</span>
<span class="s3">from </span><span class="s1">typing </span><span class="s3">import </span><span class="s1">Any</span><span class="s3">, </span><span class="s1">Callable</span><span class="s3">, </span><span class="s1">Optional</span><span class="s3">, </span><span class="s1">Sequence</span><span class="s3">, </span><span class="s1">Union</span>

<span class="s3">import </span><span class="s1">jax</span>
<span class="s3">from </span><span class="s1">jax </span><span class="s3">import </span><span class="s1">lax</span>
<span class="s3">from </span><span class="s1">jax </span><span class="s3">import </span><span class="s1">numpy </span><span class="s3">as </span><span class="s1">jnp</span>
<span class="s3">from </span><span class="s1">jax.config </span><span class="s3">import </span><span class="s1">config</span>
<span class="s3">from </span><span class="s1">jax._src </span><span class="s3">import </span><span class="s1">test_util </span><span class="s3">as </span><span class="s1">jtu</span>
<span class="s3">from </span><span class="s1">jax._src </span><span class="s3">import </span><span class="s1">dtypes</span>
<span class="s3">from </span><span class="s1">jax.experimental.jax2tf.tests </span><span class="s3">import </span><span class="s1">primitive_harness</span>
<span class="s3">import </span><span class="s1">numpy </span><span class="s3">as </span><span class="s1">np</span>

<span class="s1">DType = Any</span>


<span class="s3">class </span><span class="s1">Jax2TfLimitation(primitive_harness.Limitation):</span>
  <span class="s2">&quot;&quot;&quot;Specific primitive limitations for jax2tf. 
 
  See the primitive_test module docstring for details. 
  &quot;&quot;&quot;</span>

  <span class="s0"># Bitmask values for encoding limitations specific to native lowering</span>
  <span class="s1">FOR_NATIVE = </span><span class="s4">1</span>
  <span class="s1">FOR_NON_NATIVE = </span><span class="s4">2</span>

  <span class="s3">def </span><span class="s1">__init__(</span>
      <span class="s1">self</span><span class="s3">,</span>
      <span class="s1">description: str</span><span class="s3">,</span>
      <span class="s1">*</span><span class="s3">,</span>
      <span class="s1">devices: Union[str</span><span class="s3">, </span><span class="s1">Sequence[str]] = (</span><span class="s5">&quot;cpu&quot;</span><span class="s3">, </span><span class="s5">&quot;gpu&quot;</span><span class="s3">, </span><span class="s5">&quot;tpu&quot;</span><span class="s1">)</span><span class="s3">,</span>
      <span class="s1">dtypes: Sequence[DType] = ()</span><span class="s3">,</span>
      <span class="s1">enabled: bool = </span><span class="s3">True,</span>
      <span class="s0"># jax2tf specific</span>
      <span class="s1">modes=(</span><span class="s5">&quot;eager&quot;</span><span class="s3">, </span><span class="s5">&quot;graph&quot;</span><span class="s3">, </span><span class="s5">&quot;compiled&quot;</span><span class="s1">)</span><span class="s3">,</span>
      <span class="s1">native_serialization=FOR_NON_NATIVE</span><span class="s3">,</span>
      <span class="s1">skip_tf_run=</span><span class="s3">False,</span>
      <span class="s1">expect_tf_error: bool = </span><span class="s3">True,</span>
      <span class="s1">skip_comparison=</span><span class="s3">False,</span>
      <span class="s1">custom_assert: Optional[Callable] = </span><span class="s3">None,</span>
      <span class="s1">tol=</span><span class="s3">None</span><span class="s1">):</span>
    <span class="s2">&quot;&quot;&quot;See the primitive_harness.Limitation common arguments. 
 
    Args : 
      modes: one of &quot;eager&quot;, &quot;graph&quot;, &quot;compiled&quot; 
      for_native_serialization: A bitmask with some of {FOR_NATIVE, FOR_NON_NATIVE} 
        to specify how the limitation applies to native and non-native lowering. 
      skip_tf_run: if set will skip the TF execution. Use this sparingly, 
        prefer `expect_tf_error`. Use only when the test cannot recover from 
        the TF error. 
      expect_tf_error: if set, then expect a TF error in the given mode when 
        executing the result of jax2tf conversion. If not set, then the 
        limitation must have a custom_assert or non-default tol. 
      skip_comparison: skips the numeric comparison. 
      tol: a tolerance to use for both atol and rtol. We will use the maximum 
        tolerance over all the applicable limitations, irrespective of their 
        order. 
      custom_assert: if given, then execute as 
        `custom_assert(tst, result_jax, result_tf, args=args, tol=tol, err_msg)` 
        , where `tst` is the current TestCase instance, and args are the input 
        arguments that the harness created. The `tol` is the maximum tolerance 
        based on the applicable limitations. `err_msg` is passed to NumPy 
        assert methods. 
        `result_tf` is already converted to NumPy arrays. 
    &quot;&quot;&quot;</span>
    <span class="s1">super().__init__(</span>
        <span class="s1">description</span><span class="s3">, </span><span class="s1">devices=devices</span><span class="s3">, </span><span class="s1">dtypes=dtypes</span><span class="s3">, </span><span class="s1">enabled=enabled)</span>
    <span class="s3">if </span><span class="s1">isinstance(modes</span><span class="s3">, </span><span class="s1">str):</span>
      <span class="s1">modes = (modes</span><span class="s3">,</span><span class="s1">)</span>
    <span class="s3">assert </span><span class="s1">all(m </span><span class="s3">in </span><span class="s1">[</span><span class="s5">&quot;eager&quot;</span><span class="s3">, </span><span class="s5">&quot;graph&quot;</span><span class="s3">, </span><span class="s5">&quot;compiled&quot;</span><span class="s1">] </span><span class="s3">for </span><span class="s1">m </span><span class="s3">in </span><span class="s1">modes)</span><span class="s3">, </span><span class="s5">&quot;Invalid modes: {modes}&quot;</span>
    <span class="s1">self.modes = modes</span>
    <span class="s1">self.native_serialization = native_serialization</span>
    <span class="s1">self.expect_tf_error = expect_tf_error</span>
    <span class="s1">self.skip_tf_run = skip_tf_run</span>
    <span class="s1">self.custom_assert = custom_assert</span>
    <span class="s1">self.tol = tol</span>
    <span class="s1">self.skip_comparison = skip_comparison</span>

  <span class="s3">def </span><span class="s1">get_max_tolerance_limitation(</span>
      <span class="s1">self</span><span class="s3">, </span><span class="s1">limitations: Sequence[</span><span class="s5">&quot;Jax2TfLimitation&quot;</span><span class="s1">]</span>
  <span class="s1">) -&gt; Optional[</span><span class="s5">&quot;Jax2TfLimitation&quot;</span><span class="s1">]:</span>
    <span class="s2">&quot;&quot;&quot;Pick the tolerance limitation that establishes the maximum tolerance.&quot;&quot;&quot;</span>
    <span class="s0"># TODO: it would be best if the limitations with tolerance are mutually exclusive</span>
    <span class="s0"># and we don't have to compute the maximum</span>
    <span class="s0"># TODO: we made this an instance method only so that we don't have to import</span>
    <span class="s0"># this module from tf_test.util.</span>
    <span class="s1">max_tol_lim = </span><span class="s3">None</span>
    <span class="s3">for </span><span class="s1">l </span><span class="s3">in </span><span class="s1">limitations:</span>
      <span class="s3">if </span><span class="s1">l.tol </span><span class="s3">is not None</span><span class="s1">:</span>
        <span class="s3">if </span><span class="s1">max_tol_lim </span><span class="s3">is None or </span><span class="s1">l.tol &gt; max_tol_lim.tol:</span>
          <span class="s1">max_tol_lim = l</span>
    <span class="s3">return </span><span class="s1">max_tol_lim</span>

  <span class="s3">def </span><span class="s1">filter(  </span><span class="s0"># type: ignore[override]</span>
      <span class="s1">self</span><span class="s3">,</span>
      <span class="s1">dtype: Optional[DType] = </span><span class="s3">None,</span>
      <span class="s1">device: Optional[str] = </span><span class="s3">None,</span>
      <span class="s1">mode: Optional[str] = </span><span class="s3">None</span><span class="s1">) -&gt; bool:</span>
    <span class="s2">&quot;&quot;&quot;Checks if this limitation is enabled for dtype and device and mode.&quot;&quot;&quot;</span>
    <span class="s1">native_serialization_mask = (</span>
        <span class="s1">Jax2TfLimitation.FOR_NATIVE</span>
        <span class="s3">if </span><span class="s1">config.jax2tf_default_native_serialization</span>
        <span class="s3">else </span><span class="s1">Jax2TfLimitation.FOR_NON_NATIVE)</span>
    <span class="s3">return </span><span class="s1">((mode </span><span class="s3">is None or </span><span class="s1">mode </span><span class="s3">in </span><span class="s1">self.modes) </span><span class="s3">and</span>
            <span class="s1">(self.native_serialization &amp; native_serialization_mask) </span><span class="s3">and</span>
            <span class="s1">super().filter(device=device</span><span class="s3">, </span><span class="s1">dtype=dtype))</span>

  <span class="s1">@classmethod</span>
  <span class="s3">def </span><span class="s1">limitations_for_harness(</span>
      <span class="s1">cls</span><span class="s3">, </span><span class="s1">harness: primitive_harness.Harness) -&gt; Sequence[</span><span class="s5">&quot;Jax2TfLimitation&quot;</span><span class="s1">]:</span>
    <span class="s1">group_method = getattr(cls</span><span class="s3">, </span><span class="s1">harness.group_name</span><span class="s3">, None</span><span class="s1">)</span>
    <span class="s3">if </span><span class="s1">harness.group_name </span><span class="s3">in </span><span class="s1">cls.harness_groups_no_limitations:</span>
      <span class="s3">assert </span><span class="s1">group_method </span><span class="s3">is None, </span><span class="s1">(</span>
          <span class="s5">f&quot;Harness group '</span><span class="s3">{</span><span class="s1">harness.group_name</span><span class="s3">}</span><span class="s5">' is both in &quot;</span>
          <span class="s5">f&quot;'harness_groups_no_limitations' and has a custom &quot;</span>
          <span class="s5">f&quot;Jax2TfLimitation.classmethod defined (see module docstring)&quot;</span><span class="s1">)</span>
      <span class="s3">return </span><span class="s1">[]</span>
    <span class="s3">else</span><span class="s1">:</span>
      <span class="s3">assert </span><span class="s1">group_method </span><span class="s3">is not None, </span><span class="s1">(</span>
          <span class="s5">f&quot;Harness group '</span><span class="s3">{</span><span class="s1">harness.group_name</span><span class="s3">}</span><span class="s5">' must be either part of &quot;</span>
          <span class="s5">f&quot;'harness_groups_no_limitations' or must have a custom &quot;</span>
          <span class="s5">f&quot;Jax2TfLimitation.classmethod defined (see module docstring)&quot;</span><span class="s1">)</span>
      <span class="s1">limitations = group_method(harness)</span>
      <span class="s3">assert </span><span class="s1">isinstance(limitations</span><span class="s3">, </span><span class="s1">(list</span><span class="s3">, </span><span class="s1">tuple))</span>
      <span class="s3">return </span><span class="s1">limitations</span>

  <span class="s0"># We keep here the explicit set of groups for which we don't have limitations</span>
  <span class="s1">harness_groups_no_limitations = {</span>
      <span class="s5">&quot;abs&quot;</span><span class="s3">, </span><span class="s5">&quot;add&quot;</span><span class="s3">, </span><span class="s5">&quot;add_any&quot;</span><span class="s3">, </span><span class="s5">&quot;and&quot;</span><span class="s3">, </span><span class="s5">&quot;atan2&quot;</span><span class="s3">, </span><span class="s5">&quot;bitcast_convert_type&quot;</span><span class="s3">,</span>
      <span class="s5">&quot;broadcast&quot;</span><span class="s3">, </span><span class="s5">&quot;broadcast_in_dim&quot;</span><span class="s3">, </span><span class="s5">&quot;cbrt&quot;</span><span class="s3">, </span><span class="s5">&quot;ceil&quot;</span><span class="s3">, </span><span class="s5">&quot;clamp&quot;</span><span class="s3">, </span><span class="s5">&quot;concatenate&quot;</span><span class="s3">,</span>
      <span class="s5">&quot;cos&quot;</span><span class="s3">, </span><span class="s5">&quot;cosh&quot;</span><span class="s3">, </span><span class="s5">&quot;complex&quot;</span><span class="s3">, </span><span class="s5">&quot;conj&quot;</span><span class="s3">, </span><span class="s5">&quot;convert_element_type&quot;</span><span class="s3">, </span><span class="s5">&quot;cummax&quot;</span><span class="s3">,</span>
      <span class="s5">&quot;cummin&quot;</span><span class="s3">, </span><span class="s5">&quot;device_put&quot;</span><span class="s3">, </span><span class="s5">&quot;dynamic_slice&quot;</span><span class="s3">, </span><span class="s5">&quot;dynamic_update_slice&quot;</span><span class="s3">, </span><span class="s5">&quot;exp&quot;</span><span class="s3">,</span>
      <span class="s5">&quot;eq&quot;</span><span class="s3">, </span><span class="s5">&quot;floor&quot;</span><span class="s3">, </span><span class="s5">&quot;gather&quot;</span><span class="s3">, </span><span class="s5">&quot;ge&quot;</span><span class="s3">, </span><span class="s5">&quot;gt&quot;</span><span class="s3">, </span><span class="s5">&quot;imag&quot;</span><span class="s3">, </span><span class="s5">&quot;iota&quot;</span><span class="s3">, </span><span class="s5">&quot;iota_2x32_shape&quot;</span><span class="s3">,</span>
      <span class="s5">&quot;is_finite&quot;</span><span class="s3">, </span><span class="s5">&quot;le&quot;</span><span class="s3">, </span><span class="s5">&quot;logistic&quot;</span><span class="s3">, </span><span class="s5">&quot;lt&quot;</span><span class="s3">, </span><span class="s5">&quot;log&quot;</span><span class="s3">, </span><span class="s5">&quot;mul&quot;</span><span class="s3">, </span><span class="s5">&quot;ne&quot;</span><span class="s3">, </span><span class="s5">&quot;neg&quot;</span><span class="s3">, </span><span class="s5">&quot;not&quot;</span><span class="s3">,</span>
      <span class="s5">&quot;or&quot;</span><span class="s3">, </span><span class="s5">&quot;pad&quot;</span><span class="s3">, </span><span class="s5">&quot;population_count&quot;</span><span class="s3">, </span><span class="s5">&quot;random_categorical&quot;</span><span class="s3">, </span><span class="s5">&quot;random_uniform&quot;</span><span class="s3">,</span>
      <span class="s5">&quot;random_randint&quot;</span><span class="s3">, </span><span class="s5">&quot;reduce&quot;</span><span class="s3">, </span><span class="s5">&quot;reduce_and&quot;</span><span class="s3">, </span><span class="s5">&quot;reduce_precision&quot;</span><span class="s3">,</span>
      <span class="s5">&quot;reduce_prod&quot;</span><span class="s3">, </span><span class="s5">&quot;reduce_or&quot;</span><span class="s3">,</span>
      <span class="s5">&quot;reduce_sum&quot;</span><span class="s3">, </span><span class="s5">&quot;reduce_window_mul&quot;</span><span class="s3">, </span><span class="s5">&quot;reduce_window_min&quot;</span><span class="s3">,</span>
      <span class="s5">&quot;reduce_window_max&quot;</span><span class="s3">, </span><span class="s5">&quot;real&quot;</span><span class="s3">, </span><span class="s5">&quot;reshape&quot;</span><span class="s3">, </span><span class="s5">&quot;rev&quot;</span><span class="s3">, </span><span class="s5">&quot;rsqrt&quot;</span><span class="s3">, </span><span class="s5">&quot;select_n&quot;</span><span class="s3">,</span>
      <span class="s5">&quot;select_and_scatter_add&quot;</span><span class="s3">, </span><span class="s5">&quot;shift_left&quot;</span><span class="s3">, </span><span class="s5">&quot;shift_right_logical&quot;</span><span class="s3">,</span>
      <span class="s5">&quot;shift_right_arithmetic&quot;</span><span class="s3">, </span><span class="s5">&quot;sign&quot;</span><span class="s3">, </span><span class="s5">&quot;sin&quot;</span><span class="s3">, </span><span class="s5">&quot;sinh&quot;</span><span class="s3">, </span><span class="s5">&quot;slice&quot;</span><span class="s3">, </span><span class="s5">&quot;sqrt&quot;</span><span class="s3">,</span>
      <span class="s5">&quot;squeeze&quot;</span><span class="s3">, </span><span class="s5">&quot;stop_gradient&quot;</span><span class="s3">, </span><span class="s5">&quot;sub&quot;</span><span class="s3">, </span><span class="s5">&quot;tie_in&quot;</span><span class="s3">, </span><span class="s5">&quot;transpose&quot;</span><span class="s3">, </span><span class="s5">&quot;xor&quot;</span><span class="s3">,</span>
      <span class="s5">&quot;zeros_like&quot;</span>
  <span class="s1">}</span>

  <span class="s1">@classmethod</span>
  <span class="s3">def </span><span class="s1">helper_get_trig_custom_limitation(cls</span><span class="s3">, </span><span class="s1">np_inverse):</span>

    <span class="s3">def </span><span class="s1">custom_assert(tst</span><span class="s3">, </span><span class="s1">result_jax</span><span class="s3">, </span><span class="s1">result_tf</span><span class="s3">, </span><span class="s1">*</span><span class="s3">, </span><span class="s1">args</span><span class="s3">, </span><span class="s1">tol</span><span class="s3">, </span><span class="s1">err_msg):</span>
      <span class="s1">operand</span><span class="s3">, </span><span class="s1">= args</span>
      <span class="s1">tst.assertAllClose(</span>
          <span class="s1">operand</span><span class="s3">, </span><span class="s1">np_inverse(result_tf)</span><span class="s3">, </span><span class="s1">atol=tol</span><span class="s3">, </span><span class="s1">rtol=tol</span><span class="s3">, </span><span class="s1">err_msg=err_msg)</span>

    <span class="s3">return </span><span class="s1">custom_numeric(</span>
        <span class="s1">description=</span><span class="s5">&quot;May return different but still correct results&quot;</span><span class="s3">,</span>
        <span class="s1">dtypes=[np.complex64</span><span class="s3">, </span><span class="s1">np.complex128]</span><span class="s3">,</span>
        <span class="s1">custom_assert=custom_assert)</span>

  <span class="s1">@classmethod</span>
  <span class="s3">def </span><span class="s1">random_seed(cls</span><span class="s3">, </span><span class="s1">handess: primitive_harness.Harness):</span>
    <span class="s3">return </span><span class="s1">[custom_random_keys_output()]</span>

  <span class="s1">@classmethod</span>
  <span class="s3">def </span><span class="s1">random_split(cls</span><span class="s3">, </span><span class="s1">handess: primitive_harness.Harness):</span>
    <span class="s3">return </span><span class="s1">[custom_random_keys_output()]</span>

  <span class="s1">@classmethod</span>
  <span class="s3">def </span><span class="s1">random_fold_in(cls</span><span class="s3">, </span><span class="s1">handess: primitive_harness.Harness):</span>
    <span class="s3">return </span><span class="s1">[custom_random_keys_output()]</span>

  <span class="s1">@classmethod</span>
  <span class="s3">def </span><span class="s1">acos(cls</span><span class="s3">, </span><span class="s1">harness: primitive_harness.Harness):</span>
    <span class="s3">return </span><span class="s1">[</span>
        <span class="s1">custom_numeric(</span>
            <span class="s1">dtypes=[np.complex64]</span><span class="s3">,</span>
            <span class="s1">devices=(</span><span class="s5">&quot;cpu&quot;</span><span class="s3">, </span><span class="s5">&quot;gpu&quot;</span><span class="s1">)</span><span class="s3">,</span>
            <span class="s1">tol=</span><span class="s4">1e-4</span><span class="s3">,</span>
            <span class="s1">modes=(</span><span class="s5">&quot;eager&quot;</span><span class="s3">, </span><span class="s5">&quot;graph&quot;</span><span class="s3">, </span><span class="s5">&quot;compiled&quot;</span><span class="s1">))</span><span class="s3">,</span>
        <span class="s1">custom_numeric(</span>
            <span class="s1">dtypes=[np.complex128]</span><span class="s3">,</span>
            <span class="s1">devices=(</span><span class="s5">&quot;cpu&quot;</span><span class="s3">, </span><span class="s5">&quot;gpu&quot;</span><span class="s1">)</span><span class="s3">,</span>
            <span class="s1">tol=</span><span class="s4">1e-13</span><span class="s3">,</span>
            <span class="s1">modes=(</span><span class="s5">&quot;eager&quot;</span><span class="s3">, </span><span class="s5">&quot;graph&quot;</span><span class="s3">, </span><span class="s5">&quot;compiled&quot;</span><span class="s1">))</span><span class="s3">,</span>
    <span class="s1">]</span>

  <span class="s1">@classmethod</span>
  <span class="s3">def </span><span class="s1">acosh(cls</span><span class="s3">, </span><span class="s1">harness: primitive_harness.Harness):</span>
    <span class="s3">return </span><span class="s1">[</span>
        <span class="s1">custom_numeric(dtypes=[np.complex64]</span><span class="s3">, </span><span class="s1">devices=(</span><span class="s5">&quot;cpu&quot;</span><span class="s3">, </span><span class="s5">&quot;gpu&quot;</span><span class="s1">)</span><span class="s3">, </span><span class="s1">tol=</span><span class="s4">1e-3</span><span class="s1">)</span><span class="s3">,</span>
        <span class="s1">custom_numeric(dtypes=[np.complex128]</span><span class="s3">, </span><span class="s1">devices=(</span><span class="s5">&quot;cpu&quot;</span><span class="s3">, </span><span class="s5">&quot;gpu&quot;</span><span class="s1">)</span><span class="s3">, </span><span class="s1">tol=</span><span class="s4">1e-12</span><span class="s1">)</span><span class="s3">,</span>
        <span class="s1">cls.helper_get_trig_custom_limitation(np.cosh)</span>
    <span class="s1">]</span>

  <span class="s1">@classmethod</span>
  <span class="s3">def </span><span class="s1">approx_top_k(cls</span><span class="s3">, </span><span class="s1">harness: primitive_harness.Harness):</span>
    <span class="s1">supported_dtypes = jtu.supported_dtypes()</span>
    <span class="s3">return </span><span class="s1">[</span>
        <span class="s1">missing_tf_kernel(</span>
            <span class="s1">dtypes=[t </span><span class="s3">for </span><span class="s1">t </span><span class="s3">in </span><span class="s1">[jnp.bfloat16</span><span class="s3">, </span><span class="s1">np.float16</span><span class="s3">, </span><span class="s1">np.float32</span><span class="s3">, </span><span class="s1">np.float64]</span>
                    <span class="s3">if </span><span class="s1">t </span><span class="s3">in </span><span class="s1">supported_dtypes]</span><span class="s3">,</span>
            <span class="s1">devices=(</span><span class="s5">&quot;cpu&quot;</span><span class="s3">, </span><span class="s5">&quot;gpu&quot;</span><span class="s1">)</span><span class="s3">,</span>
            <span class="s1">modes=(</span><span class="s5">&quot;graph&quot;</span><span class="s3">, </span><span class="s5">&quot;eager&quot;</span><span class="s1">))</span><span class="s3">,</span>
        <span class="s1">Jax2TfLimitation(</span>
            <span class="s5">&quot;compilation not supported for float64.&quot;</span><span class="s3">,</span>
            <span class="s1">dtypes=[np.float64]</span><span class="s3">,</span>
            <span class="s1">devices=(</span><span class="s5">&quot;cpu&quot;</span><span class="s3">, </span><span class="s5">&quot;gpu&quot;</span><span class="s1">)</span><span class="s3">,</span>
            <span class="s1">modes=(</span><span class="s5">&quot;compiled&quot;</span><span class="s3">,</span><span class="s1">))]</span>

  <span class="s1">@classmethod</span>
  <span class="s3">def </span><span class="s1">argmax(cls</span><span class="s3">, </span><span class="s1">harness: primitive_harness.Harness):</span>
    <span class="s3">return </span><span class="s1">[</span>
        <span class="s1">Jax2TfLimitation(</span>
            <span class="s5">&quot;different results when the input contains NaN and enable_xla=False&quot;</span><span class="s3">,</span>
            <span class="s1">dtypes=jtu.dtypes.all_inexact</span><span class="s3">,</span>
            <span class="s1">devices=(</span><span class="s5">&quot;cpu&quot;</span><span class="s3">, </span><span class="s5">&quot;gpu&quot;</span><span class="s3">, </span><span class="s5">&quot;tpu&quot;</span><span class="s1">)</span><span class="s3">,</span>
            <span class="s1">modes=(</span><span class="s5">&quot;eager&quot;</span><span class="s3">, </span><span class="s5">&quot;graph&quot;</span><span class="s3">, </span><span class="s5">&quot;compiled&quot;</span><span class="s1">)</span><span class="s3">,</span>
            <span class="s1">expect_tf_error=</span><span class="s3">False,</span>
            <span class="s1">skip_comparison=</span><span class="s3">True,</span>
            <span class="s1">enabled=(</span><span class="s5">&quot;nan_&quot; </span><span class="s3">in </span><span class="s1">harness.name </span><span class="s3">and not </span><span class="s1">harness.params[</span><span class="s5">&quot;enable_xla&quot;</span><span class="s1">]))</span><span class="s3">,</span>
    <span class="s1">]</span>

  <span class="s1">@classmethod</span>
  <span class="s3">def </span><span class="s1">argmin(cls</span><span class="s3">, </span><span class="s1">harness: primitive_harness.Harness):</span>
    <span class="s3">return </span><span class="s1">cls.argmax(harness)</span>

  <span class="s1">@classmethod</span>
  <span class="s3">def </span><span class="s1">asin(cls</span><span class="s3">, </span><span class="s1">harness: primitive_harness.Harness):</span>
    <span class="s3">return </span><span class="s1">[</span>
        <span class="s1">custom_numeric(dtypes=[np.complex64]</span><span class="s3">, </span><span class="s1">devices=(</span><span class="s5">&quot;cpu&quot;</span><span class="s3">, </span><span class="s5">&quot;gpu&quot;</span><span class="s1">)</span><span class="s3">, </span><span class="s1">tol=</span><span class="s4">1e-4</span><span class="s3">,</span>
                       <span class="s1">modes=(</span><span class="s5">&quot;eager&quot;</span><span class="s3">, </span><span class="s5">&quot;graph&quot;</span><span class="s3">, </span><span class="s5">&quot;compiled&quot;</span><span class="s1">))</span><span class="s3">,</span>
        <span class="s1">custom_numeric(dtypes=[np.complex128]</span><span class="s3">, </span><span class="s1">devices=(</span><span class="s5">&quot;cpu&quot;</span><span class="s3">, </span><span class="s5">&quot;gpu&quot;</span><span class="s1">)</span><span class="s3">, </span><span class="s1">tol=</span><span class="s4">1e-12</span><span class="s3">,</span>
                       <span class="s1">modes=(</span><span class="s5">&quot;eager&quot;</span><span class="s3">, </span><span class="s5">&quot;graph&quot;</span><span class="s3">, </span><span class="s5">&quot;compiled&quot;</span><span class="s1">))</span><span class="s3">,</span>
        <span class="s1">cls.helper_get_trig_custom_limitation(np.sin)</span>
    <span class="s1">]</span>

  <span class="s1">@classmethod</span>
  <span class="s3">def </span><span class="s1">asinh(cls</span><span class="s3">, </span><span class="s1">harness: primitive_harness.Harness):</span>
    <span class="s3">return </span><span class="s1">[</span>
        <span class="s1">custom_numeric(dtypes=[np.complex64]</span><span class="s3">, </span><span class="s1">devices=(</span><span class="s5">&quot;cpu&quot;</span><span class="s3">, </span><span class="s5">&quot;gpu&quot;</span><span class="s1">)</span><span class="s3">, </span><span class="s1">tol=</span><span class="s4">1e-3</span><span class="s1">)</span><span class="s3">,</span>
        <span class="s1">custom_numeric(dtypes=[np.complex128]</span><span class="s3">, </span><span class="s1">devices=(</span><span class="s5">&quot;cpu&quot;</span><span class="s3">, </span><span class="s5">&quot;gpu&quot;</span><span class="s1">)</span><span class="s3">, </span><span class="s1">tol=</span><span class="s4">1e-12</span><span class="s1">)</span><span class="s3">,</span>
        <span class="s1">cls.helper_get_trig_custom_limitation(np.sinh)</span>
    <span class="s1">]</span>

  <span class="s1">@classmethod</span>
  <span class="s3">def </span><span class="s1">atan(cls</span><span class="s3">, </span><span class="s1">harness: primitive_harness.Harness):</span>
    <span class="s3">return </span><span class="s1">[</span>
        <span class="s1">custom_numeric(dtypes=[np.complex64]</span><span class="s3">, </span><span class="s1">devices=(</span><span class="s5">&quot;cpu&quot;</span><span class="s3">, </span><span class="s5">&quot;gpu&quot;</span><span class="s1">)</span><span class="s3">, </span><span class="s1">tol=</span><span class="s4">1e-5</span><span class="s1">)</span><span class="s3">,</span>
        <span class="s1">custom_numeric(dtypes=[np.complex128]</span><span class="s3">, </span><span class="s1">devices=(</span><span class="s5">&quot;cpu&quot;</span><span class="s3">, </span><span class="s5">&quot;gpu&quot;</span><span class="s1">)</span><span class="s3">, </span><span class="s1">tol=</span><span class="s4">1e-12</span><span class="s1">)</span><span class="s3">,</span>
        <span class="s1">cls.helper_get_trig_custom_limitation(np.tan)</span>
    <span class="s1">]</span>

  <span class="s1">@classmethod</span>
  <span class="s3">def </span><span class="s1">atanh(cls</span><span class="s3">, </span><span class="s1">harness: primitive_harness.Harness):</span>
    <span class="s3">return </span><span class="s1">[</span>
        <span class="s1">custom_numeric(dtypes=[np.float64]</span><span class="s3">, </span><span class="s1">tol=</span><span class="s4">1e-14</span><span class="s1">)</span><span class="s3">,</span>
        <span class="s1">custom_numeric(dtypes=[np.complex64]</span><span class="s3">, </span><span class="s1">tol=</span><span class="s4">1e-3</span><span class="s1">)</span><span class="s3">,</span>
        <span class="s1">custom_numeric(dtypes=[np.complex128]</span><span class="s3">, </span><span class="s1">devices=(</span><span class="s5">&quot;cpu&quot;</span><span class="s3">, </span><span class="s5">&quot;gpu&quot;</span><span class="s1">)</span><span class="s3">, </span><span class="s1">tol=</span><span class="s4">1e-12</span><span class="s1">)</span><span class="s3">,</span>
        <span class="s1">cls.helper_get_trig_custom_limitation(np.tanh)</span>
    <span class="s1">]</span>

  <span class="s1">@classmethod</span>
  <span class="s3">def </span><span class="s1">bessel_i0e(cls</span><span class="s3">, </span><span class="s1">harness: primitive_harness.Harness):</span>
    <span class="s3">return </span><span class="s1">[</span>
        <span class="s1">missing_tf_kernel(</span>
            <span class="s1">dtypes=[dtypes.bfloat16]</span><span class="s3">,</span>
            <span class="s1">devices=(</span><span class="s5">&quot;cpu&quot;</span><span class="s3">, </span><span class="s5">&quot;gpu&quot;</span><span class="s1">)</span><span class="s3">,</span>
            <span class="s1">modes=(</span><span class="s5">&quot;eager&quot;</span><span class="s3">, </span><span class="s5">&quot;graph&quot;</span><span class="s1">))</span>
    <span class="s1">]</span>

  <span class="s1">@classmethod</span>
  <span class="s3">def </span><span class="s1">bessel_i1e(cls</span><span class="s3">, </span><span class="s1">harness: primitive_harness.Harness):</span>
    <span class="s3">return </span><span class="s1">cls.bessel_i0e(harness)</span>

  <span class="s1">@classmethod</span>
  <span class="s3">def </span><span class="s1">cholesky(cls</span><span class="s3">, </span><span class="s1">harness: primitive_harness.Harness):</span>

    <span class="s3">def </span><span class="s1">custom_assert(tst</span><span class="s3">, </span><span class="s1">result_jax</span><span class="s3">, </span><span class="s1">result_tf</span><span class="s3">, </span><span class="s1">*</span><span class="s3">, </span><span class="s1">tol</span><span class="s3">, </span><span class="s1">err_msg</span><span class="s3">, </span><span class="s1">**_):</span>
      <span class="s0"># cholesky_p returns garbage in the strictly upper triangular part of the</span>
      <span class="s0"># result, so we can safely ignore that part.</span>
      <span class="s1">tst.assertAllClose(</span>
          <span class="s1">jnp.tril(result_jax)</span><span class="s3">, </span><span class="s1">result_tf</span><span class="s3">, </span><span class="s1">atol=tol</span><span class="s3">, </span><span class="s1">err_msg=err_msg)</span>

    <span class="s3">return </span><span class="s1">[</span>
        <span class="s0"># TODO: very high tolerance</span>
        <span class="s1">custom_numeric(</span>
            <span class="s1">dtypes=[np.float32</span><span class="s3">, </span><span class="s1">np.complex64]</span><span class="s3">,</span>
            <span class="s1">tol=</span><span class="s4">1e-2</span><span class="s3">,</span>
            <span class="s1">devices=(</span><span class="s5">&quot;cpu&quot;</span><span class="s3">, </span><span class="s5">&quot;gpu&quot;</span><span class="s1">)</span><span class="s3">,</span>
            <span class="s1">modes=(</span><span class="s5">&quot;eager&quot;</span><span class="s3">, </span><span class="s5">&quot;graph&quot;</span><span class="s3">, </span><span class="s5">&quot;compiled&quot;</span><span class="s1">))</span><span class="s3">,</span>
        <span class="s1">custom_numeric(</span>
            <span class="s1">dtypes=[np.float64</span><span class="s3">, </span><span class="s1">np.complex128]</span><span class="s3">,</span>
            <span class="s1">tol=</span><span class="s4">1e-6</span><span class="s3">,</span>
            <span class="s1">devices=(</span><span class="s5">&quot;cpu&quot;</span><span class="s3">, </span><span class="s5">&quot;gpu&quot;</span><span class="s1">)</span><span class="s3">,</span>
            <span class="s1">modes=(</span><span class="s5">&quot;eager&quot;</span><span class="s3">, </span><span class="s5">&quot;graph&quot;</span><span class="s3">, </span><span class="s5">&quot;compiled&quot;</span><span class="s1">))</span><span class="s3">,</span>
        <span class="s1">custom_numeric(</span>
            <span class="s1">dtypes=[dtypes.bfloat16</span><span class="s3">, </span><span class="s1">np.float16]</span><span class="s3">,</span>
            <span class="s1">tol=</span><span class="s4">5e-2</span><span class="s3">,</span>
            <span class="s1">devices=(</span><span class="s5">&quot;cpu&quot;</span><span class="s3">, </span><span class="s5">&quot;gpu&quot;</span><span class="s1">)</span><span class="s3">,</span>
            <span class="s1">modes=(</span><span class="s5">&quot;eager&quot;</span><span class="s3">, </span><span class="s5">&quot;graph&quot;</span><span class="s3">, </span><span class="s5">&quot;compiled&quot;</span><span class="s1">))</span><span class="s3">,</span>
        <span class="s1">custom_numeric(</span>
            <span class="s1">custom_assert=custom_assert</span><span class="s3">,</span>
            <span class="s1">description=(</span>
                <span class="s5">&quot;May return different values in the strictly upper triangular &quot;</span>
                <span class="s5">&quot;part of the result. This does not matter for correctness, &quot;</span>
                <span class="s5">&quot;because this part of the matrix is not considered in the result.&quot;</span>
            <span class="s1">)</span><span class="s3">,</span>
            <span class="s1">modes=(</span><span class="s5">&quot;eager&quot;</span><span class="s3">, </span><span class="s5">&quot;graph&quot;</span><span class="s3">, </span><span class="s5">&quot;compiled&quot;</span><span class="s1">))</span>
    <span class="s1">]</span>

  <span class="s1">@classmethod</span>
  <span class="s3">def </span><span class="s1">conv_general_dilated(cls</span><span class="s3">, </span><span class="s1">harness: primitive_harness.Harness):</span>
    <span class="s3">return </span><span class="s1">[</span>
        <span class="s0"># Even in compiled mode, for GPU we see a bit of discrepancy but</span>
        <span class="s0"># very minor.</span>
        <span class="s1">custom_numeric(dtypes=[np.float32]</span><span class="s3">, </span><span class="s1">devices=</span><span class="s5">&quot;gpu&quot;</span><span class="s3">,</span>
                       <span class="s1">modes=(</span><span class="s5">&quot;eager&quot;</span><span class="s3">, </span><span class="s5">&quot;graph&quot;</span><span class="s3">, </span><span class="s5">&quot;compiled&quot;</span><span class="s1">)</span><span class="s3">,</span>
                       <span class="s1">tol=</span><span class="s4">1e-5</span><span class="s1">)</span><span class="s3">,</span>
        <span class="s1">custom_numeric(dtypes=[np.float32]</span><span class="s3">, </span><span class="s1">devices=</span><span class="s5">&quot;cpu&quot;</span><span class="s3">,</span>
                       <span class="s1">modes=(</span><span class="s5">&quot;eager&quot;</span><span class="s3">, </span><span class="s5">&quot;graph&quot;</span><span class="s3">, </span><span class="s5">&quot;compiled&quot;</span><span class="s1">)</span><span class="s3">,</span>
                       <span class="s1">tol=</span><span class="s4">1e-4</span><span class="s3">,</span>
                       <span class="s1">native_serialization=Jax2TfLimitation.FOR_NATIVE | Jax2TfLimitation.FOR_NON_NATIVE)</span><span class="s3">,</span>
        <span class="s1">custom_numeric(description=</span><span class="s5">&quot;higher numeric inaccuracy when `enable_xla=False`&quot;</span><span class="s3">,</span>
                       <span class="s1">modes=(</span><span class="s5">&quot;eager&quot;</span><span class="s3">, </span><span class="s5">&quot;graph&quot;</span><span class="s3">, </span><span class="s5">&quot;compiled&quot;</span><span class="s1">)</span><span class="s3">,</span>
                       <span class="s1">enabled=(</span><span class="s3">not </span><span class="s1">harness.params[</span><span class="s5">&quot;enable_xla&quot;</span><span class="s1">])</span><span class="s3">,</span>
                       <span class="s1">tol=</span><span class="s4">5e-3</span><span class="s1">)</span>
    <span class="s1">]</span>

  <span class="s1">@classmethod</span>
  <span class="s3">def </span><span class="s1">cumlogsumexp(cls</span><span class="s3">, </span><span class="s1">harness):</span>
    <span class="s3">return </span><span class="s1">[</span>
        <span class="s0"># JAX uses a different lowering for CPU and GPU.</span>
        <span class="s1">custom_numeric(</span>
            <span class="s1">dtypes=(np.float16</span><span class="s3">, </span><span class="s1">jnp.bfloat16)</span><span class="s3">,</span>
            <span class="s1">devices=(</span><span class="s5">&quot;cpu&quot;</span><span class="s3">, </span><span class="s5">&quot;gpu&quot;</span><span class="s1">)</span><span class="s3">,</span>
            <span class="s1">modes=(</span><span class="s5">&quot;eager&quot;</span><span class="s3">, </span><span class="s5">&quot;graph&quot;</span><span class="s3">, </span><span class="s5">&quot;compiled&quot;</span><span class="s1">)</span><span class="s3">,</span>
            <span class="s1">tol=</span><span class="s4">5e-1</span><span class="s1">)</span>
    <span class="s1">]</span>


  <span class="s1">@classmethod</span>
  <span class="s3">def </span><span class="s1">cumprod(cls</span><span class="s3">, </span><span class="s1">harness):</span>
    <span class="s3">return </span><span class="s1">[</span>
        <span class="s0"># JAX uses a different lowering for CPU and GPU.</span>
        <span class="s1">custom_numeric(</span>
            <span class="s1">dtypes=(np.float16</span><span class="s3">, </span><span class="s1">jnp.bfloat16)</span><span class="s3">,</span>
            <span class="s1">devices=(</span><span class="s5">&quot;cpu&quot;</span><span class="s3">, </span><span class="s5">&quot;gpu&quot;</span><span class="s1">)</span><span class="s3">,</span>
            <span class="s1">modes=(</span><span class="s5">&quot;eager&quot;</span><span class="s3">, </span><span class="s5">&quot;graph&quot;</span><span class="s3">, </span><span class="s5">&quot;compiled&quot;</span><span class="s1">)</span><span class="s3">,</span>
            <span class="s1">tol=</span><span class="s4">5e-1</span><span class="s1">)</span>
    <span class="s1">]</span>

  <span class="s1">@classmethod</span>
  <span class="s3">def </span><span class="s1">cumsum(cls</span><span class="s3">, </span><span class="s1">harness):</span>
    <span class="s3">return </span><span class="s1">[</span>
        <span class="s0"># JAX uses a different lowering for CPU and GPU.</span>
        <span class="s1">custom_numeric(</span>
            <span class="s1">dtypes=(np.float16</span><span class="s3">, </span><span class="s1">jnp.bfloat16)</span><span class="s3">,</span>
            <span class="s1">devices=(</span><span class="s5">&quot;cpu&quot;</span><span class="s3">, </span><span class="s5">&quot;gpu&quot;</span><span class="s1">)</span><span class="s3">,</span>
            <span class="s1">modes=(</span><span class="s5">&quot;eager&quot;</span><span class="s3">, </span><span class="s5">&quot;graph&quot;</span><span class="s3">, </span><span class="s5">&quot;compiled&quot;</span><span class="s1">)</span><span class="s3">,</span>
            <span class="s1">tol=</span><span class="s4">5e-1</span><span class="s1">)</span>
    <span class="s1">]</span>

  <span class="s1">@classmethod</span>
  <span class="s3">def </span><span class="s1">custom_linear_solve(cls</span><span class="s3">, </span><span class="s1">harness: primitive_harness.Harness):</span>
    <span class="s3">return </span><span class="s1">[</span>
        <span class="s1">Jax2TfLimitation(</span>
            <span class="s5">&quot;TODO: large numerical discrepancy&quot;</span><span class="s3">,</span>
            <span class="s1">dtypes=[np.float32]</span><span class="s3">,</span>
            <span class="s1">devices=</span><span class="s5">&quot;tpu&quot;</span><span class="s3">,</span>
            <span class="s1">expect_tf_error=</span><span class="s3">False,</span>
            <span class="s1">skip_comparison=</span><span class="s3">True</span><span class="s1">)</span><span class="s3">,</span>
        <span class="s1">custom_numeric(dtypes=[np.float32]</span><span class="s3">, </span><span class="s1">devices=</span><span class="s5">&quot;tpu&quot;</span><span class="s3">, </span><span class="s1">tol=</span><span class="s4">0.01</span><span class="s1">)</span><span class="s3">,</span>
        <span class="s1">custom_numeric(tol=</span><span class="s4">1e-3</span><span class="s1">)</span><span class="s3">,</span>
    <span class="s1">]</span>

  <span class="s1">@classmethod</span>
  <span class="s3">def </span><span class="s1">digamma(cls</span><span class="s3">, </span><span class="s1">harness: primitive_harness.Harness):</span>
    <span class="s1">dtype = harness.dtype</span>

    <span class="s0"># In the bfloat16 case, TF and lax both return NaN in undefined cases.</span>
    <span class="s0"># digamma is not defined at 0 and -1</span>
    <span class="s3">def </span><span class="s1">custom_assert(tst</span><span class="s3">, </span><span class="s1">result_jax</span><span class="s3">, </span><span class="s1">result_tf</span><span class="s3">, </span><span class="s1">*</span><span class="s3">, </span><span class="s1">args</span><span class="s3">, </span><span class="s1">tol</span><span class="s3">, </span><span class="s1">err_msg):</span>
      <span class="s0"># lax.digamma returns NaN and tf.math.digamma returns inf</span>
      <span class="s1">arg</span><span class="s3">, </span><span class="s1">= args</span>
      <span class="s1">special_cases = (arg == </span><span class="s4">0.</span><span class="s1">) | (arg == -</span><span class="s4">1.</span><span class="s1">)</span>
      <span class="s1">nr_special_cases = np.count_nonzero(special_cases)</span>
      <span class="s1">tst.assertAllClose(</span>
          <span class="s1">np.full((nr_special_cases</span><span class="s3">,</span><span class="s1">)</span><span class="s3">, </span><span class="s1">dtype(np.nan))</span><span class="s3">,</span>
          <span class="s1">result_jax[special_cases]</span><span class="s3">,</span>
          <span class="s1">err_msg=err_msg)</span>
      <span class="s1">tst.assertAllClose(</span>
          <span class="s1">np.full((nr_special_cases</span><span class="s3">,</span><span class="s1">)</span><span class="s3">, </span><span class="s1">dtype(np.inf))</span><span class="s3">,</span>
          <span class="s1">result_tf[special_cases]</span><span class="s3">,</span>
          <span class="s1">err_msg=err_msg)</span>
      <span class="s0"># non-special cases are equal</span>
      <span class="s1">tst.assertAllClose(</span>
          <span class="s1">result_jax[~special_cases]</span><span class="s3">,</span>
          <span class="s1">result_tf[~special_cases]</span><span class="s3">,</span>
          <span class="s1">atol=tol</span><span class="s3">,</span>
          <span class="s1">rtol=tol</span><span class="s3">,</span>
          <span class="s1">err_msg=err_msg)</span>

    <span class="s3">return </span><span class="s1">[</span>
        <span class="s1">missing_tf_kernel(</span>
            <span class="s1">dtypes=[dtypes.bfloat16]</span><span class="s3">,</span>
            <span class="s1">devices=(</span><span class="s5">&quot;cpu&quot;</span><span class="s3">, </span><span class="s5">&quot;gpu&quot;</span><span class="s1">)</span><span class="s3">,</span>
            <span class="s1">modes=(</span><span class="s5">&quot;eager&quot;</span><span class="s3">, </span><span class="s5">&quot;graph&quot;</span><span class="s1">))</span><span class="s3">,</span>
        <span class="s1">custom_numeric(dtypes=[np.float64]</span><span class="s3">, </span><span class="s1">tol=</span><span class="s4">1e-13</span><span class="s1">)</span><span class="s3">,</span>
        <span class="s1">custom_numeric(dtypes=[np.float32]</span><span class="s3">, </span><span class="s1">devices=[</span><span class="s5">&quot;cpu&quot;</span><span class="s3">, </span><span class="s5">&quot;gpu&quot;</span><span class="s1">]</span><span class="s3">, </span><span class="s1">tol=</span><span class="s4">1e-3</span><span class="s1">)</span><span class="s3">,</span>
        <span class="s1">custom_numeric(</span>
            <span class="s1">dtypes=[dtypes.bfloat16]</span><span class="s3">,</span>
            <span class="s1">custom_assert=custom_assert</span><span class="s3">,</span>
            <span class="s1">description=(</span>
                <span class="s5">&quot;May return different results at singularity points 0 and -1.&quot;</span>
                <span class="s5">&quot;JAX returns nan and TF returns inf&quot;</span><span class="s1">))</span>
    <span class="s1">]</span>

  <span class="s1">@classmethod</span>
  <span class="s3">def </span><span class="s1">div(cls</span><span class="s3">, </span><span class="s1">harness: primitive_harness.Harness):</span>
    <span class="s3">return </span><span class="s1">[</span>
        <span class="s1">Jax2TfLimitation(</span>
            <span class="s5">&quot;TF integer division fails if divisor contains 0; JAX returns NaN&quot;</span><span class="s3">,</span>
            <span class="s1">dtypes=[</span>
                <span class="s1">np.uint8</span><span class="s3">, </span><span class="s1">np.int8</span><span class="s3">, </span><span class="s1">np.uint16</span><span class="s3">, </span><span class="s1">np.uint32</span><span class="s3">, </span><span class="s1">np.uint64</span><span class="s3">, </span><span class="s1">np.int8</span><span class="s3">,</span>
                <span class="s1">np.int16</span><span class="s3">, </span><span class="s1">np.int32</span><span class="s3">, </span><span class="s1">np.int64</span>
            <span class="s1">]</span><span class="s3">,</span>
            <span class="s0"># Only the harnesses with &quot;singularity&quot; will have divide by 0</span>
            <span class="s1">enabled=(</span><span class="s5">&quot;singularity&quot; </span><span class="s3">in </span><span class="s1">harness.name))</span>
    <span class="s1">]</span>

  <span class="s1">@classmethod</span>
  <span class="s3">def </span><span class="s1">dot_general(cls</span><span class="s3">, </span><span class="s1">harness: primitive_harness.Harness):</span>
    <span class="s1">prefer_elem = harness.params[</span><span class="s5">&quot;preferred_element_type&quot;</span><span class="s1">]</span>
    <span class="s3">return </span><span class="s1">[</span>
        <span class="s1">missing_tf_kernel(dtypes=[np.bool_]</span><span class="s3">,</span><span class="s1">)</span><span class="s3">,</span>
        <span class="s0"># TODO(b/189287598)</span>
        <span class="s1">Jax2TfLimitation(</span>
            <span class="s5">&quot;Non-deterministic NaN for dot_general with preferred_element_type on GPU (b/189287598)&quot;</span><span class="s3">,</span>
            <span class="s1">dtypes=[</span>
                <span class="s1">jnp.bfloat16</span><span class="s3">, </span><span class="s1">np.float16</span><span class="s3">, </span><span class="s1">np.float32</span><span class="s3">, </span><span class="s1">np.complex64</span>
            <span class="s1">]</span><span class="s3">,</span>
            <span class="s1">devices=</span><span class="s5">&quot;gpu&quot;</span><span class="s3">,</span>
            <span class="s1">modes=(</span><span class="s5">&quot;eager&quot;</span><span class="s3">, </span><span class="s5">&quot;graph&quot;</span><span class="s3">, </span><span class="s5">&quot;compiled&quot;</span><span class="s1">)</span><span class="s3">,</span>
            <span class="s1">enabled=(prefer_elem </span><span class="s3">is not None</span><span class="s1">)</span><span class="s3">,</span>
            <span class="s1">skip_comparison=</span><span class="s3">True</span><span class="s1">)</span><span class="s3">,</span>
        <span class="s0"># TODO(b/241740367) - note this only occurs when X64 is enabled.</span>
        <span class="s1">Jax2TfLimitation(</span>
            <span class="s5">&quot;Large tolerances when upcasting with preferred_element_type on CPU (b/241740367)&quot;</span><span class="s3">,</span>
            <span class="s1">devices=[</span><span class="s5">&quot;cpu&quot;</span><span class="s3">, </span><span class="s5">&quot;gpu&quot;</span><span class="s3">, </span><span class="s5">&quot;tpu&quot;</span><span class="s1">]</span><span class="s3">,</span>
            <span class="s1">enabled=prefer_elem </span><span class="s3">and </span><span class="s1">np.dtype(harness.dtype) &lt; np.dtype(prefer_elem)</span><span class="s3">,</span>
            <span class="s1">skip_comparison=</span><span class="s3">True</span><span class="s1">)</span><span class="s3">,</span>
        <span class="s0"># JAX performs float16 matmuls in float32 on CPU, so the JAX result</span>
        <span class="s0"># may be more precise.</span>
        <span class="s1">custom_numeric(dtypes=[np.float16]</span><span class="s3">, </span><span class="s1">devices=[</span><span class="s5">&quot;cpu&quot;</span><span class="s1">]</span><span class="s3">, </span><span class="s1">tol=</span><span class="s4">1e-2</span><span class="s3">,</span>
                       <span class="s1">modes=(</span><span class="s5">&quot;eager&quot;</span><span class="s3">, </span><span class="s5">&quot;graph&quot;</span><span class="s3">, </span><span class="s5">&quot;compiled&quot;</span><span class="s1">))</span><span class="s3">,</span>
    <span class="s1">]</span>

  <span class="s1">@classmethod</span>
  <span class="s3">def </span><span class="s1">eig(cls</span><span class="s3">, </span><span class="s1">harness: primitive_harness.Harness):</span>
    <span class="s1">compute_left_eigenvectors = harness.params[</span><span class="s5">&quot;compute_left_eigenvectors&quot;</span><span class="s1">]</span>
    <span class="s1">compute_right_eigenvectors = harness.params[</span><span class="s5">&quot;compute_right_eigenvectors&quot;</span><span class="s1">]</span>
    <span class="s1">dtype = harness.dtype</span>

    <span class="s3">def </span><span class="s1">custom_assert(tst</span><span class="s3">, </span><span class="s1">result_jax</span><span class="s3">, </span><span class="s1">result_tf</span><span class="s3">, </span><span class="s1">*</span><span class="s3">, </span><span class="s1">args</span><span class="s3">, </span><span class="s1">tol</span><span class="s3">, </span><span class="s1">err_msg):</span>
      <span class="s1">operand</span><span class="s3">, </span><span class="s1">= args</span>
      <span class="s1">inner_dimension = operand.shape[-</span><span class="s4">1</span><span class="s1">]</span>

      <span class="s0"># Test ported from tests.linlag_test.testEig</span>
      <span class="s0"># Norm, adjusted for dimension and type.</span>
      <span class="s3">def </span><span class="s1">norm(x):</span>
        <span class="s1">norm = np.linalg.norm(x</span><span class="s3">, </span><span class="s1">axis=(-</span><span class="s4">2</span><span class="s3">, </span><span class="s1">-</span><span class="s4">1</span><span class="s1">))</span>
        <span class="s3">return </span><span class="s1">norm / ((inner_dimension + </span><span class="s4">1</span><span class="s1">) * jnp.finfo(dtype).eps)</span>

      <span class="s3">def </span><span class="s1">check_right_eigenvectors(a</span><span class="s3">, </span><span class="s1">w</span><span class="s3">, </span><span class="s1">vr):</span>
        <span class="s1">tst.assertTrue(</span>
            <span class="s1">np.all(norm(np.matmul(a</span><span class="s3">, </span><span class="s1">vr) - w[...</span><span class="s3">, None, </span><span class="s1">:] * vr) &lt; </span><span class="s4">100</span><span class="s1">))</span>

      <span class="s3">def </span><span class="s1">check_left_eigenvectors(a</span><span class="s3">, </span><span class="s1">w</span><span class="s3">, </span><span class="s1">vl):</span>
        <span class="s1">rank = len(a.shape)</span>
        <span class="s1">aH = jnp.conj(a.transpose(list(range(rank - </span><span class="s4">2</span><span class="s1">)) + [rank - </span><span class="s4">1</span><span class="s3">, </span><span class="s1">rank - </span><span class="s4">2</span><span class="s1">]))</span>
        <span class="s1">wC = jnp.conj(w)</span>
        <span class="s1">check_right_eigenvectors(aH</span><span class="s3">, </span><span class="s1">wC</span><span class="s3">, </span><span class="s1">vl)</span>

      <span class="s3">def </span><span class="s1">check_eigenvalue_is_in_array(eigenvalue</span><span class="s3">, </span><span class="s1">eigenvalues_array):</span>
        <span class="s1">tol = </span><span class="s3">None</span>
        <span class="s0"># TODO(bchetioui): numerical discrepancies</span>
        <span class="s3">if </span><span class="s1">dtype </span><span class="s3">in </span><span class="s1">[np.float32</span><span class="s3">, </span><span class="s1">np.complex64]:</span>
          <span class="s1">tol = </span><span class="s4">1e-4</span>
        <span class="s3">elif </span><span class="s1">dtype </span><span class="s3">in </span><span class="s1">[np.float64</span><span class="s3">, </span><span class="s1">np.complex128]:</span>
          <span class="s1">tol = </span><span class="s4">1e-13</span>
        <span class="s1">closest_diff = min(abs(eigenvalues_array - eigenvalue))</span>
        <span class="s1">tst.assertAllClose(</span>
            <span class="s1">closest_diff</span><span class="s3">,</span>
            <span class="s1">np.array(</span><span class="s4">0.</span><span class="s3">, </span><span class="s1">closest_diff.dtype)</span><span class="s3">,</span>
            <span class="s1">atol=tol</span><span class="s3">,</span>
            <span class="s1">err_msg=err_msg)</span>

      <span class="s1">all_w_jax</span><span class="s3">, </span><span class="s1">all_w_tf = result_jax[</span><span class="s4">0</span><span class="s1">]</span><span class="s3">, </span><span class="s1">result_tf[</span><span class="s4">0</span><span class="s1">]</span>
      <span class="s3">for </span><span class="s1">idx </span><span class="s3">in </span><span class="s1">itertools.product(*map(range</span><span class="s3">, </span><span class="s1">operand.shape[:-</span><span class="s4">2</span><span class="s1">])):</span>
        <span class="s1">w_jax</span><span class="s3">, </span><span class="s1">w_tf = all_w_jax[idx]</span><span class="s3">, </span><span class="s1">all_w_tf[idx]</span>
        <span class="s3">for </span><span class="s1">i </span><span class="s3">in </span><span class="s1">range(inner_dimension):</span>
          <span class="s1">check_eigenvalue_is_in_array(w_jax[i]</span><span class="s3">, </span><span class="s1">w_tf)</span>
          <span class="s1">check_eigenvalue_is_in_array(w_tf[i]</span><span class="s3">, </span><span class="s1">w_jax)</span>

      <span class="s3">if </span><span class="s1">compute_left_eigenvectors:</span>
        <span class="s1">check_left_eigenvectors(operand</span><span class="s3">, </span><span class="s1">all_w_tf</span><span class="s3">, </span><span class="s1">result_tf[</span><span class="s4">1</span><span class="s1">])</span>
      <span class="s3">if </span><span class="s1">compute_right_eigenvectors:</span>
        <span class="s1">check_right_eigenvectors(operand</span><span class="s3">, </span><span class="s1">all_w_tf</span><span class="s3">,</span>
                                 <span class="s1">result_tf[</span><span class="s4">1 </span><span class="s1">+ compute_left_eigenvectors])</span>

    <span class="s3">return </span><span class="s1">[</span>
        <span class="s0"># Eig does not work in JAX on gpu or tpu</span>
        <span class="s1">Jax2TfLimitation(</span>
            <span class="s5">&quot;function not compilable&quot;</span><span class="s3">, </span><span class="s1">modes=</span><span class="s5">&quot;compiled&quot;</span><span class="s3">, </span><span class="s1">devices=</span><span class="s5">&quot;cpu&quot;</span><span class="s1">)</span><span class="s3">,</span>
        <span class="s1">Jax2TfLimitation(</span>
            <span class="s5">&quot;TF Conversion of eig is not implemented when both compute_left_eigenvectors and compute_right_eigenvectors are set to True&quot;</span><span class="s3">,</span>
            <span class="s1">enabled=(compute_left_eigenvectors </span><span class="s3">and </span><span class="s1">compute_right_eigenvectors))</span><span class="s3">,</span>
        <span class="s1">custom_numeric(</span>
            <span class="s1">custom_assert=custom_assert</span><span class="s3">,</span>
            <span class="s1">description=(</span><span class="s5">&quot;May return the eigenvalues and eigenvectors in a &quot;</span>
                         <span class="s5">&quot;potentially different order. The eigenvectors may &quot;</span>
                         <span class="s5">&quot;also be different, but equally valid.&quot;</span><span class="s1">))</span>
    <span class="s1">]</span>

  <span class="s1">@classmethod</span>
  <span class="s3">def </span><span class="s1">eigh(cls</span><span class="s3">, </span><span class="s1">harness: primitive_harness.Harness):</span>
    <span class="s1">dtype = harness.dtype</span>

    <span class="s3">def </span><span class="s1">custom_assert(tst</span><span class="s3">, </span><span class="s1">result_jax</span><span class="s3">, </span><span class="s1">result_tf</span><span class="s3">, </span><span class="s1">*</span><span class="s3">, </span><span class="s1">args</span><span class="s3">, </span><span class="s1">tol</span><span class="s3">, </span><span class="s1">err_msg):</span>
      <span class="s1">operand</span><span class="s3">, </span><span class="s1">= args</span>
      <span class="s1">inner_dimension = operand.shape[-</span><span class="s4">1</span><span class="s1">]</span>

      <span class="s3">def </span><span class="s1">check_right_eigenvectors(a</span><span class="s3">, </span><span class="s1">w</span><span class="s3">, </span><span class="s1">vr):</span>
        <span class="s1">tol = </span><span class="s4">1e-16</span>
        <span class="s0"># TODO(bchetioui): tolerance needs to be very high in compiled mode,</span>
        <span class="s0"># specifically for eigenvectors.</span>
        <span class="s3">if </span><span class="s1">dtype == np.float64:</span>
          <span class="s1">tol = </span><span class="s4">2e-5</span>
        <span class="s3">elif </span><span class="s1">dtype == np.float32:</span>
          <span class="s1">tol = </span><span class="s4">1e-2</span>
        <span class="s3">elif </span><span class="s1">dtype </span><span class="s3">in </span><span class="s1">[dtypes.bfloat16</span><span class="s3">, </span><span class="s1">np.complex64]:</span>
          <span class="s1">tol = </span><span class="s4">1e-3</span>
        <span class="s3">elif </span><span class="s1">dtype == np.complex128:</span>
          <span class="s1">tol = </span><span class="s4">2e-5</span>
        <span class="s1">tst.assertAllClose(</span>
            <span class="s1">np.matmul(a</span><span class="s3">, </span><span class="s1">vr) - w[...</span><span class="s3">, None, </span><span class="s1">:] * vr</span><span class="s3">,</span>
            <span class="s1">np.zeros(a.shape</span><span class="s3">, </span><span class="s1">dtype=vr.dtype)</span><span class="s3">,</span>
            <span class="s1">atol=tol</span><span class="s3">,</span>
            <span class="s0"># For bfloat16 the np.matmul returns float32 result.</span>
            <span class="s1">check_dtypes=</span><span class="s3">False,</span>
            <span class="s1">err_msg=err_msg)</span>

      <span class="s3">def </span><span class="s1">check_eigenvalue_is_in_array(eigenvalue</span><span class="s3">, </span><span class="s1">eigenvalues_array):</span>
        <span class="s1">tol = </span><span class="s3">None</span>
        <span class="s3">if </span><span class="s1">dtype </span><span class="s3">in </span><span class="s1">[dtypes.bfloat16</span><span class="s3">, </span><span class="s1">np.float32</span><span class="s3">, </span><span class="s1">np.complex64]:</span>
          <span class="s1">tol = </span><span class="s4">1e-3</span>
        <span class="s3">elif </span><span class="s1">dtype </span><span class="s3">in </span><span class="s1">[np.float64</span><span class="s3">, </span><span class="s1">np.complex128]:</span>
          <span class="s1">tol = </span><span class="s4">1e-5</span>
        <span class="s1">closest_diff = min(abs(eigenvalues_array - eigenvalue))</span>
        <span class="s1">tst.assertAllClose(</span>
            <span class="s1">closest_diff</span><span class="s3">,</span>
            <span class="s1">np.array(</span><span class="s4">0.</span><span class="s3">, </span><span class="s1">closest_diff.dtype)</span><span class="s3">,</span>
            <span class="s1">atol=tol</span><span class="s3">,</span>
            <span class="s1">err_msg=err_msg)</span>

      <span class="s1">_</span><span class="s3">, </span><span class="s1">all_w_jax = result_jax</span>
      <span class="s1">all_vr_tf</span><span class="s3">, </span><span class="s1">all_w_tf = result_tf</span>

      <span class="s3">for </span><span class="s1">idx </span><span class="s3">in </span><span class="s1">itertools.product(*map(range</span><span class="s3">, </span><span class="s1">operand.shape[:-</span><span class="s4">2</span><span class="s1">])):</span>
        <span class="s1">w_jax</span><span class="s3">, </span><span class="s1">w_tf = all_w_jax[idx]</span><span class="s3">, </span><span class="s1">all_w_tf[idx]</span>
        <span class="s3">for </span><span class="s1">i </span><span class="s3">in </span><span class="s1">range(inner_dimension):</span>
          <span class="s1">check_eigenvalue_is_in_array(w_jax[i]</span><span class="s3">, </span><span class="s1">w_tf)</span>
          <span class="s1">check_eigenvalue_is_in_array(w_tf[i]</span><span class="s3">, </span><span class="s1">w_jax)</span>

      <span class="s1">check_right_eigenvectors(operand</span><span class="s3">, </span><span class="s1">all_w_tf</span><span class="s3">, </span><span class="s1">all_vr_tf)</span>

    <span class="s3">return </span><span class="s1">[</span>
        <span class="s1">missing_tf_kernel(</span>
            <span class="s1">dtypes=[dtypes.bfloat16]</span><span class="s3">,</span>
            <span class="s1">devices=</span><span class="s5">&quot;tpu&quot;</span><span class="s3">,</span>
            <span class="s1">enabled=(harness.params[</span><span class="s5">&quot;shape&quot;</span><span class="s1">] != (</span><span class="s4">0</span><span class="s3">, </span><span class="s4">0</span><span class="s1">))</span><span class="s3">,  </span><span class="s0"># This actually works!</span>
        <span class="s1">)</span><span class="s3">,</span>
        <span class="s1">Jax2TfLimitation(</span>
            <span class="s5">&quot;TODO: numeric discrepancies&quot;</span><span class="s3">,</span>
            <span class="s1">dtypes=[np.float16]</span><span class="s3">,</span>
            <span class="s1">devices=</span><span class="s5">&quot;tpu&quot;</span><span class="s3">,</span>
            <span class="s1">expect_tf_error=</span><span class="s3">False,</span>
            <span class="s1">skip_comparison=</span><span class="s3">True</span><span class="s1">)</span><span class="s3">,</span>
        <span class="s1">custom_numeric(</span>
            <span class="s1">custom_assert=custom_assert</span><span class="s3">,</span>
            <span class="s1">description=(</span><span class="s5">&quot;May return the eigenvalues and eigenvectors in a &quot;</span>
                         <span class="s5">&quot;potentially different order. The eigenvectors may &quot;</span>
                         <span class="s5">&quot;also be different, but equally valid.&quot;</span><span class="s1">)</span><span class="s3">,</span>
            <span class="s1">modes=(</span><span class="s5">&quot;eager&quot;</span><span class="s3">, </span><span class="s5">&quot;graph&quot;</span><span class="s3">, </span><span class="s5">&quot;compiled&quot;</span><span class="s1">))</span>
    <span class="s1">]</span>

  <span class="s1">@classmethod</span>
  <span class="s3">def </span><span class="s1">erf(cls</span><span class="s3">, </span><span class="s1">harness: primitive_harness.Harness):</span>
    <span class="s3">return </span><span class="s1">[]</span>

  <span class="s1">@classmethod</span>
  <span class="s3">def </span><span class="s1">erfc(cls</span><span class="s3">, </span><span class="s1">harness: primitive_harness.Harness):</span>
    <span class="s3">return </span><span class="s1">[]</span>

  <span class="s1">@classmethod</span>
  <span class="s3">def </span><span class="s1">erf_inv(cls</span><span class="s3">, </span><span class="s1">harness: primitive_harness.Harness):</span>
    <span class="s0"># erf_inv is not defined for arg &lt;= -1 or arg &gt;= 1</span>
    <span class="s3">def </span><span class="s1">custom_assert(tst</span><span class="s3">, </span><span class="s1">result_jax</span><span class="s3">, </span><span class="s1">result_tf</span><span class="s3">, </span><span class="s1">*</span><span class="s3">, </span><span class="s1">args</span><span class="s3">, </span><span class="s1">tol</span><span class="s3">,</span>
                      <span class="s1">err_msg):  </span><span class="s0"># noqa: F811</span>
      <span class="s1">arg</span><span class="s3">, </span><span class="s1">= args</span>
      <span class="s0"># for arg &lt; -1 or arg &gt; 1</span>
      <span class="s0"># lax.erf_inv returns NaN; tf.math.erf_inv return +/- inf</span>
      <span class="s1">special_cases = (arg &lt; -</span><span class="s4">1.</span><span class="s1">) | (arg &gt; </span><span class="s4">1.</span><span class="s1">)</span>
      <span class="s0"># non-special cases are equal</span>
      <span class="s1">tst.assertAllClose(</span>
          <span class="s1">result_jax[~special_cases]</span><span class="s3">,</span>
          <span class="s1">result_tf[~special_cases]</span><span class="s3">,</span>
          <span class="s1">atol=tol</span><span class="s3">,</span>
          <span class="s1">rtol=tol</span><span class="s3">,</span>
          <span class="s1">err_msg=err_msg)</span>

    <span class="s3">return </span><span class="s1">[</span>
        <span class="s1">missing_tf_kernel(</span>
            <span class="s1">dtypes=[dtypes.bfloat16</span><span class="s3">, </span><span class="s1">np.float16]</span><span class="s3">,</span>
            <span class="s1">devices=(</span><span class="s5">&quot;cpu&quot;</span><span class="s3">, </span><span class="s5">&quot;gpu&quot;</span><span class="s1">)</span><span class="s3">,</span>
            <span class="s1">modes=(</span><span class="s5">&quot;eager&quot;</span><span class="s3">, </span><span class="s5">&quot;graph&quot;</span><span class="s1">))</span><span class="s3">,</span>
        <span class="s1">custom_numeric(dtypes=[np.float32</span><span class="s3">, </span><span class="s1">np.float64]</span><span class="s3">, </span><span class="s1">tol=</span><span class="s4">1e-4</span><span class="s1">)</span><span class="s3">,</span>
        <span class="s1">custom_numeric(</span>
            <span class="s1">dtypes=[np.float32</span><span class="s3">, </span><span class="s1">np.float64]</span><span class="s3">,</span>
            <span class="s1">custom_assert=custom_assert</span><span class="s3">,</span>
            <span class="s1">description=(</span>
                <span class="s5">&quot;May return different results at undefined points (&lt; -1 or &gt; 1):&quot;</span>
                <span class="s5">&quot; JAX returns `NaN` and TF returns `+inf` or `-inf`.&quot;</span><span class="s1">))</span><span class="s3">,</span>
    <span class="s1">]</span>

  <span class="s1">@classmethod</span>
  <span class="s3">def </span><span class="s1">expm1(cls</span><span class="s3">, </span><span class="s1">harness: primitive_harness.Harness):</span>
    <span class="s3">return </span><span class="s1">[custom_numeric(dtypes=[np.float64]</span><span class="s3">, </span><span class="s1">tol=</span><span class="s4">1e-5</span><span class="s1">)]</span>

  <span class="s1">@classmethod</span>
  <span class="s3">def </span><span class="s1">fft(cls</span><span class="s3">, </span><span class="s1">harness):</span>
    <span class="s3">return </span><span class="s1">[</span>
        <span class="s1">Jax2TfLimitation(</span>
            <span class="s5">&quot;TF function not compilableble&quot;</span><span class="s3">,</span>
            <span class="s1">devices=(</span><span class="s5">&quot;cpu&quot;</span><span class="s3">, </span><span class="s5">&quot;gpu&quot;</span><span class="s1">)</span><span class="s3">,</span>
            <span class="s1">dtypes=[np.float64]</span><span class="s3">,</span>
            <span class="s1">modes=</span><span class="s5">&quot;compiled&quot;</span><span class="s1">)</span><span class="s3">,</span>
        <span class="s1">Jax2TfLimitation(</span>
            <span class="s5">&quot;TF function not compilableble for IFFT and IRFFT&quot;</span><span class="s3">,</span>
            <span class="s1">devices=(</span><span class="s5">&quot;cpu&quot;</span><span class="s3">, </span><span class="s5">&quot;gpu&quot;</span><span class="s1">)</span><span class="s3">,</span>
            <span class="s1">dtypes=[np.complex128]</span><span class="s3">,</span>
            <span class="s1">modes=</span><span class="s5">&quot;compiled&quot;</span><span class="s3">,</span>
            <span class="s1">enabled=(str(harness.params[</span><span class="s5">&quot;fft_type&quot;</span><span class="s1">]) </span><span class="s3">in </span><span class="s1">[</span><span class="s5">&quot;FftType.IFFT&quot;</span><span class="s3">,</span>
                                                         <span class="s5">&quot;FftType.IRFFT&quot;</span><span class="s1">]))</span><span class="s3">,</span>
        <span class="s0"># TODO: very high tolerance</span>
        <span class="s1">custom_numeric(tol=</span><span class="s4">1e-3</span><span class="s3">, </span><span class="s1">modes=(</span><span class="s5">&quot;eager&quot;</span><span class="s3">, </span><span class="s5">&quot;graph&quot;</span><span class="s3">, </span><span class="s5">&quot;compiled&quot;</span><span class="s1">))</span><span class="s3">,</span>
    <span class="s1">]</span>

  <span class="s1">@classmethod</span>
  <span class="s3">def </span><span class="s1">_pow_test_util(cls</span><span class="s3">, </span><span class="s1">harness: primitive_harness.Harness):</span>

    <span class="s3">def </span><span class="s1">custom_assert(tst</span><span class="s3">, </span><span class="s1">result_jax</span><span class="s3">, </span><span class="s1">result_tf</span><span class="s3">, </span><span class="s1">*</span><span class="s3">, </span><span class="s1">args</span><span class="s3">, </span><span class="s1">tol</span><span class="s3">, </span><span class="s1">err_msg):</span>
      <span class="s0"># NaNs are mismatched, but assertAllClose will also behave weirdly for</span>
      <span class="s0"># complex numbers containing np.inf as one of their components. See</span>
      <span class="s0"># https://github.com/numpy/numpy/issues/15959 for more details.</span>
      <span class="s1">mask = (</span>
          <span class="s1">np.isnan(result_jax) + np.isnan(result_tf) + np.isinf(result_jax) +</span>
          <span class="s1">np.isinf(result_tf))</span>
      <span class="s1">tst.assertAllClose(</span>
          <span class="s1">result_jax[~mask]</span><span class="s3">, </span><span class="s1">result_tf[~mask]</span><span class="s3">, </span><span class="s1">rtol=tol</span><span class="s3">, </span><span class="s1">err_msg=err_msg)</span>

    <span class="s3">return </span><span class="s1">[</span>
        <span class="s1">custom_numeric(</span>
            <span class="s1">dtypes=[np.float32</span><span class="s3">, </span><span class="s1">np.complex64]</span><span class="s3">, </span><span class="s1">devices=(</span><span class="s5">&quot;cpu&quot;</span><span class="s3">, </span><span class="s5">&quot;gpu&quot;</span><span class="s1">)</span><span class="s3">,</span>
            <span class="s1">tol=</span><span class="s4">1e-3</span><span class="s1">)</span><span class="s3">,</span>
        <span class="s1">custom_numeric(</span>
            <span class="s1">dtypes=[np.float64</span><span class="s3">, </span><span class="s1">np.complex128]</span><span class="s3">,</span>
            <span class="s1">devices=(</span><span class="s5">&quot;cpu&quot;</span><span class="s3">, </span><span class="s5">&quot;gpu&quot;</span><span class="s1">)</span><span class="s3">,</span>
            <span class="s1">tol=</span><span class="s4">5e-5</span><span class="s1">)</span><span class="s3">,</span>
        <span class="s1">custom_numeric(</span>
            <span class="s1">dtypes=[np.complex64</span><span class="s3">, </span><span class="s1">np.complex128]</span><span class="s3">,</span>
            <span class="s1">custom_assert=custom_assert</span><span class="s3">,</span>
        <span class="s1">)</span>
    <span class="s1">]</span>

  <span class="s1">@classmethod</span>
  <span class="s3">def </span><span class="s1">igamma(cls</span><span class="s3">, </span><span class="s1">harness: primitive_harness.Harness):</span>
    <span class="s1">dtype = harness.dtype</span>

    <span class="s0"># igamma is not defined when the first argument is &lt;=0</span>
    <span class="s3">def </span><span class="s1">custom_assert(tst</span><span class="s3">, </span><span class="s1">result_jax</span><span class="s3">, </span><span class="s1">result_tf</span><span class="s3">, </span><span class="s1">*</span><span class="s3">, </span><span class="s1">args</span><span class="s3">, </span><span class="s1">tol</span><span class="s3">, </span><span class="s1">err_msg):</span>
      <span class="s1">arg1</span><span class="s3">, </span><span class="s1">arg2 = args</span>
      <span class="s0"># lax.igamma returns NaN when arg1 == arg2 == 0; tf.math.igamma returns 0</span>
      <span class="s1">special_cases = (arg1 == </span><span class="s4">0.</span><span class="s1">) &amp; (arg2 == </span><span class="s4">0.</span><span class="s1">)</span>
      <span class="s1">nr_special_cases = np.count_nonzero(special_cases)</span>
      <span class="s1">tst.assertAllClose(</span>
          <span class="s1">np.full((nr_special_cases</span><span class="s3">,</span><span class="s1">)</span><span class="s3">, </span><span class="s1">np.nan</span><span class="s3">, </span><span class="s1">dtype=dtype)</span><span class="s3">,</span>
          <span class="s1">result_jax[special_cases])</span>
      <span class="s1">tst.assertAllClose(</span>
          <span class="s1">np.full((nr_special_cases</span><span class="s3">,</span><span class="s1">)</span><span class="s3">, </span><span class="s4">0.</span><span class="s3">, </span><span class="s1">dtype=dtype)</span><span class="s3">,</span>
          <span class="s1">result_tf[special_cases])</span>
      <span class="s0"># non-special cases are equal</span>
      <span class="s1">tst.assertAllClose(</span>
          <span class="s1">result_jax[~special_cases]</span><span class="s3">,</span>
          <span class="s1">result_tf[~special_cases]</span><span class="s3">,</span>
          <span class="s1">atol=tol</span><span class="s3">,</span>
          <span class="s1">rtol=tol</span><span class="s3">,</span>
          <span class="s1">err_msg=err_msg)</span>

    <span class="s3">return </span><span class="s1">[</span>
        <span class="s1">missing_tf_kernel(</span>
            <span class="s1">dtypes=[dtypes.bfloat16</span><span class="s3">, </span><span class="s1">np.float16]</span><span class="s3">,</span>
            <span class="s1">devices=(</span><span class="s5">&quot;cpu&quot;</span><span class="s3">, </span><span class="s5">&quot;gpu&quot;</span><span class="s1">)</span><span class="s3">,</span>
            <span class="s1">modes=(</span><span class="s5">&quot;eager&quot;</span><span class="s3">, </span><span class="s5">&quot;graph&quot;</span><span class="s1">))</span><span class="s3">,</span>
        <span class="s1">custom_numeric(</span>
            <span class="s1">custom_assert=custom_assert</span><span class="s3">,</span>
            <span class="s1">description=(</span>
                <span class="s5">&quot;May return different results at undefined points &quot;</span>
                <span class="s5">&quot;(both arguments 0). JAX returns `NaN` and TF returns 0 or &quot;</span>
                <span class="s5">&quot;JAX returns 1 and TF returns `NaN`&quot;</span><span class="s1">))</span>
    <span class="s1">]</span>

  <span class="s1">@classmethod</span>
  <span class="s3">def </span><span class="s1">igammac(cls</span><span class="s3">, </span><span class="s1">harness: primitive_harness.Harness):</span>
    <span class="s1">dtype = harness.dtype</span>

    <span class="s0"># igammac is not defined when the first argument is &lt;=0</span>
    <span class="s3">def </span><span class="s1">custom_assert(tst</span><span class="s3">, </span><span class="s1">result_jax</span><span class="s3">, </span><span class="s1">result_tf</span><span class="s3">, </span><span class="s1">*</span><span class="s3">, </span><span class="s1">args</span><span class="s3">, </span><span class="s1">tol</span><span class="s3">,</span>
                      <span class="s1">err_msg):  </span><span class="s0"># noqa: F811</span>
      <span class="s1">arg1</span><span class="s3">, </span><span class="s1">arg2 = args</span>
      <span class="s0"># lax.igammac returns 1. when arg1 &lt;= 0; tf.math.igammac returns NaN</span>
      <span class="s1">special_cases = (arg1 &lt;= </span><span class="s4">0.</span><span class="s1">) | (arg2 &lt;= </span><span class="s4">0</span><span class="s1">)</span>
      <span class="s1">nr_special_cases = np.count_nonzero(special_cases)</span>
      <span class="s1">tst.assertAllClose(</span>
          <span class="s1">np.full((nr_special_cases</span><span class="s3">,</span><span class="s1">)</span><span class="s3">, </span><span class="s4">1.</span><span class="s3">, </span><span class="s1">dtype=dtype)</span><span class="s3">,</span>
          <span class="s1">result_jax[special_cases]</span><span class="s3">,</span>
          <span class="s1">err_msg=err_msg)</span>
      <span class="s1">tst.assertAllClose(</span>
          <span class="s1">np.full((nr_special_cases</span><span class="s3">,</span><span class="s1">)</span><span class="s3">, </span><span class="s1">np.nan</span><span class="s3">, </span><span class="s1">dtype=dtype)</span><span class="s3">,</span>
          <span class="s1">result_tf[special_cases]</span><span class="s3">,</span>
          <span class="s1">err_msg=err_msg)</span>
      <span class="s0"># non-special cases are equal</span>
      <span class="s1">tst.assertAllClose(</span>
          <span class="s1">result_jax[~special_cases]</span><span class="s3">,</span>
          <span class="s1">result_tf[~special_cases]</span><span class="s3">,</span>
          <span class="s1">atol=tol</span><span class="s3">,</span>
          <span class="s1">rtol=tol</span><span class="s3">,</span>
          <span class="s1">err_msg=err_msg)</span>

    <span class="s3">return </span><span class="s1">[</span>
        <span class="s1">missing_tf_kernel(</span>
            <span class="s1">dtypes=[dtypes.bfloat16</span><span class="s3">, </span><span class="s1">np.float16]</span><span class="s3">,</span>
            <span class="s1">devices=(</span><span class="s5">&quot;cpu&quot;</span><span class="s3">, </span><span class="s5">&quot;gpu&quot;</span><span class="s1">)</span><span class="s3">,</span>
            <span class="s1">modes=(</span><span class="s5">&quot;eager&quot;</span><span class="s3">, </span><span class="s5">&quot;graph&quot;</span><span class="s1">))</span><span class="s3">,</span>
        <span class="s1">custom_numeric(dtypes=[np.float64]</span><span class="s3">, </span><span class="s1">tol=</span><span class="s4">1e-9</span><span class="s1">)</span><span class="s3">,</span>
        <span class="s1">custom_numeric(devices=</span><span class="s5">&quot;gpu&quot;</span><span class="s3">, </span><span class="s1">tol=</span><span class="s4">1e-3</span><span class="s1">)</span><span class="s3">,</span>
        <span class="s1">custom_numeric(</span>
            <span class="s1">custom_assert=custom_assert</span><span class="s3">,</span>
            <span class="s1">devices=(</span><span class="s5">&quot;cpu&quot;</span><span class="s3">, </span><span class="s5">&quot;gpu&quot;</span><span class="s1">)</span><span class="s3">,</span>
            <span class="s1">description=(</span>
                <span class="s5">&quot;May return different results at undefined points &quot;</span>
                <span class="s5">&quot;(both arguments less or equal 0). JAX returns `NaN` and TF returns 0 or &quot;</span>
                <span class="s5">&quot;JAX returns 1 and TF returns `NaN`&quot;</span><span class="s1">))</span><span class="s3">,</span>
    <span class="s1">]</span>

  <span class="s1">@classmethod</span>
  <span class="s3">def </span><span class="s1">integer_pow(cls</span><span class="s3">, </span><span class="s1">harness: primitive_harness.Harness):</span>
    <span class="s1">y = harness.params[</span><span class="s5">&quot;y&quot;</span><span class="s1">]</span>
    <span class="s3">return </span><span class="s1">[</span>
        <span class="s0"># TODO: on TPU, for f16, we get different results with eager mode</span>
        <span class="s0"># than with compiled mode.</span>
        <span class="s1">Jax2TfLimitation(</span>
            <span class="s5">&quot;Different overflow behavior. &quot;</span><span class="s3">,</span>
            <span class="s1">dtypes=[np.float16</span><span class="s3">, </span><span class="s1">jnp.bfloat16]</span><span class="s3">,</span>
            <span class="s1">devices=</span><span class="s5">&quot;tpu&quot;</span><span class="s3">,</span>
            <span class="s1">expect_tf_error=</span><span class="s3">False,</span>
            <span class="s1">modes=(</span><span class="s5">&quot;eager&quot;</span><span class="s3">, </span><span class="s5">&quot;graph&quot;</span><span class="s1">)</span><span class="s3">,</span>
            <span class="s1">skip_comparison=</span><span class="s3">True</span><span class="s1">)</span><span class="s3">,</span>
        <span class="s1">Jax2TfLimitation(</span>
            <span class="s5">&quot;Different overflow behavior for large exponents. &quot;</span><span class="s3">,</span>
            <span class="s1">dtypes=[</span>
                <span class="s1">np.int8</span><span class="s3">, </span><span class="s1">np.int16</span><span class="s3">, </span><span class="s1">np.int32</span><span class="s3">, </span><span class="s1">np.int64</span><span class="s3">, </span><span class="s1">np.float16</span><span class="s3">, </span><span class="s1">jnp.bfloat16</span><span class="s3">,</span>
                <span class="s1">np.float32</span><span class="s3">, </span><span class="s1">np.complex64</span><span class="s3">, </span><span class="s1">np.complex128</span>
            <span class="s1">]</span><span class="s3">,</span>
            <span class="s1">enabled=(abs(y) &gt; </span><span class="s4">10</span><span class="s1">)</span><span class="s3">,</span>
            <span class="s1">expect_tf_error=</span><span class="s3">False,</span>
            <span class="s1">modes=(</span><span class="s5">&quot;eager&quot;</span><span class="s3">, </span><span class="s5">&quot;graph&quot;</span><span class="s1">)</span><span class="s3">,</span>
            <span class="s1">skip_comparison=</span><span class="s3">True</span><span class="s1">)</span><span class="s3">,</span>
        <span class="s1">custom_numeric(dtypes=[dtypes.bfloat16]</span><span class="s3">, </span><span class="s1">tol=</span><span class="s4">2e-2</span><span class="s1">)</span>
    <span class="s1">] + list(cls._pow_test_util(harness))</span>

  <span class="s1">@classmethod</span>
  <span class="s3">def </span><span class="s1">pow(cls</span><span class="s3">, </span><span class="s1">harness: primitive_harness.Harness):</span>
    <span class="s3">return </span><span class="s1">cls._pow_test_util(harness)</span>

  <span class="s1">@classmethod</span>
  <span class="s3">def </span><span class="s1">lgamma(cls</span><span class="s3">, </span><span class="s1">harness: primitive_harness.Harness):</span>
    <span class="s3">return </span><span class="s1">[</span>
        <span class="s1">missing_tf_kernel(</span>
            <span class="s1">dtypes=[dtypes.bfloat16]</span><span class="s3">,</span>
            <span class="s1">devices=(</span><span class="s5">&quot;cpu&quot;</span><span class="s3">, </span><span class="s5">&quot;gpu&quot;</span><span class="s1">)</span><span class="s3">,</span>
            <span class="s1">modes=(</span><span class="s5">&quot;eager&quot;</span><span class="s3">, </span><span class="s5">&quot;graph&quot;</span><span class="s1">))</span><span class="s3">,</span>
        <span class="s1">custom_numeric(dtypes=[np.float64]</span><span class="s3">, </span><span class="s1">tol=</span><span class="s4">1e-11</span><span class="s1">)</span><span class="s3">,</span>
        <span class="s1">custom_numeric(dtypes=[np.float32]</span><span class="s3">, </span><span class="s1">tol=</span><span class="s4">1e-3</span><span class="s1">)</span>
    <span class="s1">]</span>

  <span class="s1">@classmethod</span>
  <span class="s3">def </span><span class="s1">log1p(cls</span><span class="s3">, </span><span class="s1">harness: primitive_harness.Harness):</span>
    <span class="s3">return </span><span class="s1">[</span>
        <span class="s1">custom_numeric(dtypes=[np.complex128]</span><span class="s3">, </span><span class="s1">tol=</span><span class="s4">3e-14</span><span class="s1">)</span><span class="s3">,</span>
        <span class="s1">custom_numeric(dtypes=[np.float64]</span><span class="s3">, </span><span class="s1">tol=</span><span class="s4">1e-10</span><span class="s1">)</span><span class="s3">,</span>
        <span class="s1">custom_numeric(dtypes=[np.float32]</span><span class="s3">, </span><span class="s1">tol=</span><span class="s4">1e-3</span><span class="s1">)</span>
    <span class="s1">]</span>

  <span class="s1">@classmethod</span>
  <span class="s3">def </span><span class="s1">lu(cls</span><span class="s3">, </span><span class="s1">harness: primitive_harness.Harness):</span>
    <span class="s1">dtype = harness.dtype</span>

    <span class="s3">def </span><span class="s1">custom_assert(tst</span><span class="s3">, </span><span class="s1">result_jax</span><span class="s3">, </span><span class="s1">result_tf</span><span class="s3">, </span><span class="s1">*</span><span class="s3">, </span><span class="s1">args</span><span class="s3">, </span><span class="s1">tol</span><span class="s3">, </span><span class="s1">err_msg):</span>
      <span class="s1">operand</span><span class="s3">, </span><span class="s1">= args</span>
      <span class="s1">lu</span><span class="s3">, </span><span class="s1">pivots</span><span class="s3">, </span><span class="s1">perm = result_tf</span>
      <span class="s1">batch_dims = operand.shape[:-</span><span class="s4">2</span><span class="s1">]</span>
      <span class="s1">m</span><span class="s3">, </span><span class="s1">n = operand.shape[-</span><span class="s4">2</span><span class="s1">]</span><span class="s3">, </span><span class="s1">operand.shape[-</span><span class="s4">1</span><span class="s1">]</span>

      <span class="s3">def </span><span class="s1">_make_permutation_matrix(perm):</span>
        <span class="s1">result = []</span>
        <span class="s3">for </span><span class="s1">idx </span><span class="s3">in </span><span class="s1">itertools.product(*map(range</span><span class="s3">, </span><span class="s1">operand.shape[:-</span><span class="s4">1</span><span class="s1">])):</span>
          <span class="s1">result += [</span><span class="s4">0 </span><span class="s3">if </span><span class="s1">c != perm[idx] </span><span class="s3">else </span><span class="s4">1 </span><span class="s3">for </span><span class="s1">c </span><span class="s3">in </span><span class="s1">range(m)]</span>
        <span class="s1">result = np.reshape(np.array(result</span><span class="s3">, </span><span class="s1">dtype=dtype)</span><span class="s3">, </span><span class="s1">[*batch_dims</span><span class="s3">, </span><span class="s1">m</span><span class="s3">, </span><span class="s1">m])</span>
        <span class="s3">return </span><span class="s1">result</span>

      <span class="s1">k = min(m</span><span class="s3">, </span><span class="s1">n)</span>
      <span class="s1">l = jnp.tril(lu</span><span class="s3">, </span><span class="s1">-</span><span class="s4">1</span><span class="s1">)[...</span><span class="s3">, </span><span class="s1">:</span><span class="s3">, </span><span class="s1">:k] + jnp.eye(m</span><span class="s3">, </span><span class="s1">k</span><span class="s3">, </span><span class="s1">dtype=dtype)</span>
      <span class="s1">u = jnp.triu(lu)[...</span><span class="s3">, </span><span class="s1">:k</span><span class="s3">, </span><span class="s1">:]</span>
      <span class="s1">p_mat = _make_permutation_matrix(perm)</span>

      <span class="s1">tst.assertArraysEqual(</span>
          <span class="s1">lax.linalg.lu_pivots_to_permutation(pivots</span><span class="s3">, </span><span class="s1">m)</span><span class="s3">, </span><span class="s1">perm)</span>
      <span class="s1">tst.assertAllClose(</span>
          <span class="s1">jnp.matmul(p_mat</span><span class="s3">, </span><span class="s1">operand)</span><span class="s3">,</span>
          <span class="s1">jnp.matmul(l</span><span class="s3">, </span><span class="s1">u)</span><span class="s3">,</span>
          <span class="s1">atol=tol</span><span class="s3">,</span>
          <span class="s1">rtol=tol</span><span class="s3">,</span>
          <span class="s1">err_msg=err_msg)</span>

    <span class="s3">return </span><span class="s1">[</span>
        <span class="s1">custom_numeric(</span>
            <span class="s1">dtypes=[np.float32</span><span class="s3">, </span><span class="s1">np.complex64]</span><span class="s3">, </span><span class="s1">devices=</span><span class="s5">&quot;tpu&quot;</span><span class="s3">, </span><span class="s1">tol=</span><span class="s4">0.1</span><span class="s1">)</span><span class="s3">,</span>
        <span class="s1">custom_numeric(</span>
            <span class="s1">dtypes=[np.float32</span><span class="s3">, </span><span class="s1">np.complex64]</span><span class="s3">, </span><span class="s1">devices=(</span><span class="s5">&quot;cpu&quot;</span><span class="s3">, </span><span class="s5">&quot;gpu&quot;</span><span class="s1">)</span><span class="s3">,</span>
            <span class="s1">tol=</span><span class="s4">1e-5</span><span class="s1">)</span><span class="s3">,</span>
        <span class="s1">custom_numeric(</span>
            <span class="s1">dtypes=[np.float64</span><span class="s3">, </span><span class="s1">np.complex128]</span><span class="s3">,</span>
            <span class="s1">modes=(</span><span class="s5">&quot;eager&quot;</span><span class="s3">, </span><span class="s5">&quot;graph&quot;</span><span class="s1">)</span><span class="s3">,</span>
            <span class="s1">tol=</span><span class="s4">1e-13</span><span class="s1">)</span><span class="s3">,</span>
        <span class="s1">custom_numeric(</span>
            <span class="s1">dtypes=[np.float64</span><span class="s3">, </span><span class="s1">np.complex128]</span><span class="s3">, </span><span class="s1">modes=(</span><span class="s5">&quot;compiled&quot;</span><span class="s1">)</span><span class="s3">, </span><span class="s1">tol=</span><span class="s4">1e-14</span><span class="s1">)</span><span class="s3">,</span>
        <span class="s1">custom_numeric(</span>
            <span class="s1">custom_assert=custom_assert</span><span class="s3">,</span>
            <span class="s1">description=(</span><span class="s5">&quot;May return different, but also correct, results when &quot;</span>
                         <span class="s5">&quot;the decomposition is not unique&quot;</span><span class="s1">)</span><span class="s3">,</span>
            <span class="s1">devices=(</span><span class="s5">&quot;cpu&quot;</span><span class="s3">, </span><span class="s5">&quot;gpu&quot;</span><span class="s1">)</span><span class="s3">,</span>
            <span class="s1">modes=(</span><span class="s5">&quot;eager&quot;</span><span class="s3">, </span><span class="s5">&quot;graph&quot;</span><span class="s3">, </span><span class="s5">&quot;compiled&quot;</span><span class="s1">))</span><span class="s3">,</span>
    <span class="s1">]</span>

  <span class="s1">@classmethod</span>
  <span class="s3">def </span><span class="s1">max(cls</span><span class="s3">, </span><span class="s1">harness: primitive_harness.Harness):</span>
    <span class="s0"># TODO(bchetioui): discrepancies between TF &amp; JAX when comparing with NaN;</span>
    <span class="s0"># JAX always returns NaN, while TF returns the value NaN is compared with.</span>
    <span class="s3">def </span><span class="s1">custom_assert(tst</span><span class="s3">, </span><span class="s1">result_jax</span><span class="s3">, </span><span class="s1">result_tf</span><span class="s3">, </span><span class="s1">err_msg</span><span class="s3">, </span><span class="s1">**_):</span>
      <span class="s1">mask = np.isnan(result_jax)</span>
      <span class="s1">tst.assertAllClose(result_jax[~mask]</span><span class="s3">, </span><span class="s1">result_tf[~mask]</span><span class="s3">, </span><span class="s1">err_msg=err_msg)</span>

    <span class="s3">return </span><span class="s1">[</span>
        <span class="s1">custom_numeric(</span>
            <span class="s1">custom_assert=custom_assert</span><span class="s3">,</span>
            <span class="s1">description=(</span>
                <span class="s5">&quot;May return different values when one of the values is NaN. &quot;</span>
                <span class="s5">&quot;JAX always returns NaN, while TF returns the value NaN is compared with.&quot;</span>
            <span class="s1">)</span><span class="s3">,</span>
            <span class="s1">modes=(</span><span class="s5">&quot;eager&quot;</span><span class="s3">, </span><span class="s5">&quot;graph&quot;</span><span class="s3">, </span><span class="s5">&quot;compiled&quot;</span><span class="s1">)</span><span class="s3">,</span>
            <span class="s1">native_serialization=Jax2TfLimitation.FOR_NON_NATIVE)</span><span class="s3">,</span>
        <span class="s0"># TODO(b/269996580)</span>
        <span class="s1">custom_numeric(</span>
            <span class="s1">custom_assert=custom_assert</span><span class="s3">,</span>
            <span class="s1">devices=</span><span class="s5">&quot;cpu&quot;</span><span class="s3">,</span>
            <span class="s1">description=(</span>
                <span class="s5">&quot;TF and JAX use different values of the compiler flag &quot;</span>
                <span class="s5">&quot;xla_cpu_enable_fast_min_max compiler flag and therefore have &quot;</span>
                <span class="s5">&quot;different behavior of NaN propagation through min/max.&quot;</span>
            <span class="s1">)</span><span class="s3">,</span>
            <span class="s1">modes=(</span><span class="s5">&quot;eager&quot;</span><span class="s3">, </span><span class="s5">&quot;graph&quot;</span><span class="s3">, </span><span class="s5">&quot;compiled&quot;</span><span class="s1">)</span><span class="s3">,</span>
            <span class="s1">native_serialization=Jax2TfLimitation.FOR_NATIVE)</span>
    <span class="s1">]</span>

  <span class="s1">@classmethod</span>
  <span class="s3">def </span><span class="s1">min(cls</span><span class="s3">, </span><span class="s1">harness: primitive_harness.Harness):</span>
    <span class="s0"># TODO(bchetioui): discrepancies between TF &amp; JAX when comparing with NaN;</span>
    <span class="s0"># JAX always returns NaN, while TF returns the value NaN is compared with.</span>
    <span class="s3">def </span><span class="s1">custom_assert(tst</span><span class="s3">, </span><span class="s1">result_jax</span><span class="s3">, </span><span class="s1">result_tf</span><span class="s3">, </span><span class="s1">*</span><span class="s3">, </span><span class="s1">err_msg</span><span class="s3">, </span><span class="s1">**_):</span>
      <span class="s1">mask = np.isnan(result_jax)</span>
      <span class="s1">tst.assertAllClose(result_jax[~mask]</span><span class="s3">, </span><span class="s1">result_tf[~mask]</span><span class="s3">, </span><span class="s1">err_msg=err_msg)</span>

    <span class="s3">return </span><span class="s1">[</span>
        <span class="s1">custom_numeric(</span>
            <span class="s1">custom_assert=custom_assert</span><span class="s3">,</span>
            <span class="s1">description=(</span>
                <span class="s5">&quot;May return different values when one of the values is NaN. &quot;</span>
                <span class="s5">&quot;JAX always returns NaN, while TF returns the value NaN is compared with.&quot;</span>
            <span class="s1">)</span><span class="s3">,</span>
            <span class="s1">modes=(</span><span class="s5">&quot;eager&quot;</span><span class="s3">, </span><span class="s5">&quot;graph&quot;</span><span class="s3">, </span><span class="s5">&quot;compiled&quot;</span><span class="s1">)</span><span class="s3">,</span>
            <span class="s1">native_serialization=Jax2TfLimitation.FOR_NON_NATIVE)</span><span class="s3">,</span>
        <span class="s0"># TODO(b/269996580)</span>
        <span class="s1">custom_numeric(</span>
            <span class="s1">custom_assert=custom_assert</span><span class="s3">,</span>
            <span class="s1">devices=</span><span class="s5">&quot;cpu&quot;</span><span class="s3">,</span>
            <span class="s1">description=(</span>
                <span class="s5">&quot;TF and JAX use different values of the compiler flag &quot;</span>
                <span class="s5">&quot;xla_cpu_enable_fast_min_max compiler flag and therefore have &quot;</span>
                <span class="s5">&quot;different behavior of NaN propagation through min/max.&quot;</span>
            <span class="s1">)</span><span class="s3">,</span>
            <span class="s1">modes=(</span><span class="s5">&quot;eager&quot;</span><span class="s3">, </span><span class="s5">&quot;graph&quot;</span><span class="s3">, </span><span class="s5">&quot;compiled&quot;</span><span class="s1">)</span><span class="s3">,</span>
            <span class="s1">native_serialization=Jax2TfLimitation.FOR_NATIVE)</span>
    <span class="s1">]</span>

  <span class="s1">@classmethod</span>
  <span class="s3">def </span><span class="s1">nextafter(cls</span><span class="s3">, </span><span class="s1">harness: primitive_harness.Harness):</span>
    <span class="s3">return </span><span class="s1">[missing_tf_kernel(dtypes=[np.float16</span><span class="s3">, </span><span class="s1">dtypes.bfloat16])]</span>

  <span class="s1">@classmethod</span>
  <span class="s3">def </span><span class="s1">qr(cls</span><span class="s3">, </span><span class="s1">harness: primitive_harness.Harness):</span>
    <span class="s0"># See https://github.com/google/jax/pull/3775#issuecomment-659407824;</span>
    <span class="s0">#     # jit_compile=True breaks for complex types.</span>
    <span class="s0"># TODO: see https://github.com/google/jax/pull/3775#issuecomment-659407824.</span>
    <span class="s0"># - for now, the performance of the HLO QR implementation called when</span>
    <span class="s0">#   compiling with TF is expected to have worse performance than the</span>
    <span class="s0">#   custom calls made in JAX.</span>
    <span class="s3">return </span><span class="s1">[</span>
        <span class="s1">custom_numeric(</span>
            <span class="s1">dtypes=[np.float64</span><span class="s3">, </span><span class="s1">np.complex128]</span><span class="s3">,</span>
            <span class="s1">devices=(</span><span class="s5">&quot;cpu&quot;</span><span class="s3">, </span><span class="s5">&quot;gpu&quot;</span><span class="s1">)</span><span class="s3">,</span>
            <span class="s1">modes=(</span><span class="s5">&quot;eager&quot;</span><span class="s3">, </span><span class="s5">&quot;graph&quot;</span><span class="s3">, </span><span class="s5">&quot;compiled&quot;</span><span class="s1">)</span><span class="s3">,</span>
            <span class="s1">tol=</span><span class="s4">1e-13</span><span class="s1">)</span><span class="s3">,</span>
        <span class="s1">custom_numeric(</span>
            <span class="s1">dtypes=[np.float32</span><span class="s3">, </span><span class="s1">np.complex64]</span><span class="s3">,</span>
            <span class="s1">devices=(</span><span class="s5">&quot;cpu&quot;</span><span class="s3">, </span><span class="s5">&quot;gpu&quot;</span><span class="s1">)</span><span class="s3">,</span>
            <span class="s1">modes=(</span><span class="s5">&quot;eager&quot;</span><span class="s3">, </span><span class="s5">&quot;graph&quot;</span><span class="s3">, </span><span class="s5">&quot;compiled&quot;</span><span class="s1">)</span><span class="s3">,</span>
            <span class="s1">tol=</span><span class="s4">1e-4</span><span class="s1">)</span><span class="s3">,</span>
        <span class="s1">missing_tf_kernel(</span>
            <span class="s1">dtypes=[dtypes.bfloat16]</span><span class="s3">,</span>
            <span class="s1">devices=</span><span class="s5">&quot;tpu&quot;</span><span class="s3">,</span>
        <span class="s1">)</span>
    <span class="s1">]</span>

  <span class="s1">@classmethod</span>
  <span class="s3">def </span><span class="s1">random_gamma(cls</span><span class="s3">, </span><span class="s1">harness: primitive_harness.Harness):</span>
    <span class="s3">return </span><span class="s1">[custom_numeric(devices=</span><span class="s5">&quot;tpu&quot;</span><span class="s3">, </span><span class="s1">tol=</span><span class="s4">1e-3</span><span class="s1">)]</span>

  <span class="s1">@classmethod</span>
  <span class="s3">def </span><span class="s1">reduce_max(cls</span><span class="s3">, </span><span class="s1">harness: primitive_harness.Harness):</span>
    <span class="s0"># Unlike reduce_window_max, we use a native TF op: tf.reduce_max, which</span>
    <span class="s0"># does not work for complex</span>
    <span class="s3">return </span><span class="s1">[missing_tf_kernel(dtypes=[np.complex64</span><span class="s3">, </span><span class="s1">np.complex128])]</span>

  <span class="s1">@classmethod</span>
  <span class="s3">def </span><span class="s1">reduce_min(cls</span><span class="s3">, </span><span class="s1">harness: primitive_harness.Harness):</span>
    <span class="s3">return </span><span class="s1">cls.reduce_max(harness)</span>

  <span class="s1">@classmethod</span>
  <span class="s3">def </span><span class="s1">reduce_window_add(cls</span><span class="s3">, </span><span class="s1">harness: primitive_harness.Harness):</span>
    <span class="s3">return </span><span class="s1">[</span>
        <span class="s1">Jax2TfLimitation(</span>
            <span class="s5">&quot;Small deviations on GPU for large inputs and enable_xla=False&quot;</span><span class="s3">,</span>
            <span class="s1">dtypes=[np.float32]</span><span class="s3">,</span>
            <span class="s1">devices=</span><span class="s5">&quot;gpu&quot;</span><span class="s3">,</span>
            <span class="s1">modes=(</span><span class="s5">&quot;eager&quot;</span><span class="s3">, </span><span class="s5">&quot;graph&quot;</span><span class="s3">, </span><span class="s5">&quot;compiled&quot;</span><span class="s1">)</span><span class="s3">,</span>
            <span class="s1">expect_tf_error=</span><span class="s3">False,</span>
            <span class="s1">skip_comparison=</span><span class="s3">False,</span>
            <span class="s1">enabled=</span><span class="s3">not </span><span class="s1">harness.params[</span><span class="s5">&quot;enable_xla&quot;</span><span class="s1">]</span><span class="s3">,</span>
            <span class="s1">tol=</span><span class="s4">3e-5</span><span class="s1">)</span><span class="s3">,</span>
        <span class="s1">Jax2TfLimitation(</span>
            <span class="s5">&quot;Large deviations on TPU for enable_xla=False&quot;</span><span class="s3">,</span>
            <span class="s1">dtypes=[np.float16</span><span class="s3">, </span><span class="s1">np.float32]</span><span class="s3">,</span>
            <span class="s1">devices=</span><span class="s5">&quot;tpu&quot;</span><span class="s3">,</span>
            <span class="s1">modes=(</span><span class="s5">&quot;eager&quot;</span><span class="s3">, </span><span class="s5">&quot;graph&quot;</span><span class="s3">, </span><span class="s5">&quot;compiled&quot;</span><span class="s1">)</span><span class="s3">,</span>
            <span class="s1">expect_tf_error=</span><span class="s3">False,</span>
            <span class="s1">skip_comparison=</span><span class="s3">True,</span>
            <span class="s1">enabled=</span><span class="s3">not </span><span class="s1">harness.params[</span><span class="s5">&quot;enable_xla&quot;</span><span class="s1">])</span><span class="s3">,</span>
    <span class="s1">]</span>

  <span class="s1">@classmethod</span>
  <span class="s3">def </span><span class="s1">regularized_incomplete_beta(cls</span><span class="s3">, </span><span class="s1">harness: primitive_harness.Harness):</span>
    <span class="s3">return </span><span class="s1">[</span>
        <span class="s1">custom_numeric(dtypes=[np.float64]</span><span class="s3">, </span><span class="s1">tol=</span><span class="s4">1e-14</span><span class="s1">)</span><span class="s3">,</span>
        <span class="s1">missing_tf_kernel(dtypes=[np.float16</span><span class="s3">, </span><span class="s1">dtypes.bfloat16])</span>
    <span class="s1">]</span>

  <span class="s1">@classmethod</span>
  <span class="s3">def </span><span class="s1">rem(cls</span><span class="s3">, </span><span class="s1">harness: primitive_harness.Harness):</span>
    <span class="s3">return </span><span class="s1">[</span>
        <span class="s1">Jax2TfLimitation(</span>
            <span class="s5">&quot;TF integer division fails if divisor contains 0; JAX returns NaN&quot;</span><span class="s3">,</span>
            <span class="s1">dtypes=[</span>
                <span class="s1">np.uint8</span><span class="s3">, </span><span class="s1">np.int8</span><span class="s3">, </span><span class="s1">np.uint16</span><span class="s3">, </span><span class="s1">np.uint32</span><span class="s3">, </span><span class="s1">np.uint64</span><span class="s3">, </span><span class="s1">np.int8</span><span class="s3">,</span>
                <span class="s1">np.int16</span><span class="s3">, </span><span class="s1">np.int32</span><span class="s3">, </span><span class="s1">np.int64</span>
            <span class="s1">]</span><span class="s3">,</span>
            <span class="s1">skip_comparison=</span><span class="s3">True,</span>
            <span class="s0"># Only the harnesses with &quot;singularity&quot; will have divide by 0</span>
            <span class="s1">enabled=(</span><span class="s5">&quot;singularity&quot; </span><span class="s3">in </span><span class="s1">harness.name))</span><span class="s3">,</span>
        <span class="s1">Jax2TfLimitation(</span>
            <span class="s5">&quot;TF division of inf by inf returns inf while in JAX returns nan&quot;</span><span class="s3">,</span>
            <span class="s1">dtypes=[</span>
                <span class="s1">np.float32</span><span class="s3">,</span>
            <span class="s1">]</span><span class="s3">,</span>
            <span class="s1">devices=</span><span class="s5">&quot;gpu&quot;</span><span class="s3">,</span>
            <span class="s1">skip_comparison=</span><span class="s3">True,</span>
            <span class="s1">enabled=(</span><span class="s5">&quot;singularity_inf_by_inf&quot; </span><span class="s3">in </span><span class="s1">harness.name))</span><span class="s3">,</span>
    <span class="s1">]</span>

  <span class="s1">@classmethod</span>
  <span class="s3">def </span><span class="s1">rng_bit_generator(cls</span><span class="s3">, </span><span class="s1">harness: primitive_harness.Harness):</span>
    <span class="s3">return </span><span class="s1">[]</span>

  <span class="s1">@classmethod</span>
  <span class="s3">def </span><span class="s1">round(cls</span><span class="s3">, </span><span class="s1">harness: primitive_harness.Harness):</span>
    <span class="s3">return </span><span class="s1">[</span>
        <span class="s1">missing_tf_kernel(</span>
            <span class="s1">dtypes=[dtypes.bfloat16]</span><span class="s3">,</span>
            <span class="s1">devices=(</span><span class="s5">&quot;cpu&quot;</span><span class="s3">, </span><span class="s5">&quot;gpu&quot;</span><span class="s1">)</span><span class="s3">,</span>
            <span class="s1">modes=(</span><span class="s5">&quot;eager&quot;</span><span class="s3">, </span><span class="s5">&quot;graph&quot;</span><span class="s1">))</span>
    <span class="s1">]</span>

  <span class="s1">@classmethod</span>
  <span class="s3">def </span><span class="s1">scatter(cls</span><span class="s3">, </span><span class="s1">harness):</span>
    <span class="s3">return </span><span class="s1">[</span>
        <span class="s1">Jax2TfLimitation(</span>
            <span class="s5">&quot;out-of-bounds scatters are not supported in graph and eager mode&quot;</span><span class="s3">,</span>
            <span class="s1">dtypes=jtu.dtypes.all_inexact</span><span class="s3">,</span>
            <span class="s1">devices=(</span><span class="s5">&quot;cpu&quot;</span><span class="s3">, </span><span class="s5">&quot;gpu&quot;</span><span class="s3">, </span><span class="s5">&quot;tpu&quot;</span><span class="s1">)</span><span class="s3">,</span>
            <span class="s1">modes=(</span><span class="s5">&quot;eager&quot;</span><span class="s3">, </span><span class="s5">&quot;graph&quot;</span><span class="s1">)</span><span class="s3">,</span>
            <span class="s1">expect_tf_error=</span><span class="s3">True,</span>
            <span class="s1">skip_comparison=</span><span class="s3">True,</span>
            <span class="s1">enabled=(</span><span class="s5">&quot;modes_out_of_bounds&quot; </span><span class="s3">in </span><span class="s1">harness.name </span><span class="s3">and not </span><span class="s1">harness.params[</span><span class="s5">&quot;enable_xla&quot;</span><span class="s1">]))</span><span class="s3">,</span>
    <span class="s1">]</span>

  <span class="s1">@classmethod</span>
  <span class="s3">def </span><span class="s1">scatter_add(cls</span><span class="s3">, </span><span class="s1">harness):</span>
    <span class="s3">return </span><span class="s1">cls.scatter(harness)</span>

  <span class="s1">@classmethod</span>
  <span class="s3">def </span><span class="s1">scatter_mul(cls</span><span class="s3">, </span><span class="s1">harness):</span>
    <span class="s3">return </span><span class="s1">cls.scatter(harness)</span>

  <span class="s1">@classmethod</span>
  <span class="s3">def </span><span class="s1">scatter_max(cls</span><span class="s3">, </span><span class="s1">harness):</span>
    <span class="s3">return </span><span class="s1">cls.scatter(harness)</span>

  <span class="s1">@classmethod</span>
  <span class="s3">def </span><span class="s1">scatter_min(cls</span><span class="s3">, </span><span class="s1">harness):</span>
    <span class="s3">return </span><span class="s1">cls.scatter(harness)</span>

  <span class="s1">@classmethod</span>
  <span class="s3">def </span><span class="s1">select_and_gather_add(cls</span><span class="s3">, </span><span class="s1">harness):</span>
    <span class="s3">return </span><span class="s1">[</span>
        <span class="s0"># This JAX primitives is not not exposed directly in the JAX API</span>
        <span class="s0"># but arises from JVP of `lax.reduce_window` for reducers</span>
        <span class="s0"># `lax.max` or `lax.min`. It also arises from second-order</span>
        <span class="s0"># VJP of the same. Implemented using XlaReduceWindow.</span>
        <span class="s1">Jax2TfLimitation((</span>
            <span class="s5">&quot;jax2tf unimplemented for 64-bit inputs because the current implementation &quot;</span>
            <span class="s5">&quot;relies on packing two values into a single value. This can be &quot;</span>
            <span class="s5">&quot;fixed by using a variadic XlaReduceWindow, when available&quot;</span><span class="s1">)</span><span class="s3">,</span>
                         <span class="s1">dtypes=[np.float64]</span><span class="s3">,</span>
                         <span class="s1">devices=(</span><span class="s5">&quot;cpu&quot;</span><span class="s3">, </span><span class="s5">&quot;gpu&quot;</span><span class="s1">))</span>
    <span class="s1">]</span>

  <span class="s1">@classmethod</span>
  <span class="s3">def </span><span class="s1">sort(cls</span><span class="s3">, </span><span class="s1">harness: primitive_harness.Harness):</span>
    <span class="s3">return </span><span class="s1">[</span>
        <span class="s1">Jax2TfLimitation(</span>
            <span class="s0"># I think that this is because TF is running on CPU even for GPU tests?</span>
            <span class="s5">&quot;TODO: TF non-stable multiple-array sort&quot;</span><span class="s3">,</span>
            <span class="s1">devices=</span><span class="s5">&quot;gpu&quot;</span><span class="s3">,</span>
            <span class="s1">enabled=(harness.params[</span><span class="s5">&quot;num_arrays&quot;</span><span class="s1">] &gt; </span><span class="s4">1 </span><span class="s3">and</span>
                     <span class="s3">not </span><span class="s1">harness.params[</span><span class="s5">&quot;is_stable&quot;</span><span class="s1">])</span><span class="s3">,</span>
            <span class="s1">expect_tf_error=</span><span class="s3">False,</span>
            <span class="s1">skip_comparison=</span><span class="s3">True</span><span class="s1">)</span><span class="s3">,</span>
    <span class="s1">]</span>

  <span class="s1">@classmethod</span>
  <span class="s3">def </span><span class="s1">svd(cls</span><span class="s3">, </span><span class="s1">harness: primitive_harness.Harness):</span>
    <span class="s0"># TODO: slow test</span>
    <span class="s1">compute_uv = harness.params[</span><span class="s5">&quot;compute_uv&quot;</span><span class="s1">]</span>

    <span class="s0"># Both `r_jax` and `r_tf` are 3-Tuples containing the SVD results:</span>
    <span class="s0"># `S` (singular values), `U` (left singular vectors), and `Vh` (the</span>
    <span class="s0"># adjoint of the right singular vectors). Note that the TF results are</span>
    <span class="s0"># obtained through `_svd` in jax/experimental/jax2tf/jax2tf.py.</span>
    <span class="s3">def </span><span class="s1">custom_assert(tst</span><span class="s3">, </span><span class="s1">r_jax</span><span class="s3">, </span><span class="s1">r_tf</span><span class="s3">, </span><span class="s1">*</span><span class="s3">, </span><span class="s1">args</span><span class="s3">, </span><span class="s1">tol</span><span class="s3">, </span><span class="s1">err_msg):</span>

      <span class="s3">def </span><span class="s1">reconstruct_operand(result):</span>
        <span class="s0"># Reconstructing operand as documented in numpy.linalg.svd (see</span>
        <span class="s0"># https://numpy.org/doc/stable/reference/generated/numpy.linalg.svd.html)</span>
        <span class="s1">s</span><span class="s3">, </span><span class="s1">u</span><span class="s3">, </span><span class="s1">v = result</span>
        <span class="s1">U = u[...</span><span class="s3">, </span><span class="s1">:s.shape[-</span><span class="s4">1</span><span class="s1">]]</span>
        <span class="s1">V = v[...</span><span class="s3">, </span><span class="s1">:s.shape[-</span><span class="s4">1</span><span class="s1">]</span><span class="s3">, </span><span class="s1">:]</span>
        <span class="s1">S = s[...</span><span class="s3">, None, </span><span class="s1">:]</span>
        <span class="s3">return </span><span class="s1">jnp.matmul(U * S</span><span class="s3">, </span><span class="s1">V</span><span class="s3">, </span><span class="s1">precision=lax.Precision.HIGHEST)</span>

      <span class="s0"># Compares the shapes.</span>
      <span class="s3">def </span><span class="s1">compare_shapes(r_jax</span><span class="s3">, </span><span class="s1">r_tf):</span>
        <span class="s1">shapes_jax = [result.shape </span><span class="s3">for </span><span class="s1">result </span><span class="s3">in </span><span class="s1">r_jax]</span>
        <span class="s1">shapes_tf = [result.shape </span><span class="s3">for </span><span class="s1">result </span><span class="s3">in </span><span class="s1">r_tf]</span>
        <span class="s1">tst.assertEqual(shapes_jax</span><span class="s3">, </span><span class="s1">shapes_tf)</span>

      <span class="s0"># Compares reconstructed operand.</span>
      <span class="s0"># Computes backward error https://www.netlib.org/lapack/lug/node97.html</span>
      <span class="s0"># and uses the maximum backward error if there are batch dimensions.</span>
      <span class="s0"># The backward error is bounded by some constant multiplying the machine</span>
      <span class="s0"># precision.</span>
      <span class="s0"># TODO: Compares the operand instead of the reconstructed operand.</span>
      <span class="s3">def </span><span class="s1">compare_reconstructed_operand(r_jax</span><span class="s3">, </span><span class="s1">r_tf</span><span class="s3">, </span><span class="s1">tol):</span>
        <span class="s1">operand_jax = reconstruct_operand(r_jax)</span>
        <span class="s1">operand_tf = reconstruct_operand(r_tf)</span>
        <span class="s1">error_norm = jnp.linalg.norm(operand_jax - operand_tf</span><span class="s3">,</span>
                                              <span class="s1">axis=(-</span><span class="s4">2</span><span class="s3">, </span><span class="s1">-</span><span class="s4">1</span><span class="s1">))</span>
        <span class="s1">backward_error = (error_norm /</span>
                          <span class="s1">jnp.linalg.norm(operand_jax</span><span class="s3">, </span><span class="s1">axis=(-</span><span class="s4">2</span><span class="s3">, </span><span class="s1">-</span><span class="s4">1</span><span class="s1">)))</span>
        <span class="s1">max_backward_error = jnp.amax(backward_error)</span>
        <span class="s1">tst.assertLess(max_backward_error</span><span class="s3">, </span><span class="s1">tol)</span>

      <span class="s0"># Computes the absolute gap between singular value `\sigma_i` and the</span>
      <span class="s0"># nearest other singular value and for all singular values. The absolute</span>
      <span class="s0"># gap is used to approximate the upper bound of angular difference</span>
      <span class="s0"># between the computed and the true singular vectors. If the matrix is</span>
      <span class="s0"># rectangular `m != n`, the gap for the smallest nonzero singular value</span>
      <span class="s0"># should also consider the gap between it and zero. Note that this code</span>
      <span class="s0"># relies on the singular values being in descending order.</span>
      <span class="s3">def </span><span class="s1">compute_absolute_gap(s</span><span class="s3">, </span><span class="s1">m</span><span class="s3">, </span><span class="s1">n):</span>
        <span class="s1">forward_appendant = np.Inf </span><span class="s3">if </span><span class="s1">m == n </span><span class="s3">else </span><span class="s4">0</span>
        <span class="s1">forward_diff = jnp.diff(s</span><span class="s3">, </span><span class="s1">axis=-</span><span class="s4">1</span><span class="s3">, </span><span class="s1">append=forward_appendant)</span>
        <span class="s1">backward_diff = jnp.diff(</span>
            <span class="s1">s[...</span><span class="s3">, </span><span class="s1">::-</span><span class="s4">1</span><span class="s1">]</span><span class="s3">, </span><span class="s1">axis=-</span><span class="s4">1</span><span class="s3">, </span><span class="s1">append=np.Inf)[...</span><span class="s3">, </span><span class="s1">::-</span><span class="s4">1</span><span class="s1">]</span>
        <span class="s1">absolute_gap = jnp.minimum(jnp.abs(forward_diff)</span><span class="s3">,</span>
                                   <span class="s1">jnp.abs(backward_diff))</span>
        <span class="s3">return </span><span class="s1">absolute_gap</span>

      <span class="s0"># See `CompareSingularVectors` in</span>
      <span class="s0"># tensorflow/python/kernel_tests/linalg/svd_op_test.py</span>
      <span class="s3">def </span><span class="s1">compare_singular_vectors(x</span><span class="s3">, </span><span class="s1">y</span><span class="s3">, </span><span class="s1">*</span><span class="s3">, </span><span class="s1">error_bound):</span>
        <span class="s0"># Singular vectors are only unique up to sign (complex phase factor for</span>
        <span class="s0"># complex matrices), so we normalize the sign first.</span>
        <span class="s1">sum_of_ratios = jnp.sum(jnp.divide(y</span><span class="s3">, </span><span class="s1">x)</span><span class="s3">, </span><span class="s1">-</span><span class="s4">2</span><span class="s3">, </span><span class="s1">keepdims=</span><span class="s3">True</span><span class="s1">)</span>
        <span class="s1">phases = jnp.divide(sum_of_ratios</span><span class="s3">, </span><span class="s1">jnp.abs(sum_of_ratios))</span>
        <span class="s1">x *= phases</span>

        <span class="s0"># Note that in general `sqrt(sum(squares))` is not a stable way to</span>
        <span class="s0"># compute l2 vector norms, but it should be OK for normalization</span>
        <span class="s0"># factors of vectors with norm ~= 1 as here.</span>
        <span class="s3">def </span><span class="s1">dot_column_wise(a</span><span class="s3">, </span><span class="s1">b):</span>
          <span class="s1">output = jnp.sum(jnp.einsum(</span><span class="s5">'...ij,...ij-&gt;...ij'</span><span class="s3">, </span><span class="s1">a.conj()</span><span class="s3">, </span><span class="s1">b</span><span class="s3">,</span>
                                      <span class="s1">precision=lax.Precision.HIGHEST)</span><span class="s3">,</span>
                           <span class="s1">axis=-</span><span class="s4">2</span><span class="s1">)</span>
          <span class="s3">return </span><span class="s1">jnp.real(output)</span>

        <span class="s1">cos_angular_diff = (</span>
            <span class="s1">dot_column_wise(x</span><span class="s3">, </span><span class="s1">y) /</span>
            <span class="s1">jnp.sqrt(dot_column_wise(x</span><span class="s3">, </span><span class="s1">x) * dot_column_wise(y</span><span class="s3">, </span><span class="s1">y)))</span>

        <span class="s0"># Values of `\cos(angular_diff)` outside the interval [0, 1] are clipped</span>
        <span class="s0"># to the interval edges. For example, `\cos(angular_diff)` could contain</span>
        <span class="s0"># values like 1.0000001 on float32, which are clipped to 1.0. It is</span>
        <span class="s0"># possible that anything other than `cos_angular_diff` can be outside</span>
        <span class="s0"># the interval [0, 1] due to roundoff.</span>
        <span class="s1">cos_angular_diff = jnp.clip(cos_angular_diff</span><span class="s3">, </span><span class="s1">a_min=</span><span class="s4">0.0</span><span class="s3">, </span><span class="s1">a_max=</span><span class="s4">1.0</span><span class="s1">)</span>

        <span class="s1">angular_diff = jnp.arccos(cos_angular_diff)</span>

        <span class="s0"># TODO: removes the slack factor on the angular difference.</span>
        <span class="s0"># It is possible that the singular vectors are not accurate to much more</span>
        <span class="s0"># than O(\sqrt(eps)), which is likely a property of the SVD algorithms</span>
        <span class="s0"># in question; revisit with better understanding of the SVD algorithms.</span>
        <span class="s3">if </span><span class="s1">x.dtype </span><span class="s3">in </span><span class="s1">[np.float32</span><span class="s3">, </span><span class="s1">np.complex64]:</span>
          <span class="s1">slack_factor = </span><span class="s4">2E4</span>
        <span class="s3">elif </span><span class="s1">x.dtype </span><span class="s3">in </span><span class="s1">[np.float64</span><span class="s3">, </span><span class="s1">np.complex128]:</span>
          <span class="s1">slack_factor = </span><span class="s4">2E9</span>

        <span class="s1">np.testing.assert_array_less(angular_diff</span><span class="s3">,</span>
                                     <span class="s1">slack_factor * error_bound)</span>

      <span class="s3">if </span><span class="s1">compute_uv:</span>
        <span class="s0"># Compares the shapes.</span>
        <span class="s1">compare_shapes(r_jax</span><span class="s3">, </span><span class="s1">r_tf)</span>

        <span class="s0"># Compares the singular values. Each computed singular value `\sigma_i`</span>
        <span class="s0"># differs from the true `\sigma_i`* by at most</span>
        <span class="s0"># `|\sigma_i - \sigma_i*| &lt;= \epsilon \sigma_1`, where `\sigma_1` is the</span>
        <span class="s0"># largest singular value and `\epsilon` denotes the machine precision.</span>
        <span class="s1">s_jax</span><span class="s3">, </span><span class="s1">s_tf = r_jax[</span><span class="s4">0</span><span class="s1">]</span><span class="s3">, </span><span class="s1">r_tf[</span><span class="s4">0</span><span class="s1">]</span>
        <span class="s1">tst.assertAllClose(s_jax</span><span class="s3">, </span><span class="s1">s_tf</span><span class="s3">, </span><span class="s1">atol=tol</span><span class="s3">, </span><span class="s1">rtol=tol</span><span class="s3">, </span><span class="s1">err_msg=err_msg)</span>

        <span class="s0"># Compares the reconstructed operand.</span>
        <span class="s1">compare_reconstructed_operand(r_jax</span><span class="s3">, </span><span class="s1">r_tf</span><span class="s3">, </span><span class="s1">tol)</span>

        <span class="s0"># Compares the singular vectors.</span>
        <span class="s0"># We only compare the first `rank` singular vectors since the remainder</span>
        <span class="s0"># forms an arbitrary orthonormal basis for the (row- or column-) null</span>
        <span class="s0"># space, whose exact value depends on implementation details.</span>
        <span class="s0"># TODO: A better estimation on the rank?</span>
        <span class="s1">rank = r_jax[</span><span class="s4">0</span><span class="s1">].shape[-</span><span class="s4">1</span><span class="s1">]</span>

        <span class="s0"># Computes the upper bound for angular difference of singular vectors.</span>
        <span class="s0"># The upper bound has the shape of `[..., k]`, where `...` denotes the</span>
        <span class="s0"># batch dimensions and `k` is the number of nonzero singular values.</span>
        <span class="s1">m = r_jax[</span><span class="s4">1</span><span class="s1">].shape[-</span><span class="s4">2</span><span class="s1">]</span>
        <span class="s1">n = r_jax[</span><span class="s4">2</span><span class="s1">].shape[-</span><span class="s4">2</span><span class="s1">]</span>
        <span class="s1">absolute_gap = compute_absolute_gap(r_jax[</span><span class="s4">0</span><span class="s1">]</span><span class="s3">, </span><span class="s1">m</span><span class="s3">, </span><span class="s1">n)</span>
        <span class="s1">epsilon = jnp.finfo(r_jax[</span><span class="s4">0</span><span class="s1">].dtype).eps</span>
        <span class="s1">sigma_largest = (r_jax[</span><span class="s4">0</span><span class="s1">][...</span><span class="s3">, </span><span class="s4">0</span><span class="s1">])[...</span><span class="s3">, None</span><span class="s1">]</span>
        <span class="s1">upperbound_singular_vectors = epsilon * sigma_largest / absolute_gap</span>
        <span class="s1">upperbound_singular_vectors = upperbound_singular_vectors[...</span><span class="s3">, </span><span class="s1">:rank]</span>

        <span class="s0"># Left singular vectors.</span>
        <span class="s1">u_jax = r_jax[</span><span class="s4">1</span><span class="s1">][...</span><span class="s3">, </span><span class="s1">:rank]</span>
        <span class="s1">u_tf = r_tf[</span><span class="s4">1</span><span class="s1">][...</span><span class="s3">, </span><span class="s1">:rank]</span>
        <span class="s1">compare_singular_vectors(u_jax</span><span class="s3">, </span><span class="s1">u_tf</span><span class="s3">,</span>
                                 <span class="s1">error_bound=upperbound_singular_vectors)</span>

        <span class="s0"># Right singular vectors.</span>
        <span class="s1">v_jax = jnp.swapaxes(r_jax[</span><span class="s4">2</span><span class="s1">][...</span><span class="s3">, </span><span class="s1">:rank</span><span class="s3">, </span><span class="s1">:]</span><span class="s3">, </span><span class="s1">-</span><span class="s4">2</span><span class="s3">, </span><span class="s1">-</span><span class="s4">1</span><span class="s1">).conj()</span>
        <span class="s1">v_tf = jnp.swapaxes(r_tf[</span><span class="s4">2</span><span class="s1">][...</span><span class="s3">, </span><span class="s1">:rank</span><span class="s3">, </span><span class="s1">:]</span><span class="s3">, </span><span class="s1">-</span><span class="s4">2</span><span class="s3">, </span><span class="s1">-</span><span class="s4">1</span><span class="s1">).conj()</span>
        <span class="s1">compare_singular_vectors(v_jax</span><span class="s3">, </span><span class="s1">v_tf</span><span class="s3">,</span>
                                 <span class="s1">error_bound=upperbound_singular_vectors)</span>
      <span class="s3">else</span><span class="s1">:</span>
        <span class="s1">tst.assertAllClose(r_jax</span><span class="s3">, </span><span class="s1">r_tf</span><span class="s3">, </span><span class="s1">atol=tol</span><span class="s3">, </span><span class="s1">rtol=tol</span><span class="s3">, </span><span class="s1">err_msg=err_msg)</span>

    <span class="s3">return </span><span class="s1">[</span>
        <span class="s0"># Works in JAX for complex due to custom calls on cpu and gpu</span>
        <span class="s1">Jax2TfLimitation(</span>
            <span class="s5">&quot;function not compilable. Implemented using `tf.linalg.svd` and `tf.linalg.adjoint`&quot;</span><span class="s3">,</span>
            <span class="s1">dtypes=[np.complex64</span><span class="s3">, </span><span class="s1">np.complex128]</span><span class="s3">,</span>
            <span class="s1">devices=(</span><span class="s5">&quot;cpu&quot;</span><span class="s3">, </span><span class="s5">&quot;gpu&quot;</span><span class="s1">)</span><span class="s3">,</span>
            <span class="s1">modes=(</span><span class="s5">&quot;compiled&quot;</span><span class="s3">,</span><span class="s1">))</span><span class="s3">,</span>
        <span class="s1">Jax2TfLimitation(</span>
            <span class="s5">&quot;Large numerical discrepancy&quot;</span><span class="s3">,</span>
            <span class="s1">dtypes=[np.float16]</span><span class="s3">,</span>
            <span class="s1">devices=(</span><span class="s5">&quot;tpu&quot;</span><span class="s1">)</span><span class="s3">,</span>
            <span class="s1">modes=(</span><span class="s5">&quot;eager&quot;</span><span class="s3">, </span><span class="s5">&quot;graph&quot;</span><span class="s3">, </span><span class="s5">&quot;compiled&quot;</span><span class="s1">)</span><span class="s3">,</span>
            <span class="s1">skip_comparison=</span><span class="s3">True</span><span class="s1">)</span><span class="s3">,</span>
        <span class="s1">missing_tf_kernel(dtypes=[dtypes.bfloat16]</span><span class="s3">, </span><span class="s1">devices=</span><span class="s5">&quot;tpu&quot;</span><span class="s1">)</span><span class="s3">,</span>
        <span class="s1">missing_tf_kernel(dtypes=[np.complex64</span><span class="s3">, </span><span class="s1">np.complex128]</span><span class="s3">,</span>
                          <span class="s1">modes=(</span><span class="s5">&quot;compiled&quot;</span><span class="s3">, </span><span class="s5">&quot;graph&quot;</span><span class="s1">)</span><span class="s3">,</span>
                          <span class="s1">devices=</span><span class="s5">&quot;tpu&quot;</span><span class="s1">)</span><span class="s3">,</span>
        <span class="s1">custom_numeric(</span>
            <span class="s1">tol=</span><span class="s4">1e-4</span><span class="s3">,</span>
            <span class="s1">dtypes=[np.float32</span><span class="s3">, </span><span class="s1">np.complex64]</span><span class="s3">,</span>
            <span class="s1">devices=(</span><span class="s5">&quot;cpu&quot;</span><span class="s3">, </span><span class="s5">&quot;gpu&quot;</span><span class="s1">)</span><span class="s3">,</span>
            <span class="s1">modes=(</span><span class="s5">&quot;eager&quot;</span><span class="s3">, </span><span class="s5">&quot;graph&quot;</span><span class="s3">, </span><span class="s5">&quot;compiled&quot;</span><span class="s1">))</span><span class="s3">,</span>
        <span class="s0"># TODO: this is very low tolerance for f64</span>
        <span class="s1">custom_numeric(</span>
            <span class="s1">tol=</span><span class="s4">1e-4</span><span class="s3">,</span>
            <span class="s1">dtypes=[np.float64</span><span class="s3">, </span><span class="s1">np.complex128]</span><span class="s3">,</span>
            <span class="s1">devices=(</span><span class="s5">&quot;cpu&quot;</span><span class="s3">, </span><span class="s5">&quot;gpu&quot;</span><span class="s1">)</span><span class="s3">,</span>
            <span class="s1">modes=(</span><span class="s5">&quot;eager&quot;</span><span class="s3">, </span><span class="s5">&quot;graph&quot;</span><span class="s3">, </span><span class="s5">&quot;compiled&quot;</span><span class="s1">))</span><span class="s3">,</span>
        <span class="s1">custom_numeric(</span>
            <span class="s1">tol=</span><span class="s4">1e-4</span><span class="s3">,</span>
            <span class="s1">description=</span><span class="s5">&quot;custom numeric comparison when compute_uv on CPU/GPU&quot;</span><span class="s3">,</span>
            <span class="s1">custom_assert=custom_assert</span><span class="s3">,</span>
            <span class="s1">devices=(</span><span class="s5">&quot;cpu&quot;</span><span class="s3">, </span><span class="s5">&quot;gpu&quot;</span><span class="s1">)</span><span class="s3">,</span>
            <span class="s1">modes=(</span><span class="s5">&quot;eager&quot;</span><span class="s3">, </span><span class="s5">&quot;graph&quot;</span><span class="s3">, </span><span class="s5">&quot;compiled&quot;</span><span class="s1">)</span><span class="s3">,</span>
            <span class="s1">enabled=(compute_uv == </span><span class="s3">True</span><span class="s1">))</span><span class="s3">,</span>
        <span class="s1">custom_numeric(</span>
            <span class="s1">tol=</span><span class="s4">1e-2</span><span class="s3">,</span>
            <span class="s1">description=</span><span class="s5">&quot;custom numeric comparison when compute_uv on TPU&quot;</span><span class="s3">,</span>
            <span class="s1">dtypes=[np.float32</span><span class="s3">, </span><span class="s1">np.float64</span><span class="s3">, </span><span class="s1">np.complex64</span><span class="s3">, </span><span class="s1">np.complex128]</span><span class="s3">,</span>
            <span class="s1">custom_assert=custom_assert</span><span class="s3">,</span>
            <span class="s1">devices=(</span><span class="s5">&quot;tpu&quot;</span><span class="s1">)</span><span class="s3">,</span>
            <span class="s1">modes=(</span><span class="s5">&quot;eager&quot;</span><span class="s3">, </span><span class="s5">&quot;graph&quot;</span><span class="s3">, </span><span class="s5">&quot;compiled&quot;</span><span class="s1">)</span><span class="s3">,</span>
            <span class="s1">enabled=(compute_uv == </span><span class="s3">True</span><span class="s1">))</span><span class="s3">,</span>
    <span class="s1">]</span>

  <span class="s1">@classmethod</span>
  <span class="s3">def </span><span class="s1">tan(cls</span><span class="s3">, </span><span class="s1">harness):</span>
    <span class="s3">return </span><span class="s1">[</span>
        <span class="s1">custom_numeric(dtypes=[np.complex64]</span><span class="s3">, </span><span class="s1">devices=</span><span class="s5">&quot;tpu&quot;</span><span class="s3">, </span><span class="s1">tol=</span><span class="s4">1e-4</span><span class="s1">)</span><span class="s3">,</span>
        <span class="s1">custom_numeric(dtypes=[np.complex64]</span><span class="s3">, </span><span class="s1">devices=(</span><span class="s5">&quot;cpu&quot;</span><span class="s3">, </span><span class="s5">&quot;gpu&quot;</span><span class="s1">)</span><span class="s3">, </span><span class="s1">tol=</span><span class="s4">1e-3</span><span class="s1">)</span><span class="s3">,</span>
        <span class="s1">custom_numeric(dtypes=[np.complex128]</span><span class="s3">, </span><span class="s1">devices=(</span><span class="s5">&quot;cpu&quot;</span><span class="s3">, </span><span class="s5">&quot;gpu&quot;</span><span class="s1">)</span><span class="s3">, </span><span class="s1">tol=</span><span class="s4">1e-12</span><span class="s1">)</span>
    <span class="s1">]</span>

  <span class="s1">@classmethod</span>
  <span class="s3">def </span><span class="s1">tanh(cls</span><span class="s3">, </span><span class="s1">harness):</span>
    <span class="s3">return </span><span class="s1">[</span>
        <span class="s1">custom_numeric(dtypes=[np.complex128]</span><span class="s3">, </span><span class="s1">tol=</span><span class="s4">1e-7</span><span class="s1">)</span><span class="s3">,</span>
        <span class="s1">custom_numeric(dtypes=[np.complex64]</span><span class="s3">, </span><span class="s1">tol=</span><span class="s4">1e-4</span><span class="s1">)</span>
    <span class="s1">]</span>

  <span class="s1">@classmethod</span>
  <span class="s3">def </span><span class="s1">top_k(cls</span><span class="s3">, </span><span class="s1">harness):</span>

    <span class="s3">def </span><span class="s1">custom_assert(tst</span><span class="s3">, </span><span class="s1">result_jax</span><span class="s3">, </span><span class="s1">result_tf</span><span class="s3">, </span><span class="s1">*</span><span class="s3">, </span><span class="s1">err_msg</span><span class="s3">, </span><span class="s1">**_):</span>
      <span class="s3">assert </span><span class="s1">len(result_jax) == len(result_tf)</span>
      <span class="s0"># TODO: TF and JAX sort [inf, nan] differently.</span>
      <span class="s1">first_arr_jax</span><span class="s3">, </span><span class="s1">first_arr_tf = result_jax[</span><span class="s4">0</span><span class="s1">]</span><span class="s3">, </span><span class="s1">result_tf[</span><span class="s4">0</span><span class="s1">]</span>
      <span class="s3">if </span><span class="s1">np.all(first_arr_jax == first_arr_tf):</span>
        <span class="s3">for </span><span class="s1">arr_jax</span><span class="s3">, </span><span class="s1">arr_tf </span><span class="s3">in </span><span class="s1">zip(result_jax</span><span class="s3">, </span><span class="s1">result_tf):</span>
          <span class="s1">tst.assertArraysEqual(arr_jax</span><span class="s3">, </span><span class="s1">arr_tf</span><span class="s3">, </span><span class="s1">err_msg=err_msg)</span>
      <span class="s3">else</span><span class="s1">:</span>
        <span class="s1">mask_jax = np.isnan(first_arr_jax) | np.isinf(first_arr_jax)</span>
        <span class="s1">mask_tf = np.isnan(first_arr_tf) | np.isinf(first_arr_tf)</span>
        <span class="s1">tst.assertArraysEqual(</span>
            <span class="s1">first_arr_jax[~mask_jax]</span><span class="s3">, </span><span class="s1">first_arr_tf[~mask_tf]</span><span class="s3">, </span><span class="s1">err_msg=err_msg)</span>

    <span class="s3">return </span><span class="s1">[</span>
        <span class="s1">custom_numeric(</span>
            <span class="s1">dtypes=[np.float16</span><span class="s3">, </span><span class="s1">dtypes.bfloat16</span><span class="s3">, </span><span class="s1">np.float32</span><span class="s3">, </span><span class="s1">np.float64]</span><span class="s3">,</span>
            <span class="s1">custom_assert=custom_assert</span><span class="s3">,</span>
            <span class="s1">description=(</span>
                <span class="s5">&quot;Produces different results when the array contains `inf` and `NaN`&quot;</span>
                <span class="s5">&quot; (they are sorted differently in TF vs. XLA).&quot;</span><span class="s1">))</span>
    <span class="s1">]</span>

  <span class="s1">@classmethod</span>
  <span class="s3">def </span><span class="s1">triangular_solve(cls</span><span class="s3">, </span><span class="s1">harness: primitive_harness.Harness):</span>
    <span class="s3">return </span><span class="s1">[</span>
        <span class="s1">missing_tf_kernel(</span>
            <span class="s1">dtypes=[dtypes.bfloat16]</span><span class="s3">,</span>
            <span class="s1">devices=(</span><span class="s5">&quot;gpu&quot;</span><span class="s3">, </span><span class="s5">&quot;cpu&quot;</span><span class="s1">)</span><span class="s3">,</span>
            <span class="s1">modes=(</span><span class="s5">&quot;eager&quot;</span><span class="s3">, </span><span class="s5">&quot;graph&quot;</span><span class="s1">))</span><span class="s3">,</span>
        <span class="s1">missing_tf_kernel(</span>
            <span class="s1">dtypes=[np.float16]</span><span class="s3">,</span>
            <span class="s1">devices=(</span><span class="s5">&quot;gpu&quot;</span><span class="s3">, </span><span class="s5">&quot;cpu&quot;</span><span class="s1">)</span><span class="s3">,</span>
            <span class="s1">modes=(</span><span class="s5">&quot;eager&quot;</span><span class="s3">, </span><span class="s5">&quot;graph&quot;</span><span class="s1">))</span><span class="s3">,</span>
        <span class="s1">custom_numeric(dtypes=[np.float32]</span><span class="s3">, </span><span class="s1">tol=</span><span class="s4">5e-3</span><span class="s1">)</span>
    <span class="s1">]</span>

  <span class="s1">@classmethod</span>
  <span class="s3">def </span><span class="s1">tridiagonal_solve(cls</span><span class="s3">, </span><span class="s1">harness: primitive_harness.Harness):</span>
    <span class="s3">return </span><span class="s1">[]</span>

<span class="s3">def </span><span class="s1">custom_numeric(</span>
    <span class="s1">*</span><span class="s3">,</span>
    <span class="s1">description=</span><span class="s5">&quot;custom numeric comparison&quot;</span><span class="s3">,</span>
    <span class="s1">dtypes=()</span><span class="s3">,  </span><span class="s0"># All</span>
    <span class="s1">modes=(</span>
        <span class="s5">&quot;eager&quot;</span><span class="s3">,</span>
        <span class="s5">&quot;graph&quot;</span><span class="s3">,</span>
    <span class="s1">)</span><span class="s3">,  </span><span class="s0"># By default we should not need tolerance for</span>
    <span class="s0"># &quot;compiled&quot;</span>
    <span class="s1">devices=(</span><span class="s5">&quot;cpu&quot;</span><span class="s3">, </span><span class="s5">&quot;gpu&quot;</span><span class="s3">, </span><span class="s5">&quot;tpu&quot;</span><span class="s1">)</span><span class="s3">,</span>
    <span class="s1">custom_assert=</span><span class="s3">None,</span>
    <span class="s1">enabled=</span><span class="s3">True,</span>
    <span class="s1">native_serialization=Jax2TfLimitation.FOR_NON_NATIVE</span><span class="s3">,</span>
    <span class="s1">tol=</span><span class="s3">None</span><span class="s1">) -&gt; Jax2TfLimitation:</span>

  <span class="s3">return </span><span class="s1">Jax2TfLimitation(</span>
      <span class="s1">description</span><span class="s3">,</span>
      <span class="s1">expect_tf_error=</span><span class="s3">False,</span>
      <span class="s1">dtypes=dtypes</span><span class="s3">,</span>
      <span class="s1">devices=devices</span><span class="s3">,</span>
      <span class="s1">modes=modes</span><span class="s3">,</span>
      <span class="s1">custom_assert=custom_assert</span><span class="s3">,</span>
      <span class="s1">enabled=enabled</span><span class="s3">,</span>
      <span class="s1">native_serialization=native_serialization</span><span class="s3">,</span>
      <span class="s1">tol=tol)</span>

<span class="s3">def </span><span class="s1">custom_random_keys_output():</span>
  <span class="s3">def </span><span class="s1">custom_assert(tst</span><span class="s3">, </span><span class="s1">result_jax</span><span class="s3">, </span><span class="s1">result_tf</span><span class="s3">, </span><span class="s1">*</span><span class="s3">, </span><span class="s1">args</span><span class="s3">, </span><span class="s1">tol</span><span class="s3">, </span><span class="s1">err_msg):</span>
    <span class="s0"># TODO(frostig): Don't need this conditional once we always</span>
    <span class="s0"># enable_custom_prng. We can even assert the isinstance instead.</span>
    <span class="s3">def </span><span class="s1">unwrap_keys(keys):</span>
      <span class="s3">if </span><span class="s1">isinstance(keys</span><span class="s3">, </span><span class="s1">jax.random.KeyArray):</span>
        <span class="s3">return </span><span class="s1">jax._src.prng.random_unwrap(keys)</span>
      <span class="s3">else</span><span class="s1">:</span>
        <span class="s3">return </span><span class="s1">keys</span>

    <span class="s1">tst.assertAllClose(unwrap_keys(result_jax)</span><span class="s3">, </span><span class="s1">result_tf</span><span class="s3">,</span>
                       <span class="s1">atol=tol</span><span class="s3">, </span><span class="s1">rtol=tol</span><span class="s3">, </span><span class="s1">err_msg=err_msg)</span>

  <span class="s3">return </span><span class="s1">custom_numeric(</span>
      <span class="s1">description=</span><span class="s5">&quot;Returns JAX key arrays, so compare underlying base array&quot;</span><span class="s3">,</span>
      <span class="s1">modes=(</span><span class="s5">&quot;eager&quot;</span><span class="s3">, </span><span class="s5">&quot;graph&quot;</span><span class="s3">, </span><span class="s5">&quot;compiled&quot;</span><span class="s1">)</span><span class="s3">,</span>
      <span class="s1">custom_assert=custom_assert)</span>


<span class="s3">def </span><span class="s1">missing_tf_kernel(*</span><span class="s3">,</span>
    <span class="s1">description=</span><span class="s5">&quot;op not defined for dtype&quot;</span><span class="s3">,</span>
    <span class="s1">dtypes</span><span class="s3">,</span>
    <span class="s1">modes=(</span><span class="s5">&quot;eager&quot;</span><span class="s3">, </span><span class="s5">&quot;graph&quot;</span><span class="s3">, </span><span class="s5">&quot;compiled&quot;</span><span class="s1">)</span><span class="s3">,</span>
    <span class="s1">devices=(</span><span class="s5">&quot;cpu&quot;</span><span class="s3">, </span><span class="s5">&quot;gpu&quot;</span><span class="s3">, </span><span class="s5">&quot;tpu&quot;</span><span class="s1">)</span><span class="s3">,</span>
    <span class="s1">native_serialization = Jax2TfLimitation.FOR_NON_NATIVE</span><span class="s3">,</span>
    <span class="s1">enabled=</span><span class="s3">True</span><span class="s1">) -&gt; Jax2TfLimitation:</span>

  <span class="s3">return </span><span class="s1">Jax2TfLimitation(</span>
      <span class="s1">description</span><span class="s3">, </span><span class="s1">dtypes=dtypes</span><span class="s3">, </span><span class="s1">devices=devices</span><span class="s3">, </span><span class="s1">modes=modes</span><span class="s3">, </span><span class="s1">enabled=enabled</span><span class="s3">,</span>
      <span class="s1">native_serialization=native_serialization)</span>
</pre>
</body>
</html>