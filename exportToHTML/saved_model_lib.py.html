<html>
<head>
<title>saved_model_lib.py</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<style type="text/css">
.s0 { color: #808080;}
.s1 { color: #a9b7c6;}
.s2 { color: #629755; font-style: italic;}
.s3 { color: #cc7832;}
.s4 { color: #6a8759;}
.s5 { color: #6897bb;}
</style>
</head>
<body bgcolor="#2b2b2b">
<table CELLSPACING=0 CELLPADDING=5 COLS=1 WIDTH="100%" BGCOLOR="#606060" >
<tr><td><center>
<font face="Arial, Helvetica" color="#000000">
saved_model_lib.py</font>
</center></td></tr></table>
<pre><span class="s0"># Copyright 2020 The JAX Authors.</span>
<span class="s0">#</span>
<span class="s0"># Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);</span>
<span class="s0"># you may not use this file except in compliance with the License.</span>
<span class="s0"># You may obtain a copy of the License at</span>
<span class="s0">#</span>
<span class="s0">#     https://www.apache.org/licenses/LICENSE-2.0</span>
<span class="s0">#</span>
<span class="s0"># Unless required by applicable law or agreed to in writing, software</span>
<span class="s0"># distributed under the License is distributed on an &quot;AS IS&quot; BASIS,</span>
<span class="s0"># WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span>
<span class="s0"># See the License for the specific language governing permissions and</span>
<span class="s0"># limitations under the License.</span>
<span class="s2">&quot;&quot;&quot;Defines a helper function for creating a SavedModel from a jax2tf trained model. 
 
This has been tested with TensorFlow Hub, TensorFlow JavaScript, 
and TensorFlow Serving. 
 
Note that the code in this file is provided only as an example. The functions 
generated by `jax2tf.convert` are standard TensorFlow functions and you can 
save them in a SavedModel using standard TensorFlow code. This decoupling 
of jax2tf from SavedModel is important, because it allows the user to have full 
control over what metadata is saved in the SavedModel. Please copy and 
customize this function as needed. 
&quot;&quot;&quot;</span>

<span class="s3">from </span><span class="s1">typing </span><span class="s3">import </span><span class="s1">Any</span><span class="s3">, </span><span class="s1">Callable</span><span class="s3">, </span><span class="s1">Sequence</span><span class="s3">, </span><span class="s1">Optional</span><span class="s3">, </span><span class="s1">Union</span>

<span class="s3">from </span><span class="s1">jax.experimental </span><span class="s3">import </span><span class="s1">jax2tf  </span><span class="s0"># type: ignore[import]</span>
<span class="s3">import </span><span class="s1">tensorflow </span><span class="s3">as </span><span class="s1">tf  </span><span class="s0"># type: ignore[import]</span>


<span class="s3">def </span><span class="s1">convert_and_save_model(</span>
    <span class="s1">jax_fn: Callable[[Any</span><span class="s3">, </span><span class="s1">Any]</span><span class="s3">, </span><span class="s1">Any]</span><span class="s3">,</span>
    <span class="s1">params</span><span class="s3">,</span>
    <span class="s1">model_dir: str</span><span class="s3">,</span>
    <span class="s1">*</span><span class="s3">,</span>
    <span class="s1">input_signatures: Sequence[tf.TensorSpec]</span><span class="s3">,</span>
    <span class="s1">polymorphic_shapes: Optional[Union[str</span><span class="s3">, </span><span class="s1">jax2tf.PolyShape]] = </span><span class="s3">None,</span>
    <span class="s1">with_gradient: bool = </span><span class="s3">False,</span>
    <span class="s1">enable_xla: bool = </span><span class="s3">True,</span>
    <span class="s1">compile_model: bool = </span><span class="s3">True,</span>
    <span class="s1">saved_model_options: Optional[tf.saved_model.SaveOptions] = </span><span class="s3">None</span><span class="s1">):</span>
  <span class="s2">&quot;&quot;&quot;Convert a JAX function and saves a SavedModel. 
 
  This is an example, we do not promise backwards compatibility for this code. 
  For serious uses, please copy and and expand it as needed (see note at the top 
  of the module). 
 
  Use this function if you have a trained ML model that has both a prediction 
  function and trained parameters, which you want to save separately from the 
  function graph as variables (e.g., to avoid limits on the size of the 
  GraphDef, or to enable fine-tuning.) If you don't have such parameters, 
  you can still use this library function but probably don't need it 
  (see jax2tf/README.md for some simple examples). 
 
  In order to use this wrapper you must first convert your model to a function 
  with two arguments: the parameters and the input on which you want to do 
  inference. Both arguments may be np.ndarray or (nested) 
  tuples/lists/dictionaries thereof. 
 
  See the README.md for a discussion of how to prepare Flax and Haiku models. 
 
  Args: 
    jax_fn: a JAX function taking two arguments, the parameters and the inputs. 
      Both arguments may be (nested) tuples/lists/dictionaries of np.ndarray. 
    params: the parameters, to be used as first argument for `jax_fn`. These 
      must be (nested) tuples/lists/dictionaries of np.ndarray, and will be 
      saved as the variables of the SavedModel. 
    model_dir: the directory where the model should be saved. 
    input_signatures: the input signatures for the second argument of `jax_fn` 
      (the input). A signature must be a `tensorflow.TensorSpec` instance, or a 
      (nested) tuple/list/dictionary thereof with a structure matching the 
      second argument of `jax_fn`. The first input_signature will be saved as 
      the default serving signature. The additional signatures will be used 
      only to ensure that the `jax_fn` is traced and converted to TF for the 
      corresponding input shapes. 
    with_gradient: the value to use for the `with_gradient` parameter for 
      `jax2tf.convert`. 
    enable_xla: the value to use for the `enable_xla` parameter for 
      `jax2tf.convert`. 
    compile_model: use TensorFlow jit_compiler on the SavedModel. This 
      is needed if the SavedModel will be used for TensorFlow serving. 
    polymorphic_shapes: if given then it will be used as the 
      `polymorphic_shapes` argument to jax2tf.convert for the second parameter of 
      `jax_fn`. In this case, a single `input_signatures` is supported, and 
      should have `None` in the polymorphic dimensions. 
    saved_model_options: options to pass to savedmodel.save. 
  &quot;&quot;&quot;</span>
  <span class="s3">if not </span><span class="s1">input_signatures:</span>
    <span class="s3">raise </span><span class="s1">ValueError(</span><span class="s4">&quot;At least one input_signature must be given&quot;</span><span class="s1">)</span>
  <span class="s3">if </span><span class="s1">polymorphic_shapes </span><span class="s3">is not None</span><span class="s1">:</span>
    <span class="s3">if </span><span class="s1">len(input_signatures) &gt; </span><span class="s5">1</span><span class="s1">:</span>
      <span class="s3">raise </span><span class="s1">ValueError(</span><span class="s4">&quot;For shape-polymorphic conversion a single &quot;</span>
                       <span class="s4">&quot;input_signature is supported.&quot;</span><span class="s1">)</span>
  <span class="s1">tf_fn = jax2tf.convert(</span>
    <span class="s1">jax_fn</span><span class="s3">,</span>
    <span class="s1">with_gradient=with_gradient</span><span class="s3">,</span>
    <span class="s1">polymorphic_shapes=[</span><span class="s3">None, </span><span class="s1">polymorphic_shapes]</span><span class="s3">,</span>
    <span class="s1">enable_xla=enable_xla)</span>

  <span class="s0"># Create tf.Variables for the parameters. If you want more useful variable</span>
  <span class="s0"># names, you can use `tree.map_structure_with_path` from the `dm-tree` package</span>
  <span class="s1">param_vars = tf.nest.map_structure(</span>
    <span class="s3">lambda </span><span class="s1">param: tf.Variable(param</span><span class="s3">, </span><span class="s1">trainable=with_gradient)</span><span class="s3">,</span>
    <span class="s1">params)</span>
  <span class="s1">tf_graph = tf.function(</span><span class="s3">lambda </span><span class="s1">inputs: tf_fn(param_vars</span><span class="s3">, </span><span class="s1">inputs)</span><span class="s3">,</span>
                         <span class="s1">autograph=</span><span class="s3">False,</span>
                         <span class="s1">jit_compile=compile_model)</span>

  <span class="s1">signatures = {}</span>
  <span class="s0"># This signature is needed for TensorFlow Serving use.</span>
  <span class="s1">signatures[tf.saved_model.DEFAULT_SERVING_SIGNATURE_DEF_KEY] = \</span>
    <span class="s1">tf_graph.get_concrete_function(input_signatures[</span><span class="s5">0</span><span class="s1">])</span>
  <span class="s3">for </span><span class="s1">input_signature </span><span class="s3">in </span><span class="s1">input_signatures[</span><span class="s5">1</span><span class="s1">:]:</span>
    <span class="s0"># If there are more signatures, trace and cache a TF function for each one</span>
    <span class="s1">tf_graph.get_concrete_function(input_signature)</span>
  <span class="s1">wrapper = _ReusableSavedModelWrapper(tf_graph</span><span class="s3">, </span><span class="s1">param_vars)</span>
  <span class="s3">if </span><span class="s1">with_gradient:</span>
    <span class="s3">if not </span><span class="s1">saved_model_options:</span>
      <span class="s1">saved_model_options = tf.saved_model.SaveOptions(experimental_custom_gradients=</span><span class="s3">True</span><span class="s1">)</span>
    <span class="s3">else</span><span class="s1">:</span>
      <span class="s1">saved_model_options.experimental_custom_gradients = </span><span class="s3">True</span>
  <span class="s1">tf.saved_model.save(wrapper</span><span class="s3">, </span><span class="s1">model_dir</span><span class="s3">, </span><span class="s1">signatures=signatures</span><span class="s3">,</span>
                      <span class="s1">options=saved_model_options)</span>


<span class="s3">class </span><span class="s1">_ReusableSavedModelWrapper(tf.train.Checkpoint):</span>
  <span class="s2">&quot;&quot;&quot;Wraps a function and its parameters for saving to a SavedModel. 
 
  Implements the interface described at 
  https://www.tensorflow.org/hub/reusable_saved_models. 
  &quot;&quot;&quot;</span>

  <span class="s3">def </span><span class="s1">__init__(self</span><span class="s3">, </span><span class="s1">tf_graph</span><span class="s3">, </span><span class="s1">param_vars):</span>
    <span class="s2">&quot;&quot;&quot;Args: 
 
      tf_graph: a tf.function taking one argument (the inputs), which can be 
         be tuples/lists/dictionaries of np.ndarray or tensors. The function 
         may have references to the tf.Variables in `param_vars`. 
      param_vars: the parameters, as tuples/lists/dictionaries of tf.Variable, 
         to be saved as the variables of the SavedModel. 
    &quot;&quot;&quot;</span>
    <span class="s1">super().__init__()</span>
    <span class="s0"># Implement the interface from https://www.tensorflow.org/hub/reusable_saved_models</span>
    <span class="s1">self.variables = tf.nest.flatten(param_vars)</span>
    <span class="s1">self.trainable_variables = [v </span><span class="s3">for </span><span class="s1">v </span><span class="s3">in </span><span class="s1">self.variables </span><span class="s3">if </span><span class="s1">v.trainable]</span>
    <span class="s0"># If you intend to prescribe regularization terms for users of the model,</span>
    <span class="s0"># add them as @tf.functions with no inputs to this list. Else drop this.</span>
    <span class="s1">self.regularization_losses = []</span>
    <span class="s1">self.__call__ = tf_graph</span>
</pre>
</body>
</html>