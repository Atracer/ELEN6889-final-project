<html>
<head>
<title>lax.py</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<style type="text/css">
.s0 { color: #808080;}
.s1 { color: #a9b7c6;}
.s2 { color: #cc7832;}
.s3 { color: #6a8759;}
.s4 { color: #6897bb;}
.s5 { color: #629755; font-style: italic;}
</style>
</head>
<body bgcolor="#2b2b2b">
<table CELLSPACING=0 CELLPADDING=5 COLS=1 WIDTH="100%" BGCOLOR="#606060" >
<tr><td><center>
<font face="Arial, Helvetica" color="#000000">
lax.py</font>
</center></td></tr></table>
<pre><span class="s0"># Copyright 2018 The JAX Authors.</span>
<span class="s0">#</span>
<span class="s0"># Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);</span>
<span class="s0"># you may not use this file except in compliance with the License.</span>
<span class="s0"># You may obtain a copy of the License at</span>
<span class="s0">#</span>
<span class="s0">#     https://www.apache.org/licenses/LICENSE-2.0</span>
<span class="s0">#</span>
<span class="s0"># Unless required by applicable law or agreed to in writing, software</span>
<span class="s0"># distributed under the License is distributed on an &quot;AS IS&quot; BASIS,</span>
<span class="s0"># WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span>
<span class="s0"># See the License for the specific language governing permissions and</span>
<span class="s0"># limitations under the License.</span>

<span class="s2">import </span><span class="s1">builtins</span>
<span class="s2">import </span><span class="s1">enum</span>
<span class="s2">import </span><span class="s1">functools</span>
<span class="s2">from </span><span class="s1">functools </span><span class="s2">import </span><span class="s1">partial</span>
<span class="s2">import </span><span class="s1">itertools</span>
<span class="s2">import </span><span class="s1">math</span>
<span class="s2">import </span><span class="s1">operator</span>
<span class="s2">from </span><span class="s1">typing </span><span class="s2">import </span><span class="s1">(Any</span><span class="s2">, </span><span class="s1">Callable</span><span class="s2">, </span><span class="s1">Optional</span><span class="s2">, </span><span class="s1">Sequence</span><span class="s2">, </span><span class="s1">Tuple</span><span class="s2">, </span><span class="s1">List</span><span class="s2">,</span>
                    <span class="s1">TypeVar</span><span class="s2">, </span><span class="s1">Union</span><span class="s2">, </span><span class="s1">cast </span><span class="s2">as </span><span class="s1">type_cast</span><span class="s2">, </span><span class="s1">overload)</span>
<span class="s2">import </span><span class="s1">warnings</span>

<span class="s2">import </span><span class="s1">numpy </span><span class="s2">as </span><span class="s1">np</span>

<span class="s2">import </span><span class="s1">jax</span>
<span class="s2">from </span><span class="s1">jax </span><span class="s2">import </span><span class="s1">tree_util</span>
<span class="s2">from </span><span class="s1">jax.tree_util </span><span class="s2">import </span><span class="s1">tree_map</span>

<span class="s2">from </span><span class="s1">jax._src </span><span class="s2">import </span><span class="s1">ad_util</span>
<span class="s2">from </span><span class="s1">jax._src </span><span class="s2">import </span><span class="s1">api</span>
<span class="s2">from </span><span class="s1">jax._src </span><span class="s2">import </span><span class="s1">api_util</span>
<span class="s2">from </span><span class="s1">jax._src </span><span class="s2">import </span><span class="s1">array</span>
<span class="s2">from </span><span class="s1">jax._src </span><span class="s2">import </span><span class="s1">core</span>
<span class="s2">from </span><span class="s1">jax._src </span><span class="s2">import </span><span class="s1">device_array</span>
<span class="s2">from </span><span class="s1">jax._src </span><span class="s2">import </span><span class="s1">dispatch</span>
<span class="s2">from </span><span class="s1">jax._src </span><span class="s2">import </span><span class="s1">dtypes</span>
<span class="s2">from </span><span class="s1">jax._src </span><span class="s2">import </span><span class="s1">effects</span>
<span class="s2">from </span><span class="s1">jax._src </span><span class="s2">import </span><span class="s1">linear_util </span><span class="s2">as </span><span class="s1">lu</span>
<span class="s2">from </span><span class="s1">jax._src </span><span class="s2">import </span><span class="s1">pretty_printer </span><span class="s2">as </span><span class="s1">pp</span>
<span class="s2">from </span><span class="s1">jax._src </span><span class="s2">import </span><span class="s1">source_info_util</span>
<span class="s2">from </span><span class="s1">jax._src </span><span class="s2">import </span><span class="s1">util</span>
<span class="s2">from </span><span class="s1">jax._src.abstract_arrays </span><span class="s2">import </span><span class="s1">array_types</span>
<span class="s2">from </span><span class="s1">jax._src.config </span><span class="s2">import </span><span class="s1">config</span>
<span class="s2">from </span><span class="s1">jax._src.core </span><span class="s2">import </span><span class="s1">(Primitive</span><span class="s2">, </span><span class="s1">UnshapedArray</span><span class="s2">, </span><span class="s1">ShapedArray</span><span class="s2">, </span><span class="s1">ConcreteArray</span><span class="s2">,</span>
                           <span class="s1">raise_to_shaped</span><span class="s2">, </span><span class="s1">abstract_token</span><span class="s2">, </span><span class="s1">canonicalize_shape)</span>
<span class="s2">from </span><span class="s1">jax._src.interpreters </span><span class="s2">import </span><span class="s1">ad</span>
<span class="s2">from </span><span class="s1">jax._src.interpreters </span><span class="s2">import </span><span class="s1">batching</span>
<span class="s2">from </span><span class="s1">jax._src.interpreters </span><span class="s2">import </span><span class="s1">mlir</span>
<span class="s2">from </span><span class="s1">jax._src.interpreters </span><span class="s2">import </span><span class="s1">partial_eval </span><span class="s2">as </span><span class="s1">pe</span>
<span class="s2">from </span><span class="s1">jax._src.interpreters </span><span class="s2">import </span><span class="s1">pxla</span>
<span class="s2">from </span><span class="s1">jax._src.interpreters </span><span class="s2">import </span><span class="s1">xla</span>
<span class="s2">from </span><span class="s1">jax._src.interpreters.batching </span><span class="s2">import </span><span class="s1">ConcatAxis</span>
<span class="s2">from </span><span class="s1">jax._src.lax </span><span class="s2">import </span><span class="s1">slicing</span>
<span class="s2">from </span><span class="s1">jax._src.lax.utils </span><span class="s2">import </span><span class="s1">(</span>
  <span class="s1">_input_dtype</span><span class="s2">,</span>
  <span class="s1">standard_abstract_eval</span><span class="s2">,</span>
  <span class="s1">standard_multi_result_abstract_eval</span><span class="s2">,</span>
  <span class="s1">standard_named_shape_rule</span><span class="s2">,</span>
  <span class="s1">standard_primitive</span><span class="s2">,</span>
<span class="s1">)</span>
<span class="s2">from </span><span class="s1">jax._src.lib </span><span class="s2">import </span><span class="s1">pytree</span>
<span class="s2">from </span><span class="s1">jax._src </span><span class="s2">import </span><span class="s1">xla_bridge</span>
<span class="s2">from </span><span class="s1">jax._src.lib </span><span class="s2">import </span><span class="s1">xla_client</span>
<span class="s2">from </span><span class="s1">jax._src.lib.mlir </span><span class="s2">import </span><span class="s1">ir</span>
<span class="s2">from </span><span class="s1">jax._src.lib.mlir.dialects </span><span class="s2">import </span><span class="s1">chlo</span>
<span class="s2">from </span><span class="s1">jax._src.lib.mlir.dialects </span><span class="s2">import </span><span class="s1">hlo</span>
<span class="s2">from </span><span class="s1">jax._src.sharding_impls </span><span class="s2">import </span><span class="s1">PmapSharding</span>
<span class="s2">from </span><span class="s1">jax._src.typing </span><span class="s2">import </span><span class="s1">Array</span><span class="s2">, </span><span class="s1">ArrayLike</span><span class="s2">, </span><span class="s1">DTypeLike</span><span class="s2">, </span><span class="s1">Shape</span>
<span class="s2">from </span><span class="s1">jax._src.util </span><span class="s2">import </span><span class="s1">(cache</span><span class="s2">, </span><span class="s1">safe_zip</span><span class="s2">, </span><span class="s1">safe_map</span><span class="s2">, </span><span class="s1">canonicalize_axis</span><span class="s2">,</span>
                           <span class="s1">split_list)</span>

<span class="s1">xb = xla_bridge</span>
<span class="s1">xc = xla_client</span>
<span class="s1">xops = xla_client.ops</span>
<span class="s1">xe = xla_client._xla</span>

<span class="s1">_max = builtins.max</span>
<span class="s1">_min = builtins.min</span>
<span class="s1">_reduce = functools.reduce</span>

<span class="s1">T = TypeVar(</span><span class="s3">&quot;T&quot;</span><span class="s1">)</span>

<span class="s1">map</span><span class="s2">, </span><span class="s1">unsafe_map = safe_map</span><span class="s2">, </span><span class="s1">map</span>
<span class="s1">zip</span><span class="s2">, </span><span class="s1">unsafe_zip = safe_zip</span><span class="s2">, </span><span class="s1">zip</span>

<span class="s2">def </span><span class="s1">_validate_shapes(shapes: Sequence[Shape]):</span>
  <span class="s2">def </span><span class="s1">_check_static_shape(shape: Shape):</span>
    <span class="s1">checked = canonicalize_shape(shape)</span>
    <span class="s2">if not </span><span class="s1">all(idx &gt;= </span><span class="s4">0 </span><span class="s2">for </span><span class="s1">idx </span><span class="s2">in </span><span class="s1">checked):</span>
      <span class="s1">msg = </span><span class="s3">f&quot;Only non-negative indices are allowed when broadcasting&quot; </span><span class="s1">\</span>
            <span class="s3">f&quot; static shapes, but got shape </span><span class="s2">{</span><span class="s1">shape</span><span class="s2">!r}</span><span class="s3">.&quot;</span>
      <span class="s2">raise </span><span class="s1">TypeError(msg)</span>

  <span class="s2">assert </span><span class="s1">shapes</span>
  <span class="s2">if </span><span class="s1">config.jax_dynamic_shapes:</span>
    <span class="s0"># pass dynamic shapes through unchecked</span>
    <span class="s2">return</span>
  <span class="s2">else</span><span class="s1">:</span>
    <span class="s1">map(_check_static_shape</span><span class="s2">, </span><span class="s1">shapes)</span>

<span class="s2">def </span><span class="s1">_try_broadcast_shapes(</span>
    <span class="s1">shapes: Sequence[Tuple[int</span><span class="s2">, </span><span class="s1">...]]) -&gt; Optional[Tuple[int</span><span class="s2">, </span><span class="s1">...]]:</span>
  <span class="s2">if </span><span class="s1">len(shapes) == </span><span class="s4">1</span><span class="s1">: </span><span class="s2">return </span><span class="s1">shapes[</span><span class="s4">0</span><span class="s1">]</span>
  <span class="s1">rank</span><span class="s2">, </span><span class="s1">*others = {len(shape) </span><span class="s2">for </span><span class="s1">shape </span><span class="s2">in </span><span class="s1">shapes}</span>
  <span class="s2">if </span><span class="s1">others: </span><span class="s2">return None  </span><span class="s0"># must have consistent rank</span>
  <span class="s2">if not </span><span class="s1">rank: </span><span class="s2">return </span><span class="s1">()  </span><span class="s0"># scalar case</span>
  <span class="s1">result_shape = []</span>
  <span class="s2">for </span><span class="s1">ds </span><span class="s2">in </span><span class="s1">unsafe_zip(*shapes):</span>
    <span class="s2">if </span><span class="s1">all(core.same_referent(d</span><span class="s2">, </span><span class="s1">ds[</span><span class="s4">0</span><span class="s1">]) </span><span class="s2">for </span><span class="s1">d </span><span class="s2">in </span><span class="s1">ds[</span><span class="s4">1</span><span class="s1">:]):</span>
      <span class="s0"># if all axes are identical objects, the resulting size is the object</span>
      <span class="s1">result_shape.append(ds[</span><span class="s4">0</span><span class="s1">])</span>
    <span class="s2">else</span><span class="s1">:</span>
      <span class="s0"># if all dims are equal (or 1), the result is the non-1 size (or 1)</span>
      <span class="s1">non_1s = [d </span><span class="s2">for </span><span class="s1">d </span><span class="s2">in </span><span class="s1">ds </span><span class="s2">if not </span><span class="s1">core.symbolic_equal_dim(d</span><span class="s2">, </span><span class="s4">1</span><span class="s1">)]</span>
      <span class="s2">if not </span><span class="s1">non_1s:</span>
        <span class="s1">result_shape.append(</span><span class="s4">1</span><span class="s1">)</span>
      <span class="s2">elif </span><span class="s1">all(core.symbolic_equal_dim(non_1s[</span><span class="s4">0</span><span class="s1">]</span><span class="s2">, </span><span class="s1">d) </span><span class="s2">for </span><span class="s1">d </span><span class="s2">in </span><span class="s1">non_1s[</span><span class="s4">1</span><span class="s1">:]):</span>
        <span class="s1">result_shape.append(non_1s[</span><span class="s4">0</span><span class="s1">])</span>
      <span class="s2">else</span><span class="s1">:</span>
        <span class="s2">return None</span>
  <span class="s2">return </span><span class="s1">tuple(result_shape)</span>

<span class="s1">@overload</span>
<span class="s2">def </span><span class="s1">broadcast_shapes(*shapes: Tuple[int</span><span class="s2">, </span><span class="s1">...]) -&gt; Tuple[int</span><span class="s2">, </span><span class="s1">...]: ...</span>

<span class="s1">@overload</span>
<span class="s2">def </span><span class="s1">broadcast_shapes(*shapes: Tuple[Union[int</span><span class="s2">, </span><span class="s1">core.Tracer]</span><span class="s2">, </span><span class="s1">...]</span>
                     <span class="s1">) -&gt; Tuple[Union[int</span><span class="s2">, </span><span class="s1">core.Tracer]</span><span class="s2">, </span><span class="s1">...]: ...</span>

<span class="s2">def </span><span class="s1">broadcast_shapes(*shapes):</span>
  <span class="s5">&quot;&quot;&quot;Returns the shape that results from NumPy broadcasting of `shapes`.&quot;&quot;&quot;</span>
  <span class="s0"># NOTE: We have both cached and uncached versions to handle Tracers in shapes.</span>
  <span class="s2">try</span><span class="s1">:</span>
    <span class="s2">return </span><span class="s1">_broadcast_shapes_cached(*shapes)</span>
  <span class="s2">except</span><span class="s1">:</span>
    <span class="s2">return </span><span class="s1">_broadcast_shapes_uncached(*shapes)</span>

<span class="s1">@cache()</span>
<span class="s2">def </span><span class="s1">_broadcast_shapes_cached(*shapes: Tuple[int</span><span class="s2">, </span><span class="s1">...]) -&gt; Tuple[int</span><span class="s2">, </span><span class="s1">...]:</span>
  <span class="s2">return </span><span class="s1">_broadcast_shapes_uncached(*shapes)</span>

<span class="s2">def </span><span class="s1">_broadcast_shapes_uncached(*shapes):</span>
  <span class="s1">_validate_shapes(shapes)</span>
  <span class="s1">fst</span><span class="s2">, </span><span class="s1">*rst = shapes</span>
  <span class="s2">if not </span><span class="s1">rst: </span><span class="s2">return </span><span class="s1">fst</span>

  <span class="s0"># First check if we need only rank promotion (and not singleton-broadcasting).</span>
  <span class="s2">try</span><span class="s1">: </span><span class="s2">return </span><span class="s1">_reduce(_broadcast_ranks</span><span class="s2">, </span><span class="s1">rst</span><span class="s2">, </span><span class="s1">fst)</span>
  <span class="s2">except </span><span class="s1">ValueError: </span><span class="s2">pass</span>

  <span class="s0"># Next try singleton-broadcasting, padding out ranks using singletons.</span>
  <span class="s1">ndim = _max(len(shape) </span><span class="s2">for </span><span class="s1">shape </span><span class="s2">in </span><span class="s1">shapes)</span>
  <span class="s1">shape_list = [(</span><span class="s4">1</span><span class="s2">,</span><span class="s1">) * (ndim - len(shape)) + shape </span><span class="s2">for </span><span class="s1">shape </span><span class="s2">in </span><span class="s1">shapes]</span>
  <span class="s1">result_shape = _try_broadcast_shapes(shape_list)</span>
  <span class="s2">if </span><span class="s1">result_shape </span><span class="s2">is None</span><span class="s1">:</span>
    <span class="s2">raise </span><span class="s1">ValueError(</span><span class="s3">f&quot;Incompatible shapes for broadcasting: shapes=</span><span class="s2">{</span><span class="s1">list(shapes)</span><span class="s2">}</span><span class="s3">&quot;</span><span class="s1">)</span>
  <span class="s2">return </span><span class="s1">result_shape</span>

<span class="s2">def </span><span class="s1">_broadcast_ranks(s1</span><span class="s2">, </span><span class="s1">s2):</span>
  <span class="s2">if </span><span class="s1">len(s1) &gt; len(s2):</span>
    <span class="s1">s1</span><span class="s2">, </span><span class="s1">s2 = s2</span><span class="s2">, </span><span class="s1">s1</span>
  <span class="s2">assert </span><span class="s1">len(s1) &lt;= len(s2)</span>
  <span class="s1">s1_ = s2[len(s2) - len(s1):]</span>
  <span class="s2">if </span><span class="s1">core.symbolic_equal_shape(s1_</span><span class="s2">, </span><span class="s1">s1): </span><span class="s2">return </span><span class="s1">s2</span>
  <span class="s2">else</span><span class="s1">: </span><span class="s2">raise </span><span class="s1">ValueError</span>

<span class="s2">def </span><span class="s1">_identity(x): </span><span class="s2">return </span><span class="s1">x</span>

<span class="s2">def </span><span class="s1">_extract_tracers_dyn_shape(</span>
    <span class="s1">shape: Sequence[Union[int</span><span class="s2">, </span><span class="s1">core.Tracer]]</span>
  <span class="s1">) -&gt; Tuple[List[core.Tracer]</span><span class="s2">, </span><span class="s1">List[Optional[int]]]:</span>
  <span class="s0"># Given a sequence representing a shape, pull out Tracers, replacing with None</span>
  <span class="s2">if </span><span class="s1">config.jax_dynamic_shapes:</span>
    <span class="s0"># We must gate this behavior under a flag because otherwise the errors</span>
    <span class="s0"># raised are different (and have worse source provenance information).</span>
    <span class="s1">dyn_shape = [d </span><span class="s2">for </span><span class="s1">d </span><span class="s2">in </span><span class="s1">shape </span><span class="s2">if </span><span class="s1">isinstance(d</span><span class="s2">, </span><span class="s1">core.Tracer)]</span>
    <span class="s1">static_shape = [</span><span class="s2">None if </span><span class="s1">isinstance(d</span><span class="s2">, </span><span class="s1">core.Tracer) </span><span class="s2">else </span><span class="s1">d </span><span class="s2">for </span><span class="s1">d </span><span class="s2">in </span><span class="s1">shape]</span>
    <span class="s2">return </span><span class="s1">dyn_shape</span><span class="s2">, </span><span class="s1">static_shape</span>
  <span class="s2">else</span><span class="s1">:</span>
    <span class="s2">return </span><span class="s1">[]</span><span class="s2">, </span><span class="s1">list(shape)  </span><span class="s0"># type: ignore</span>

<span class="s2">def </span><span class="s1">_merge_dyn_shape(</span>
    <span class="s1">static_shape: Sequence[Optional[int]]</span><span class="s2">,</span>
    <span class="s1">dyn_shape: Sequence[Any]</span><span class="s2">,</span>
  <span class="s1">) -&gt; Tuple[Union[int</span><span class="s2">, </span><span class="s1">mlir.Value</span><span class="s2">, </span><span class="s1">core.Tracer]</span><span class="s2">, </span><span class="s1">...]:</span>
  <span class="s0"># Replace Nones in static_shape with elements of dyn_shape, in order</span>
  <span class="s1">dyn_shape_it = iter(dyn_shape)</span>
  <span class="s1">shape = tuple(next(dyn_shape_it) </span><span class="s2">if </span><span class="s1">d </span><span class="s2">is None else </span><span class="s1">d </span><span class="s2">for </span><span class="s1">d </span><span class="s2">in </span><span class="s1">static_shape)</span>
  <span class="s2">assert </span><span class="s1">next(dyn_shape_it</span><span class="s2">, None</span><span class="s1">) </span><span class="s2">is None</span>
  <span class="s2">return </span><span class="s1">shape</span>

<span class="s2">def </span><span class="s1">_dyn_shape_staging_rule(trace</span><span class="s2">, </span><span class="s1">prim</span><span class="s2">, </span><span class="s1">out_aval</span><span class="s2">, </span><span class="s1">*args</span><span class="s2">, </span><span class="s1">**params):</span>
  <span class="s1">source_info = source_info_util.current()</span>
  <span class="s1">out_tracer = pe.DynamicJaxprTracer(trace</span><span class="s2">, </span><span class="s1">out_aval</span><span class="s2">, </span><span class="s1">source_info)</span>
  <span class="s1">eqn = pe.new_jaxpr_eqn([trace.getvar(x) </span><span class="s2">for </span><span class="s1">x </span><span class="s2">in </span><span class="s1">args]</span><span class="s2">,</span>
                         <span class="s1">[trace.makevar(out_tracer)]</span><span class="s2">,</span>
                         <span class="s1">prim</span><span class="s2">, </span><span class="s1">params</span><span class="s2">, </span><span class="s1">core.no_effects</span><span class="s2">, </span><span class="s1">source_info)</span>
  <span class="s1">trace.frame.add_eqn(eqn)</span>
  <span class="s2">return </span><span class="s1">out_tracer</span>


<span class="s0">### traceables</span>

<span class="s2">def </span><span class="s1">neg(x: ArrayLike) -&gt; Array:</span>
  <span class="s5">r&quot;&quot;&quot;Elementwise negation: :math:`-x`.&quot;&quot;&quot;</span>
  <span class="s2">return </span><span class="s1">neg_p.bind(x)</span>

<span class="s2">def </span><span class="s1">sign(x: ArrayLike) -&gt; Array:</span>
  <span class="s5">r&quot;&quot;&quot;Elementwise sign. 
 
  For floating-point inputs, returns 
  :math:`\mathrm{sign}(x) = \begin{cases} 
  -1 &amp; x &lt; 0\\ 
  -0 &amp; x = -0\\ 
  \mathit{NaN} &amp; x = \mathit{NaN}\\ 
  +0 &amp; x = +0\\ 
  1 &amp; x &gt; 0 
  \end{cases}` 
 
  For signed integer inputs, returns 
  :math:`\mathrm{sign}(x) = \begin{cases} 
  -1 &amp; x &lt; 0\\ 
  0 &amp; x = 0\\ 
  1 &amp; x &gt; 0 
  \end{cases}` 
 
  For complex inputs, returns the complex phase, i.e. 
  :math:`\mathrm{sign}(x) = \frac{x}{|x|}`. 
  &quot;&quot;&quot;</span>
  <span class="s2">return </span><span class="s1">sign_p.bind(x)</span>

<span class="s2">def </span><span class="s1">nextafter(x1: ArrayLike</span><span class="s2">, </span><span class="s1">x2: ArrayLike) -&gt; Array:</span>
  <span class="s5">r&quot;&quot;&quot;Returns the next representable value after `x1` in the direction of `x2`. 
 
  Note that in some environments flush-denormal-to-zero semantics is used. 
  This means that, around zero, this function returns strictly non-zero 
  values which appear as zero in any operations. Consider this example:: 
 
    &gt;&gt;&gt; jnp.nextafter(0, 1)  # denormal numbers are representable 
    Array(1.e-45, dtype=float32, weak_type=True) 
    &gt;&gt;&gt; jnp.nextafter(0, 1) * 1  # but are flushed to zero 
    Array(0., dtype=float32, weak_type=True) 
 
  For the smallest usable (i.e. normal) float, use ``tiny`` of ``jnp.finfo``. 
  &quot;&quot;&quot;</span>
  <span class="s2">return </span><span class="s1">nextafter_p.bind(x1</span><span class="s2">, </span><span class="s1">x2)</span>

<span class="s2">def </span><span class="s1">floor(x: ArrayLike) -&gt; Array:</span>
  <span class="s5">r&quot;&quot;&quot;Elementwise floor: :math:`\left\lfloor x \right\rfloor`.&quot;&quot;&quot;</span>
  <span class="s2">return </span><span class="s1">floor_p.bind(x)</span>

<span class="s2">def </span><span class="s1">ceil(x: ArrayLike) -&gt; Array:</span>
  <span class="s5">r&quot;&quot;&quot;Elementwise ceiling: :math:`\left\lceil x \right\rceil`.&quot;&quot;&quot;</span>
  <span class="s2">return </span><span class="s1">ceil_p.bind(x)</span>

<span class="s2">class </span><span class="s1">RoundingMethod(enum.IntEnum):</span>
  <span class="s1">AWAY_FROM_ZERO = </span><span class="s4">0</span>
  <span class="s1">TO_NEAREST_EVEN = </span><span class="s4">1</span>

<span class="s2">def </span><span class="s1">round(x: ArrayLike</span><span class="s2">,</span>
          <span class="s1">rounding_method: RoundingMethod = RoundingMethod.AWAY_FROM_ZERO</span>
          <span class="s1">) -&gt; Array:</span>
  <span class="s5">r&quot;&quot;&quot;Elementwise round. 
 
  Rounds values to the nearest integer. 
 
  Args: 
    x: an array or scalar value to round. 
    rounding_method: the method to use when rounding halfway values 
      (e.g., `0.5`). See ``lax.RoundingMethod`` for the list of possible 
      values. 
 
  Returns: 
    An array containing the elementwise rounding of x. 
  &quot;&quot;&quot;</span>
  <span class="s1">rounding_method = RoundingMethod(rounding_method)</span>
  <span class="s2">return </span><span class="s1">round_p.bind(x</span><span class="s2">, </span><span class="s1">rounding_method=rounding_method)</span>

<span class="s2">def </span><span class="s1">is_finite(x: ArrayLike) -&gt; Array:</span>
  <span class="s5">r&quot;&quot;&quot;Elementwise :math:`\mathrm{isfinite}`. 
 
  For each element x returns `True` if and only if x is not :math:`\pm\infty` or 
  :math:`\mathit{NaN}`. 
  &quot;&quot;&quot;</span>
  <span class="s2">return </span><span class="s1">is_finite_p.bind(x)</span>

<span class="s2">def </span><span class="s1">exp(x: ArrayLike) -&gt; Array:</span>
  <span class="s5">r&quot;&quot;&quot;Elementwise exponential: :math:`e^x`.&quot;&quot;&quot;</span>
  <span class="s2">return </span><span class="s1">exp_p.bind(x)</span>

<span class="s2">def </span><span class="s1">expm1(x: ArrayLike) -&gt; Array:</span>
  <span class="s5">r&quot;&quot;&quot;Elementwise :math:`e^{x} - 1`.&quot;&quot;&quot;</span>
  <span class="s2">return </span><span class="s1">expm1_p.bind(x)</span>

<span class="s2">def </span><span class="s1">log(x: ArrayLike) -&gt; Array:</span>
  <span class="s5">r&quot;&quot;&quot;Elementwise natural logarithm: :math:`\mathrm{log}(x)`.&quot;&quot;&quot;</span>
  <span class="s2">return </span><span class="s1">log_p.bind(x)</span>

<span class="s2">def </span><span class="s1">log1p(x: ArrayLike) -&gt; Array:</span>
  <span class="s5">r&quot;&quot;&quot;Elementwise :math:`\mathrm{log}(1 + x)`.&quot;&quot;&quot;</span>
  <span class="s2">return </span><span class="s1">log1p_p.bind(x)</span>

<span class="s2">def </span><span class="s1">tanh(x: ArrayLike) -&gt; Array:</span>
  <span class="s5">r&quot;&quot;&quot;Elementwise hyperbolic tangent: :math:`\mathrm{tanh}(x)`.&quot;&quot;&quot;</span>
  <span class="s2">return </span><span class="s1">tanh_p.bind(x)</span>

<span class="s2">def </span><span class="s1">logistic(x: ArrayLike) -&gt; Array:</span>
  <span class="s5">r&quot;&quot;&quot;Elementwise logistic (sigmoid) function: :math:`\frac{1}{1 + e^{-x}}`.&quot;&quot;&quot;</span>
  <span class="s2">return </span><span class="s1">logistic_p.bind(x)</span>

<span class="s2">def </span><span class="s1">sin(x: ArrayLike) -&gt; Array:</span>
  <span class="s5">r&quot;&quot;&quot;Elementwise sine: :math:`\mathrm{sin}(x)`.&quot;&quot;&quot;</span>
  <span class="s2">return </span><span class="s1">sin_p.bind(x)</span>

<span class="s2">def </span><span class="s1">cos(x: ArrayLike) -&gt; Array:</span>
  <span class="s5">r&quot;&quot;&quot;Elementwise cosine: :math:`\mathrm{cos}(x)`.&quot;&quot;&quot;</span>
  <span class="s2">return </span><span class="s1">cos_p.bind(x)</span>

<span class="s2">def </span><span class="s1">atan2(x: ArrayLike</span><span class="s2">, </span><span class="s1">y: ArrayLike) -&gt; Array:</span>
  <span class="s5">r&quot;&quot;&quot;Elementwise arc tangent of two variables: 
    :math:`\mathrm{atan}({x \over y})`.&quot;&quot;&quot;</span>
  <span class="s2">return </span><span class="s1">atan2_p.bind(x</span><span class="s2">, </span><span class="s1">y)</span>

<span class="s2">def </span><span class="s1">real(x: ArrayLike) -&gt; Array:</span>
  <span class="s5">r&quot;&quot;&quot;Elementwise extract real part: :math:`\mathrm{Re}(x)`. 
 
  Returns the real part of a complex number. 
  &quot;&quot;&quot;</span>
  <span class="s2">return </span><span class="s1">real_p.bind(x)</span>

<span class="s2">def </span><span class="s1">imag(x: ArrayLike) -&gt; Array:</span>
  <span class="s5">r&quot;&quot;&quot;Elementwise extract imaginary part: :math:`\mathrm{Im}(x)`. 
 
  Returns the imaginary part of a complex number. 
  &quot;&quot;&quot;</span>
  <span class="s2">return </span><span class="s1">imag_p.bind(x)</span>

<span class="s2">def </span><span class="s1">complex(x: ArrayLike</span><span class="s2">, </span><span class="s1">y: ArrayLike) -&gt; Array:</span>
  <span class="s5">r&quot;&quot;&quot;Elementwise make complex number: :math:`x + jy`. 
 
  Builds a complex number from real and imaginary parts. 
  &quot;&quot;&quot;</span>
  <span class="s2">return </span><span class="s1">complex_p.bind(x</span><span class="s2">, </span><span class="s1">y)</span>

<span class="s2">def </span><span class="s1">conj(x: ArrayLike) -&gt; Array:</span>
  <span class="s5">r&quot;&quot;&quot;Elementwise complex conjugate function: :math:`\overline{x}`.&quot;&quot;&quot;</span>
  <span class="s2">return </span><span class="s1">conj_p.bind(x</span><span class="s2">, </span><span class="s1">input_dtype=_dtype(x))</span>

<span class="s2">def </span><span class="s1">abs(x: ArrayLike) -&gt; Array:</span>
  <span class="s5">r&quot;&quot;&quot;Elementwise absolute value: :math:`|x|`.&quot;&quot;&quot;</span>
  <span class="s2">return </span><span class="s1">abs_p.bind(x)</span>

<span class="s2">def </span><span class="s1">pow(x: ArrayLike</span><span class="s2">, </span><span class="s1">y: ArrayLike) -&gt; Array:</span>
  <span class="s5">r&quot;&quot;&quot;Elementwise power: :math:`x^y`.&quot;&quot;&quot;</span>
  <span class="s2">return </span><span class="s1">pow_p.bind(x</span><span class="s2">, </span><span class="s1">y)</span>

<span class="s2">def </span><span class="s1">integer_pow(x: ArrayLike</span><span class="s2">, </span><span class="s1">y: int) -&gt; Array:</span>
  <span class="s5">r&quot;&quot;&quot;Elementwise power: :math:`x^y`, where :math:`y` is a fixed integer.&quot;&quot;&quot;</span>
  <span class="s2">return </span><span class="s1">integer_pow_p.bind(x</span><span class="s2">, </span><span class="s1">y=y)</span>

<span class="s2">def </span><span class="s1">sqrt(x: ArrayLike) -&gt; Array:</span>
  <span class="s5">r&quot;&quot;&quot;Elementwise square root: :math:`\sqrt{x}`.&quot;&quot;&quot;</span>
  <span class="s2">return </span><span class="s1">sqrt_p.bind(x)</span>

<span class="s2">def </span><span class="s1">rsqrt(x: ArrayLike) -&gt; Array:</span>
  <span class="s5">r&quot;&quot;&quot;Elementwise reciprocal square root:  :math:`1 \over \sqrt{x}`.&quot;&quot;&quot;</span>
  <span class="s2">return </span><span class="s1">rsqrt_p.bind(x)</span>

<span class="s2">def </span><span class="s1">cbrt(x: ArrayLike) -&gt; Array:</span>
  <span class="s5">r&quot;&quot;&quot;Elementwise cube root: :math:`\sqrt[3]{x}`.&quot;&quot;&quot;</span>
  <span class="s2">return </span><span class="s1">cbrt_p.bind(x)</span>

<span class="s2">def </span><span class="s1">bitwise_not(x: ArrayLike) -&gt; Array:</span>
  <span class="s5">r&quot;&quot;&quot;Elementwise NOT: :math:`\neg x`.&quot;&quot;&quot;</span>
  <span class="s2">return </span><span class="s1">not_p.bind(x)</span>

<span class="s2">def </span><span class="s1">bitwise_and(x: ArrayLike</span><span class="s2">, </span><span class="s1">y: ArrayLike) -&gt; Array:</span>
  <span class="s5">r&quot;&quot;&quot;Elementwise AND: :math:`x \wedge y`.&quot;&quot;&quot;</span>
  <span class="s2">return </span><span class="s1">and_p.bind(x</span><span class="s2">, </span><span class="s1">y)</span>

<span class="s2">def </span><span class="s1">bitwise_or(x: ArrayLike</span><span class="s2">, </span><span class="s1">y: ArrayLike) -&gt; Array:</span>
  <span class="s5">r&quot;&quot;&quot;Elementwise OR: :math:`x \vee y`.&quot;&quot;&quot;</span>
  <span class="s2">return </span><span class="s1">or_p.bind(x</span><span class="s2">, </span><span class="s1">y)</span>

<span class="s2">def </span><span class="s1">bitwise_xor(x: ArrayLike</span><span class="s2">, </span><span class="s1">y: ArrayLike) -&gt; Array:</span>
  <span class="s5">r&quot;&quot;&quot;Elementwise exclusive OR: :math:`x \oplus y`.&quot;&quot;&quot;</span>
  <span class="s2">return </span><span class="s1">xor_p.bind(x</span><span class="s2">, </span><span class="s1">y)</span>

<span class="s2">def </span><span class="s1">population_count(x: ArrayLike) -&gt; Array:</span>
  <span class="s5">r&quot;&quot;&quot;Elementwise popcount, count the number of set bits in each element.&quot;&quot;&quot;</span>
  <span class="s2">return </span><span class="s1">population_count_p.bind(x)</span>

<span class="s2">def </span><span class="s1">clz(x: ArrayLike) -&gt; Array:</span>
  <span class="s5">r&quot;&quot;&quot;Elementwise count-leading-zeros.&quot;&quot;&quot;</span>
  <span class="s2">return </span><span class="s1">clz_p.bind(x)</span>

<span class="s2">def </span><span class="s1">add(x: ArrayLike</span><span class="s2">, </span><span class="s1">y: ArrayLike) -&gt; Array:</span>
  <span class="s5">r&quot;&quot;&quot;Elementwise addition: :math:`x + y`.&quot;&quot;&quot;</span>
  <span class="s2">return </span><span class="s1">add_p.bind(x</span><span class="s2">, </span><span class="s1">y)</span>

<span class="s2">def </span><span class="s1">sub(x: ArrayLike</span><span class="s2">, </span><span class="s1">y: ArrayLike) -&gt; Array:</span>
  <span class="s5">r&quot;&quot;&quot;Elementwise subtraction: :math:`x - y`.&quot;&quot;&quot;</span>
  <span class="s2">return </span><span class="s1">sub_p.bind(x</span><span class="s2">, </span><span class="s1">y)</span>

<span class="s2">def </span><span class="s1">mul(x: ArrayLike</span><span class="s2">, </span><span class="s1">y: ArrayLike) -&gt; Array:</span>
  <span class="s5">r&quot;&quot;&quot;Elementwise multiplication: :math:`x \times y`.&quot;&quot;&quot;</span>
  <span class="s2">return </span><span class="s1">mul_p.bind(x</span><span class="s2">, </span><span class="s1">y)</span>

<span class="s2">def </span><span class="s1">div(x: ArrayLike</span><span class="s2">, </span><span class="s1">y: ArrayLike) -&gt; Array:</span>
  <span class="s5">r&quot;&quot;&quot;Elementwise division: :math:`x \over y`. 
 
  Integer division overflow 
  (division by zero or signed division of INT_SMIN with -1) 
  produces an implementation defined value. 
  &quot;&quot;&quot;</span>
  <span class="s2">return </span><span class="s1">div_p.bind(x</span><span class="s2">, </span><span class="s1">y)</span>

<span class="s2">def </span><span class="s1">rem(x: ArrayLike</span><span class="s2">, </span><span class="s1">y: ArrayLike) -&gt; Array:</span>
  <span class="s5">r&quot;&quot;&quot;Elementwise remainder: :math:`x \bmod y`. 
 
  The sign of the result is taken from the dividend, 
  and the absolute value of the result is always 
  less than the divisor's absolute value. 
 
  Integer division overflow 
  (remainder by zero or remainder of INT_SMIN with -1) 
  produces an implementation defined value. 
  &quot;&quot;&quot;</span>
  <span class="s2">return </span><span class="s1">rem_p.bind(x</span><span class="s2">, </span><span class="s1">y)</span>

<span class="s2">def </span><span class="s1">max(x: ArrayLike</span><span class="s2">, </span><span class="s1">y: ArrayLike) -&gt; Array:</span>
  <span class="s5">r&quot;&quot;&quot;Elementwise maximum: :math:`\mathrm{max}(x, y)` 
 
  For complex numbers, uses a lexicographic comparison on the 
  `(real, imaginary)` pairs.&quot;&quot;&quot;</span>
  <span class="s2">return </span><span class="s1">max_p.bind(x</span><span class="s2">, </span><span class="s1">y)</span>

<span class="s2">def </span><span class="s1">min(x: ArrayLike</span><span class="s2">, </span><span class="s1">y: ArrayLike) -&gt; Array:</span>
  <span class="s5">r&quot;&quot;&quot;Elementwise minimum:  :math:`\mathrm{min}(x, y)` 
 
  For complex numbers, uses a lexicographic comparison on the 
  `(real, imaginary)` pairs.&quot;&quot;&quot;</span>
  <span class="s2">return </span><span class="s1">min_p.bind(x</span><span class="s2">, </span><span class="s1">y)</span>

<span class="s2">def </span><span class="s1">shift_left(x: ArrayLike</span><span class="s2">, </span><span class="s1">y: ArrayLike) -&gt; Array:</span>
  <span class="s5">r&quot;&quot;&quot;Elementwise left shift: :math:`x \ll y`.&quot;&quot;&quot;</span>
  <span class="s2">return </span><span class="s1">shift_left_p.bind(x</span><span class="s2">, </span><span class="s1">y)</span>

<span class="s2">def </span><span class="s1">shift_right_arithmetic(x: ArrayLike</span><span class="s2">, </span><span class="s1">y: ArrayLike) -&gt; Array:</span>
  <span class="s5">r&quot;&quot;&quot;Elementwise arithmetic right shift: :math:`x \gg y`.&quot;&quot;&quot;</span>
  <span class="s2">return </span><span class="s1">shift_right_arithmetic_p.bind(x</span><span class="s2">, </span><span class="s1">y)</span>

<span class="s2">def </span><span class="s1">shift_right_logical(x: ArrayLike</span><span class="s2">, </span><span class="s1">y: ArrayLike) -&gt; Array:</span>
  <span class="s5">r&quot;&quot;&quot;Elementwise logical right shift: :math:`x \gg y`.&quot;&quot;&quot;</span>
  <span class="s2">return </span><span class="s1">shift_right_logical_p.bind(x</span><span class="s2">, </span><span class="s1">y)</span>

<span class="s2">def </span><span class="s1">eq(x: ArrayLike</span><span class="s2">, </span><span class="s1">y: ArrayLike) -&gt; Array:</span>
  <span class="s5">r&quot;&quot;&quot;Elementwise equals: :math:`x = y`.&quot;&quot;&quot;</span>
  <span class="s2">return </span><span class="s1">eq_p.bind(x</span><span class="s2">, </span><span class="s1">y)</span>

<span class="s2">def </span><span class="s1">ne(x: ArrayLike</span><span class="s2">, </span><span class="s1">y: ArrayLike) -&gt; Array:</span>
  <span class="s5">r&quot;&quot;&quot;Elementwise not-equals: :math:`x \neq y`.&quot;&quot;&quot;</span>
  <span class="s2">return </span><span class="s1">ne_p.bind(x</span><span class="s2">, </span><span class="s1">y)</span>

<span class="s2">def </span><span class="s1">ge(x: ArrayLike</span><span class="s2">, </span><span class="s1">y: ArrayLike) -&gt; Array:</span>
  <span class="s5">r&quot;&quot;&quot;Elementwise greater-than-or-equals: :math:`x \geq y`.&quot;&quot;&quot;</span>
  <span class="s2">return </span><span class="s1">ge_p.bind(x</span><span class="s2">, </span><span class="s1">y)</span>

<span class="s2">def </span><span class="s1">gt(x: ArrayLike</span><span class="s2">, </span><span class="s1">y: ArrayLike) -&gt; Array:</span>
  <span class="s5">r&quot;&quot;&quot;Elementwise greater-than: :math:`x &gt; y`.&quot;&quot;&quot;</span>
  <span class="s2">return </span><span class="s1">gt_p.bind(x</span><span class="s2">, </span><span class="s1">y)</span>

<span class="s2">def </span><span class="s1">le(x: ArrayLike</span><span class="s2">, </span><span class="s1">y: ArrayLike) -&gt; Array:</span>
  <span class="s5">r&quot;&quot;&quot;Elementwise less-than-or-equals: :math:`x \leq y`.&quot;&quot;&quot;</span>
  <span class="s2">return </span><span class="s1">le_p.bind(x</span><span class="s2">, </span><span class="s1">y)</span>

<span class="s2">def </span><span class="s1">lt(x: ArrayLike</span><span class="s2">, </span><span class="s1">y: ArrayLike) -&gt; Array:</span>
  <span class="s5">r&quot;&quot;&quot;Elementwise less-than: :math:`x &lt; y`.&quot;&quot;&quot;</span>
  <span class="s2">return </span><span class="s1">lt_p.bind(x</span><span class="s2">, </span><span class="s1">y)</span>

<span class="s2">def </span><span class="s1">convert_element_type(operand: ArrayLike</span><span class="s2">, </span><span class="s1">new_dtype: DTypeLike) -&gt; Array:</span>
  <span class="s5">&quot;&quot;&quot;Elementwise cast. 
 
  Wraps XLA's `ConvertElementType 
  &lt;https://www.tensorflow.org/xla/operation_semantics#convertelementtype&gt;`_ 
  operator, which performs an elementwise conversion from one type to another. 
  Similar to a C++ `static_cast`. 
 
  Args: 
    operand: an array or scalar value to be cast 
    new_dtype: a NumPy dtype representing the target type. 
 
  Returns: 
    An array with the same shape as `operand`, cast elementwise to `new_dtype`. 
  &quot;&quot;&quot;</span>
  <span class="s2">if </span><span class="s1">hasattr(operand</span><span class="s2">, </span><span class="s3">'__jax_array__'</span><span class="s1">):</span>
    <span class="s1">operand = operand.__jax_array__()  </span><span class="s0"># type: ignore</span>
  <span class="s2">return </span><span class="s1">_convert_element_type(operand</span><span class="s2">, </span><span class="s1">new_dtype</span><span class="s2">, </span><span class="s1">weak_type=</span><span class="s2">False</span><span class="s1">)</span>

<span class="s2">def </span><span class="s1">_convert_element_type(operand: ArrayLike</span><span class="s2">, </span><span class="s1">new_dtype: Optional[DTypeLike] = </span><span class="s2">None,</span>
                          <span class="s1">weak_type: bool = </span><span class="s2">False</span><span class="s1">):</span>
  <span class="s2">if </span><span class="s1">(core.is_opaque_dtype(new_dtype) </span><span class="s2">or</span>
      <span class="s1">core.is_opaque_dtype(getattr(operand</span><span class="s2">, </span><span class="s3">'dtype'</span><span class="s2">, None</span><span class="s1">))):</span>
    <span class="s2">return </span><span class="s1">convert_element_type_p.bind(operand</span><span class="s2">, </span><span class="s1">new_dtype=new_dtype</span><span class="s2">,</span>
                                       <span class="s1">weak_type=bool(weak_type))</span>

  <span class="s0"># Don't canonicalize old_dtype because x64 context might cause</span>
  <span class="s0"># un-canonicalized operands to be passed in.</span>
  <span class="s1">old_dtype = dtypes.dtype(operand</span><span class="s2">, </span><span class="s1">canonicalize=</span><span class="s2">False</span><span class="s1">)</span>
  <span class="s1">old_weak_type = dtypes.is_weakly_typed(operand)</span>
  <span class="s2">if </span><span class="s1">new_dtype </span><span class="s2">is None</span><span class="s1">:</span>
    <span class="s1">new_dtype = old_dtype</span>
  <span class="s2">else</span><span class="s1">:</span>
    <span class="s1">new_dtype = np.dtype(new_dtype)</span>
  <span class="s1">new_dtype = dtypes.dtype(new_dtype</span><span class="s2">, </span><span class="s1">canonicalize=</span><span class="s2">True</span><span class="s1">)</span>

  <span class="s2">if </span><span class="s1">(dtypes.issubdtype(old_dtype</span><span class="s2">, </span><span class="s1">np.complexfloating) </span><span class="s2">and</span>
      <span class="s2">not </span><span class="s1">dtypes.issubdtype(new_dtype</span><span class="s2">, </span><span class="s1">np.complexfloating)):</span>
    <span class="s1">msg = </span><span class="s3">&quot;Casting complex values to real discards the imaginary part&quot;</span>
    <span class="s1">warnings.warn(msg</span><span class="s2">, </span><span class="s1">np.ComplexWarning</span><span class="s2">, </span><span class="s1">stacklevel=</span><span class="s4">2</span><span class="s1">)</span>

  <span class="s0"># Python has big integers, but convert_element_type(2 ** 100, np.float32) need</span>
  <span class="s0"># not be an error since the target dtype fits the value. Handle this case by</span>
  <span class="s0"># converting to a NumPy array before calling bind. Without this step, we'd</span>
  <span class="s0"># first canonicalize the input to a value of dtype int32 or int64, leading to</span>
  <span class="s0"># an overflow error.</span>
  <span class="s2">if </span><span class="s1">type(operand) </span><span class="s2">is </span><span class="s1">int:</span>
    <span class="s1">operand = np.asarray(operand</span><span class="s2">, </span><span class="s1">new_dtype)</span>
    <span class="s1">old_weak_type = </span><span class="s2">False</span>

  <span class="s2">if </span><span class="s1">(old_dtype</span><span class="s2">, </span><span class="s1">old_weak_type) == (new_dtype</span><span class="s2">, </span><span class="s1">weak_type) </span><span class="s2">and </span><span class="s1">isinstance(operand</span><span class="s2">, </span><span class="s1">Array):</span>
    <span class="s2">return </span><span class="s1">type_cast(Array</span><span class="s2">, </span><span class="s1">operand)</span>
  <span class="s2">else</span><span class="s1">:</span>
    <span class="s2">return </span><span class="s1">convert_element_type_p.bind(operand</span><span class="s2">, </span><span class="s1">new_dtype=new_dtype</span><span class="s2">,</span>
                                       <span class="s1">weak_type=bool(weak_type))</span>

<span class="s2">def </span><span class="s1">bitcast_convert_type(operand: ArrayLike</span><span class="s2">, </span><span class="s1">new_dtype: DTypeLike) -&gt; Array:</span>
  <span class="s5">&quot;&quot;&quot;Elementwise bitcast. 
 
  Wraps XLA's `BitcastConvertType 
  &lt;https://www.tensorflow.org/xla/operation_semantics#bitcastconverttype&gt;`_ 
  operator, which performs a bit cast from one type to another. 
 
  The output shape depends on the size of the input and output dtypes with 
  the following logic:: 
 
    if new_dtype.itemsize == operand.dtype.itemsize: 
      output_shape = operand.shape 
    if new_dtype.itemsize &lt; operand.dtype.itemsize: 
      output_shape = (*operand.shape, operand.dtype.itemsize // new_dtype.itemsize) 
    if new_dtype.itemsize &gt; operand.dtype.itemsize: 
      assert operand.shape[-1] * operand.dtype.itemsize == new_dtype.itemsize 
      output_shape = operand.shape[:-1] 
 
  Args: 
    operand: an array or scalar value to be cast 
    new_dtype: the new type. Should be a NumPy type. 
 
  Returns: 
    An array of shape `output_shape` (see above) and type `new_dtype`, 
    constructed from the same bits as operand. 
  &quot;&quot;&quot;</span>
  <span class="s1">new_dtype = dtypes.canonicalize_dtype(new_dtype)</span>
  <span class="s2">return </span><span class="s1">bitcast_convert_type_p.bind(operand</span><span class="s2">, </span><span class="s1">new_dtype=new_dtype)</span>

<span class="s2">def </span><span class="s1">clamp(min: ArrayLike</span><span class="s2">, </span><span class="s1">x: ArrayLike</span><span class="s2">, </span><span class="s1">max: ArrayLike) -&gt; Array:</span>
  <span class="s5">r&quot;&quot;&quot;Elementwise clamp. 
 
  Returns :math:`\mathrm{clamp}(x) = \begin{cases} 
  \mathit{min} &amp; \text{if } x &lt; \mathit{min},\\ 
  \mathit{max} &amp; \text{if } x &gt; \mathit{max},\\ 
  x &amp; \text{otherwise} 
  \end{cases}`. 
  &quot;&quot;&quot;</span>
  <span class="s2">return </span><span class="s1">clamp_p.bind(min</span><span class="s2">, </span><span class="s1">x</span><span class="s2">, </span><span class="s1">max)</span>

<span class="s2">def </span><span class="s1">concatenate(operands: Union[Array</span><span class="s2">, </span><span class="s1">Sequence[ArrayLike]]</span><span class="s2">, </span><span class="s1">dimension: int) -&gt; Array:</span>
  <span class="s5">&quot;&quot;&quot;Concatenates a sequence of arrays along `dimension`. 
 
  Wraps XLA's `Concatenate 
  &lt;https://www.tensorflow.org/xla/operation_semantics#concatenate&gt;`_ 
  operator. 
 
  Args: 
    operands: a sequence of arrays to concatenate. The arrays must have equal 
      shapes, except in the `dimension` axis. 
    dimension: the dimension along which to concatenate the arrays. 
 
  Returns: 
    An array containing the concatenation. 
  &quot;&quot;&quot;</span>
  <span class="s2">if </span><span class="s1">len(operands) == </span><span class="s4">0</span><span class="s1">:</span>
    <span class="s2">raise </span><span class="s1">ValueError(</span><span class="s3">&quot;concatenate requires a non-empty sequences of arrays&quot;</span><span class="s1">)</span>
  <span class="s2">if </span><span class="s1">len(operands) == </span><span class="s4">1</span><span class="s1">:</span>
    <span class="s1">op</span><span class="s2">, </span><span class="s1">= operands</span>
    <span class="s2">if </span><span class="s1">isinstance(op</span><span class="s2">, </span><span class="s1">Array):</span>
      <span class="s2">return </span><span class="s1">type_cast(Array</span><span class="s2">, </span><span class="s1">op)</span>
  <span class="s2">return </span><span class="s1">concatenate_p.bind(*operands</span><span class="s2">, </span><span class="s1">dimension=dimension)</span>


<span class="s2">class </span><span class="s1">_enum_descriptor:</span>
  <span class="s2">def </span><span class="s1">__init__(self</span><span class="s2">, </span><span class="s1">val):</span>
    <span class="s1">self.val = val</span>
  <span class="s2">def </span><span class="s1">__get__(self</span><span class="s2">, </span><span class="s1">_</span><span class="s2">, </span><span class="s1">owner):</span>
    <span class="s2">return </span><span class="s1">owner(self.val)</span>


<span class="s2">class </span><span class="s1">Precision(xla_client.PrecisionConfig.Precision):  </span><span class="s0"># type: ignore</span>
  <span class="s5">&quot;&quot;&quot;Precision enum for lax functions 
 
  The `precision` argument to JAX functions generally controls the tradeoff 
  between speed and accuracy for array computations on accelerator backends, 
  (i.e. TPU and GPU). Members are: 
 
  DEFAULT: 
    Fastest mode, but least accurate. Performs computations in bfloat16. 
    Aliases: ``'default'``, ``'fastest'``, ``'bfloat16'``. 
  HIGH: 
    Slower but more accurate. Performs float32 computations in 3 bfloat16 
    passes, or using tensorfloat32 where available. Aliases: ``'high'``, 
    ``'bfloat16_3x'``, ``'tensorfloat32'``. 
  HIGHEST: 
    Slowest but most accurate. Performs computations in float32 or float64 
    as applicable. Aliases: ``'highest'``, ``'float32'``. 
  &quot;&quot;&quot;</span>
  <span class="s0"># Wrap enum values with this class.</span>
  <span class="s1">DEFAULT = _enum_descriptor(</span><span class="s3">'default'</span><span class="s1">)</span>
  <span class="s1">HIGH = _enum_descriptor(</span><span class="s3">'high'</span><span class="s1">)</span>
  <span class="s1">HIGHEST = _enum_descriptor(</span><span class="s3">'highest'</span><span class="s1">)</span>

  <span class="s1">_strings = {</span>
      <span class="s3">'highest'</span><span class="s1">:       xla_client.PrecisionConfig.Precision.HIGHEST</span><span class="s2">,</span>
      <span class="s3">'float32'</span><span class="s1">:       xla_client.PrecisionConfig.Precision.HIGHEST</span><span class="s2">,</span>
      <span class="s3">'high'</span><span class="s1">:          xla_client.PrecisionConfig.Precision.HIGH</span><span class="s2">,</span>
      <span class="s3">'bfloat16_3x'</span><span class="s1">:   xla_client.PrecisionConfig.Precision.HIGH</span><span class="s2">,</span>
      <span class="s3">'tensorfloat32'</span><span class="s1">: xla_client.PrecisionConfig.Precision.HIGH</span><span class="s2">,</span>
      <span class="s3">'default'</span><span class="s1">:       xla_client.PrecisionConfig.Precision.DEFAULT</span><span class="s2">,</span>
      <span class="s3">'bfloat16'</span><span class="s1">:      xla_client.PrecisionConfig.Precision.DEFAULT</span><span class="s2">,</span>
      <span class="s3">'fastest'</span><span class="s1">:       xla_client.PrecisionConfig.Precision.DEFAULT</span><span class="s2">,</span>
      <span class="s2">None</span><span class="s1">:            xla_client.PrecisionConfig.Precision.DEFAULT</span><span class="s2">,</span>
  <span class="s1">}</span>
  <span class="s2">def </span><span class="s1">__init__(self</span><span class="s2">, </span><span class="s1">arg0):</span>
    <span class="s1">arg0 = self._strings.get(arg0</span><span class="s2">, </span><span class="s1">arg0)</span>
    <span class="s1">super().__init__(arg0)</span>

  <span class="s2">def </span><span class="s1">__str__(self) -&gt; str:</span>
    <span class="s2">return </span><span class="s1">self.name</span>


<span class="s1">PrecisionType = Precision</span>
<span class="s1">PrecisionLike = Union[</span><span class="s2">None, </span><span class="s1">str</span><span class="s2">, </span><span class="s1">PrecisionType</span><span class="s2">, </span><span class="s1">Tuple[str</span><span class="s2">, </span><span class="s1">str]</span><span class="s2">,</span>
                      <span class="s1">Tuple[PrecisionType</span><span class="s2">, </span><span class="s1">PrecisionType]]</span>

<span class="s2">def </span><span class="s1">dot(lhs: Array</span><span class="s2">, </span><span class="s1">rhs: Array</span><span class="s2">, </span><span class="s1">precision: PrecisionLike = </span><span class="s2">None,</span>
        <span class="s1">preferred_element_type: Optional[DTypeLike] = </span><span class="s2">None</span><span class="s1">) -&gt; Array:</span>
  <span class="s5">&quot;&quot;&quot;Vector/vector, matrix/vector, and matrix/matrix multiplication. 
 
  Wraps XLA's `Dot 
  &lt;https://www.tensorflow.org/xla/operation_semantics#dot&gt;`_ 
  operator. 
 
  For more general contraction, see the `dot_general` operator. 
 
  Args: 
    lhs: an array of dimension 1 or 2. 
    rhs: an array of dimension 1 or 2. 
    precision: Optional. Either ``None``, which means the default precision for 
      the backend, a :class:`~jax.lax.Precision` enum value (``Precision.DEFAULT``, 
      ``Precision.HIGH`` or ``Precision.HIGHEST``) or a tuple of two 
      :class:`~jax.lax.Precision` enums indicating precision of ``lhs``` and ``rhs``. 
    preferred_element_type: Optional. Either ``None``, which means the default 
      accumulation type for the input types, or a datatype, indicating to 
      accumulate results to and return a result with that datatype. 
 
  Returns: 
    An array containing the product. 
  &quot;&quot;&quot;</span>
  <span class="s2">if </span><span class="s4">1 </span><span class="s1">&lt;= lhs.ndim &lt;= </span><span class="s4">2 </span><span class="s2">and </span><span class="s4">1 </span><span class="s1">&lt;= rhs.ndim &lt;= </span><span class="s4">2 </span><span class="s2">and </span><span class="s1">core.symbolic_equal_dim(lhs.shape[-</span><span class="s4">1</span><span class="s1">]</span><span class="s2">, </span><span class="s1">rhs.shape[</span><span class="s4">0</span><span class="s1">]):</span>
    <span class="s2">return </span><span class="s1">dot_general(lhs</span><span class="s2">, </span><span class="s1">rhs</span><span class="s2">, </span><span class="s1">(((lhs.ndim - </span><span class="s4">1</span><span class="s2">,</span><span class="s1">)</span><span class="s2">, </span><span class="s1">(</span><span class="s4">0</span><span class="s2">,</span><span class="s1">))</span><span class="s2">, </span><span class="s1">(()</span><span class="s2">, </span><span class="s1">()))</span><span class="s2">,</span>
                       <span class="s1">precision=precision</span><span class="s2">,</span>
                       <span class="s1">preferred_element_type=preferred_element_type)</span>
  <span class="s2">else</span><span class="s1">:</span>
    <span class="s2">raise </span><span class="s1">TypeError(</span><span class="s3">&quot;Incompatible shapes for dot: got {} and {}.&quot;</span><span class="s1">.format(</span>
        <span class="s1">lhs.shape</span><span class="s2">, </span><span class="s1">rhs.shape))</span>


<span class="s1">DotDimensionNumbers = Tuple[Tuple[Sequence[int]</span><span class="s2">, </span><span class="s1">Sequence[int]]</span><span class="s2">,</span>
                            <span class="s1">Tuple[Sequence[int]</span><span class="s2">, </span><span class="s1">Sequence[int]]]</span>

<span class="s2">def </span><span class="s1">dot_general(lhs: ArrayLike</span><span class="s2">, </span><span class="s1">rhs: ArrayLike</span><span class="s2">, </span><span class="s1">dimension_numbers: DotDimensionNumbers</span><span class="s2">,</span>
                <span class="s1">precision: PrecisionLike = </span><span class="s2">None,</span>
                <span class="s1">preferred_element_type: Optional[DTypeLike] = </span><span class="s2">None</span><span class="s1">) -&gt; Array:</span>
  <span class="s5">&quot;&quot;&quot;General dot product/contraction operator. 
 
  Wraps XLA's `DotGeneral 
  &lt;https://www.tensorflow.org/xla/operation_semantics#dotgeneral&gt;`_ 
  operator. 
 
  The semantics of ``dot_general`` are complicated, but most users should not have to 
  use it directly. Instead, you can use higher-level functions like :func:`jax.numpy.dot`, 
  :func:`jax.numpy.matmul`, :func:`jax.numpy.tensordot`, :func:`jax.numpy.einsum`, 
  and others which will construct appropriate calls to ``dot_general`` under the hood. 
  If you really want to understand ``dot_general`` itself, we recommend reading XLA's 
  `DotGeneral  &lt;https://www.tensorflow.org/xla/operation_semantics#dotgeneral&gt;`_ 
  operator documentation. 
 
  Args: 
    lhs: an array 
    rhs: an array 
    dimension_numbers: a tuple of tuples of sequences of ints of the form 
      ``((lhs_contracting_dims, rhs_contracting_dims), (lhs_batch_dims, rhs_batch_dims))`` 
    precision: Optional. Either ``None``, which means the default precision for 
      the backend, a :class:`~jax.lax.Precision` enum value (``Precision.DEFAULT``, 
      ``Precision.HIGH`` or ``Precision.HIGHEST``) or a tuple of two 
      :class:`~jax.lax.Precision` enums indicating precision of ``lhs``` and ``rhs``. 
    preferred_element_type: Optional. Either ``None``, which means the default 
      accumulation type for the input types, or a datatype, indicating to 
      accumulate results to and return a result with that datatype. 
 
  Returns: 
    An array whose first dimensions are the (shared) batch dimensions, followed by 
    the ``lhs`` non-contracting/non-batch dimensions, and finally the ``rhs`` 
    non-contracting/non-batch dimensions. 
  &quot;&quot;&quot;</span>
  <span class="s1">(lhs_contract</span><span class="s2">, </span><span class="s1">rhs_contract)</span><span class="s2">, </span><span class="s1">(lhs_batch</span><span class="s2">, </span><span class="s1">rhs_batch) = dimension_numbers</span>
  <span class="s1">cdims = (api_util._ensure_index_tuple(lhs_contract)</span><span class="s2">,</span>
           <span class="s1">api_util._ensure_index_tuple(rhs_contract))</span>
  <span class="s1">bdims = (api_util._ensure_index_tuple(lhs_batch)</span><span class="s2">,</span>
           <span class="s1">api_util._ensure_index_tuple(rhs_batch))</span>
  <span class="s1">preferred_element_type = (</span>
      <span class="s2">None if </span><span class="s1">preferred_element_type </span><span class="s2">is None else</span>
      <span class="s1">dtypes.canonicalize_dtype(np.dtype(preferred_element_type)))</span>
  <span class="s2">return </span><span class="s1">dot_general_p.bind(lhs</span><span class="s2">, </span><span class="s1">rhs</span><span class="s2">,</span>
                            <span class="s1">dimension_numbers=(cdims</span><span class="s2">, </span><span class="s1">bdims)</span><span class="s2">,</span>
                            <span class="s1">precision=canonicalize_precision(precision)</span><span class="s2">,</span>
                            <span class="s1">preferred_element_type=preferred_element_type)</span>

<span class="s2">def </span><span class="s1">broadcast(operand: ArrayLike</span><span class="s2">, </span><span class="s1">sizes: Sequence[int]) -&gt; Array:</span>
  <span class="s5">&quot;&quot;&quot;Broadcasts an array, adding new leading dimensions 
 
  Args: 
    operand: an array 
    sizes: a sequence of integers, giving the sizes of new leading dimensions 
      to add to the front of the array. 
 
  Returns: 
    An array containing the result. 
 
  See Also: 
    jax.lax.broadcast_in_dim : add new dimensions at any location in the array shape. 
  &quot;&quot;&quot;</span>
  <span class="s1">dims = tuple(range(len(sizes)</span><span class="s2">, </span><span class="s1">len(sizes) + np.ndim(operand)))</span>
  <span class="s2">return </span><span class="s1">broadcast_in_dim(operand</span><span class="s2">, </span><span class="s1">tuple(sizes) + np.shape(operand)</span><span class="s2">, </span><span class="s1">dims)</span>

<span class="s2">def </span><span class="s1">broadcast_in_dim(operand: ArrayLike</span><span class="s2">, </span><span class="s1">shape: Shape</span><span class="s2">,</span>
                     <span class="s1">broadcast_dimensions: Sequence[int]) -&gt; Array:</span>
  <span class="s5">&quot;&quot;&quot;Wraps XLA's `BroadcastInDim 
  &lt;https://www.tensorflow.org/xla/operation_semantics#broadcastindim&gt;`_ 
  operator. 
 
  Args: 
    operand: an array 
    shape: the shape of the target array 
    broadcast_dimensions: to which dimension in the target shape each dimension 
      of the operand shape corresponds to 
 
  Returns: 
    An array containing the result. 
 
  See Also: 
    jax.lax.broadcast : simpler interface to add new leading dimensions. 
  &quot;&quot;&quot;</span>
  <span class="s2">if </span><span class="s1">np.ndim(operand) == len(shape) </span><span class="s2">and not </span><span class="s1">len(broadcast_dimensions) </span><span class="s2">and </span><span class="s1">isinstance(operand</span><span class="s2">, </span><span class="s1">Array):</span>
    <span class="s2">return </span><span class="s1">type_cast(Array</span><span class="s2">, </span><span class="s1">operand)</span>
  <span class="s2">if </span><span class="s1">config.jax_dynamic_shapes:</span>
    <span class="s0"># We must gate this behavior under a flag because otherwise the errors</span>
    <span class="s0"># raised are different (and have worse source provenance information).</span>
    <span class="s1">dyn_shape</span><span class="s2">, </span><span class="s1">static_shape = _extract_tracers_dyn_shape(shape)</span>
  <span class="s2">else</span><span class="s1">:</span>
    <span class="s1">dyn_shape</span><span class="s2">, </span><span class="s1">static_shape = []</span><span class="s2">, </span><span class="s1">shape  </span><span class="s0"># type: ignore</span>
  <span class="s2">return </span><span class="s1">broadcast_in_dim_p.bind(</span>
      <span class="s1">operand</span><span class="s2">, </span><span class="s1">*dyn_shape</span><span class="s2">, </span><span class="s1">shape=tuple(static_shape)</span><span class="s2">,</span>
      <span class="s1">broadcast_dimensions=tuple(broadcast_dimensions))</span>

<span class="s2">def </span><span class="s1">broadcast_to_rank(x: Array</span><span class="s2">, </span><span class="s1">rank: int) -&gt; Array:</span>
  <span class="s5">&quot;&quot;&quot;Adds leading dimensions of ``1`` to give ``x`` rank ``rank``.&quot;&quot;&quot;</span>
  <span class="s2">return </span><span class="s1">broadcast(x</span><span class="s2">, </span><span class="s1">(</span><span class="s4">1</span><span class="s2">,</span><span class="s1">) * (rank - x.ndim))</span>

<span class="s2">def </span><span class="s1">reshape(operand: ArrayLike</span><span class="s2">, </span><span class="s1">new_sizes: Shape</span><span class="s2">,</span>
            <span class="s1">dimensions: Optional[Sequence[int]] = </span><span class="s2">None</span><span class="s1">) -&gt; Array:</span>
  <span class="s5">&quot;&quot;&quot;Wraps XLA's `Reshape 
  &lt;https://www.tensorflow.org/xla/operation_semantics#reshape&gt;`_ 
  operator. 
 
  For inserting/removing dimensions of size 1, prefer using ``lax.squeeze`` / 
  ``lax.expand_dims``. These preserve information about axis identity that may 
  be useful for advanced transformation rules. 
 
  Args: 
    operand: array to be reshaped. 
    new_sizes: sequence of integers specifying the resulting shape. The size 
      of the final array must match the size of the input. 
    dimensions: optional sequence of integers specifying the permutation order of 
      the input shape. If specified, the length must match ``operand.shape``. 
 
  Returns: 
    out: reshaped array. 
 
  Examples: 
    Simple reshaping from one to two dimensions: 
 
    &gt;&gt;&gt; x = jnp.arange(6) 
    &gt;&gt;&gt; y = reshape(x, (2, 3)) 
    &gt;&gt;&gt; y 
    Array([[0, 1, 2], 
                 [3, 4, 5]], dtype=int32) 
 
    Reshaping back to one dimension: 
 
    &gt;&gt;&gt; reshape(y, (6,)) 
    Array([0, 1, 2, 3, 4, 5], dtype=int32) 
 
    Reshaping to one dimension with permutation of dimensions: 
 
    &gt;&gt;&gt; reshape(y, (6,), (1, 0)) 
    Array([0, 3, 1, 4, 2, 5], dtype=int32) 
  &quot;&quot;&quot;</span>
  <span class="s1">new_sizes = canonicalize_shape(new_sizes)  </span><span class="s0"># TODO</span>
  <span class="s1">new_sizes = tuple(new_sizes)</span>
  <span class="s1">same_shape = core.symbolic_equal_shape(np.shape(operand)</span><span class="s2">, </span><span class="s1">new_sizes)</span>
  <span class="s2">if </span><span class="s1">dimensions </span><span class="s2">is None</span><span class="s1">:</span>
    <span class="s1">same_dims = </span><span class="s2">True</span>
    <span class="s1">dims = </span><span class="s2">None</span>
  <span class="s2">else</span><span class="s1">:</span>
    <span class="s1">dims = api_util._ensure_index_tuple(dimensions)</span>
    <span class="s1">same_dims = tuple(dims) == tuple(range(np.ndim(operand)))</span>
  <span class="s2">if </span><span class="s1">np.shape(operand) </span><span class="s2">and </span><span class="s1">same_shape </span><span class="s2">and </span><span class="s1">same_dims </span><span class="s2">and </span><span class="s1">isinstance(operand</span><span class="s2">, </span><span class="s1">Array):</span>
    <span class="s2">return </span><span class="s1">type_cast(Array</span><span class="s2">, </span><span class="s1">operand)</span>
  <span class="s2">else</span><span class="s1">:</span>
    <span class="s1">dyn_shape</span><span class="s2">, </span><span class="s1">static_new_sizes = _extract_tracers_dyn_shape(new_sizes)</span>

    <span class="s2">return </span><span class="s1">reshape_p.bind(</span>
      <span class="s1">operand</span><span class="s2">, </span><span class="s1">*dyn_shape</span><span class="s2">, </span><span class="s1">new_sizes=tuple(static_new_sizes)</span><span class="s2">,</span>
      <span class="s1">dimensions=</span><span class="s2">None if </span><span class="s1">dims </span><span class="s2">is None or </span><span class="s1">same_dims </span><span class="s2">else </span><span class="s1">dims)</span>

<span class="s2">def </span><span class="s1">pad(operand: ArrayLike</span><span class="s2">, </span><span class="s1">padding_value: ArrayLike</span><span class="s2">,</span>
        <span class="s1">padding_config: Sequence[Tuple[int</span><span class="s2">, </span><span class="s1">int</span><span class="s2">, </span><span class="s1">int]]) -&gt; Array:</span>
  <span class="s5">&quot;&quot;&quot;Applies low, high, and/or interior padding to an array. 
 
  Wraps XLA's `Pad 
  &lt;https://www.tensorflow.org/xla/operation_semantics#pad&gt;`_ 
  operator. 
 
  Args: 
    operand: an array to be padded. 
    padding_value: the value to be inserted as padding. Must have the same dtype 
      as ``operand``. 
    padding_config: a sequence of ``(low, high, interior)`` tuples of integers, 
      giving the amount of low, high, and interior (dilation) padding to insert 
      in each dimension. 
 
  Returns: 
    The ``operand`` array with padding value ``padding_value`` inserted in each 
    dimension according to the ``padding_config``. 
  &quot;&quot;&quot;</span>
  <span class="s2">return </span><span class="s1">pad_p.bind(operand</span><span class="s2">, </span><span class="s1">padding_value</span><span class="s2">, </span><span class="s1">padding_config=tuple(padding_config))</span>

<span class="s2">def </span><span class="s1">rev(operand: ArrayLike</span><span class="s2">, </span><span class="s1">dimensions: Sequence[int]) -&gt; Array:</span>
  <span class="s5">&quot;&quot;&quot;Wraps XLA's `Rev 
  &lt;https://www.tensorflow.org/xla/operation_semantics#rev_reverse&gt;`_ 
  operator. 
  &quot;&quot;&quot;</span>
  <span class="s2">return </span><span class="s1">rev_p.bind(operand</span><span class="s2">, </span><span class="s1">dimensions=tuple(dimensions))</span>

<span class="s2">def </span><span class="s1">select(pred: ArrayLike</span><span class="s2">, </span><span class="s1">on_true: ArrayLike</span><span class="s2">, </span><span class="s1">on_false: ArrayLike) -&gt; Array:</span>
  <span class="s5">&quot;&quot;&quot;Selects between two branches based on a boolean predicate. 
 
  Wraps XLA's `Select 
  &lt;https://www.tensorflow.org/xla/operation_semantics#select&gt;`_ 
  operator. 
 
  In general :func:`~jax.lax.select` leads to evaluation of both branches, although 
  the compiler may elide computations if possible. For a similar function that 
  usually evaluates only a single branch, see :func:`~jax.lax.cond`. 
 
  Args: 
    pred: boolean array 
    on_true: array containing entries to return where ``pred`` is True. Must have 
      the same shape as ``pred``, and the same shape and dtype as ``on_false``. 
    on_false: array containing entries to return where ``pred`` is False. Must have 
      the same shape as ``pred``, and the same shape and dtype as ``on_true``. 
 
  Returns: 
    result: array with same shape and dtype as ``on_true`` and ``on_false``. 
  &quot;&quot;&quot;</span>
  <span class="s0"># Caution! The select_n_p primitive has the *opposite* order of arguments to</span>
  <span class="s0"># select(). This is because it implements `select_n`.</span>
  <span class="s2">return </span><span class="s1">select_n_p.bind(pred</span><span class="s2">, </span><span class="s1">on_false</span><span class="s2">, </span><span class="s1">on_true)</span>

<span class="s2">def </span><span class="s1">select_n(which: ArrayLike</span><span class="s2">, </span><span class="s1">*cases: ArrayLike) -&gt; Array:</span>
  <span class="s5">&quot;&quot;&quot;Selects array values from multiple cases. 
 
  Generalizes XLA's `Select 
  &lt;https://www.tensorflow.org/xla/operation_semantics#select&gt;`_ 
  operator. Unlike XLA's version, the operator is variadic and can select 
  from many cases using an integer `pred`. 
 
  Args: 
    which: determines which case should be returned. Must be an array containing 
      either a boolean or integer values. May either be a scalar or have 
      shape matching ``cases``. For each array element, the value of ``which`` 
      determines which of ``cases`` is taken. ``which`` must be in the range 
      ``[0 .. len(cases))``; for values outside that range the behavior is 
      implementation-defined. 
    *cases: a non-empty list of array cases. All must have equal dtypes and 
      equal shapes. 
  Returns: 
    An array with shape and dtype equal to the cases, whose values are chosen 
    according to ``which``. 
  &quot;&quot;&quot;</span>
  <span class="s2">if </span><span class="s1">len(cases) == </span><span class="s4">0</span><span class="s1">:</span>
    <span class="s2">raise </span><span class="s1">ValueError(</span><span class="s3">&quot;select_n() must have at least one case&quot;</span><span class="s1">)</span>
  <span class="s2">return </span><span class="s1">select_n_p.bind(which</span><span class="s2">, </span><span class="s1">*cases)</span>


<span class="s2">def </span><span class="s1">transpose(operand: ArrayLike</span><span class="s2">, </span><span class="s1">permutation: Sequence[int]) -&gt; Array:</span>
  <span class="s5">&quot;&quot;&quot;Wraps XLA's `Transpose 
  &lt;https://www.tensorflow.org/xla/operation_semantics#transpose&gt;`_ 
  operator. 
  &quot;&quot;&quot;</span>
  <span class="s1">permutation = tuple(operator.index(d) </span><span class="s2">for </span><span class="s1">d </span><span class="s2">in </span><span class="s1">permutation)</span>
  <span class="s2">if </span><span class="s1">permutation == tuple(range(np.ndim(operand))) </span><span class="s2">and </span><span class="s1">isinstance(operand</span><span class="s2">, </span><span class="s1">Array):</span>
    <span class="s2">return </span><span class="s1">type_cast(Array</span><span class="s2">, </span><span class="s1">operand)</span>
  <span class="s2">else</span><span class="s1">:</span>
    <span class="s2">return </span><span class="s1">transpose_p.bind(operand</span><span class="s2">, </span><span class="s1">permutation=permutation)</span>

<span class="s2">def </span><span class="s1">argmin(operand: ArrayLike</span><span class="s2">, </span><span class="s1">axis: int</span><span class="s2">,</span>
           <span class="s1">index_dtype: DTypeLike) -&gt; Array:</span>
  <span class="s5">&quot;&quot;&quot;Computes the index of the minimum element along ``axis``.&quot;&quot;&quot;</span>
  <span class="s2">return </span><span class="s1">argmin_p.bind(operand</span><span class="s2">, </span><span class="s1">axes=(axis</span><span class="s2">,</span><span class="s1">)</span><span class="s2">,</span>
                       <span class="s1">index_dtype=dtypes.canonicalize_dtype(index_dtype))</span>

<span class="s2">def </span><span class="s1">argmax(operand: ArrayLike</span><span class="s2">, </span><span class="s1">axis: int</span><span class="s2">,</span>
           <span class="s1">index_dtype: DTypeLike) -&gt; Array:</span>
  <span class="s5">&quot;&quot;&quot;Computes the index of the maximum element along ``axis``.&quot;&quot;&quot;</span>
  <span class="s2">return </span><span class="s1">argmax_p.bind(operand</span><span class="s2">, </span><span class="s1">axes=(axis</span><span class="s2">,</span><span class="s1">)</span><span class="s2">,</span>
                       <span class="s1">index_dtype=dtypes.canonicalize_dtype(index_dtype))</span>

<span class="s2">def </span><span class="s1">reduce(operands: Any</span><span class="s2">,</span>
           <span class="s1">init_values: Any</span><span class="s2">,</span>
           <span class="s1">computation: Callable[[Any</span><span class="s2">, </span><span class="s1">Any]</span><span class="s2">, </span><span class="s1">Any]</span><span class="s2">,</span>
           <span class="s1">dimensions: Sequence[int]) -&gt; Any:</span>
  <span class="s5">&quot;&quot;&quot;Wraps XLA's `Reduce 
  &lt;https://www.tensorflow.org/xla/operation_semantics#reduce&gt;`_ 
  operator. 
 
  ``init_values`` and ``computation`` together must form a `monoid 
  &lt;https://en.wikipedia.org/wiki/Monoid&gt;`_ 
  for correctness. That is ``init_values`` must be an identity of 
  ``computation``, and ``computation`` must be associative. XLA may exploit both 
  of these properties during code generation; if either is violated the result 
  is undefined. 
  &quot;&quot;&quot;</span>
  <span class="s1">flat_operands</span><span class="s2">, </span><span class="s1">operand_tree = tree_util.tree_flatten(operands)</span>
  <span class="s1">flat_init_values</span><span class="s2">, </span><span class="s1">init_value_tree = tree_util.tree_flatten(init_values)</span>
  <span class="s2">if </span><span class="s1">operand_tree != init_value_tree:</span>
    <span class="s2">raise </span><span class="s1">ValueError(</span><span class="s3">'Operands must have the same tree structure as init_values:'</span>
                     <span class="s3">f' </span><span class="s2">{</span><span class="s1">operand_tree</span><span class="s2">} </span><span class="s3">vs. </span><span class="s2">{</span><span class="s1">init_value_tree</span><span class="s2">}</span><span class="s3">'</span><span class="s1">)</span>
  <span class="s2">if </span><span class="s1">len(flat_operands) != len(flat_init_values):</span>
    <span class="s2">raise </span><span class="s1">ValueError(</span><span class="s3">'Must have same total number of operands as init_values: '</span>
                     <span class="s3">f' </span><span class="s2">{</span><span class="s1">len(flat_operands)</span><span class="s2">} </span><span class="s3">vs. </span><span class="s2">{</span><span class="s1">len(flat_init_values)</span><span class="s2">}</span><span class="s3">'</span><span class="s1">)</span>
  <span class="s1">monoid_reducer = _get_monoid_reducer(computation</span><span class="s2">, </span><span class="s1">flat_init_values)</span>
  <span class="s2">if </span><span class="s1">monoid_reducer:</span>
    <span class="s0"># monoid reducers bypass the weak_type_rule, so we set it explicitly.</span>
    <span class="s1">weak_type = dtypes.is_weakly_typed(*flat_operands) </span><span class="s2">and </span><span class="s1">dtypes.is_weakly_typed(*flat_init_values)</span>
    <span class="s2">return </span><span class="s1">_convert_element_type(monoid_reducer(*flat_operands</span><span class="s2">, </span><span class="s1">dimensions)</span><span class="s2">,</span>
                                 <span class="s1">weak_type=weak_type)</span>
  <span class="s2">else</span><span class="s1">:</span>
    <span class="s1">flat_init_avals = safe_map(_abstractify</span><span class="s2">, </span><span class="s1">flat_init_values)</span>
    <span class="s1">jaxpr</span><span class="s2">, </span><span class="s1">consts</span><span class="s2">, </span><span class="s1">out_tree = _variadic_reduction_jaxpr(</span>
        <span class="s1">computation</span><span class="s2">, </span><span class="s1">tuple(flat_init_avals)</span><span class="s2">, </span><span class="s1">init_value_tree)</span>
    <span class="s1">out = reduce_p.bind(*flat_operands</span><span class="s2">, </span><span class="s1">*flat_init_values</span><span class="s2">, </span><span class="s1">computation=computation</span><span class="s2">,</span>
                        <span class="s1">jaxpr=jaxpr</span><span class="s2">, </span><span class="s1">consts=consts</span><span class="s2">, </span><span class="s1">dimensions=tuple(dimensions))</span>
    <span class="s2">return </span><span class="s1">tree_util.tree_unflatten(out_tree</span><span class="s2">, </span><span class="s1">out)</span>

<span class="s1">@cache()</span>
<span class="s2">def </span><span class="s1">_reduction_jaxpr(computation</span><span class="s2">, </span><span class="s1">aval):</span>
  <span class="s1">@lu.wrap_init</span>
  <span class="s2">def </span><span class="s1">comp(x</span><span class="s2">, </span><span class="s1">y):</span>
    <span class="s1">result = computation(x</span><span class="s2">, </span><span class="s1">y)</span>
    <span class="s2">if not </span><span class="s1">(isinstance(result</span><span class="s2">, </span><span class="s1">core.Tracer) </span><span class="s2">or </span><span class="s1">core.valid_jaxtype(result)):</span>
      <span class="s2">raise </span><span class="s1">ValueError(</span>
          <span class="s3">f&quot;Invalid return type from reduction function: </span><span class="s2">{</span><span class="s1">type(result)</span><span class="s2">}\n</span><span class="s3">&quot;</span>
          <span class="s3">f&quot;Reduction functions should only return an array.</span><span class="s2">\n</span><span class="s3">&quot;</span>
          <span class="s3">f&quot;Full return value: </span><span class="s2">{</span><span class="s1">result</span><span class="s2">}</span><span class="s3">&quot;</span><span class="s1">)</span>
    <span class="s2">return </span><span class="s1">(result</span><span class="s2">,</span><span class="s1">)</span>
  <span class="s1">jaxpr</span><span class="s2">, </span><span class="s1">_</span><span class="s2">, </span><span class="s1">consts = pe.trace_to_jaxpr_dynamic(comp</span><span class="s2">, </span><span class="s1">(aval</span><span class="s2">, </span><span class="s1">aval))</span>
  <span class="s2">if </span><span class="s1">any(isinstance(c</span><span class="s2">, </span><span class="s1">core.Tracer) </span><span class="s2">for </span><span class="s1">c </span><span class="s2">in </span><span class="s1">consts):</span>
    <span class="s2">raise </span><span class="s1">NotImplementedError(</span>
        <span class="s3">&quot;Reduction computations can't close over Tracers. Please open an issue &quot;</span>
        <span class="s3">&quot;at https://github.com/google/jax.&quot;</span><span class="s1">)</span>
  <span class="s2">return </span><span class="s1">jaxpr</span><span class="s2">, </span><span class="s1">tuple(consts)</span>

<span class="s1">@cache()</span>
<span class="s2">def </span><span class="s1">_variadic_reduction_jaxpr(computation</span><span class="s2">, </span><span class="s1">flat_avals</span><span class="s2">, </span><span class="s1">aval_tree):</span>
  <span class="s1">avals = tree_util.tree_unflatten(aval_tree</span><span class="s2">, </span><span class="s1">flat_avals)</span>
  <span class="s1">flat_in_avals</span><span class="s2">, </span><span class="s1">in_tree = tree_util.tree_flatten((avals</span><span class="s2">, </span><span class="s1">avals))</span>
  <span class="s1">comp = lu.wrap_init(computation)</span>
  <span class="s1">flat_comp</span><span class="s2">, </span><span class="s1">out_tree = api_util.flatten_fun_nokwargs(comp</span><span class="s2">, </span><span class="s1">in_tree)</span>
  <span class="s1">jaxpr</span><span class="s2">, </span><span class="s1">_</span><span class="s2">, </span><span class="s1">consts = pe.trace_to_jaxpr_dynamic(flat_comp</span><span class="s2">, </span><span class="s1">tuple(flat_in_avals))</span>
  <span class="s2">if </span><span class="s1">any(isinstance(c</span><span class="s2">, </span><span class="s1">core.Tracer) </span><span class="s2">for </span><span class="s1">c </span><span class="s2">in </span><span class="s1">consts):</span>
    <span class="s2">raise </span><span class="s1">NotImplementedError(</span>
        <span class="s3">&quot;Reduction computations can't close over Tracers. Please open an issue &quot;</span>
        <span class="s3">&quot;at https://github.com/google/jax.&quot;</span><span class="s1">)</span>
  <span class="s2">return </span><span class="s1">jaxpr</span><span class="s2">, </span><span class="s1">tuple(consts)</span><span class="s2">, </span><span class="s1">out_tree()</span>

<span class="s2">def </span><span class="s1">_get_monoid_reducer(monoid_op: Callable</span><span class="s2">,</span>
                        <span class="s1">xs: Sequence[Array]) -&gt; Optional[Callable]:</span>
  <span class="s2">if </span><span class="s1">len(xs) != </span><span class="s4">1</span><span class="s1">:</span>
    <span class="s2">return None</span>
  <span class="s1">x</span><span class="s2">, </span><span class="s1">= xs</span>
  <span class="s1">aval = core.get_aval(x)</span>
  <span class="s1">dtype = _dtype(x)</span>
  <span class="s2">if </span><span class="s1">(type(aval) </span><span class="s2">is </span><span class="s1">ConcreteArray) </span><span class="s2">and </span><span class="s1">aval.shape == ():</span>
    <span class="s0"># allow bitwise reductions for boolean and integer types</span>
    <span class="s1">_is_intlike = dtype == np.bool_ </span><span class="s2">or </span><span class="s1">dtypes.issubdtype(dtype</span><span class="s2">, </span><span class="s1">np.integer)</span>
    <span class="s2">if </span><span class="s1">monoid_op </span><span class="s2">is </span><span class="s1">add:</span>
      <span class="s2">return </span><span class="s1">_reduce_sum </span><span class="s2">if </span><span class="s1">np.equal(aval.val</span><span class="s2">, </span><span class="s4">0</span><span class="s1">) </span><span class="s2">else None</span>
    <span class="s2">elif </span><span class="s1">monoid_op </span><span class="s2">is </span><span class="s1">mul:</span>
      <span class="s2">return </span><span class="s1">_reduce_prod </span><span class="s2">if </span><span class="s1">np.equal(aval.val</span><span class="s2">, </span><span class="s4">1</span><span class="s1">) </span><span class="s2">else None</span>
    <span class="s2">elif </span><span class="s1">monoid_op </span><span class="s2">is </span><span class="s1">bitwise_or </span><span class="s2">and </span><span class="s1">_is_intlike:</span>
      <span class="s2">return </span><span class="s1">_reduce_or </span><span class="s2">if </span><span class="s1">np.equal(aval.val</span><span class="s2">, </span><span class="s1">_get_bitwise_or_identity(dtype)) </span><span class="s2">else None</span>
    <span class="s2">elif </span><span class="s1">monoid_op </span><span class="s2">is </span><span class="s1">bitwise_and </span><span class="s2">and </span><span class="s1">_is_intlike:</span>
      <span class="s2">return </span><span class="s1">_reduce_and </span><span class="s2">if </span><span class="s1">np.equal(aval.val</span><span class="s2">, </span><span class="s1">_get_bitwise_and_identity(dtype)) </span><span class="s2">else None</span>
    <span class="s2">elif </span><span class="s1">monoid_op </span><span class="s2">is </span><span class="s1">bitwise_xor </span><span class="s2">and </span><span class="s1">_is_intlike:</span>
      <span class="s2">return </span><span class="s1">_reduce_xor </span><span class="s2">if </span><span class="s1">np.equal(aval.val</span><span class="s2">, </span><span class="s1">_get_bitwise_or_identity(dtype)) </span><span class="s2">else None</span>
    <span class="s2">elif </span><span class="s1">monoid_op </span><span class="s2">is </span><span class="s1">max:</span>
      <span class="s2">return </span><span class="s1">_reduce_max </span><span class="s2">if </span><span class="s1">np.equal(aval.val</span><span class="s2">, </span><span class="s1">_get_max_identity(dtype)) </span><span class="s2">else None</span>
    <span class="s2">elif </span><span class="s1">monoid_op </span><span class="s2">is </span><span class="s1">min:</span>
      <span class="s2">return </span><span class="s1">_reduce_min </span><span class="s2">if </span><span class="s1">np.equal(aval.val</span><span class="s2">, </span><span class="s1">_get_min_identity(dtype)) </span><span class="s2">else None</span>
  <span class="s2">return None</span>

<span class="s2">def </span><span class="s1">_get_bitwise_and_identity(dtype: DTypeLike) -&gt; np.ndarray:</span>
  <span class="s2">return </span><span class="s1">np.array(-</span><span class="s4">1</span><span class="s1">).astype(dtype)</span>

<span class="s2">def </span><span class="s1">_get_bitwise_or_identity(dtype: DTypeLike) -&gt; np.ndarray:</span>
  <span class="s2">return </span><span class="s1">np.array(</span><span class="s4">0</span><span class="s2">, </span><span class="s1">dtype)</span>

<span class="s2">def </span><span class="s1">_get_sum_identity(dtype: DTypeLike) -&gt; np.ndarray:</span>
  <span class="s2">return </span><span class="s1">np.array(</span><span class="s4">0</span><span class="s2">, </span><span class="s1">dtype)</span>

<span class="s2">def </span><span class="s1">_get_prod_identity(dtype: DTypeLike) -&gt; np.ndarray:</span>
  <span class="s2">return </span><span class="s1">np.array(</span><span class="s4">1</span><span class="s2">, </span><span class="s1">dtype)</span>

<span class="s2">def </span><span class="s1">_get_max_identity(dtype: DTypeLike) -&gt; np.ndarray:</span>
  <span class="s2">if </span><span class="s1">dtypes.issubdtype(dtype</span><span class="s2">, </span><span class="s1">np.inexact):</span>
    <span class="s2">return </span><span class="s1">np.array(-np.inf</span><span class="s2">, </span><span class="s1">dtype)</span>
  <span class="s2">elif </span><span class="s1">dtypes.issubdtype(dtype</span><span class="s2">, </span><span class="s1">np.integer):</span>
    <span class="s2">return </span><span class="s1">np.array(dtypes.iinfo(dtype).min</span><span class="s2">, </span><span class="s1">dtype)</span>
  <span class="s2">elif </span><span class="s1">dtypes.issubdtype(dtype</span><span class="s2">, </span><span class="s1">np.bool_):</span>
    <span class="s2">return </span><span class="s1">np.array(</span><span class="s2">False, </span><span class="s1">np.bool_)</span>
  <span class="s2">else</span><span class="s1">:</span>
    <span class="s2">raise </span><span class="s1">ValueError(</span><span class="s3">f&quot;Unsupported dtype for max: </span><span class="s2">{</span><span class="s1">dtype</span><span class="s2">}</span><span class="s3">&quot;</span><span class="s1">)</span>

<span class="s2">def </span><span class="s1">_get_min_identity(dtype: DTypeLike) -&gt; np.ndarray:</span>
  <span class="s2">if </span><span class="s1">dtypes.issubdtype(dtype</span><span class="s2">, </span><span class="s1">np.inexact):</span>
    <span class="s2">return </span><span class="s1">np.array(np.inf</span><span class="s2">, </span><span class="s1">dtype)</span>
  <span class="s2">elif </span><span class="s1">dtypes.issubdtype(dtype</span><span class="s2">, </span><span class="s1">np.integer):</span>
    <span class="s2">return </span><span class="s1">np.array(dtypes.iinfo(dtype).max</span><span class="s2">, </span><span class="s1">dtype)</span>
  <span class="s2">elif </span><span class="s1">dtypes.issubdtype(dtype</span><span class="s2">, </span><span class="s1">np.bool_):</span>
    <span class="s2">return </span><span class="s1">np.array(</span><span class="s2">True, </span><span class="s1">np.bool_)</span>
  <span class="s2">else</span><span class="s1">:</span>
    <span class="s2">raise </span><span class="s1">ValueError(</span><span class="s3">f&quot;Unsupported dtype for min: </span><span class="s2">{</span><span class="s1">dtype</span><span class="s2">}</span><span class="s3">&quot;</span><span class="s1">)</span>

<span class="s2">def </span><span class="s1">_reduce_sum(operand: ArrayLike</span><span class="s2">, </span><span class="s1">axes: Sequence[int]) -&gt; Array:</span>
  <span class="s2">return </span><span class="s1">reduce_sum_p.bind(operand</span><span class="s2">, </span><span class="s1">axes=tuple(axes))</span>

<span class="s2">def </span><span class="s1">_reduce_prod(operand: ArrayLike</span><span class="s2">, </span><span class="s1">axes: Sequence[int]) -&gt; Array:</span>
  <span class="s2">return </span><span class="s1">reduce_prod_p.bind(operand</span><span class="s2">, </span><span class="s1">axes=tuple(axes))</span>

<span class="s2">def </span><span class="s1">_reduce_max(operand: ArrayLike</span><span class="s2">, </span><span class="s1">axes: Sequence[int]) -&gt; Array:</span>
  <span class="s2">return </span><span class="s1">reduce_max_p.bind(operand</span><span class="s2">, </span><span class="s1">axes=tuple(axes))</span>

<span class="s2">def </span><span class="s1">_reduce_min(operand: ArrayLike</span><span class="s2">, </span><span class="s1">axes: Sequence[int]) -&gt; Array:</span>
  <span class="s2">return </span><span class="s1">reduce_min_p.bind(operand</span><span class="s2">, </span><span class="s1">axes=tuple(axes))</span>

<span class="s2">def </span><span class="s1">_reduce_or(operand: ArrayLike</span><span class="s2">, </span><span class="s1">axes: Sequence[int]) -&gt; Array:</span>
  <span class="s2">return </span><span class="s1">reduce_or_p.bind(operand</span><span class="s2">, </span><span class="s1">axes=tuple(axes))</span>

<span class="s2">def </span><span class="s1">_reduce_and(operand: ArrayLike</span><span class="s2">, </span><span class="s1">axes: Sequence[int]) -&gt; Array:</span>
  <span class="s2">return </span><span class="s1">reduce_and_p.bind(operand</span><span class="s2">, </span><span class="s1">axes=tuple(axes))</span>

<span class="s2">def </span><span class="s1">_reduce_xor(operand: ArrayLike</span><span class="s2">, </span><span class="s1">axes: Sequence[int]) -&gt; Array:</span>
  <span class="s2">return </span><span class="s1">reduce_xor_p.bind(operand</span><span class="s2">, </span><span class="s1">axes=tuple(axes))</span>

<span class="s1">@overload</span>
<span class="s2">def </span><span class="s1">sort(operand: Sequence[Array]</span><span class="s2">, </span><span class="s1">dimension: int = -</span><span class="s4">1</span><span class="s2">,</span>
         <span class="s1">is_stable: bool = </span><span class="s2">True, </span><span class="s1">num_keys: int = </span><span class="s4">1</span><span class="s1">) -&gt; Tuple[Array</span><span class="s2">, </span><span class="s1">...]: ...</span>

<span class="s1">@overload</span>
<span class="s2">def </span><span class="s1">sort(operand: Array</span><span class="s2">, </span><span class="s1">dimension: int = -</span><span class="s4">1</span><span class="s2">,</span>
         <span class="s1">is_stable: bool = </span><span class="s2">True, </span><span class="s1">num_keys: int = </span><span class="s4">1</span><span class="s1">) -&gt; Array: ...</span>

<span class="s2">def </span><span class="s1">sort(operand: Union[Array</span><span class="s2">, </span><span class="s1">Sequence[Array]]</span><span class="s2">, </span><span class="s1">dimension: int = -</span><span class="s4">1</span><span class="s2">,</span>
         <span class="s1">is_stable: bool = </span><span class="s2">True, </span><span class="s1">num_keys: int = </span><span class="s4">1</span><span class="s1">) -&gt; Union[Array</span><span class="s2">, </span><span class="s1">Tuple[Array</span><span class="s2">, </span><span class="s1">...]]:</span>
  <span class="s5">&quot;&quot;&quot;Wraps XLA's `Sort 
  &lt;https://www.tensorflow.org/xla/operation_semantics#sort&gt;`_ operator. 
 
  For floating point inputs, -0.0 and 0.0 are treated as equivalent, and NaN values 
  are sorted to the end of the array. For complex inputs, the sort order is 
  lexicographic over the real and imaginary parts, with the real part primary. 
 
  Args: 
    operand : Array or sequence of arrays 
    dimension : integer dimension along which to sort. Default: -1. 
    is_stable : boolean specifying whether to use a stable sort. Default: True. 
    num_keys : number of operands to treat as sort keys. Default: 1. 
      For num_keys &gt; 1, the sort order will be determined lexicographically using 
      the first `num_keys` arrays, with the first key being primary. 
      The remaining operands will be returned with the same permutation. 
 
  Returns: 
    operand : sorted version of the input or inputs. 
  &quot;&quot;&quot;</span>
  <span class="s2">if </span><span class="s1">isinstance(operand</span><span class="s2">, </span><span class="s1">Sequence):</span>
    <span class="s2">if </span><span class="s1">len(operand) == </span><span class="s4">0</span><span class="s1">:</span>
      <span class="s2">raise </span><span class="s1">TypeError(</span><span class="s3">&quot;Sort requires at least one operand&quot;</span><span class="s1">)</span>
    <span class="s2">if not </span><span class="s1">(</span><span class="s4">1 </span><span class="s1">&lt;= num_keys &lt;= len(operand)):</span>
      <span class="s2">raise </span><span class="s1">ValueError(</span><span class="s3">f&quot;</span><span class="s2">{</span><span class="s1">num_keys=</span><span class="s2">} </span><span class="s3">must be between 1 and </span><span class="s2">{</span><span class="s1">len(operand)=</span><span class="s2">}</span><span class="s3">&quot;</span><span class="s1">)</span>
    <span class="s1">dimension = canonicalize_axis(dimension</span><span class="s2">, </span><span class="s1">len(operand[</span><span class="s4">0</span><span class="s1">].shape))</span>
    <span class="s2">return </span><span class="s1">tuple(sort_p.bind(*operand</span><span class="s2">, </span><span class="s1">dimension=dimension</span><span class="s2">,</span>
                             <span class="s1">is_stable=is_stable</span><span class="s2">,</span>
                             <span class="s1">num_keys=num_keys))</span>
  <span class="s2">else</span><span class="s1">:</span>
    <span class="s2">if </span><span class="s1">num_keys != </span><span class="s4">1</span><span class="s1">:</span>
      <span class="s2">raise </span><span class="s1">ValueError(</span><span class="s3">f&quot;</span><span class="s2">{</span><span class="s1">num_keys=</span><span class="s2">} </span><span class="s3">must equal 1 for a single operand.&quot;</span><span class="s1">)</span>
    <span class="s1">dimension = canonicalize_axis(dimension</span><span class="s2">, </span><span class="s1">len(operand.shape))</span>
    <span class="s2">return </span><span class="s1">sort_p.bind(operand</span><span class="s2">, </span><span class="s1">dimension=dimension</span><span class="s2">, </span><span class="s1">is_stable=is_stable</span><span class="s2">, </span><span class="s1">num_keys=</span><span class="s4">1</span><span class="s1">)[</span><span class="s4">0</span><span class="s1">]</span>

<span class="s2">def </span><span class="s1">sort_key_val(keys: Array</span><span class="s2">, </span><span class="s1">values: ArrayLike</span><span class="s2">, </span><span class="s1">dimension: int = -</span><span class="s4">1</span><span class="s2">,</span>
                 <span class="s1">is_stable: bool = </span><span class="s2">True</span><span class="s1">) -&gt; Tuple[Array</span><span class="s2">, </span><span class="s1">Array]:</span>
  <span class="s5">&quot;&quot;&quot;Sorts ``keys`` along ``dimension`` and applies the same permutation to ``values``.&quot;&quot;&quot;</span>
  <span class="s1">dimension = canonicalize_axis(dimension</span><span class="s2">, </span><span class="s1">len(keys.shape))</span>
  <span class="s1">k</span><span class="s2">, </span><span class="s1">v = sort_p.bind(keys</span><span class="s2">, </span><span class="s1">values</span><span class="s2">, </span><span class="s1">dimension=dimension</span><span class="s2">, </span><span class="s1">is_stable=is_stable</span><span class="s2">, </span><span class="s1">num_keys=</span><span class="s4">1</span><span class="s1">)</span>
  <span class="s2">return </span><span class="s1">k</span><span class="s2">, </span><span class="s1">v</span>

<span class="s2">def </span><span class="s1">top_k(operand: ArrayLike</span><span class="s2">, </span><span class="s1">k: int) -&gt; Tuple[Array</span><span class="s2">, </span><span class="s1">Array]:</span>
  <span class="s5">&quot;&quot;&quot;Returns top ``k`` values and their indices along the last axis of ``operand``. 
 
  Args: 
    operand: N-dimensional array of non-complex type. 
    k: integer specifying the number of top entries. 
 
  Returns: 
    values: array containing the top k values along the last axis. 
    indices: array containing the indices corresponding to values. 
 
  See also: 
  - :func:`jax.lax.approx_max_k` 
  - :func:`jax.lax.approx_min_k` 
  &quot;&quot;&quot;</span>
  <span class="s1">k = int(k)</span>
  <span class="s2">if </span><span class="s1">k &lt; </span><span class="s4">0</span><span class="s1">:</span>
    <span class="s2">raise </span><span class="s1">ValueError(</span><span class="s3">f&quot;k argument to top_k must be nonnegative, got </span><span class="s2">{</span><span class="s1">k</span><span class="s2">}</span><span class="s3">&quot;</span><span class="s1">)</span>
  <span class="s2">return </span><span class="s1">top_k_p.bind(operand</span><span class="s2">, </span><span class="s1">k=k)</span>

<span class="s2">def </span><span class="s1">tie_in(x: Any</span><span class="s2">, </span><span class="s1">y: T) -&gt; T:</span>
  <span class="s5">&quot;&quot;&quot;Deprecated. Ignores ``x`` and returns ``y``.&quot;&quot;&quot;</span>
  <span class="s2">return </span><span class="s1">y</span>

<span class="s2">def </span><span class="s1">full(shape: Shape</span><span class="s2">, </span><span class="s1">fill_value: ArrayLike</span><span class="s2">, </span><span class="s1">dtype: Optional[DTypeLike] = </span><span class="s2">None</span><span class="s1">) -&gt; Array:</span>
  <span class="s5">&quot;&quot;&quot;Returns an array of `shape` filled with `fill_value`. 
 
  Args: 
    shape: sequence of integers, describing the shape of the output array. 
    fill_value: the value to fill the new array with. 
    dtype: the type of the output array, or `None`. If not `None`, `fill_value` 
      will be cast to `dtype`. 
  &quot;&quot;&quot;</span>
  <span class="s1">shape = canonicalize_shape(shape)</span>
  <span class="s2">if </span><span class="s1">np.shape(fill_value):</span>
    <span class="s1">msg = </span><span class="s3">&quot;full must be called with scalar fill_value, got fill_value.shape {}.&quot;</span>
    <span class="s2">raise </span><span class="s1">TypeError(msg.format(np.shape(fill_value)))</span>
  <span class="s1">weak_type = dtype </span><span class="s2">is None and </span><span class="s1">dtypes.is_weakly_typed(fill_value)</span>
  <span class="s1">dtype = dtypes.canonicalize_dtype(dtype </span><span class="s2">or </span><span class="s1">_dtype(fill_value))</span>
  <span class="s1">fill_value = _convert_element_type(fill_value</span><span class="s2">, </span><span class="s1">dtype</span><span class="s2">, </span><span class="s1">weak_type)</span>
  <span class="s2">return </span><span class="s1">broadcast(fill_value</span><span class="s2">, </span><span class="s1">shape)</span>

<span class="s2">def </span><span class="s1">zeros_like_shaped_array(aval: ShapedArray) -&gt; Array:</span>
  <span class="s2">assert </span><span class="s1">isinstance(aval</span><span class="s2">, </span><span class="s1">ShapedArray)</span>
  <span class="s2">if </span><span class="s1">aval.dtype == dtypes.float0:</span>
    <span class="s1">scalar_zero = np.zeros(()</span><span class="s2">, </span><span class="s1">dtype=aval.dtype)</span>
  <span class="s2">else</span><span class="s1">:</span>
    <span class="s1">scalar_zero = _convert_element_type(</span><span class="s4">0</span><span class="s2">, </span><span class="s1">aval.dtype</span><span class="s2">, </span><span class="s1">aval.weak_type)</span>
  <span class="s2">return </span><span class="s1">broadcast(scalar_zero</span><span class="s2">, </span><span class="s1">aval.shape)</span>

<span class="s1">ad_util.aval_zeros_likers[ShapedArray] = zeros_like_shaped_array</span>

<span class="s2">def </span><span class="s1">iota(dtype: DTypeLike</span><span class="s2">, </span><span class="s1">size: int) -&gt; Array:</span>
  <span class="s5">&quot;&quot;&quot;Wraps XLA's `Iota 
  &lt;https://www.tensorflow.org/xla/operation_semantics#iota&gt;`_ 
  operator. 
  &quot;&quot;&quot;</span>
  <span class="s2">return </span><span class="s1">broadcasted_iota(dtype</span><span class="s2">, </span><span class="s1">(size</span><span class="s2">,</span><span class="s1">)</span><span class="s2">, </span><span class="s4">0</span><span class="s1">)</span>

<span class="s2">def </span><span class="s1">broadcasted_iota(dtype: DTypeLike</span><span class="s2">, </span><span class="s1">shape: Shape</span><span class="s2">, </span><span class="s1">dimension: int) -&gt; Array:</span>
  <span class="s5">&quot;&quot;&quot;Convenience wrapper around ``iota``.&quot;&quot;&quot;</span>
  <span class="s1">dtype = dtypes.canonicalize_dtype(dtype)</span>
  <span class="s1">shape = canonicalize_shape(shape)</span>
  <span class="s1">dynamic_shape = [d </span><span class="s2">for </span><span class="s1">d </span><span class="s2">in </span><span class="s1">shape </span><span class="s2">if </span><span class="s1">isinstance(d</span><span class="s2">, </span><span class="s1">core.Tracer)]</span>
  <span class="s1">static_shape = [</span><span class="s2">None if </span><span class="s1">isinstance(d</span><span class="s2">, </span><span class="s1">core.Tracer) </span><span class="s2">else </span><span class="s1">d </span><span class="s2">for </span><span class="s1">d </span><span class="s2">in </span><span class="s1">shape]</span>
  <span class="s1">dimension = core.concrete_or_error(</span>
      <span class="s1">int</span><span class="s2">, </span><span class="s1">dimension</span><span class="s2">, </span><span class="s3">&quot;dimension argument of lax.broadcasted_iota&quot;</span><span class="s1">)</span>
  <span class="s2">return </span><span class="s1">iota_p.bind(*dynamic_shape</span><span class="s2">, </span><span class="s1">dtype=dtype</span><span class="s2">, </span><span class="s1">shape=tuple(static_shape)</span><span class="s2">,</span>
                     <span class="s1">dimension=dimension)</span>

<span class="s2">def </span><span class="s1">_eye(dtype: DTypeLike</span><span class="s2">, </span><span class="s1">shape: Shape</span><span class="s2">, </span><span class="s1">offset: int) -&gt; Array:</span>
  <span class="s5">&quot;&quot;&quot;Like numpy.eye, create a 2D array with ones on a diagonal.&quot;&quot;&quot;</span>
  <span class="s1">offset = int(offset)</span>
  <span class="s1">dtype = dtypes.canonicalize_dtype(dtype)</span>
  <span class="s1">bool_eye = eq(add(broadcasted_iota(np.int32</span><span class="s2">, </span><span class="s1">shape</span><span class="s2">, </span><span class="s4">0</span><span class="s1">)</span><span class="s2">, </span><span class="s1">np.int32(offset))</span><span class="s2">,</span>
                <span class="s1">broadcasted_iota(np.int32</span><span class="s2">, </span><span class="s1">shape</span><span class="s2">, </span><span class="s4">1</span><span class="s1">))</span>
  <span class="s2">return </span><span class="s1">convert_element_type_p.bind(bool_eye</span><span class="s2">, </span><span class="s1">new_dtype=dtype</span><span class="s2">, </span><span class="s1">weak_type=</span><span class="s2">False</span><span class="s1">)</span>

<span class="s2">def </span><span class="s1">_delta(dtype: DTypeLike</span><span class="s2">, </span><span class="s1">shape: Shape</span><span class="s2">, </span><span class="s1">axes: Sequence[int]) -&gt; Array:</span>
  <span class="s5">&quot;&quot;&quot;This utility function exists for creating Kronecker delta arrays.&quot;&quot;&quot;</span>
  <span class="s1">axes = map(int</span><span class="s2">, </span><span class="s1">axes)</span>
  <span class="s1">dtype = dtypes.canonicalize_dtype(dtype)</span>
  <span class="s1">base_shape = tuple(np.take(shape</span><span class="s2">, </span><span class="s1">axes))  </span><span class="s0"># type: ignore[arg-type]</span>
  <span class="s1">iotas = [broadcasted_iota(np.uint32</span><span class="s2">, </span><span class="s1">base_shape</span><span class="s2">, </span><span class="s1">i)</span>
           <span class="s2">for </span><span class="s1">i </span><span class="s2">in </span><span class="s1">range(len(base_shape))]</span>
  <span class="s1">eyes = [eq(i1</span><span class="s2">, </span><span class="s1">i2) </span><span class="s2">for </span><span class="s1">i1</span><span class="s2">, </span><span class="s1">i2 </span><span class="s2">in </span><span class="s1">zip(iotas[:-</span><span class="s4">1</span><span class="s1">]</span><span class="s2">, </span><span class="s1">iotas[</span><span class="s4">1</span><span class="s1">:])]</span>
  <span class="s1">result = convert_element_type_p.bind(_reduce(operator.and_</span><span class="s2">, </span><span class="s1">eyes)</span><span class="s2">,</span>
                                       <span class="s1">new_dtype=dtype</span><span class="s2">, </span><span class="s1">weak_type=</span><span class="s2">False</span><span class="s1">)</span>
  <span class="s2">return </span><span class="s1">broadcast_in_dim(result</span><span class="s2">, </span><span class="s1">shape</span><span class="s2">, </span><span class="s1">axes)</span>

<span class="s2">def </span><span class="s1">_tri(dtype: DTypeLike</span><span class="s2">, </span><span class="s1">shape: Shape</span><span class="s2">, </span><span class="s1">offset: int) -&gt; Array:</span>
  <span class="s5">&quot;&quot;&quot;Like numpy.tri, create a 2D array with ones below a diagonal.&quot;&quot;&quot;</span>
  <span class="s1">offset = int(offset)</span>
  <span class="s1">dtype = dtypes.canonicalize_dtype(dtype)</span>
  <span class="s1">bool_tri = ge(add(broadcasted_iota(np.int32</span><span class="s2">, </span><span class="s1">shape</span><span class="s2">, </span><span class="s4">0</span><span class="s1">)</span><span class="s2">, </span><span class="s1">np.int32(offset))</span><span class="s2">,</span>
                <span class="s1">broadcasted_iota(np.int32</span><span class="s2">, </span><span class="s1">shape</span><span class="s2">, </span><span class="s4">1</span><span class="s1">))</span>
  <span class="s2">return </span><span class="s1">convert_element_type_p.bind(bool_tri</span><span class="s2">, </span><span class="s1">new_dtype=dtype</span><span class="s2">, </span><span class="s1">weak_type=</span><span class="s2">False</span><span class="s1">)</span>

<span class="s2">def </span><span class="s1">stop_gradient(x: T) -&gt; T:</span>
  <span class="s5">&quot;&quot;&quot;Stops gradient computation. 
 
  Operationally ``stop_gradient`` is the identity function, that is, it returns 
  argument `x` unchanged. However, ``stop_gradient`` prevents the flow of 
  gradients during forward or reverse-mode automatic differentiation. If there 
  are multiple nested gradient computations, ``stop_gradient`` stops gradients 
  for all of them. 
 
  For example: 
 
  &gt;&gt;&gt; jax.grad(lambda x: x**2)(3.) 
  Array(6., dtype=float32, weak_type=True) 
  &gt;&gt;&gt; jax.grad(lambda x: jax.lax.stop_gradient(x)**2)(3.) 
  Array(0., dtype=float32, weak_type=True) 
  &gt;&gt;&gt; jax.grad(jax.grad(lambda x: x**2))(3.) 
  Array(2., dtype=float32, weak_type=True) 
  &gt;&gt;&gt; jax.grad(jax.grad(lambda x: jax.lax.stop_gradient(x)**2))(3.) 
  Array(0., dtype=float32, weak_type=True) 
  &quot;&quot;&quot;</span>
  <span class="s2">def </span><span class="s1">stop(x):</span>
    <span class="s0"># only bind primitive on inexact dtypes, to avoid some staging</span>
    <span class="s2">if </span><span class="s1">core.has_opaque_dtype(x):</span>
      <span class="s2">return </span><span class="s1">x</span>
    <span class="s2">elif </span><span class="s1">(dtypes.issubdtype(_dtype(x)</span><span class="s2">, </span><span class="s1">np.floating) </span><span class="s2">or</span>
        <span class="s1">dtypes.issubdtype(_dtype(x)</span><span class="s2">, </span><span class="s1">np.complexfloating)):</span>
      <span class="s2">return </span><span class="s1">ad_util.stop_gradient_p.bind(x)</span>
    <span class="s2">else</span><span class="s1">:</span>
      <span class="s2">return </span><span class="s1">x</span>
  <span class="s2">return </span><span class="s1">tree_map(stop</span><span class="s2">, </span><span class="s1">x)</span>

<span class="s2">def </span><span class="s1">reduce_precision(operand: Union[float</span><span class="s2">, </span><span class="s1">ArrayLike]</span><span class="s2">,</span>
                     <span class="s1">exponent_bits: int</span><span class="s2">,</span>
                     <span class="s1">mantissa_bits: int) -&gt; Array:</span>
  <span class="s5">&quot;&quot;&quot;Wraps XLA's `ReducePrecision 
  &lt;https://www.tensorflow.org/xla/operation_semantics#reduceprecision&gt;`_ 
  operator. 
  &quot;&quot;&quot;</span>
  <span class="s1">exponent_bits = core.concrete_or_error(</span>
    <span class="s1">operator.index</span><span class="s2">, </span><span class="s1">exponent_bits</span><span class="s2">, </span><span class="s3">&quot;exponent_bits argument of lax.reduce_precision&quot;</span><span class="s1">)</span>
  <span class="s1">mantissa_bits = core.concrete_or_error(</span>
    <span class="s1">operator.index</span><span class="s2">, </span><span class="s1">mantissa_bits</span><span class="s2">, </span><span class="s3">&quot;mantissa_bits argument of lax.reduce_precision&quot;</span><span class="s1">)</span>
  <span class="s2">return </span><span class="s1">reduce_precision_p.bind(operand</span><span class="s2">, </span><span class="s1">exponent_bits=exponent_bits</span><span class="s2">, </span><span class="s1">mantissa_bits=mantissa_bits)</span>

<span class="s2">def </span><span class="s1">squeeze(array: ArrayLike</span><span class="s2">, </span><span class="s1">dimensions: Sequence[int]) -&gt; Array:</span>
  <span class="s5">&quot;&quot;&quot;Squeeze any number of size 1 dimensions from an array.&quot;&quot;&quot;</span>
  <span class="s1">ndim = np.ndim(array)</span>
  <span class="s1">dimensions = tuple(sorted(canonicalize_axis(i</span><span class="s2">, </span><span class="s1">ndim) </span><span class="s2">for </span><span class="s1">i </span><span class="s2">in </span><span class="s1">dimensions))</span>
  <span class="s2">if not </span><span class="s1">dimensions </span><span class="s2">and </span><span class="s1">isinstance(array</span><span class="s2">, </span><span class="s1">Array):</span>
    <span class="s2">return </span><span class="s1">type_cast(Array</span><span class="s2">, </span><span class="s1">array)</span>
  <span class="s2">return </span><span class="s1">squeeze_p.bind(array</span><span class="s2">, </span><span class="s1">dimensions=dimensions)</span>

<span class="s2">def </span><span class="s1">expand_dims(array: ArrayLike</span><span class="s2">, </span><span class="s1">dimensions: Sequence[int]) -&gt; Array:</span>
  <span class="s5">&quot;&quot;&quot;Insert any number of size 1 dimensions into an array.&quot;&quot;&quot;</span>
  <span class="s2">if </span><span class="s1">len(set(dimensions)) != len(dimensions):</span>
    <span class="s2">raise </span><span class="s1">ValueError(</span><span class="s3">f'repeated axis in lax.expand_dims: </span><span class="s2">{</span><span class="s1">dimensions</span><span class="s2">}</span><span class="s3">'</span><span class="s1">)</span>
  <span class="s1">ndim_out = np.ndim(array) + len(dimensions)</span>
  <span class="s1">dims = [canonicalize_axis(i</span><span class="s2">, </span><span class="s1">ndim_out) </span><span class="s2">for </span><span class="s1">i </span><span class="s2">in </span><span class="s1">dimensions]</span>
  <span class="s2">if </span><span class="s1">len(set(dims)) != len(dims):  </span><span class="s0"># check again after canonicalizing</span>
    <span class="s2">raise </span><span class="s1">ValueError(</span><span class="s3">f'repeated axis in lax.expand_dims: </span><span class="s2">{</span><span class="s1">dims</span><span class="s2">}</span><span class="s3">'</span><span class="s1">)</span>
  <span class="s1">dims_set = frozenset(dims)</span>
  <span class="s1">result_shape = list(np.shape(array))</span>
  <span class="s2">for </span><span class="s1">i </span><span class="s2">in </span><span class="s1">sorted(dims_set):</span>
    <span class="s1">result_shape.insert(i</span><span class="s2">, </span><span class="s4">1</span><span class="s1">)</span>
  <span class="s1">broadcast_dims = [i </span><span class="s2">for </span><span class="s1">i </span><span class="s2">in </span><span class="s1">range(ndim_out) </span><span class="s2">if </span><span class="s1">i </span><span class="s2">not in </span><span class="s1">dims_set]</span>
  <span class="s2">return </span><span class="s1">broadcast_in_dim(array</span><span class="s2">, </span><span class="s1">result_shape</span><span class="s2">, </span><span class="s1">broadcast_dims)</span>


<span class="s0">### convenience wrappers around traceables</span>

<span class="s2">def </span><span class="s1">full_like(x: ArrayLike</span><span class="s2">, </span><span class="s1">fill_value: ArrayLike</span><span class="s2">, </span><span class="s1">dtype: Optional[DTypeLike] = </span><span class="s2">None,</span>
              <span class="s1">shape: Optional[Shape] = </span><span class="s2">None</span><span class="s1">) -&gt; Array:</span>
  <span class="s5">&quot;&quot;&quot;Create a full array like np.full based on the example array `x`. 
 
  Args: 
    x: example array-like, used for shape and dtype information. 
    fill_value: a scalar value to fill the entries of the output array. 
    dtype: optional, a dtype parameter for the output ndarray. 
    shape: optional, a shape parameter for the output ndarray. 
 
  Returns: 
    An ndarray with the same shape as `x` with its entries set equal to 
    `fill_value`, similar to the output of np.full. 
  &quot;&quot;&quot;</span>
  <span class="s1">fill_shape = np.shape(x) </span><span class="s2">if </span><span class="s1">shape </span><span class="s2">is None else </span><span class="s1">canonicalize_shape(shape)</span>
  <span class="s1">weak_type = dtype </span><span class="s2">is None and </span><span class="s1">dtypes.is_weakly_typed(x)</span>
  <span class="s1">dtype = dtype </span><span class="s2">or </span><span class="s1">_dtype(x)</span>
  <span class="s1">val = full(fill_shape</span><span class="s2">, </span><span class="s1">_convert_element_type(fill_value</span><span class="s2">, </span><span class="s1">dtype</span><span class="s2">, </span><span class="s1">weak_type))</span>
  <span class="s0"># If the sharding is SingleDeviceSharding then don't take the `if` branch</span>
  <span class="s0"># because `val` is already an array with SingleDeviceSharding making this an</span>
  <span class="s0"># optimization.</span>
  <span class="s0"># TODO(yashkatariya,mattjj): `x` and `val` should have the same sharding,</span>
  <span class="s0"># probably in the form of a primitive like `val = match_sharding_p.bind(x, val)`</span>
  <span class="s0"># (so it works in staged-out code as well as 'eager' code). Related to</span>
  <span class="s0"># equi-sharding.</span>
  <span class="s2">if </span><span class="s1">shape </span><span class="s2">is None and </span><span class="s1">isinstance(x</span><span class="s2">, </span><span class="s1">array.ArrayImpl):</span>
    <span class="s1">sharding = x.sharding  </span><span class="s0"># type: ignore[union-attr]</span>
    <span class="s2">if </span><span class="s1">(</span><span class="s2">not </span><span class="s1">dispatch.is_single_device_sharding(sharding) </span><span class="s2">and</span>
        <span class="s2">not </span><span class="s1">isinstance(sharding</span><span class="s2">, </span><span class="s1">PmapSharding)):</span>
      <span class="s2">return </span><span class="s1">array.make_array_from_callback(</span>
          <span class="s1">type_cast(array.Shape</span><span class="s2">, </span><span class="s1">fill_shape)</span><span class="s2">, </span><span class="s1">sharding</span><span class="s2">, lambda </span><span class="s1">idx: val[idx])</span>
  <span class="s2">return </span><span class="s1">val</span>


<span class="s2">def </span><span class="s1">collapse(operand: Array</span><span class="s2">, </span><span class="s1">start_dimension: int</span><span class="s2">,</span>
             <span class="s1">stop_dimension: int) -&gt; Array:</span>
  <span class="s5">&quot;&quot;&quot;Collapses dimensions of an array into a single dimension. 
 
  For example, if ``operand`` is an array with shape ``[2, 3, 4]``, 
  ``collapse(operand, 0, 2).shape == [6, 4]``. The elements of the collapsed 
  dimension are laid out major-to-minor, i.e., with the lowest-numbered 
  dimension as the slowest varying dimension. 
 
  Args: 
    operand: an input array. 
    start_dimension: the start of the dimensions to collapse (inclusive). 
    stop_dimension: the end of the dimensions to collapse (exclusive). 
 
  Returns: 
    An array where dimensions ``[start_dimension, stop_dimension)`` have been 
    collapsed (raveled) into a single dimension. 
  &quot;&quot;&quot;</span>
  <span class="s1">lo</span><span class="s2">, </span><span class="s1">hi = start_dimension</span><span class="s2">, </span><span class="s1">stop_dimension</span>
  <span class="s1">size = math.prod(operand.shape[lo:hi])</span>
  <span class="s1">new_shape = operand.shape[:lo] + (size</span><span class="s2">,</span><span class="s1">) + operand.shape[hi:]</span>
  <span class="s2">return </span><span class="s1">reshape(operand</span><span class="s2">, </span><span class="s1">new_shape)</span>


<span class="s2">def </span><span class="s1">batch_matmul(lhs: Array</span><span class="s2">, </span><span class="s1">rhs: Array</span><span class="s2">,</span>
                 <span class="s1">precision: PrecisionLike = </span><span class="s2">None</span><span class="s1">) -&gt; Array:</span>
  <span class="s5">&quot;&quot;&quot;Batch matrix multiplication.&quot;&quot;&quot;</span>
  <span class="s2">if </span><span class="s1">_min(lhs.ndim</span><span class="s2">, </span><span class="s1">rhs.ndim) &lt; </span><span class="s4">2</span><span class="s1">:</span>
    <span class="s2">raise </span><span class="s1">ValueError(</span><span class="s3">'Arguments to batch_matmul must be at least 2D, got {}, {}'</span>
                     <span class="s1">.format(lhs.ndim</span><span class="s2">, </span><span class="s1">rhs.ndim))</span>
  <span class="s2">if </span><span class="s1">lhs.ndim != rhs.ndim:</span>
    <span class="s2">raise </span><span class="s1">ValueError(</span><span class="s3">'Arguments to batch_matmul must have same ndim, got {}, {}'</span>
                     <span class="s1">.format(lhs.ndim</span><span class="s2">, </span><span class="s1">rhs.ndim))</span>
  <span class="s1">lhs_contract = (lhs.ndim - </span><span class="s4">1</span><span class="s2">,</span><span class="s1">)</span>
  <span class="s1">rhs_contract = (rhs.ndim - </span><span class="s4">2</span><span class="s2">,</span><span class="s1">)</span>
  <span class="s1">batch = tuple(range(lhs.ndim - </span><span class="s4">2</span><span class="s1">))</span>
  <span class="s2">return </span><span class="s1">dot_general(lhs</span><span class="s2">, </span><span class="s1">rhs</span><span class="s2">, </span><span class="s1">((lhs_contract</span><span class="s2">, </span><span class="s1">rhs_contract)</span><span class="s2">, </span><span class="s1">(batch</span><span class="s2">, </span><span class="s1">batch))</span><span class="s2">,</span>
                     <span class="s1">precision=precision)</span>


<span class="s0"># These functions also exist in the XLA client library, but we treat them</span>
<span class="s0"># as non-primitive to maintain a smaller set of autodiff primitives.</span>

<span class="s2">def </span><span class="s1">square(x: ArrayLike) -&gt; Array:</span>
  <span class="s5">r&quot;&quot;&quot;Elementwise square: :math:`x^2`.&quot;&quot;&quot;</span>
  <span class="s2">return </span><span class="s1">integer_pow(x</span><span class="s2">, </span><span class="s4">2</span><span class="s1">)</span>

<span class="s2">def </span><span class="s1">reciprocal(x: ArrayLike) -&gt; Array:</span>
  <span class="s5">r&quot;&quot;&quot;Elementwise reciprocal: :math:`1 \over x`.&quot;&quot;&quot;</span>
  <span class="s2">return </span><span class="s1">integer_pow(x</span><span class="s2">, </span><span class="s1">-</span><span class="s4">1</span><span class="s1">)</span>

<span class="s2">def </span><span class="s1">_upcast_fp16_for_computation(f):</span>
  <span class="s1">@functools.wraps(f)</span>
  <span class="s2">def </span><span class="s1">f_wrapped(x):</span>
    <span class="s1">dtype = _dtype(x)</span>
    <span class="s2">if </span><span class="s1">dtype == np.float16 </span><span class="s2">or </span><span class="s1">dtype == dtypes.bfloat16:</span>
      <span class="s2">return </span><span class="s1">convert_element_type(</span>
        <span class="s1">f(convert_element_type(x</span><span class="s2">, </span><span class="s1">np.float32))</span><span class="s2">, </span><span class="s1">dtype)</span>
    <span class="s2">return </span><span class="s1">f(x)</span>

  <span class="s2">return </span><span class="s1">f_wrapped</span>

<span class="s2">def </span><span class="s1">tan(x: ArrayLike) -&gt; Array:</span>
  <span class="s5">r&quot;&quot;&quot;Elementwise tangent: :math:`\mathrm{tan}(x)`.&quot;&quot;&quot;</span>
  <span class="s2">return </span><span class="s1">tan_p.bind(x)</span>

<span class="s2">def </span><span class="s1">asin(x: ArrayLike) -&gt; Array:</span>
  <span class="s5">r&quot;&quot;&quot;Elementwise arc sine: :math:`\mathrm{asin}(x)`.&quot;&quot;&quot;</span>
  <span class="s2">return </span><span class="s1">asin_p.bind(x)</span>

<span class="s2">def </span><span class="s1">acos(x: ArrayLike) -&gt; Array:</span>
  <span class="s5">r&quot;&quot;&quot;Elementwise arc cosine: :math:`\mathrm{acos}(x)`.&quot;&quot;&quot;</span>
  <span class="s2">return </span><span class="s1">acos_p.bind(x)</span>

<span class="s2">def </span><span class="s1">atan(x: ArrayLike) -&gt; Array:</span>
  <span class="s5">r&quot;&quot;&quot;Elementwise arc tangent: :math:`\mathrm{atan}(x)`.&quot;&quot;&quot;</span>
  <span class="s2">return </span><span class="s1">atan_p.bind(x)</span>

<span class="s2">def </span><span class="s1">sinh(x: ArrayLike) -&gt; Array:</span>
  <span class="s5">r&quot;&quot;&quot;Elementwise hyperbolic sine: :math:`\mathrm{sinh}(x)`.&quot;&quot;&quot;</span>
  <span class="s2">return </span><span class="s1">sinh_p.bind(x)</span>

<span class="s2">def </span><span class="s1">cosh(x: ArrayLike) -&gt; Array:</span>
  <span class="s5">r&quot;&quot;&quot;Elementwise hyperbolic cosine: :math:`\mathrm{cosh}(x)`.&quot;&quot;&quot;</span>
  <span class="s2">return </span><span class="s1">cosh_p.bind(x)</span>

<span class="s2">def </span><span class="s1">asinh(x: ArrayLike) -&gt; Array:</span>
  <span class="s5">r&quot;&quot;&quot;Elementwise inverse hyperbolic sine: :math:`\mathrm{asinh}(x)`.&quot;&quot;&quot;</span>
  <span class="s2">return </span><span class="s1">asinh_p.bind(x)</span>

<span class="s2">def </span><span class="s1">acosh(x: ArrayLike) -&gt; Array:</span>
  <span class="s5">r&quot;&quot;&quot;Elementwise inverse hyperbolic cosine: :math:`\mathrm{acosh}(x)`.&quot;&quot;&quot;</span>
  <span class="s2">return </span><span class="s1">acosh_p.bind(x)</span>

<span class="s2">def </span><span class="s1">atanh(x: ArrayLike) -&gt; Array:</span>
  <span class="s5">r&quot;&quot;&quot;Elementwise inverse hyperbolic tangent: :math:`\mathrm{atanh}(x)`.&quot;&quot;&quot;</span>
  <span class="s2">return </span><span class="s1">atanh_p.bind(x)</span>


<span class="s0"># Add some methods to ShapedArray that rely on lax primitives</span>

<span class="s1">ShapedArray.broadcast = core.aval_method(broadcast)</span>
<span class="s1">ShapedArray.transpose = core.aval_method(transpose)  </span><span class="s0"># clobbered by lax_numpy</span>
<span class="s1">ShapedArray.reshape = core.aval_method(reshape)      </span><span class="s0"># clobbered by lax_numpy</span>

<span class="s2">def </span><span class="s1">_iter(tracer):</span>
  <span class="s2">if </span><span class="s1">tracer.ndim == </span><span class="s4">0</span><span class="s1">:</span>
    <span class="s2">raise </span><span class="s1">TypeError(</span><span class="s3">&quot;iteration over a 0-d array&quot;</span><span class="s1">)  </span><span class="s0"># same as numpy error</span>
  <span class="s2">else</span><span class="s1">:</span>
    <span class="s1">n = int(tracer.shape[</span><span class="s4">0</span><span class="s1">])</span>
    <span class="s2">if </span><span class="s1">any(isinstance(d</span><span class="s2">, </span><span class="s1">core.Tracer) </span><span class="s2">for </span><span class="s1">d </span><span class="s2">in </span><span class="s1">tracer.shape):</span>
      <span class="s2">return </span><span class="s1">(slicing.dynamic_index_in_dim(tracer</span><span class="s2">, </span><span class="s1">i</span><span class="s2">, </span><span class="s1">keepdims=</span><span class="s2">False</span><span class="s1">)</span>
              <span class="s2">for </span><span class="s1">i </span><span class="s2">in </span><span class="s1">range(n))</span>
    <span class="s2">else</span><span class="s1">:</span>
      <span class="s2">return </span><span class="s1">(slicing.index_in_dim(tracer</span><span class="s2">, </span><span class="s1">i</span><span class="s2">, </span><span class="s1">keepdims=</span><span class="s2">False</span><span class="s1">) </span><span class="s2">for </span><span class="s1">i </span><span class="s2">in </span><span class="s1">range(n))</span>
<span class="s1">ShapedArray._iter = staticmethod(_iter)</span>
<span class="s1">core.DShapedArray._iter = staticmethod(_iter)</span>

<span class="s0"># Add some ad handlers that use (or could use) lax primitives</span>

<span class="s2">def </span><span class="s1">zeros_like_array(x: ArrayLike) -&gt; Array:</span>
  <span class="s2">return </span><span class="s1">full_like(x</span><span class="s2">, </span><span class="s4">0</span><span class="s1">)</span>

<span class="s2">for </span><span class="s1">t </span><span class="s2">in </span><span class="s1">itertools.chain(</span>
    <span class="s1">dtypes.python_scalar_dtypes.keys()</span><span class="s2">, </span><span class="s1">array_types</span><span class="s2">,</span>
    <span class="s1">device_array.device_array_types</span><span class="s2">, </span><span class="s1">[array.ArrayImpl]):</span>
  <span class="s1">ad_util.jaxval_adders[t] = add</span>
<span class="s1">ad_util.jaxval_zeros_likers[device_array._DeviceArray] = zeros_like_array</span>
<span class="s1">ad_util.jaxval_zeros_likers[device_array.Buffer] = zeros_like_array</span>
<span class="s1">ad_util.jaxval_zeros_likers[array.ArrayImpl] = zeros_like_array</span>


<span class="s0">### primitives</span>


<span class="s1">_fixed_dtype = </span><span class="s2">lambda </span><span class="s1">dtype: </span><span class="s2">lambda </span><span class="s1">*args</span><span class="s2">, </span><span class="s1">**kwargs: dtypes.canonicalize_dtype(dtype)</span>
<span class="s1">_complex_basetype = </span><span class="s2">lambda </span><span class="s1">dtype: np.abs(np.zeros(()</span><span class="s2">, </span><span class="s1">dtype)).dtype</span>

<span class="s1">_strip_weak_type = </span><span class="s2">lambda </span><span class="s1">*args</span><span class="s2">, </span><span class="s1">**_: </span><span class="s2">False</span>


<span class="s2">def </span><span class="s1">unop_dtype_rule(result_dtype</span><span class="s2">, </span><span class="s1">accepted_dtypes</span><span class="s2">, </span><span class="s1">name</span><span class="s2">, </span><span class="s1">aval</span><span class="s2">, </span><span class="s1">**kwargs):</span>
  <span class="s2">if not </span><span class="s1">any(dtypes.issubdtype(aval.dtype</span><span class="s2">, </span><span class="s1">t) </span><span class="s2">for </span><span class="s1">t </span><span class="s2">in </span><span class="s1">accepted_dtypes):</span>
    <span class="s1">msg = </span><span class="s3">'{} does not accept dtype {}. Accepted dtypes are subtypes of {}.'</span>
    <span class="s1">typename = str(np.dtype(aval.dtype).name)</span>
    <span class="s1">accepted_typenames = (t.__name__ </span><span class="s2">for </span><span class="s1">t </span><span class="s2">in </span><span class="s1">accepted_dtypes)</span>
    <span class="s2">raise </span><span class="s1">TypeError(msg.format(name</span><span class="s2">, </span><span class="s1">typename</span><span class="s2">, </span><span class="s3">', '</span><span class="s1">.join(accepted_typenames)))</span>
  <span class="s2">return </span><span class="s1">result_dtype(aval.dtype)</span>


<span class="s2">def </span><span class="s1">unop(result_dtype</span><span class="s2">, </span><span class="s1">accepted_dtypes</span><span class="s2">, </span><span class="s1">name):</span>
  <span class="s1">dtype_rule = partial(unop_dtype_rule</span><span class="s2">, </span><span class="s1">result_dtype</span><span class="s2">, </span><span class="s1">accepted_dtypes</span><span class="s2">, </span><span class="s1">name)</span>
  <span class="s1">weak_type_rule = partial(_naryop_weak_type_rule</span><span class="s2">, </span><span class="s1">name)</span>
  <span class="s1">prim = standard_primitive(_attrgetter(</span><span class="s3">'shape'</span><span class="s1">)</span><span class="s2">, </span><span class="s1">dtype_rule</span><span class="s2">, </span><span class="s1">name</span><span class="s2">,</span>
                            <span class="s1">weak_type_rule=weak_type_rule)</span>
  <span class="s1">batching.defvectorized(prim)</span>
  <span class="s1">pe.def_trivial_padding(prim)</span>
  <span class="s2">return </span><span class="s1">prim</span>
<span class="s1">standard_unop = partial(unop</span><span class="s2">, </span><span class="s1">_identity)</span>
<span class="s1">_attrgetter = </span><span class="s2">lambda </span><span class="s1">name: </span><span class="s2">lambda </span><span class="s1">x</span><span class="s2">, </span><span class="s1">**kwargs: getattr(x</span><span class="s2">, </span><span class="s1">name)</span>


<span class="s2">def </span><span class="s1">naryop_dtype_rule(result_dtype</span><span class="s2">, </span><span class="s1">accepted_dtypes</span><span class="s2">, </span><span class="s1">name</span><span class="s2">, </span><span class="s1">*avals</span><span class="s2">, </span><span class="s1">**kwargs):</span>
  <span class="s1">aval_dtypes = [aval.dtype </span><span class="s2">for </span><span class="s1">aval </span><span class="s2">in </span><span class="s1">avals]</span>
  <span class="s2">for </span><span class="s1">i</span><span class="s2">, </span><span class="s1">(aval_dtype</span><span class="s2">, </span><span class="s1">types) </span><span class="s2">in </span><span class="s1">enumerate(zip(aval_dtypes</span><span class="s2">, </span><span class="s1">accepted_dtypes)):</span>
    <span class="s2">if not </span><span class="s1">any(dtypes.issubdtype(aval_dtype</span><span class="s2">, </span><span class="s1">t) </span><span class="s2">for </span><span class="s1">t </span><span class="s2">in </span><span class="s1">types):</span>
      <span class="s2">if </span><span class="s1">aval_dtype == dtypes.float0:</span>
        <span class="s2">raise </span><span class="s1">TypeError(</span>
            <span class="s3">f&quot;Called </span><span class="s2">{</span><span class="s1">name</span><span class="s2">} </span><span class="s3">with a float0 at position </span><span class="s2">{</span><span class="s1">i</span><span class="s2">}</span><span class="s3">. &quot;</span>
            <span class="s3">&quot;float0s do not support any operations by design, because they &quot;</span>
            <span class="s3">&quot;are not compatible with non-trivial vector spaces. No implicit dtype &quot;</span>
            <span class="s3">&quot;conversion is done. You can use np.zeros_like(arr, dtype=np.float) &quot;</span>
            <span class="s3">&quot;to cast a float0 array to a regular zeros array. </span><span class="s2">\n</span><span class="s3">&quot;</span>
            <span class="s3">&quot;If you didn't expect to get a float0 you might have accidentally &quot;</span>
            <span class="s3">&quot;taken a gradient with respect to an integer argument.&quot;</span><span class="s1">)</span>
      <span class="s2">else</span><span class="s1">:</span>
        <span class="s1">msg = (</span><span class="s3">'{} does not accept dtype {} at position {}. '</span>
               <span class="s3">'Accepted dtypes at position {} are subtypes of {}.'</span><span class="s1">)</span>
        <span class="s1">typename = str(np.dtype(aval_dtype).name)</span>
        <span class="s1">typenames = </span><span class="s3">', '</span><span class="s1">.join(t.__name__ </span><span class="s2">for </span><span class="s1">t </span><span class="s2">in </span><span class="s1">types)</span>
        <span class="s2">raise </span><span class="s1">TypeError(msg.format(name</span><span class="s2">, </span><span class="s1">typename</span><span class="s2">, </span><span class="s1">i</span><span class="s2">, </span><span class="s1">i</span><span class="s2">, </span><span class="s1">typenames))</span>
  <span class="s1">_check_same_dtypes(name</span><span class="s2">, False, </span><span class="s1">*aval_dtypes)</span>
  <span class="s2">return </span><span class="s1">result_dtype(*avals)</span>


<span class="s2">def </span><span class="s1">broadcasting_shape_rule(name</span><span class="s2">, </span><span class="s1">*avals):</span>
  <span class="s1">shapes = [aval.shape </span><span class="s2">for </span><span class="s1">aval </span><span class="s2">in </span><span class="s1">avals </span><span class="s2">if </span><span class="s1">aval.shape]</span>
  <span class="s2">if not </span><span class="s1">shapes:</span>
    <span class="s2">return </span><span class="s1">()</span>
  <span class="s2">if </span><span class="s1">len({len(shape) </span><span class="s2">for </span><span class="s1">shape </span><span class="s2">in </span><span class="s1">shapes}) != </span><span class="s4">1</span><span class="s1">:</span>
    <span class="s1">msg = </span><span class="s3">'{}: arrays must have same number of dimensions, got {}.'</span>
    <span class="s2">raise </span><span class="s1">TypeError(msg.format(name</span><span class="s2">, </span><span class="s3">', '</span><span class="s1">.join(map(str</span><span class="s2">, </span><span class="s1">map(tuple</span><span class="s2">, </span><span class="s1">shapes)))))</span>
  <span class="s0"># TODO(mattjj): de-duplicate with _try_broadcast_shapes</span>
  <span class="s1">result_shape = []</span>
  <span class="s2">for </span><span class="s1">ds </span><span class="s2">in </span><span class="s1">zip(*shapes):</span>
    <span class="s2">if </span><span class="s1">all(core.same_referent(d</span><span class="s2">, </span><span class="s1">ds[</span><span class="s4">0</span><span class="s1">]) </span><span class="s2">for </span><span class="s1">d </span><span class="s2">in </span><span class="s1">ds[</span><span class="s4">1</span><span class="s1">:]):</span>
      <span class="s0"># if all axes are identical objects, the resulting size is the object</span>
      <span class="s1">result_shape.append(ds[</span><span class="s4">0</span><span class="s1">])</span>
    <span class="s2">else</span><span class="s1">:</span>
      <span class="s0"># if all dims are equal (or 1), the result is the non-1 size</span>
      <span class="s1">non_1s = [d </span><span class="s2">for </span><span class="s1">d </span><span class="s2">in </span><span class="s1">ds </span><span class="s2">if not </span><span class="s1">core.symbolic_equal_dim(d</span><span class="s2">, </span><span class="s4">1</span><span class="s1">)]</span>
      <span class="s2">if not </span><span class="s1">non_1s:</span>
        <span class="s1">result_shape.append(</span><span class="s4">1</span><span class="s1">)</span>
      <span class="s2">elif </span><span class="s1">all(core.symbolic_equal_dim(non_1s[</span><span class="s4">0</span><span class="s1">]</span><span class="s2">, </span><span class="s1">d) </span><span class="s2">for </span><span class="s1">d </span><span class="s2">in </span><span class="s1">non_1s[</span><span class="s4">1</span><span class="s1">:]):</span>
        <span class="s1">result_shape.append(non_1s[</span><span class="s4">0</span><span class="s1">])</span>
      <span class="s2">else</span><span class="s1">:</span>
        <span class="s2">raise </span><span class="s1">TypeError(</span><span class="s3">f'</span><span class="s2">{</span><span class="s1">name</span><span class="s2">} </span><span class="s3">got incompatible shapes for broadcasting: '</span>
                        <span class="s3">f'</span><span class="s2">{</span><span class="s3">&quot;, &quot;</span><span class="s1">.join(map(str</span><span class="s2">, </span><span class="s1">map(tuple</span><span class="s2">, </span><span class="s1">shapes)))</span><span class="s2">}</span><span class="s3">.'</span><span class="s1">)</span>

  <span class="s2">return </span><span class="s1">tuple(result_shape)</span>

<span class="s2">def </span><span class="s1">_naryop_weak_type_rule(name</span><span class="s2">, </span><span class="s1">*avals</span><span class="s2">, </span><span class="s1">**kwargs):</span>
  <span class="s2">if </span><span class="s1">any(aval.dtype == dtypes.float0 </span><span class="s2">for </span><span class="s1">aval </span><span class="s2">in </span><span class="s1">avals):</span>
    <span class="s1">pos = next(i </span><span class="s2">for </span><span class="s1">i</span><span class="s2">, </span><span class="s1">aval </span><span class="s2">in </span><span class="s1">enumerate(avals) </span><span class="s2">if </span><span class="s1">aval.dtype == dtypes.float0)</span>
    <span class="s2">raise </span><span class="s1">TypeError(</span>
        <span class="s3">f&quot;Called </span><span class="s2">{</span><span class="s1">name</span><span class="s2">} </span><span class="s3">with a float0 at position </span><span class="s2">{</span><span class="s1">pos</span><span class="s2">}</span><span class="s3">. &quot;</span>
        <span class="s3">&quot;float0s do not support any operations by design, because they &quot;</span>
        <span class="s3">&quot;are not compatible with non-trivial vector spaces. No implicit dtype &quot;</span>
        <span class="s3">&quot;conversion is done. You can use np.zeros_like(arr, dtype=np.float) &quot;</span>
        <span class="s3">&quot;to cast a float0 array to a regular zeros array. </span><span class="s2">\n</span><span class="s3">&quot;</span>
        <span class="s3">&quot;If you didn't expect to get a float0 you might have accidentally &quot;</span>
        <span class="s3">&quot;taken a gradient with respect to an integer argument.&quot;</span><span class="s1">)</span>
  <span class="s2">return </span><span class="s1">all(aval.weak_type </span><span class="s2">for </span><span class="s1">aval </span><span class="s2">in </span><span class="s1">avals)</span>

<span class="s2">def </span><span class="s1">naryop(result_dtype</span><span class="s2">, </span><span class="s1">accepted_dtypes</span><span class="s2">, </span><span class="s1">name):</span>
  <span class="s1">dtype_rule = partial(naryop_dtype_rule</span><span class="s2">, </span><span class="s1">result_dtype</span><span class="s2">, </span><span class="s1">accepted_dtypes</span><span class="s2">, </span><span class="s1">name)</span>
  <span class="s1">shape_rule = partial(broadcasting_shape_rule</span><span class="s2">, </span><span class="s1">name)</span>
  <span class="s1">weak_type_rule = partial(_naryop_weak_type_rule</span><span class="s2">, </span><span class="s1">name)</span>
  <span class="s1">prim = standard_primitive(shape_rule</span><span class="s2">, </span><span class="s1">dtype_rule</span><span class="s2">, </span><span class="s1">name</span><span class="s2">,</span>
                            <span class="s1">weak_type_rule=weak_type_rule)</span>
  <span class="s1">batching.defbroadcasting(prim)</span>
  <span class="s1">pe.def_trivial_padding(prim)</span>
  <span class="s2">return </span><span class="s1">prim</span>
<span class="s1">standard_naryop = partial(naryop</span><span class="s2">, </span><span class="s1">_input_dtype)</span>


<span class="s2">def </span><span class="s1">_broadcast_translate(op</span><span class="s2">, </span><span class="s1">ctx</span><span class="s2">, </span><span class="s1">avals_in</span><span class="s2">, </span><span class="s1">avals_out</span><span class="s2">, </span><span class="s1">*args):</span>
  <span class="s5">&quot;&quot;&quot;Variant of _standard_translate that performs explicit broadcasting. 
 
  Not all XLA library functions perform their own broadcasting.&quot;&quot;&quot;</span>
  <span class="s1">aval_out</span><span class="s2">, </span><span class="s1">= avals_out</span>
  <span class="s1">broadcasted_args = []</span>
  <span class="s2">for </span><span class="s1">aval_in</span><span class="s2">, </span><span class="s1">arg </span><span class="s2">in </span><span class="s1">zip(avals_in</span><span class="s2">, </span><span class="s1">args):</span>
    <span class="s2">if </span><span class="s1">aval_out.shape != aval_in.shape:</span>
      <span class="s1">bcast_dims = tuple(range(len(aval_out.shape) - len(aval_in.shape)</span><span class="s2">,</span>
                               <span class="s1">len(aval_out.shape)))</span>
      <span class="s1">arg = xops.BroadcastInDim(arg</span><span class="s2">, </span><span class="s1">aval_out.shape</span><span class="s2">, </span><span class="s1">bcast_dims)</span>
    <span class="s1">broadcasted_args.append(arg)</span>
  <span class="s2">return </span><span class="s1">[op(*broadcasted_args)]</span>


<span class="s0"># Like autograd.numpy.numpy_vjps.unbroadcast, this utility handles transposition</span>
<span class="s0"># involving linear primitives with implicit broadcasting.</span>
<span class="s2">def </span><span class="s1">_unbroadcast(aval</span><span class="s2">, </span><span class="s1">x):</span>
  <span class="s2">if not </span><span class="s1">isinstance(aval</span><span class="s2">, </span><span class="s1">(core.DShapedArray</span><span class="s2">, </span><span class="s1">ShapedArray)):</span>
    <span class="s2">raise </span><span class="s1">TypeError(</span><span class="s3">&quot;transpose with implicit broadcasting of unshaped values&quot;</span><span class="s1">)</span>
  <span class="s1">x_shape = np.shape(x)</span>
  <span class="s2">if </span><span class="s1">core.symbolic_equal_shape(aval.shape</span><span class="s2">, </span><span class="s1">x_shape):</span>
    <span class="s2">return </span><span class="s1">x</span>
  <span class="s2">assert not </span><span class="s1">aval.shape </span><span class="s2">or </span><span class="s1">len(x_shape) == len(aval.shape)</span>
  <span class="s2">if not </span><span class="s1">aval.shape:</span>
    <span class="s2">return </span><span class="s1">_reduce_sum(x</span><span class="s2">, </span><span class="s1">list(range(len(x_shape))))</span>
  <span class="s2">else</span><span class="s1">:</span>
    <span class="s1">dims = [i </span><span class="s2">for </span><span class="s1">i</span><span class="s2">, </span><span class="s1">(a</span><span class="s2">, </span><span class="s1">b) </span><span class="s2">in </span><span class="s1">enumerate(zip(x_shape</span><span class="s2">, </span><span class="s1">aval.shape)) </span><span class="s2">if not </span><span class="s1">core.symbolic_equal_dim(a</span><span class="s2">, </span><span class="s1">b)]</span>
    <span class="s2">if </span><span class="s1">config.jax_enable_checks: </span><span class="s2">assert </span><span class="s1">all(aval.shape[i] == </span><span class="s4">1 </span><span class="s2">for </span><span class="s1">i </span><span class="s2">in </span><span class="s1">dims)</span>
    <span class="s2">return </span><span class="s1">reshape(_reduce_sum(x</span><span class="s2">, </span><span class="s1">dims)</span><span class="s2">, </span><span class="s1">aval.shape)</span>

<span class="s2">def </span><span class="s1">_maybe_broadcast(target_shape</span><span class="s2">, </span><span class="s1">x):</span>
  <span class="s1">x_shape = np.shape(x)</span>
  <span class="s2">if </span><span class="s1">core.symbolic_equal_shape(x_shape</span><span class="s2">, </span><span class="s1">target_shape):</span>
    <span class="s2">return </span><span class="s1">x</span>
  <span class="s2">elif not </span><span class="s1">x_shape:</span>
    <span class="s2">return </span><span class="s1">broadcast_in_dim(x</span><span class="s2">, </span><span class="s1">target_shape</span><span class="s2">, </span><span class="s1">())</span>
  <span class="s2">else</span><span class="s1">:</span>
    <span class="s1">dims = [i </span><span class="s2">for </span><span class="s1">i</span><span class="s2">, </span><span class="s1">(a</span><span class="s2">, </span><span class="s1">b) </span><span class="s2">in </span><span class="s1">enumerate(zip(x_shape</span><span class="s2">, </span><span class="s1">target_shape))</span>
            <span class="s2">if </span><span class="s1">core.symbolic_equal_dim(a</span><span class="s2">, </span><span class="s1">b)]</span>
    <span class="s1">squeeze_shape = [x_shape[i] </span><span class="s2">for </span><span class="s1">i </span><span class="s2">in </span><span class="s1">dims]</span>
    <span class="s2">return </span><span class="s1">broadcast_in_dim(reshape(x</span><span class="s2">, </span><span class="s1">squeeze_shape)</span><span class="s2">, </span><span class="s1">target_shape</span><span class="s2">, </span><span class="s1">dims)</span>

<span class="s2">def </span><span class="s1">broadcast_hlo(</span>
    <span class="s1">aval_out: core.ShapedArray</span><span class="s2">, </span><span class="s1">avals: Sequence[core.ShapedArray]</span><span class="s2">,</span>
    <span class="s1">args: Sequence[ir.Value]) -&gt; Sequence[ir.Value]:</span>
  <span class="s5">&quot;&quot;&quot;Broadcasts HLO values with broadcast-compatible shapes to the same shape. 
  &quot;&quot;&quot;</span>
  <span class="s1">out = []</span>
  <span class="s2">for </span><span class="s1">aval</span><span class="s2">, </span><span class="s1">arg </span><span class="s2">in </span><span class="s1">zip(avals</span><span class="s2">, </span><span class="s1">args):</span>
    <span class="s2">if </span><span class="s1">aval.shape != aval_out.shape:</span>
      <span class="s2">assert </span><span class="s1">len(aval.shape) &lt;= len(aval_out.shape)</span><span class="s2">, </span><span class="s1">(aval</span><span class="s2">, </span><span class="s1">aval_out)</span>
      <span class="s1">dims = mlir.dense_int_elements(</span>
          <span class="s1">range(len(aval_out.shape) - len(aval.shape)</span><span class="s2">, </span><span class="s1">len(aval_out.shape)))</span>
      <span class="s2">if </span><span class="s1">any(isinstance(d</span><span class="s2">, </span><span class="s1">ir.Value) </span><span class="s2">for </span><span class="s1">d </span><span class="s2">in </span><span class="s1">aval_out.shape):</span>
        <span class="s1">arg = hlo.DynamicBroadcastInDimOp(</span>
            <span class="s1">mlir.aval_to_ir_type(aval_out)</span><span class="s2">, </span><span class="s1">arg</span><span class="s2">,</span>
            <span class="s1">mlir.shape_tensor(aval_out.shape)</span><span class="s2">, </span><span class="s1">dims).result</span>
      <span class="s2">else</span><span class="s1">:</span>
        <span class="s1">arg = hlo.BroadcastInDimOp(</span>
            <span class="s1">mlir.aval_to_ir_type(aval.update(shape=aval_out.shape))</span><span class="s2">, </span><span class="s1">arg</span><span class="s2">,</span>
            <span class="s1">dims).result</span>
    <span class="s1">out.append(arg)</span>
  <span class="s2">return </span><span class="s1">out</span>

<span class="s2">def </span><span class="s1">_nary_lower_hlo(op: Callable</span><span class="s2">, </span><span class="s1">ctx</span><span class="s2">,</span>
                    <span class="s1">*args: Union[ir.Value</span><span class="s2">, </span><span class="s1">Sequence[ir.Value]]</span><span class="s2">,</span>
                    <span class="s1">explicit_type=</span><span class="s2">False, </span><span class="s1">**params):</span>
  <span class="s5">&quot;&quot;&quot;Lowers an elementwise operator to its MLIR equivalent. 
 
  Args: 
    explicit_type: does the MLIR op require its output type to be provided? 
  &quot;&quot;&quot;</span>
  <span class="s2">del </span><span class="s1">params</span>
  <span class="s1">avals_in</span><span class="s2">, </span><span class="s1">(aval_out</span><span class="s2">,</span><span class="s1">) = ctx.avals_in</span><span class="s2">, </span><span class="s1">ctx.avals_out</span>
  <span class="s1">broadcasted_args = mlir.multi_broadcast_in_dim(</span>
      <span class="s1">ctx</span><span class="s2">, </span><span class="s1">args</span><span class="s2">, </span><span class="s1">avals_in</span><span class="s2">, </span><span class="s1">aval_out.shape)</span>

  <span class="s2">if </span><span class="s1">explicit_type:</span>
    <span class="s2">return </span><span class="s1">op(mlir.aval_to_ir_type(aval_out)</span><span class="s2">, </span><span class="s1">*broadcasted_args).results</span>
  <span class="s2">else</span><span class="s1">:</span>
    <span class="s2">return </span><span class="s1">op(*broadcasted_args).results</span>


<span class="s1">_float = {np.floating}</span>
<span class="s1">_complex = {np.complexfloating}</span>
<span class="s1">_complex_elem_types = {np.float32</span><span class="s2">, </span><span class="s1">np.float64}</span>
<span class="s1">_int = {np.integer}</span>
<span class="s1">_bool = {np.bool_}</span>

<span class="s1">_num = _int | _float | _complex</span>
<span class="s1">_any = _int | _float | _complex | _bool</span>
<span class="s1">_bool_or_int = _int | _bool</span>
<span class="s1">_ordered = _int | _float | _bool</span>

<span class="s1">neg_p = standard_unop(_num</span><span class="s2">, </span><span class="s3">'neg'</span><span class="s1">)</span>
<span class="s1">ad.deflinear2(neg_p</span><span class="s2">, lambda </span><span class="s1">t</span><span class="s2">, </span><span class="s1">operand: [neg(t)])</span>
<span class="s1">mlir.register_lowering(neg_p</span><span class="s2">, </span><span class="s1">partial(_nary_lower_hlo</span><span class="s2">, </span><span class="s1">hlo.NegOp))</span>

<span class="s1">sign_p = standard_unop(_num</span><span class="s2">, </span><span class="s3">'sign'</span><span class="s1">)</span>
<span class="s1">ad.defjvp_zero(sign_p)</span>

<span class="s2">def </span><span class="s1">_sign_lower_hlo(ctx</span><span class="s2">, </span><span class="s1">x):</span>
  <span class="s1">x_aval</span><span class="s2">, </span><span class="s1">= ctx.avals_in</span>
  <span class="s2">if </span><span class="s1">dtypes.issubdtype(x_aval.dtype</span><span class="s2">, </span><span class="s1">np.unsignedinteger):</span>
    <span class="s2">return </span><span class="s1">hlo.SelectOp(</span>
        <span class="s1">mlir.compare_hlo(x</span><span class="s2">, </span><span class="s1">mlir.full_like_aval(ctx</span><span class="s2">, </span><span class="s4">0</span><span class="s2">, </span><span class="s1">x_aval)</span><span class="s2">, </span><span class="s3">'EQ'</span><span class="s2">,</span>
                         <span class="s3">'UNSIGNED'</span><span class="s1">).result</span><span class="s2">,</span>
        <span class="s1">mlir.full_like_aval(ctx</span><span class="s2">, </span><span class="s4">0</span><span class="s2">, </span><span class="s1">x_aval)</span><span class="s2">,</span>
        <span class="s1">mlir.full_like_aval(ctx</span><span class="s2">, </span><span class="s4">1</span><span class="s2">, </span><span class="s1">x_aval)).results</span>
  <span class="s2">return </span><span class="s1">hlo.SignOp(x).results</span>

<span class="s1">mlir.register_lowering(sign_p</span><span class="s2">, </span><span class="s1">_sign_lower_hlo)</span>

<span class="s1">nextafter_p = standard_naryop([_float</span><span class="s2">, </span><span class="s1">_float]</span><span class="s2">, </span><span class="s3">'nextafter'</span><span class="s1">)</span>
<span class="s1">mlir.register_lowering(nextafter_p</span><span class="s2">, </span><span class="s1">partial(_nary_lower_hlo</span><span class="s2">, </span><span class="s1">chlo.NextAfterOp))</span>

<span class="s1">floor_p = standard_unop(_float</span><span class="s2">, </span><span class="s3">'floor'</span><span class="s1">)</span>
<span class="s1">ad.defjvp_zero(floor_p)</span>
<span class="s1">mlir.register_lowering(floor_p</span><span class="s2">, </span><span class="s1">partial(_nary_lower_hlo</span><span class="s2">, </span><span class="s1">hlo.FloorOp))</span>

<span class="s1">ceil_p = standard_unop(_float</span><span class="s2">, </span><span class="s3">'ceil'</span><span class="s1">)</span>
<span class="s1">ad.defjvp_zero(ceil_p)</span>
<span class="s1">mlir.register_lowering(ceil_p</span><span class="s2">, </span><span class="s1">partial(_nary_lower_hlo</span><span class="s2">, </span><span class="s1">hlo.CeilOp))</span>

<span class="s1">round_p = standard_unop(_float</span><span class="s2">, </span><span class="s3">'round'</span><span class="s1">)</span>
<span class="s1">ad.defjvp_zero(round_p)</span>

<span class="s2">def </span><span class="s1">_round_lower(ctx</span><span class="s2">, </span><span class="s1">x</span><span class="s2">, </span><span class="s1">*</span><span class="s2">, </span><span class="s1">rounding_method):</span>
  <span class="s2">if </span><span class="s1">rounding_method </span><span class="s2">is </span><span class="s1">RoundingMethod.AWAY_FROM_ZERO:</span>
    <span class="s2">return </span><span class="s1">hlo.RoundOp(x).results</span>
  <span class="s2">else</span><span class="s1">:</span>
    <span class="s2">assert </span><span class="s1">rounding_method </span><span class="s2">is </span><span class="s1">RoundingMethod.TO_NEAREST_EVEN</span>
    <span class="s2">return </span><span class="s1">hlo.RoundNearestEvenOp(x).results</span>
<span class="s1">mlir.register_lowering(round_p</span><span class="s2">, </span><span class="s1">_round_lower)</span>

<span class="s1">is_finite_p = unop(_fixed_dtype(np.bool_)</span><span class="s2">, </span><span class="s1">_float</span><span class="s2">, </span><span class="s3">'is_finite'</span><span class="s1">)</span>
<span class="s1">ad.defjvp_zero(is_finite_p)</span>
<span class="s1">mlir.register_lowering(is_finite_p</span><span class="s2">, </span><span class="s1">partial(_nary_lower_hlo</span><span class="s2">, </span><span class="s1">hlo.IsFiniteOp))</span>

<span class="s1">exp_p = standard_unop(_float | _complex</span><span class="s2">, </span><span class="s3">'exp'</span><span class="s1">)</span>
<span class="s1">ad.defjvp2(exp_p</span><span class="s2">, lambda </span><span class="s1">g</span><span class="s2">, </span><span class="s1">ans</span><span class="s2">, </span><span class="s1">x: mul(g</span><span class="s2">, </span><span class="s1">ans))</span>
<span class="s0"># For exp_p it is more efficient to use the reconstructed output for the vjp</span>
<span class="s0"># rule instead of computing it again from the input.</span>
<span class="s1">mlir.register_lowering(exp_p</span><span class="s2">, </span><span class="s1">partial(_nary_lower_hlo</span><span class="s2">, </span><span class="s1">hlo.ExpOp))</span>

<span class="s1">log_p = standard_unop(_float | _complex</span><span class="s2">, </span><span class="s3">'log'</span><span class="s1">)</span>
<span class="s1">ad.defjvp(log_p</span><span class="s2">, lambda </span><span class="s1">g</span><span class="s2">, </span><span class="s1">x: div(g</span><span class="s2">, </span><span class="s1">x))</span>
<span class="s1">mlir.register_lowering(log_p</span><span class="s2">, </span><span class="s1">partial(_nary_lower_hlo</span><span class="s2">, </span><span class="s1">hlo.LogOp))</span>

<span class="s1">expm1_p = standard_unop(_float | _complex</span><span class="s2">, </span><span class="s3">'expm1'</span><span class="s1">)</span>
<span class="s1">ad.defjvp2(expm1_p</span><span class="s2">, lambda </span><span class="s1">g</span><span class="s2">, </span><span class="s1">ans</span><span class="s2">, </span><span class="s1">x: mul(g</span><span class="s2">, </span><span class="s1">add(ans</span><span class="s2">, </span><span class="s1">_one(ans))))</span>
<span class="s1">mlir.register_lowering(expm1_p</span><span class="s2">, </span><span class="s1">partial(_nary_lower_hlo</span><span class="s2">, </span><span class="s1">hlo.Expm1Op))</span>

<span class="s1">log1p_p = standard_unop(_float | _complex</span><span class="s2">, </span><span class="s3">'log1p'</span><span class="s1">)</span>
<span class="s1">ad.defjvp(log1p_p</span><span class="s2">, lambda </span><span class="s1">g</span><span class="s2">, </span><span class="s1">x: div(g</span><span class="s2">, </span><span class="s1">add(x</span><span class="s2">, </span><span class="s1">_one(x))))</span>
<span class="s1">mlir.register_lowering(log1p_p</span><span class="s2">, </span><span class="s1">partial(_nary_lower_hlo</span><span class="s2">, </span><span class="s1">hlo.Log1pOp))</span>

<span class="s1">tanh_p = standard_unop(_float | _complex</span><span class="s2">, </span><span class="s3">'tanh'</span><span class="s1">)</span>
<span class="s1">ad.defjvp2(tanh_p</span><span class="s2">, lambda </span><span class="s1">g</span><span class="s2">, </span><span class="s1">ans</span><span class="s2">, </span><span class="s1">x: mul(add(g</span><span class="s2">, </span><span class="s1">mul(g</span><span class="s2">, </span><span class="s1">ans))</span><span class="s2">,</span>
                                         <span class="s1">sub(_one(x)</span><span class="s2">, </span><span class="s1">ans)))</span>
<span class="s1">mlir.register_lowering(tanh_p</span><span class="s2">, </span><span class="s1">partial(_nary_lower_hlo</span><span class="s2">, </span><span class="s1">hlo.TanhOp))</span>

<span class="s1">logistic_p = standard_unop(_float | _complex</span><span class="s2">, </span><span class="s3">'logistic'</span><span class="s1">)</span>
<span class="s1">ad.defjvp2(logistic_p</span><span class="s2">, lambda </span><span class="s1">g</span><span class="s2">, </span><span class="s1">ans</span><span class="s2">, </span><span class="s1">x: mul(g</span><span class="s2">, </span><span class="s1">mul(ans</span><span class="s2">, </span><span class="s1">sub(_one(ans)</span><span class="s2">, </span><span class="s1">ans))))</span>
<span class="s0"># TODO(phawkins): switch to LogisticOp lowering; debug numerical problems.</span>
<span class="s0"># mlir.register_lowering(logistic_p, partial(_nary_lower_hlo, hlo.LogisticOp))</span>

<span class="s2">def </span><span class="s1">logistic_impl(x):</span>
  <span class="s1">one = _const(x</span><span class="s2">, </span><span class="s4">1</span><span class="s1">)</span>
  <span class="s2">return </span><span class="s1">div(one</span><span class="s2">, </span><span class="s1">add(one</span><span class="s2">, </span><span class="s1">exp(neg(x))))</span>

<span class="s1">mlir.register_lowering(logistic_p</span><span class="s2">,</span>
                       <span class="s1">mlir.lower_fun(logistic_impl</span><span class="s2">, </span><span class="s1">multiple_results=</span><span class="s2">False</span><span class="s1">))</span>

<span class="s1">sin_p = standard_unop(_float | _complex</span><span class="s2">, </span><span class="s3">'sin'</span><span class="s1">)</span>
<span class="s1">ad.defjvp(sin_p</span><span class="s2">, lambda </span><span class="s1">g</span><span class="s2">, </span><span class="s1">x: mul(g</span><span class="s2">, </span><span class="s1">cos(x)))</span>
<span class="s1">mlir.register_lowering(sin_p</span><span class="s2">, </span><span class="s1">partial(_nary_lower_hlo</span><span class="s2">, </span><span class="s1">hlo.SineOp))</span>

<span class="s1">cos_p = standard_unop(_float | _complex</span><span class="s2">, </span><span class="s3">'cos'</span><span class="s1">)</span>
<span class="s1">ad.defjvp(cos_p</span><span class="s2">, lambda </span><span class="s1">g</span><span class="s2">, </span><span class="s1">x: neg(mul(g</span><span class="s2">, </span><span class="s1">sin(x))))</span>
<span class="s1">mlir.register_lowering(cos_p</span><span class="s2">, </span><span class="s1">partial(_nary_lower_hlo</span><span class="s2">, </span><span class="s1">hlo.CosineOp))</span>

<span class="s1">@_upcast_fp16_for_computation</span>
<span class="s2">def </span><span class="s1">_tan_impl(x):</span>
  <span class="s2">return </span><span class="s1">div(sin(x)</span><span class="s2">, </span><span class="s1">cos(x))</span>

<span class="s1">tan_p = standard_unop(_float | _complex</span><span class="s2">, </span><span class="s3">'tan'</span><span class="s1">)</span>
<span class="s1">ad.defjvp2(tan_p</span><span class="s2">, lambda </span><span class="s1">g</span><span class="s2">, </span><span class="s1">ans</span><span class="s2">, </span><span class="s1">x: mul(g</span><span class="s2">, </span><span class="s1">_const(x</span><span class="s2">, </span><span class="s4">1</span><span class="s1">) + square(ans)))</span>
<span class="s1">mlir.register_lowering(tan_p</span><span class="s2">, </span><span class="s1">partial(_nary_lower_hlo</span><span class="s2">, </span><span class="s1">chlo.TanOp))</span>

<span class="s2">def </span><span class="s1">asin_impl(x):</span>
  <span class="s2">if </span><span class="s1">dtypes.issubdtype(_dtype(x)</span><span class="s2">, </span><span class="s1">np.complexfloating):</span>
    <span class="s2">return </span><span class="s1">mul(_const(x</span><span class="s2">, </span><span class="s1">-</span><span class="s4">1j</span><span class="s1">)</span><span class="s2">, </span><span class="s1">asinh(mul(_const(x</span><span class="s2">, </span><span class="s4">1j</span><span class="s1">)</span><span class="s2">, </span><span class="s1">x)))</span>
  <span class="s2">else</span><span class="s1">:</span>
    <span class="s2">return </span><span class="s1">mul(_const(x</span><span class="s2">, </span><span class="s4">2</span><span class="s1">)</span><span class="s2">,</span>
               <span class="s1">atan2(x</span><span class="s2">, </span><span class="s1">add(_const(x</span><span class="s2">, </span><span class="s4">1</span><span class="s1">)</span><span class="s2">, </span><span class="s1">sqrt(sub(_const(x</span><span class="s2">, </span><span class="s4">1</span><span class="s1">)</span><span class="s2">, </span><span class="s1">square(x))))))</span>

<span class="s1">asin_p = standard_unop(_float | _complex</span><span class="s2">, </span><span class="s3">'asin'</span><span class="s1">)</span>
<span class="s1">ad.defjvp(asin_p</span><span class="s2">, lambda </span><span class="s1">g</span><span class="s2">, </span><span class="s1">x: mul(g</span><span class="s2">, </span><span class="s1">rsqrt(_const(x</span><span class="s2">, </span><span class="s4">1</span><span class="s1">) - square(x))))</span>
<span class="s1">mlir.register_lowering(asin_p</span><span class="s2">, </span><span class="s1">partial(_nary_lower_hlo</span><span class="s2">, </span><span class="s1">chlo.AsinOp))</span>

<span class="s2">def </span><span class="s1">acos_impl(x):</span>
  <span class="s2">if </span><span class="s1">dtypes.issubdtype(_dtype(x)</span><span class="s2">, </span><span class="s1">np.complexfloating):</span>
    <span class="s1">result = mul(_const(x</span><span class="s2">, </span><span class="s4">1j</span><span class="s1">)</span><span class="s2">, </span><span class="s1">acosh(x))</span>
    <span class="s0"># By convention, numpy chooses the branch with positive real part.</span>
    <span class="s1">rpart = real(result)</span>
    <span class="s2">return </span><span class="s1">select(</span>
      <span class="s1">gt(rpart</span><span class="s2">, </span><span class="s1">_const(rpart</span><span class="s2">, </span><span class="s4">0</span><span class="s1">))</span><span class="s2">,</span>
      <span class="s1">result</span><span class="s2">,</span>
      <span class="s1">neg(result)</span>
    <span class="s1">)</span>
  <span class="s2">else</span><span class="s1">:</span>
    <span class="s2">return </span><span class="s1">select(</span>
        <span class="s1">ne(x</span><span class="s2">, </span><span class="s1">_const(x</span><span class="s2">, </span><span class="s1">-</span><span class="s4">1.0</span><span class="s1">))</span><span class="s2">,</span>
        <span class="s1">mul(_const(x</span><span class="s2">, </span><span class="s4">2</span><span class="s1">)</span><span class="s2">,</span>
            <span class="s1">atan2(sqrt(sub(_const(x</span><span class="s2">, </span><span class="s4">1</span><span class="s1">)</span><span class="s2">, </span><span class="s1">square(x)))</span><span class="s2">, </span><span class="s1">add(_const(x</span><span class="s2">, </span><span class="s4">1</span><span class="s1">)</span><span class="s2">, </span><span class="s1">x)))</span><span class="s2">,</span>
        <span class="s1">full_like(x</span><span class="s2">, </span><span class="s1">np.pi))</span>

<span class="s1">acos_p = standard_unop(_float | _complex</span><span class="s2">, </span><span class="s3">'acos'</span><span class="s1">)</span>
<span class="s1">ad.defjvp(acos_p</span><span class="s2">, lambda </span><span class="s1">g</span><span class="s2">, </span><span class="s1">x: mul(g</span><span class="s2">, </span><span class="s1">-rsqrt(_const(x</span><span class="s2">, </span><span class="s4">1</span><span class="s1">) - square(x))))</span>
<span class="s1">mlir.register_lowering(acos_p</span><span class="s2">,</span>
                       <span class="s1">mlir.lower_fun(acos_impl</span><span class="s2">, </span><span class="s1">multiple_results=</span><span class="s2">False</span><span class="s1">))</span>

<span class="s2">def </span><span class="s1">atan_impl(x):</span>
  <span class="s2">return </span><span class="s1">atan2(x</span><span class="s2">, </span><span class="s1">_const(x</span><span class="s2">, </span><span class="s4">1</span><span class="s1">))</span>

<span class="s1">atan_p = standard_unop(_float | _complex</span><span class="s2">, </span><span class="s3">'atan'</span><span class="s1">)</span>
<span class="s1">ad.defjvp(atan_p</span><span class="s2">, lambda </span><span class="s1">g</span><span class="s2">, </span><span class="s1">x: div(g</span><span class="s2">, </span><span class="s1">_const(x</span><span class="s2">, </span><span class="s4">1</span><span class="s1">) + square(x)))</span>
<span class="s1">mlir.register_lowering(atan_p</span><span class="s2">, </span><span class="s1">partial(_nary_lower_hlo</span><span class="s2">, </span><span class="s1">chlo.AtanOp))</span>

<span class="s1">atan2_p = standard_naryop([_float | _complex</span><span class="s2">, </span><span class="s1">_float | _complex]</span><span class="s2">, </span><span class="s3">'atan2'</span><span class="s1">)</span>
<span class="s1">ad.defjvp(atan2_p</span><span class="s2">,</span>
          <span class="s2">lambda </span><span class="s1">g</span><span class="s2">, </span><span class="s1">x</span><span class="s2">, </span><span class="s1">y: g * (y / (square(x) + square(y)))</span><span class="s2">,</span>
          <span class="s2">lambda </span><span class="s1">g</span><span class="s2">, </span><span class="s1">x</span><span class="s2">, </span><span class="s1">y: g * -x / (square(x) + square(y)))</span>
<span class="s1">mlir.register_lowering(atan2_p</span><span class="s2">, </span><span class="s1">partial(_nary_lower_hlo</span><span class="s2">, </span><span class="s1">hlo.Atan2Op))</span>

<span class="s1">sinh_p = standard_unop(_float | _complex</span><span class="s2">, </span><span class="s3">'sinh'</span><span class="s1">)</span>
<span class="s1">ad.defjvp(sinh_p</span><span class="s2">, lambda </span><span class="s1">g</span><span class="s2">, </span><span class="s1">x: mul(g</span><span class="s2">, </span><span class="s1">cosh(x)))</span>
<span class="s1">mlir.register_lowering(sinh_p</span><span class="s2">, </span><span class="s1">partial(_nary_lower_hlo</span><span class="s2">, </span><span class="s1">chlo.SinhOp))</span>

<span class="s1">cosh_p = standard_unop(_float | _complex</span><span class="s2">, </span><span class="s3">'cosh'</span><span class="s1">)</span>
<span class="s1">ad.defjvp(cosh_p</span><span class="s2">, lambda </span><span class="s1">g</span><span class="s2">, </span><span class="s1">x: mul(g</span><span class="s2">, </span><span class="s1">sinh(x)))</span>
<span class="s1">mlir.register_lowering(cosh_p</span><span class="s2">, </span><span class="s1">partial(_nary_lower_hlo</span><span class="s2">, </span><span class="s1">chlo.CoshOp))</span>

<span class="s1">asinh_p = standard_unop(_float | _complex</span><span class="s2">, </span><span class="s3">'asinh'</span><span class="s1">)</span>
<span class="s1">ad.defjvp(asinh_p</span><span class="s2">, lambda </span><span class="s1">g</span><span class="s2">, </span><span class="s1">x: mul(g</span><span class="s2">, </span><span class="s1">rsqrt(square(x) + _one(x))))</span>
<span class="s1">mlir.register_lowering(asinh_p</span><span class="s2">, </span><span class="s1">partial(_nary_lower_hlo</span><span class="s2">, </span><span class="s1">chlo.AsinhOp))</span>

<span class="s1">acosh_p = standard_unop(_float | _complex</span><span class="s2">, </span><span class="s3">'acosh'</span><span class="s1">)</span>
<span class="s1">ad.defjvp(acosh_p</span><span class="s2">,</span>
          <span class="s2">lambda </span><span class="s1">g</span><span class="s2">, </span><span class="s1">x: mul(g</span><span class="s2">, </span><span class="s1">rsqrt((x - _one(x)) * (x + _one(x)))))</span>
<span class="s1">mlir.register_lowering(acosh_p</span><span class="s2">, </span><span class="s1">partial(_nary_lower_hlo</span><span class="s2">, </span><span class="s1">chlo.AcoshOp))</span>

<span class="s1">atanh_p = standard_unop(_float | _complex</span><span class="s2">, </span><span class="s3">'atanh'</span><span class="s1">)</span>
<span class="s1">ad.defjvp(atanh_p</span><span class="s2">,</span>
          <span class="s2">lambda </span><span class="s1">g</span><span class="s2">, </span><span class="s1">x: mul(reciprocal(_one(x) + x)</span><span class="s2">, </span><span class="s1">div(g</span><span class="s2">, </span><span class="s1">(_one(x) - x))))</span>
<span class="s1">mlir.register_lowering(atanh_p</span><span class="s2">, </span><span class="s1">partial(_nary_lower_hlo</span><span class="s2">, </span><span class="s1">chlo.AtanhOp))</span>

<span class="s1">real_p = unop(_complex_basetype</span><span class="s2">, </span><span class="s1">_complex</span><span class="s2">, </span><span class="s3">'real'</span><span class="s1">)</span>
<span class="s1">ad.deflinear2(real_p</span><span class="s2">, lambda </span><span class="s1">t</span><span class="s2">, </span><span class="s1">_: [complex(t</span><span class="s2">, </span><span class="s1">np.zeros(()</span><span class="s2">, </span><span class="s1">_dtype(t)))])</span>
<span class="s1">mlir.register_lowering(real_p</span><span class="s2">, </span><span class="s1">partial(_nary_lower_hlo</span><span class="s2">, </span><span class="s1">hlo.RealOp))</span>

<span class="s1">imag_p = unop(_complex_basetype</span><span class="s2">, </span><span class="s1">_complex</span><span class="s2">, </span><span class="s3">'imag'</span><span class="s1">)</span>
<span class="s1">ad.deflinear2(imag_p</span><span class="s2">, lambda </span><span class="s1">t</span><span class="s2">, </span><span class="s1">_: [complex(np.zeros(()</span><span class="s2">, </span><span class="s1">_dtype(t))</span><span class="s2">, </span><span class="s1">neg(t))])</span>
<span class="s1">mlir.register_lowering(imag_p</span><span class="s2">, </span><span class="s1">partial(_nary_lower_hlo</span><span class="s2">, </span><span class="s1">hlo.ImagOp))</span>


<span class="s2">def </span><span class="s1">_complex_transpose_rule(t</span><span class="s2">, </span><span class="s1">x</span><span class="s2">, </span><span class="s1">y):</span>
  <span class="s2">assert </span><span class="s1">ad.is_undefined_primal(x) </span><span class="s2">or </span><span class="s1">ad.is_undefined_primal(y)</span>
  <span class="s2">if </span><span class="s1">ad.is_undefined_primal(x) </span><span class="s2">and </span><span class="s1">ad.is_undefined_primal(y):</span>
    <span class="s2">if </span><span class="s1">type(t) </span><span class="s2">is </span><span class="s1">ad_util.Zero:</span>
      <span class="s2">return </span><span class="s1">[ad_util.Zero(x.aval)</span><span class="s2">, </span><span class="s1">ad_util.Zero(y.aval)]</span>
    <span class="s2">else</span><span class="s1">:</span>
      <span class="s2">return </span><span class="s1">[_unbroadcast(x.aval</span><span class="s2">, </span><span class="s1">real(t))</span><span class="s2">, </span><span class="s1">_unbroadcast(y.aval</span><span class="s2">, </span><span class="s1">imag(neg(t)))]</span>
  <span class="s2">elif </span><span class="s1">ad.is_undefined_primal(x):</span>
    <span class="s2">if </span><span class="s1">type(t) </span><span class="s2">is </span><span class="s1">ad_util.Zero:</span>
      <span class="s2">return </span><span class="s1">[ad_util.Zero(x.aval)</span><span class="s2">, None</span><span class="s1">]</span>
    <span class="s2">else</span><span class="s1">:</span>
      <span class="s2">return </span><span class="s1">[_unbroadcast(x.aval</span><span class="s2">, </span><span class="s1">real(t))</span><span class="s2">, None</span><span class="s1">]</span>
  <span class="s2">else</span><span class="s1">:</span>
    <span class="s2">if </span><span class="s1">type(t) </span><span class="s2">is </span><span class="s1">ad_util.Zero:</span>
      <span class="s2">return </span><span class="s1">[</span><span class="s2">None, </span><span class="s1">ad_util.Zero(y.aval)]</span>
    <span class="s2">else</span><span class="s1">:</span>
      <span class="s2">return </span><span class="s1">[</span><span class="s2">None, </span><span class="s1">_unbroadcast(y.aval</span><span class="s2">, </span><span class="s1">imag(neg(t)))]</span>

<span class="s1">_complex_dtype = </span><span class="s2">lambda </span><span class="s1">dtype</span><span class="s2">, </span><span class="s1">*args: (np.zeros(()</span><span class="s2">, </span><span class="s1">dtype) + np.zeros(()</span><span class="s2">, </span><span class="s1">np.complex64)).dtype</span>
<span class="s1">complex_p = naryop(_complex_dtype</span><span class="s2">, </span><span class="s1">[_complex_elem_types</span><span class="s2">, </span><span class="s1">_complex_elem_types]</span><span class="s2">,</span>
                  <span class="s3">'complex'</span><span class="s1">)</span>
<span class="s1">ad.deflinear2(complex_p</span><span class="s2">, </span><span class="s1">_complex_transpose_rule)</span>
<span class="s1">mlir.register_lowering(complex_p</span><span class="s2">, </span><span class="s1">partial(_nary_lower_hlo</span><span class="s2">, </span><span class="s1">hlo.ComplexOp))</span>

<span class="s1">conj_p = unop(_complex_dtype</span><span class="s2">, </span><span class="s1">_complex_elem_types | _complex</span><span class="s2">, </span><span class="s3">'conj'</span><span class="s1">)</span>

<span class="s2">def </span><span class="s1">_conj_impl(x</span><span class="s2">, </span><span class="s1">**kw):</span>
  <span class="s2">if </span><span class="s1">dtypes.issubdtype(x.dtype</span><span class="s2">, </span><span class="s1">np.complexfloating):</span>
    <span class="s2">return </span><span class="s1">complex(real(x)</span><span class="s2">, </span><span class="s1">-imag(x))</span>
  <span class="s2">else</span><span class="s1">:</span>
    <span class="s2">return </span><span class="s1">complex(x</span><span class="s2">, </span><span class="s1">_zeros(x))</span>

<span class="s1">mlir.register_lowering(conj_p</span><span class="s2">,</span>
                       <span class="s1">mlir.lower_fun(_conj_impl</span><span class="s2">, </span><span class="s1">multiple_results=</span><span class="s2">False</span><span class="s1">))</span>


<span class="s2">def </span><span class="s1">_conj_transpose_rule(t</span><span class="s2">, </span><span class="s1">x</span><span class="s2">, </span><span class="s1">*</span><span class="s2">, </span><span class="s1">input_dtype):</span>
  <span class="s2">assert </span><span class="s1">ad.is_undefined_primal(x)</span>
  <span class="s2">if </span><span class="s1">dtypes.issubdtype(input_dtype</span><span class="s2">, </span><span class="s1">np.complexfloating):</span>
    <span class="s2">return </span><span class="s1">[conj(t)]</span>
  <span class="s2">else</span><span class="s1">:</span>
    <span class="s2">return </span><span class="s1">[real(t)]</span>

<span class="s1">ad.primitive_jvps[conj_p] = partial(ad.linear_jvp</span><span class="s2">, </span><span class="s1">conj_p)</span>
<span class="s1">ad.primitive_transposes[conj_p] = _conj_transpose_rule</span>

<span class="s1">abs_p = unop(_complex_basetype</span><span class="s2">, </span><span class="s1">_num</span><span class="s2">, </span><span class="s3">'abs'</span><span class="s1">)</span>
<span class="s1">mlir.register_lowering(abs_p</span><span class="s2">, </span><span class="s1">partial(_nary_lower_hlo</span><span class="s2">, </span><span class="s1">hlo.AbsOp))</span>

<span class="s2">def </span><span class="s1">_abs_jvp_rule(g</span><span class="s2">, </span><span class="s1">ans</span><span class="s2">, </span><span class="s1">x):</span>
  <span class="s2">if </span><span class="s1">_iscomplex(x):</span>
    <span class="s2">return </span><span class="s1">_maybe_real(mul(g</span><span class="s2">, </span><span class="s1">div(_maybe_conj(x)</span><span class="s2">,</span>
           <span class="s1">_replace_zero(convert_element_type(ans</span><span class="s2">, </span><span class="s1">_dtype(x))))))</span>
  <span class="s2">else</span><span class="s1">:</span>
    <span class="s2">return </span><span class="s1">select(ge(x</span><span class="s2">, </span><span class="s1">_zero(x))</span><span class="s2">, </span><span class="s1">g</span><span class="s2">, </span><span class="s1">neg(g))</span>
<span class="s1">ad.defjvp2(abs_p</span><span class="s2">, </span><span class="s1">_abs_jvp_rule)</span>
<span class="s1">_maybe_conj = </span><span class="s2">lambda </span><span class="s1">x: conj(x) </span><span class="s2">if </span><span class="s1">_iscomplex(x) </span><span class="s2">else </span><span class="s1">x</span>
<span class="s1">_maybe_real = </span><span class="s2">lambda </span><span class="s1">x: real(x) </span><span class="s2">if </span><span class="s1">_iscomplex(x) </span><span class="s2">else </span><span class="s1">x</span>

<span class="s1">sqrt_p = standard_unop(_float | _complex</span><span class="s2">, </span><span class="s3">'sqrt'</span><span class="s1">)</span>
<span class="s1">ad.defjvp2(sqrt_p</span><span class="s2">, lambda </span><span class="s1">g</span><span class="s2">, </span><span class="s1">ans</span><span class="s2">, </span><span class="s1">x: mul(g</span><span class="s2">, </span><span class="s1">div(_const(x</span><span class="s2">, </span><span class="s4">0.5</span><span class="s1">)</span><span class="s2">, </span><span class="s1">ans)))</span>
<span class="s1">mlir.register_lowering(sqrt_p</span><span class="s2">, </span><span class="s1">partial(_nary_lower_hlo</span><span class="s2">, </span><span class="s1">hlo.SqrtOp))</span>

<span class="s1">rsqrt_p = standard_unop(_float | _complex</span><span class="s2">, </span><span class="s3">'rsqrt'</span><span class="s1">)</span>
<span class="s1">ad.defjvp2(rsqrt_p</span><span class="s2">,</span>
           <span class="s2">lambda </span><span class="s1">g</span><span class="s2">, </span><span class="s1">ans</span><span class="s2">, </span><span class="s1">x:</span>
           <span class="s1">mul(g</span><span class="s2">, </span><span class="s1">mul(_const(x</span><span class="s2">, </span><span class="s1">-</span><span class="s4">0.5</span><span class="s1">)</span><span class="s2">, </span><span class="s1">div(ans</span><span class="s2">, </span><span class="s1">x))))</span>
<span class="s1">mlir.register_lowering(rsqrt_p</span><span class="s2">, </span><span class="s1">partial(_nary_lower_hlo</span><span class="s2">, </span><span class="s1">hlo.RsqrtOp))</span>

<span class="s1">cbrt_p = standard_unop(_float</span><span class="s2">, </span><span class="s3">'cbrt'</span><span class="s1">)</span>
<span class="s1">ad.defjvp2(cbrt_p</span><span class="s2">,</span>
           <span class="s2">lambda </span><span class="s1">g</span><span class="s2">, </span><span class="s1">ans</span><span class="s2">, </span><span class="s1">x: mul(g</span><span class="s2">, </span><span class="s1">mul(_const(x</span><span class="s2">, </span><span class="s4">1</span><span class="s1">/</span><span class="s4">3</span><span class="s1">)</span><span class="s2">, </span><span class="s1">integer_pow(ans</span><span class="s2">, </span><span class="s1">-</span><span class="s4">2</span><span class="s1">))))</span>
<span class="s1">mlir.register_lowering(cbrt_p</span><span class="s2">, </span><span class="s1">partial(_nary_lower_hlo</span><span class="s2">, </span><span class="s1">hlo.CbrtOp))</span>

<span class="s1">pow_p = standard_naryop([_float | _complex</span><span class="s2">, </span><span class="s1">_float | _complex]</span><span class="s2">, </span><span class="s3">'pow'</span><span class="s1">)</span>

<span class="s2">def </span><span class="s1">_pow_jvp_lhs(g</span><span class="s2">, </span><span class="s1">ans</span><span class="s2">, </span><span class="s1">x</span><span class="s2">, </span><span class="s1">y):</span>
  <span class="s2">return </span><span class="s1">mul(g</span><span class="s2">, </span><span class="s1">mul(y</span><span class="s2">, </span><span class="s1">pow(x</span><span class="s2">, </span><span class="s1">sub(y</span><span class="s2">, </span><span class="s1">_ones(y)))))</span>

<span class="s2">def </span><span class="s1">_pow_jvp_rhs(g</span><span class="s2">, </span><span class="s1">ans</span><span class="s2">, </span><span class="s1">x</span><span class="s2">, </span><span class="s1">y):</span>
  <span class="s2">return </span><span class="s1">mul(g</span><span class="s2">, </span><span class="s1">mul(log(_replace_zero(x))</span><span class="s2">, </span><span class="s1">ans))</span>

<span class="s1">ad.defjvp2(pow_p</span><span class="s2">, </span><span class="s1">_pow_jvp_lhs</span><span class="s2">, </span><span class="s1">_pow_jvp_rhs)</span>
<span class="s1">mlir.register_lowering(pow_p</span><span class="s2">, </span><span class="s1">partial(_nary_lower_hlo</span><span class="s2">, </span><span class="s1">hlo.PowOp))</span>


<span class="s2">def </span><span class="s1">_integer_pow_dtype_rule(x</span><span class="s2">, </span><span class="s1">*</span><span class="s2">, </span><span class="s1">y):</span>
  <span class="s1">dtype = unop_dtype_rule(_identity</span><span class="s2">, </span><span class="s1">_int | _float | _complex</span><span class="s2">, </span><span class="s3">'integer_pow'</span><span class="s2">, </span><span class="s1">x)</span>
  <span class="s2">if </span><span class="s1">y &lt; </span><span class="s4">0 </span><span class="s2">and </span><span class="s1">dtypes.issubdtype(dtype</span><span class="s2">, </span><span class="s1">np.integer):</span>
    <span class="s2">raise </span><span class="s1">TypeError(</span><span class="s3">&quot;Integers cannot be raised to negative powers, got &quot;</span>
                    <span class="s3">f&quot;integer_pow(</span><span class="s2">{</span><span class="s1">x</span><span class="s2">}</span><span class="s3">, </span><span class="s2">{</span><span class="s1">y</span><span class="s2">}</span><span class="s3">)&quot;</span><span class="s1">)</span>
  <span class="s2">return </span><span class="s1">dtype</span>

<span class="s2">def </span><span class="s1">_integer_pow_jvp(g</span><span class="s2">, </span><span class="s1">x</span><span class="s2">, </span><span class="s1">*</span><span class="s2">, </span><span class="s1">y):</span>
  <span class="s2">return </span><span class="s1">_zeros(g) </span><span class="s2">if </span><span class="s1">y == </span><span class="s4">0 </span><span class="s2">else </span><span class="s1">mul(g</span><span class="s2">, </span><span class="s1">mul(_const(x</span><span class="s2">, </span><span class="s1">y)</span><span class="s2">, </span><span class="s1">integer_pow(x</span><span class="s2">, </span><span class="s1">y - </span><span class="s4">1</span><span class="s1">)))</span>

<span class="s1">integer_pow_p = standard_primitive(</span>
  <span class="s1">_attrgetter(</span><span class="s3">'shape'</span><span class="s1">)</span><span class="s2">, </span><span class="s1">_integer_pow_dtype_rule</span><span class="s2">, </span><span class="s3">'integer_pow'</span><span class="s1">)</span>
<span class="s1">batching.defvectorized(integer_pow_p)</span>
<span class="s1">ad.defjvp(integer_pow_p</span><span class="s2">, </span><span class="s1">_integer_pow_jvp)</span>
<span class="s1">pe.def_trivial_padding(integer_pow_p)</span>

<span class="s2">def </span><span class="s1">_integer_pow(x</span><span class="s2">, </span><span class="s1">*</span><span class="s2">, </span><span class="s1">y):</span>
  <span class="s0"># This should be kept in sync with the jax2tf translation rule.</span>
  <span class="s2">if </span><span class="s1">y == </span><span class="s4">0</span><span class="s1">:</span>
    <span class="s2">return </span><span class="s1">full_like(x</span><span class="s2">, </span><span class="s4">1</span><span class="s1">)</span>
  <span class="s1">is_reciprocal = y &lt; </span><span class="s4">0</span>
  <span class="s2">if </span><span class="s1">is_reciprocal:</span>
    <span class="s1">y = -y</span>
  <span class="s1">acc = </span><span class="s2">None</span>
  <span class="s2">while </span><span class="s1">y &gt; </span><span class="s4">0</span><span class="s1">:</span>
    <span class="s2">if </span><span class="s1">y &amp; </span><span class="s4">1</span><span class="s1">:</span>
      <span class="s1">acc = x </span><span class="s2">if </span><span class="s1">acc </span><span class="s2">is None else </span><span class="s1">mul(acc</span><span class="s2">, </span><span class="s1">x)</span>
    <span class="s1">y &gt;&gt;= </span><span class="s4">1</span>
    <span class="s2">if </span><span class="s1">y &gt; </span><span class="s4">0</span><span class="s1">:</span>
      <span class="s0"># We don't call square because it calls integer_pow.</span>
      <span class="s1">x = mul(x</span><span class="s2">, </span><span class="s1">x)</span>
  <span class="s2">return </span><span class="s1">div(full_like(acc</span><span class="s2">, </span><span class="s4">1</span><span class="s1">)</span><span class="s2">, </span><span class="s1">acc) </span><span class="s2">if </span><span class="s1">is_reciprocal </span><span class="s2">else </span><span class="s1">acc</span>


<span class="s2">def </span><span class="s1">_integer_pow_lowering(ctx</span><span class="s2">, </span><span class="s1">x</span><span class="s2">, </span><span class="s1">*</span><span class="s2">, </span><span class="s1">y):</span>
  <span class="s1">lowering = mlir.lower_fun(_integer_pow</span><span class="s2">, </span><span class="s1">multiple_results=</span><span class="s2">False</span><span class="s1">)</span>
  <span class="s0"># TODO(b/217551391): emitting an out-of-line call leads to a large</span>
  <span class="s0"># expansion when the MLIR is lowered to HLO, because the HLO lowering</span>
  <span class="s0"># clones the callee. Consider unconditionally caching when the MLIR-&gt;HLO</span>
  <span class="s0"># lowering doesn't expand the program.</span>
  <span class="s2">if </span><span class="s1">y &gt;= </span><span class="s4">4</span><span class="s1">:</span>
    <span class="s1">lowering = mlir.cache_lowering(lowering)</span>
  <span class="s2">return </span><span class="s1">lowering(ctx</span><span class="s2">, </span><span class="s1">x</span><span class="s2">, </span><span class="s1">y=y)</span>

<span class="s1">mlir.register_lowering(integer_pow_p</span><span class="s2">, </span><span class="s1">_integer_pow_lowering)</span>

<span class="s1">_replace_zero = </span><span class="s2">lambda </span><span class="s1">x: select(eq(x</span><span class="s2">, </span><span class="s1">_const(x</span><span class="s2">, </span><span class="s4">0</span><span class="s1">))</span><span class="s2">, </span><span class="s1">_ones(x)</span><span class="s2">, </span><span class="s1">x)</span>

<span class="s1">not_p = standard_unop(_bool_or_int</span><span class="s2">, </span><span class="s3">'not'</span><span class="s1">)</span>
<span class="s1">ad.defjvp_zero(not_p)</span>
<span class="s1">mlir.register_lowering(not_p</span><span class="s2">, </span><span class="s1">partial(_nary_lower_hlo</span><span class="s2">, </span><span class="s1">hlo.NotOp))</span>

<span class="s1">and_p = standard_naryop([_bool_or_int</span><span class="s2">, </span><span class="s1">_bool_or_int]</span><span class="s2">, </span><span class="s3">'and'</span><span class="s1">)</span>
<span class="s1">ad.defjvp_zero(and_p)</span>
<span class="s1">mlir.register_lowering(and_p</span><span class="s2">, </span><span class="s1">partial(_nary_lower_hlo</span><span class="s2">, </span><span class="s1">hlo.AndOp))</span>

<span class="s1">or_p = standard_naryop([_bool_or_int</span><span class="s2">, </span><span class="s1">_bool_or_int]</span><span class="s2">, </span><span class="s3">'or'</span><span class="s1">)</span>
<span class="s1">ad.defjvp_zero(or_p)</span>
<span class="s1">mlir.register_lowering(or_p</span><span class="s2">, </span><span class="s1">partial(_nary_lower_hlo</span><span class="s2">, </span><span class="s1">hlo.OrOp))</span>

<span class="s1">xor_p = standard_naryop([_bool_or_int</span><span class="s2">, </span><span class="s1">_bool_or_int]</span><span class="s2">, </span><span class="s3">'xor'</span><span class="s1">)</span>
<span class="s1">ad.defjvp_zero(xor_p)</span>
<span class="s1">mlir.register_lowering(xor_p</span><span class="s2">, </span><span class="s1">partial(_nary_lower_hlo</span><span class="s2">, </span><span class="s1">hlo.XorOp))</span>

<span class="s1">population_count_p = standard_unop(_int</span><span class="s2">, </span><span class="s3">'population_count'</span><span class="s1">)</span>
<span class="s1">mlir.register_lowering(population_count_p</span><span class="s2">,</span>
                       <span class="s1">partial(_nary_lower_hlo</span><span class="s2">, </span><span class="s1">hlo.PopulationCountOp))</span>

<span class="s1">clz_p = standard_unop(_int</span><span class="s2">, </span><span class="s3">'clz'</span><span class="s1">)</span>
<span class="s1">mlir.register_lowering(clz_p</span><span class="s2">, </span><span class="s1">partial(_nary_lower_hlo</span><span class="s2">, </span><span class="s1">hlo.ClzOp))</span>

<span class="s2">def </span><span class="s1">_add_jvp(primals</span><span class="s2">, </span><span class="s1">tangents):</span>
  <span class="s1">x</span><span class="s2">, </span><span class="s1">y = primals</span>
  <span class="s1">xdot</span><span class="s2">, </span><span class="s1">ydot = tangents</span>
  <span class="s1">primal_out = add(x</span><span class="s2">, </span><span class="s1">y)</span>
  <span class="s2">if </span><span class="s1">type(xdot) </span><span class="s2">is </span><span class="s1">type(ydot) </span><span class="s2">is </span><span class="s1">ad_util.Zero:</span>
    <span class="s2">return </span><span class="s1">primal_out</span><span class="s2">, </span><span class="s1">ad_util.Zero.from_value(primal_out)</span>
  <span class="s2">if </span><span class="s1">type(xdot) </span><span class="s2">is </span><span class="s1">ad_util.Zero:</span>
    <span class="s2">return </span><span class="s1">primal_out</span><span class="s2">, </span><span class="s1">_maybe_broadcast(primal_out.shape</span><span class="s2">, </span><span class="s1">ydot)</span>
  <span class="s2">elif </span><span class="s1">type(ydot) </span><span class="s2">is </span><span class="s1">ad_util.Zero:</span>
    <span class="s2">return </span><span class="s1">primal_out</span><span class="s2">, </span><span class="s1">_maybe_broadcast(primal_out.shape</span><span class="s2">, </span><span class="s1">xdot)</span>
  <span class="s2">else</span><span class="s1">:</span>
    <span class="s2">return </span><span class="s1">primal_out</span><span class="s2">, </span><span class="s1">add(xdot</span><span class="s2">, </span><span class="s1">ydot)</span>

<span class="s2">def </span><span class="s1">_add_transpose(t</span><span class="s2">, </span><span class="s1">x</span><span class="s2">, </span><span class="s1">y):</span>
  <span class="s0"># Morally the following assertion is true, but because we instantiate zeros in</span>
  <span class="s0"># some places (e.g. in custom_jvp) it may not always hold. For example, see</span>
  <span class="s0"># api_test.py's CustomJVPTest.test_jaxpr_zeros.</span>
  <span class="s0"># assert ad.is_undefined_primal(x) and ad.is_undefined_primal(y)</span>
  <span class="s1">x_aval = x.aval </span><span class="s2">if </span><span class="s1">ad.is_undefined_primal(x) </span><span class="s2">else </span><span class="s1">_abstractify(x)</span>
  <span class="s1">y_aval = y.aval </span><span class="s2">if </span><span class="s1">ad.is_undefined_primal(y) </span><span class="s2">else </span><span class="s1">_abstractify(y)</span>
  <span class="s2">if </span><span class="s1">type(t) </span><span class="s2">is </span><span class="s1">ad_util.Zero:</span>
    <span class="s2">return </span><span class="s1">[ad_util.Zero(x_aval)</span><span class="s2">, </span><span class="s1">ad_util.Zero(y_aval)]</span>
  <span class="s2">else</span><span class="s1">:</span>
    <span class="s2">return </span><span class="s1">[_unbroadcast(x_aval</span><span class="s2">, </span><span class="s1">t)</span><span class="s2">, </span><span class="s1">_unbroadcast(y_aval</span><span class="s2">, </span><span class="s1">t)]</span>

<span class="s2">def </span><span class="s1">_add_inverse(r</span><span class="s2">, </span><span class="s1">x</span><span class="s2">, </span><span class="s1">y):</span>
  <span class="s1">xr = r - y</span>
  <span class="s1">yr = r - x</span>
  <span class="s2">return </span><span class="s1">xr</span><span class="s2">, </span><span class="s1">yr</span>

<span class="s0"># TODO(slebedev): Why does mypy fail to infer the type here?</span>
<span class="s1">add_p: Primitive = standard_naryop([_num</span><span class="s2">, </span><span class="s1">_num]</span><span class="s2">, </span><span class="s3">'add'</span><span class="s1">)</span>
<span class="s1">ad.primitive_jvps[add_p] = _add_jvp</span>
<span class="s1">ad.primitive_transposes[add_p] = _add_transpose</span>
<span class="s1">mlir.register_lowering(add_p</span><span class="s2">, </span><span class="s1">partial(_nary_lower_hlo</span><span class="s2">, </span><span class="s1">hlo.AddOp))</span>

<span class="s2">def </span><span class="s1">_sub_jvp(primals</span><span class="s2">, </span><span class="s1">tangents):</span>
  <span class="s1">x</span><span class="s2">, </span><span class="s1">y = primals</span>
  <span class="s1">xdot</span><span class="s2">, </span><span class="s1">ydot = tangents</span>
  <span class="s1">primal_out = sub(x</span><span class="s2">, </span><span class="s1">y)</span>
  <span class="s2">if </span><span class="s1">type(xdot) </span><span class="s2">is </span><span class="s1">type(ydot) </span><span class="s2">is </span><span class="s1">ad_util.Zero:</span>
    <span class="s2">return </span><span class="s1">primal_out</span><span class="s2">, </span><span class="s1">ad_util.Zero.from_value(primal_out)</span>
  <span class="s2">if </span><span class="s1">type(xdot) </span><span class="s2">is </span><span class="s1">ad_util.Zero:</span>
    <span class="s2">return </span><span class="s1">primal_out</span><span class="s2">, </span><span class="s1">_maybe_broadcast(primal_out.shape</span><span class="s2">, </span><span class="s1">neg(ydot))</span>
  <span class="s2">elif </span><span class="s1">type(ydot) </span><span class="s2">is </span><span class="s1">ad_util.Zero:</span>
    <span class="s2">return </span><span class="s1">primal_out</span><span class="s2">, </span><span class="s1">_maybe_broadcast(primal_out.shape</span><span class="s2">, </span><span class="s1">xdot)</span>
  <span class="s2">else</span><span class="s1">:</span>
    <span class="s2">return </span><span class="s1">primal_out</span><span class="s2">, </span><span class="s1">sub(xdot</span><span class="s2">, </span><span class="s1">ydot)</span>

<span class="s2">def </span><span class="s1">_sub_transpose(t</span><span class="s2">, </span><span class="s1">x</span><span class="s2">, </span><span class="s1">y):</span>
  <span class="s0"># Morally the following assertion is true, but see the comment in add_p's</span>
  <span class="s0"># transpose rule.</span>
  <span class="s0"># assert ad.is_undefined_primal(x) and ad.is_undefined_primal(y)</span>
  <span class="s1">x_aval = x.aval </span><span class="s2">if </span><span class="s1">ad.is_undefined_primal(x) </span><span class="s2">else </span><span class="s1">_abstractify(x)</span>
  <span class="s1">y_aval = y.aval </span><span class="s2">if </span><span class="s1">ad.is_undefined_primal(y) </span><span class="s2">else </span><span class="s1">_abstractify(y)</span>
  <span class="s2">if </span><span class="s1">type(t) </span><span class="s2">is </span><span class="s1">ad_util.Zero:</span>
    <span class="s2">return </span><span class="s1">[ad_util.Zero(x_aval)</span><span class="s2">, </span><span class="s1">ad_util.Zero(y_aval)]</span>
  <span class="s2">else</span><span class="s1">:</span>
    <span class="s2">return </span><span class="s1">[_unbroadcast(x_aval</span><span class="s2">, </span><span class="s1">t)</span><span class="s2">, </span><span class="s1">_unbroadcast(y_aval</span><span class="s2">, </span><span class="s1">neg(t))]</span>

<span class="s1">sub_p = standard_naryop([_num</span><span class="s2">, </span><span class="s1">_num]</span><span class="s2">, </span><span class="s3">'sub'</span><span class="s1">)</span>
<span class="s1">ad.primitive_jvps[sub_p] = _sub_jvp</span>
<span class="s1">ad.primitive_transposes[sub_p] = _sub_transpose</span>
<span class="s1">mlir.register_lowering(sub_p</span><span class="s2">, </span><span class="s1">partial(_nary_lower_hlo</span><span class="s2">, </span><span class="s1">hlo.SubtractOp))</span>


<span class="s2">def </span><span class="s1">_mul_transpose(ct</span><span class="s2">, </span><span class="s1">x</span><span class="s2">, </span><span class="s1">y):</span>
  <span class="s2">assert </span><span class="s1">ad.is_undefined_primal(x) ^ ad.is_undefined_primal(y)</span>
  <span class="s2">if </span><span class="s1">ad.is_undefined_primal(x):</span>
    <span class="s2">if </span><span class="s1">type(ct) </span><span class="s2">is </span><span class="s1">ad_util.Zero:</span>
      <span class="s2">return </span><span class="s1">[ad_util.Zero(x.aval)</span><span class="s2">, None</span><span class="s1">]</span>
    <span class="s2">else</span><span class="s1">:</span>
      <span class="s2">return </span><span class="s1">[_unbroadcast(x.aval</span><span class="s2">, </span><span class="s1">mul(ct</span><span class="s2">, </span><span class="s1">y))</span><span class="s2">, None</span><span class="s1">]</span>
  <span class="s2">else</span><span class="s1">:</span>
    <span class="s2">if </span><span class="s1">type(ct) </span><span class="s2">is </span><span class="s1">ad_util.Zero:</span>
      <span class="s2">return </span><span class="s1">[</span><span class="s2">None, </span><span class="s1">ad_util.Zero(y.aval)]</span>
    <span class="s2">else</span><span class="s1">:</span>
      <span class="s2">return </span><span class="s1">[</span><span class="s2">None, </span><span class="s1">_unbroadcast(y.aval</span><span class="s2">, </span><span class="s1">mul(x</span><span class="s2">, </span><span class="s1">ct))]</span>

<span class="s2">def </span><span class="s1">_mul_inverse(r</span><span class="s2">, </span><span class="s1">x</span><span class="s2">, </span><span class="s1">y):</span>
  <span class="s1">xr = r / y</span>
  <span class="s1">yr = r / x</span>
  <span class="s2">return </span><span class="s1">xr</span><span class="s2">, </span><span class="s1">yr</span>

<span class="s1">mul_p = standard_naryop([_num</span><span class="s2">, </span><span class="s1">_num]</span><span class="s2">, </span><span class="s3">'mul'</span><span class="s1">)</span>
<span class="s1">ad.defjvp(mul_p</span><span class="s2">,</span>
          <span class="s2">lambda </span><span class="s1">xdot</span><span class="s2">, </span><span class="s1">x</span><span class="s2">, </span><span class="s1">y: mul(xdot</span><span class="s2">, </span><span class="s1">y)</span><span class="s2">,</span>
          <span class="s2">lambda </span><span class="s1">ydot</span><span class="s2">, </span><span class="s1">x</span><span class="s2">, </span><span class="s1">y: mul(x</span><span class="s2">, </span><span class="s1">ydot))</span>
<span class="s1">ad.primitive_transposes[mul_p] = _mul_transpose</span>
<span class="s1">mlir.register_lowering(mul_p</span><span class="s2">, </span><span class="s1">partial(_nary_lower_hlo</span><span class="s2">, </span><span class="s1">hlo.MulOp))</span>

<span class="s2">def </span><span class="s1">_div_transpose_rule(cotangent</span><span class="s2">, </span><span class="s1">x</span><span class="s2">, </span><span class="s1">y):</span>
  <span class="s2">assert </span><span class="s1">ad.is_undefined_primal(x) </span><span class="s2">and not </span><span class="s1">ad.is_undefined_primal(y)</span>
  <span class="s2">if </span><span class="s1">type(cotangent) </span><span class="s2">is </span><span class="s1">ad_util.Zero:</span>
    <span class="s2">return </span><span class="s1">[ad_util.Zero(x.aval)</span><span class="s2">, None</span><span class="s1">]</span>
  <span class="s2">else</span><span class="s1">:</span>
    <span class="s2">return </span><span class="s1">[_unbroadcast(x.aval</span><span class="s2">, </span><span class="s1">div(cotangent</span><span class="s2">, </span><span class="s1">y))</span><span class="s2">, None</span><span class="s1">]</span>
<span class="s1">div_p = standard_naryop([_num</span><span class="s2">, </span><span class="s1">_num]</span><span class="s2">, </span><span class="s3">'div'</span><span class="s1">)</span>
<span class="s1">ad.defjvp(div_p</span><span class="s2">,</span>
          <span class="s2">lambda </span><span class="s1">g</span><span class="s2">, </span><span class="s1">x</span><span class="s2">, </span><span class="s1">y: div(g</span><span class="s2">, </span><span class="s1">y)</span><span class="s2">,</span>
          <span class="s2">lambda </span><span class="s1">g</span><span class="s2">, </span><span class="s1">x</span><span class="s2">, </span><span class="s1">y: mul(mul(neg(g)</span><span class="s2">, </span><span class="s1">x)</span><span class="s2">, </span><span class="s1">integer_pow(y</span><span class="s2">, </span><span class="s1">-</span><span class="s4">2</span><span class="s1">)))</span>
<span class="s1">ad.primitive_transposes[div_p] = _div_transpose_rule</span>
<span class="s1">mlir.register_lowering(div_p</span><span class="s2">, </span><span class="s1">partial(_nary_lower_hlo</span><span class="s2">, </span><span class="s1">hlo.DivOp))</span>

<span class="s1">rem_p = standard_naryop([_int | _float</span><span class="s2">, </span><span class="s1">_int | _float]</span><span class="s2">, </span><span class="s3">'rem'</span><span class="s1">)</span>
<span class="s1">ad.defjvp(</span>
    <span class="s1">rem_p</span><span class="s2">,</span>
    <span class="s2">lambda </span><span class="s1">g</span><span class="s2">, </span><span class="s1">x</span><span class="s2">, </span><span class="s1">y: _maybe_broadcast(broadcast_shapes(np.shape(x)</span><span class="s2">, </span><span class="s1">np.shape(y))</span><span class="s2">, </span><span class="s1">g)</span><span class="s2">,</span>
    <span class="s2">lambda </span><span class="s1">g</span><span class="s2">, </span><span class="s1">x</span><span class="s2">, </span><span class="s1">y: mul(neg(g)</span><span class="s2">, </span><span class="s1">mul(sign(div(x</span><span class="s2">, </span><span class="s1">y))</span><span class="s2">, </span><span class="s1">floor(abs(div(x</span><span class="s2">, </span><span class="s1">y))))))</span>
<span class="s1">mlir.register_lowering(rem_p</span><span class="s2">, </span><span class="s1">partial(_nary_lower_hlo</span><span class="s2">, </span><span class="s1">hlo.RemOp))</span>

<span class="s2">def </span><span class="s1">_minmax_complex_lowering(x</span><span class="s2">, </span><span class="s1">y</span><span class="s2">, </span><span class="s1">*</span><span class="s2">, </span><span class="s1">lax_cmp_pick_x):</span>
  <span class="s1">result_shape = broadcast_shapes(np.shape(x)</span><span class="s2">, </span><span class="s1">np.shape(y))</span>
  <span class="s1">x = _maybe_broadcast(result_shape</span><span class="s2">, </span><span class="s1">x)</span>
  <span class="s1">y = _maybe_broadcast(result_shape</span><span class="s2">, </span><span class="s1">y)</span>
  <span class="s1">rx = real(x)</span>
  <span class="s1">ry = real(y)</span>
  <span class="s1">pick_x = select(eq(rx</span><span class="s2">, </span><span class="s1">ry)</span><span class="s2">, </span><span class="s1">lax_cmp_pick_x(imag(x)</span><span class="s2">, </span><span class="s1">imag(y))</span><span class="s2">,</span>
                  <span class="s1">lax_cmp_pick_x(rx</span><span class="s2">, </span><span class="s1">ry))</span>
  <span class="s2">return </span><span class="s1">select(pick_x</span><span class="s2">, </span><span class="s1">x</span><span class="s2">, </span><span class="s1">y)</span>

<span class="s1">max_p: core.Primitive = standard_naryop([_any</span><span class="s2">, </span><span class="s1">_any]</span><span class="s2">, </span><span class="s3">'max'</span><span class="s1">)</span>
<span class="s1">ad.defjvp2(max_p</span><span class="s2">,</span>
           <span class="s2">lambda </span><span class="s1">g</span><span class="s2">, </span><span class="s1">ans</span><span class="s2">, </span><span class="s1">x</span><span class="s2">, </span><span class="s1">y: mul(g</span><span class="s2">, </span><span class="s1">_balanced_eq(x</span><span class="s2">, </span><span class="s1">ans</span><span class="s2">, </span><span class="s1">y))</span><span class="s2">,</span>
           <span class="s2">lambda </span><span class="s1">g</span><span class="s2">, </span><span class="s1">ans</span><span class="s2">, </span><span class="s1">x</span><span class="s2">, </span><span class="s1">y: mul(g</span><span class="s2">, </span><span class="s1">_balanced_eq(y</span><span class="s2">, </span><span class="s1">ans</span><span class="s2">, </span><span class="s1">x)))</span>
<span class="s1">mlir.register_lowering(max_p</span><span class="s2">, </span><span class="s1">partial(_nary_lower_hlo</span><span class="s2">, </span><span class="s1">mlir.max_hlo))</span>

<span class="s1">min_p: core.Primitive = standard_naryop([_any</span><span class="s2">, </span><span class="s1">_any]</span><span class="s2">, </span><span class="s3">'min'</span><span class="s1">)</span>
<span class="s1">ad.defjvp2(min_p</span><span class="s2">,</span>
           <span class="s2">lambda </span><span class="s1">g</span><span class="s2">, </span><span class="s1">ans</span><span class="s2">, </span><span class="s1">x</span><span class="s2">, </span><span class="s1">y: mul(g</span><span class="s2">, </span><span class="s1">_balanced_eq(x</span><span class="s2">, </span><span class="s1">ans</span><span class="s2">, </span><span class="s1">y))</span><span class="s2">,</span>
           <span class="s2">lambda </span><span class="s1">g</span><span class="s2">, </span><span class="s1">ans</span><span class="s2">, </span><span class="s1">x</span><span class="s2">, </span><span class="s1">y: mul(g</span><span class="s2">, </span><span class="s1">_balanced_eq(y</span><span class="s2">, </span><span class="s1">ans</span><span class="s2">, </span><span class="s1">x)))</span>
<span class="s1">mlir.register_lowering(min_p</span><span class="s2">, </span><span class="s1">partial(_nary_lower_hlo</span><span class="s2">, </span><span class="s1">mlir.min_hlo))</span>

<span class="s1">shift_left_p = standard_naryop([_int</span><span class="s2">, </span><span class="s1">_int]</span><span class="s2">, </span><span class="s3">'shift_left'</span><span class="s1">)</span>
<span class="s1">ad.defjvp_zero(shift_left_p)</span>
<span class="s1">mlir.register_lowering(shift_left_p</span><span class="s2">, </span><span class="s1">partial(_nary_lower_hlo</span><span class="s2">, </span><span class="s1">hlo.ShiftLeftOp))</span>

<span class="s1">shift_right_arithmetic_p = standard_naryop([_int</span><span class="s2">, </span><span class="s1">_int]</span><span class="s2">, </span><span class="s3">'shift_right_arithmetic'</span><span class="s1">)</span>
<span class="s1">ad.defjvp_zero(shift_right_arithmetic_p)</span>
<span class="s1">mlir.register_lowering(shift_right_arithmetic_p</span><span class="s2">,</span>
                       <span class="s1">partial(_nary_lower_hlo</span><span class="s2">, </span><span class="s1">hlo.ShiftRightArithmeticOp))</span>

<span class="s1">shift_right_logical_p = standard_naryop([_int</span><span class="s2">, </span><span class="s1">_int]</span><span class="s2">, </span><span class="s3">'shift_right_logical'</span><span class="s1">)</span>
<span class="s1">ad.defjvp_zero(shift_right_logical_p)</span>
<span class="s1">mlir.register_lowering(shift_right_logical_p</span><span class="s2">,</span>
                       <span class="s1">partial(_nary_lower_hlo</span><span class="s2">, </span><span class="s1">hlo.ShiftRightLogicalOp))</span>

<span class="s2">def </span><span class="s1">_compare_lower_hlo(direction: str</span><span class="s2">, </span><span class="s1">ctx</span><span class="s2">, </span><span class="s1">x</span><span class="s2">, </span><span class="s1">y):</span>
  <span class="s1">avals_in</span><span class="s2">, </span><span class="s1">(aval_out</span><span class="s2">,</span><span class="s1">) = ctx.avals_in</span><span class="s2">, </span><span class="s1">ctx.avals_out</span>
  <span class="s1">x_dtype = avals_in[</span><span class="s4">0</span><span class="s1">].dtype</span>
  <span class="s1">x</span><span class="s2">, </span><span class="s1">y = mlir.multi_broadcast_in_dim(ctx</span><span class="s2">, </span><span class="s1">(x</span><span class="s2">, </span><span class="s1">y)</span><span class="s2">, </span><span class="s1">avals_in</span><span class="s2">, </span><span class="s1">aval_out.shape)</span>

  <span class="s2">if </span><span class="s1">dtypes.issubdtype(x_dtype</span><span class="s2">, </span><span class="s1">np.inexact):</span>
    <span class="s1">compare_type = </span><span class="s3">&quot;FLOAT&quot;</span>
  <span class="s2">elif </span><span class="s1">dtypes.issubdtype(x_dtype</span><span class="s2">, </span><span class="s1">np.signedinteger):</span>
    <span class="s1">compare_type = </span><span class="s3">&quot;SIGNED&quot;</span>
  <span class="s2">else</span><span class="s1">:</span>
    <span class="s1">compare_type = </span><span class="s3">&quot;UNSIGNED&quot;</span>
  <span class="s2">return </span><span class="s1">mlir.compare_hlo(x</span><span class="s2">, </span><span class="s1">y</span><span class="s2">, </span><span class="s1">direction</span><span class="s2">, </span><span class="s1">compare_type).results</span>

<span class="s1">eq_p = naryop(_fixed_dtype(np.bool_)</span><span class="s2">, </span><span class="s1">[_any</span><span class="s2">, </span><span class="s1">_any]</span><span class="s2">, </span><span class="s3">'eq'</span><span class="s1">)</span>
<span class="s1">ad.defjvp_zero(eq_p)</span>
<span class="s1">mlir.register_lowering(eq_p</span><span class="s2">, </span><span class="s1">partial(_compare_lower_hlo</span><span class="s2">, </span><span class="s3">&quot;EQ&quot;</span><span class="s1">))</span>

<span class="s1">ne_p = naryop(_fixed_dtype(np.bool_)</span><span class="s2">, </span><span class="s1">[_any</span><span class="s2">, </span><span class="s1">_any]</span><span class="s2">, </span><span class="s3">'ne'</span><span class="s1">)</span>
<span class="s1">ad.defjvp_zero(ne_p)</span>
<span class="s1">mlir.register_lowering(ne_p</span><span class="s2">, </span><span class="s1">partial(_compare_lower_hlo</span><span class="s2">, </span><span class="s3">&quot;NE&quot;</span><span class="s1">))</span>

<span class="s1">ge_p = naryop(_fixed_dtype(np.bool_)</span><span class="s2">, </span><span class="s1">[_ordered</span><span class="s2">, </span><span class="s1">_ordered]</span><span class="s2">, </span><span class="s3">'ge'</span><span class="s1">)</span>
<span class="s1">ad.defjvp_zero(ge_p)</span>
<span class="s1">mlir.register_lowering(ge_p</span><span class="s2">, </span><span class="s1">partial(_compare_lower_hlo</span><span class="s2">, </span><span class="s3">&quot;GE&quot;</span><span class="s1">))</span>

<span class="s1">gt_p = naryop(_fixed_dtype(np.bool_)</span><span class="s2">, </span><span class="s1">[_ordered</span><span class="s2">, </span><span class="s1">_ordered]</span><span class="s2">, </span><span class="s3">'gt'</span><span class="s1">)</span>
<span class="s1">ad.defjvp_zero(gt_p)</span>
<span class="s1">mlir.register_lowering(gt_p</span><span class="s2">, </span><span class="s1">partial(_compare_lower_hlo</span><span class="s2">, </span><span class="s3">&quot;GT&quot;</span><span class="s1">))</span>

<span class="s1">le_p = naryop(_fixed_dtype(np.bool_)</span><span class="s2">, </span><span class="s1">[_ordered</span><span class="s2">, </span><span class="s1">_ordered]</span><span class="s2">, </span><span class="s3">'le'</span><span class="s1">)</span>
<span class="s1">ad.defjvp_zero(le_p)</span>
<span class="s1">mlir.register_lowering(le_p</span><span class="s2">, </span><span class="s1">partial(_compare_lower_hlo</span><span class="s2">, </span><span class="s3">&quot;LE&quot;</span><span class="s1">))</span>

<span class="s1">lt_p = naryop(_fixed_dtype(np.bool_)</span><span class="s2">, </span><span class="s1">[_ordered</span><span class="s2">, </span><span class="s1">_ordered]</span><span class="s2">, </span><span class="s3">'lt'</span><span class="s1">)</span>
<span class="s1">ad.defjvp_zero(lt_p)</span>
<span class="s1">mlir.register_lowering(lt_p</span><span class="s2">, </span><span class="s1">partial(_compare_lower_hlo</span><span class="s2">, </span><span class="s3">&quot;LT&quot;</span><span class="s1">))</span>


<span class="s2">def </span><span class="s1">_convert_element_type_shape_rule(operand</span><span class="s2">, </span><span class="s1">*</span><span class="s2">, </span><span class="s1">new_dtype</span><span class="s2">, </span><span class="s1">weak_type):</span>
  <span class="s2">return </span><span class="s1">operand.shape</span>

<span class="s2">def </span><span class="s1">_convert_element_type_dtype_rule(operand</span><span class="s2">, </span><span class="s1">*</span><span class="s2">, </span><span class="s1">new_dtype</span><span class="s2">, </span><span class="s1">weak_type):</span>
  <span class="s2">return </span><span class="s1">new_dtype</span>

<span class="s2">def </span><span class="s1">_convert_element_type_weak_type_rule(operand</span><span class="s2">, </span><span class="s1">*</span><span class="s2">, </span><span class="s1">new_dtype</span><span class="s2">, </span><span class="s1">weak_type):</span>
  <span class="s2">return </span><span class="s1">weak_type</span>

<span class="s2">def </span><span class="s1">_convert_element_type_transpose_rule(ct</span><span class="s2">, </span><span class="s1">operand</span><span class="s2">, </span><span class="s1">*</span><span class="s2">, </span><span class="s1">new_dtype</span><span class="s2">, </span><span class="s1">weak_type):</span>
  <span class="s2">assert </span><span class="s1">ad.is_undefined_primal(operand)</span>
  <span class="s1">old_dtype = operand.aval.dtype</span>
  <span class="s1">old_weak_type = dtypes.is_weakly_typed(operand)</span>
  <span class="s2">if </span><span class="s1">type(ct) </span><span class="s2">is </span><span class="s1">ad_util.Zero:</span>
    <span class="s2">return </span><span class="s1">[ad_util.Zero(operand.aval)]</span>
  <span class="s2">elif </span><span class="s1">core.primal_dtype_to_tangent_dtype(old_dtype) == dtypes.float0:</span>
    <span class="s2">return </span><span class="s1">[ad_util.Zero(operand.aval.update(dtype=dtypes.float0</span><span class="s2">, </span><span class="s1">weak_type=</span><span class="s2">False</span><span class="s1">))]</span>
  <span class="s2">else</span><span class="s1">:</span>
    <span class="s2">return </span><span class="s1">[convert_element_type_p.bind(ct</span><span class="s2">, </span><span class="s1">new_dtype=old_dtype</span><span class="s2">,</span>
                                        <span class="s1">weak_type=old_weak_type)]</span>

<span class="s2">def </span><span class="s1">_convert_element_type_jvp_rule(tangent</span><span class="s2">, </span><span class="s1">operand </span><span class="s2">, </span><span class="s1">*</span><span class="s2">, </span><span class="s1">new_dtype</span><span class="s2">, </span><span class="s1">weak_type):</span>
  <span class="s2">if </span><span class="s1">core.primal_dtype_to_tangent_dtype(new_dtype) == dtypes.float0:</span>
    <span class="s2">return </span><span class="s1">ad_util.Zero(tangent.aval.update(dtype=dtypes.float0</span><span class="s2">, </span><span class="s1">weak_type=</span><span class="s2">False</span><span class="s1">))</span>
  <span class="s2">else</span><span class="s1">:</span>
    <span class="s2">return </span><span class="s1">convert_element_type_p.bind(tangent</span><span class="s2">, </span><span class="s1">new_dtype=new_dtype</span><span class="s2">,</span>
                                       <span class="s1">weak_type=weak_type)</span>

<span class="s2">def </span><span class="s1">_convert_elt_type_folding_rule(consts</span><span class="s2">, </span><span class="s1">eqn):</span>
  <span class="s0"># We constant-fold convert_element_types applied to constants if those</span>
  <span class="s0"># constants are Python builtin numeric types or numpy.ndarrays (so as not</span>
  <span class="s0"># to perform any device operations when constant-folding) and if the output</span>
  <span class="s0"># type can be faithfully represented by a Python builtin numeric type or</span>
  <span class="s0"># numpy.ndarray. If those conditions are met, we output a numpy.ndarray</span>
  <span class="s0"># constant if the output type is not weak, and if the output type is weak then</span>
  <span class="s0"># we output a Python builtin numeric type.</span>
  <span class="s0"># TODO(mattjj): allow constant-folding CPU-backed JAX arrays</span>
  <span class="s1">c</span><span class="s2">, </span><span class="s1">= consts</span>
  <span class="s1">o</span><span class="s2">, </span><span class="s1">= eqn.outvars</span>
  <span class="s2">if </span><span class="s1">(type(c) </span><span class="s2">in </span><span class="s1">{np.ndarray</span><span class="s2">, </span><span class="s1">*dtypes.python_scalar_dtypes} </span><span class="s2">and</span>
      <span class="s1">isinstance(o.aval</span><span class="s2">, </span><span class="s1">core.UnshapedArray) </span><span class="s2">and not </span><span class="s1">np.shape(c) </span><span class="s2">and</span>
      <span class="s2">not </span><span class="s1">core.is_opaque_dtype(eqn.params[</span><span class="s3">'new_dtype'</span><span class="s1">])):</span>
    <span class="s1">out = np.array(c</span><span class="s2">, </span><span class="s1">eqn.params[</span><span class="s3">'new_dtype'</span><span class="s1">])</span>
    <span class="s2">if not </span><span class="s1">o.aval.weak_type:</span>
      <span class="s2">return </span><span class="s1">[out]</span><span class="s2">, None</span>
    <span class="s1">out = out.item()</span>
    <span class="s2">if </span><span class="s1">core.get_aval(out).dtype </span><span class="s2">is </span><span class="s1">o.aval.dtype:</span>
      <span class="s2">return </span><span class="s1">[out]</span><span class="s2">, None</span>
  <span class="s2">return </span><span class="s1">[</span><span class="s2">None</span><span class="s1">]</span><span class="s2">, </span><span class="s1">eqn</span>

<span class="s2">def </span><span class="s1">_convert_elt_type_fwd_rule(eqn):</span>
  <span class="s1">v</span><span class="s2">, </span><span class="s1">= eqn.invars</span>
  <span class="s2">if </span><span class="s1">(</span><span class="s2">not </span><span class="s1">core.is_opaque_dtype(eqn.params[</span><span class="s3">'new_dtype'</span><span class="s1">]) </span><span class="s2">and</span>
      <span class="s2">not </span><span class="s1">core.is_opaque_dtype(v.aval.dtype) </span><span class="s2">and</span>
      <span class="s1">v.aval.dtype == eqn.params[</span><span class="s3">'new_dtype'</span><span class="s1">] </span><span class="s2">and</span>
      <span class="s1">v.aval.weak_type == eqn.params[</span><span class="s3">'weak_type'</span><span class="s1">]):</span>
    <span class="s2">return </span><span class="s1">[v]</span><span class="s2">, None</span>
  <span class="s2">else</span><span class="s1">:</span>
    <span class="s2">return </span><span class="s1">[</span><span class="s2">None</span><span class="s1">]</span><span class="s2">, </span><span class="s1">eqn</span>

<span class="s2">def </span><span class="s1">_convert_elt_type_pp_rule(eqn</span><span class="s2">, </span><span class="s1">context</span><span class="s2">, </span><span class="s1">settings):</span>
  <span class="s0"># don't print new_dtype because the output binder shows it, don't print</span>
  <span class="s0"># weak_type when false</span>
  <span class="s1">params = dict(eqn.params)</span>
  <span class="s2">del </span><span class="s1">params[</span><span class="s3">'new_dtype'</span><span class="s1">]  </span><span class="s0"># output binder shows it</span>
  <span class="s2">if not </span><span class="s1">params[</span><span class="s3">'weak_type'</span><span class="s1">]: </span><span class="s2">del </span><span class="s1">params[</span><span class="s3">'weak_type'</span><span class="s1">]  </span><span class="s0"># don't show trivial case</span>
  <span class="s2">return </span><span class="s1">core._pp_eqn(eqn.replace(params=params)</span><span class="s2">, </span><span class="s1">context</span><span class="s2">, </span><span class="s1">settings)</span>

<span class="s1">convert_element_type_p = Primitive(</span><span class="s3">'convert_element_type'</span><span class="s1">)</span>
<span class="s1">convert_element_type_p.def_impl(partial(dispatch.apply_primitive</span><span class="s2">, </span><span class="s1">convert_element_type_p))</span>
<span class="s1">convert_element_type_p.def_abstract_eval(</span>
    <span class="s1">partial(standard_abstract_eval</span><span class="s2">, </span><span class="s1">convert_element_type_p</span><span class="s2">,</span>
            <span class="s1">_convert_element_type_shape_rule</span><span class="s2">, </span><span class="s1">_convert_element_type_dtype_rule</span><span class="s2">,</span>
            <span class="s1">_convert_element_type_weak_type_rule</span><span class="s2">, </span><span class="s1">standard_named_shape_rule))</span>
<span class="s1">ad.defjvp(convert_element_type_p</span><span class="s2">, </span><span class="s1">_convert_element_type_jvp_rule)</span>
<span class="s1">ad.primitive_transposes[convert_element_type_p] = _convert_element_type_transpose_rule</span>
<span class="s1">batching.defvectorized(convert_element_type_p)</span>
<span class="s1">pe.const_fold_rules[convert_element_type_p] = _convert_elt_type_folding_rule</span>
<span class="s1">pe.forwarding_rules[convert_element_type_p] = _convert_elt_type_fwd_rule</span>
<span class="s1">pe.def_trivial_padding(convert_element_type_p)</span>
<span class="s0"># TODO(mattjj): un-comment the next line (see #9456)</span>
<span class="s0"># core.pp_eqn_rules[convert_element_type_p] = _convert_elt_type_pp_rule</span>

<span class="s2">def </span><span class="s1">_real_dtype(dtype): </span><span class="s2">return </span><span class="s1">np.finfo(dtype).dtype</span>

<span class="s2">def </span><span class="s1">_convert_element_type_lower(ctx</span><span class="s2">, </span><span class="s1">operand</span><span class="s2">, </span><span class="s1">*</span><span class="s2">, </span><span class="s1">new_dtype</span><span class="s2">, </span><span class="s1">weak_type):</span>
  <span class="s1">aval_in</span><span class="s2">, </span><span class="s1">= ctx.avals_in</span>
  <span class="s1">aval_out</span><span class="s2">, </span><span class="s1">= ctx.avals_out</span>
  <span class="s2">if </span><span class="s1">(dtypes.issubdtype(aval_in.dtype</span><span class="s2">, </span><span class="s1">np.complexfloating) </span><span class="s2">and</span>
      <span class="s2">not </span><span class="s1">dtypes.issubdtype(new_dtype</span><span class="s2">, </span><span class="s1">np.complexfloating)):</span>
    <span class="s1">operand = hlo.RealOp(operand).result</span>
    <span class="s1">aval_in = aval_in.update(dtype=_real_dtype(aval_in.dtype))</span>
  <span class="s2">return </span><span class="s1">[mlir.convert_hlo(ctx</span><span class="s2">, </span><span class="s1">operand</span><span class="s2">, </span><span class="s1">aval_in</span><span class="s2">, </span><span class="s1">aval_out)]</span>

<span class="s1">mlir.register_lowering(convert_element_type_p</span><span class="s2">, </span><span class="s1">_convert_element_type_lower)</span>


<span class="s2">def </span><span class="s1">_bitcast_convert_type_shape_rule(operand</span><span class="s2">, </span><span class="s1">*</span><span class="s2">, </span><span class="s1">new_dtype):</span>
  <span class="s1">old_dtype = dtypes.canonicalize_dtype(operand.dtype)</span>
  <span class="s1">new_dtype = dtypes.canonicalize_dtype(new_dtype)</span>

  <span class="s2">if </span><span class="s1">old_dtype.itemsize == new_dtype.itemsize:</span>
    <span class="s2">return </span><span class="s1">operand.shape</span>
  <span class="s2">elif </span><span class="s1">old_dtype.itemsize &gt; new_dtype.itemsize:</span>
    <span class="s2">return </span><span class="s1">(*operand.shape</span><span class="s2">, </span><span class="s1">old_dtype.itemsize // new_dtype.itemsize)</span>
  <span class="s2">else</span><span class="s1">:</span>
    <span class="s1">dim_size = operand.shape[-</span><span class="s4">1</span><span class="s1">] </span><span class="s2">if </span><span class="s1">operand.shape </span><span class="s2">else </span><span class="s4">1</span>
    <span class="s2">if </span><span class="s1">dim_size * old_dtype.itemsize != new_dtype.itemsize:</span>
      <span class="s2">raise </span><span class="s1">ValueError(</span>
        <span class="s3">f&quot;Attempting to convert array of shape </span><span class="s2">{</span><span class="s1">operand.shape</span><span class="s2">} </span><span class="s3">&quot;</span>
        <span class="s3">f&quot;from </span><span class="s2">{</span><span class="s1">str(old_dtype)</span><span class="s2">} </span><span class="s3">of size </span><span class="s2">{</span><span class="s1">old_dtype.itemsize</span><span class="s2">} </span><span class="s3">&quot;</span>
        <span class="s3">f&quot;to </span><span class="s2">{</span><span class="s1">str(new_dtype)</span><span class="s2">} </span><span class="s3">of size </span><span class="s2">{</span><span class="s1">new_dtype.itemsize</span><span class="s2">}</span><span class="s3">, &quot;</span>
        <span class="s3">f&quot;but </span><span class="s2">{</span><span class="s1">dim_size</span><span class="s2">} </span><span class="s3">* </span><span class="s2">{</span><span class="s1">old_dtype.itemsize</span><span class="s2">} </span><span class="s3">!= </span><span class="s2">{</span><span class="s1">new_dtype.itemsize</span><span class="s2">}</span><span class="s3">&quot;</span><span class="s1">)</span>
    <span class="s2">return </span><span class="s1">operand.shape[:-</span><span class="s4">1</span><span class="s1">]</span>

<span class="s2">def </span><span class="s1">_bitcast_convert_type_dtype_rule(operand</span><span class="s2">, </span><span class="s1">*</span><span class="s2">, </span><span class="s1">new_dtype):</span>
  <span class="s1">old_dtype = dtypes.canonicalize_dtype(operand.dtype)</span>
  <span class="s1">new_dtype = dtypes.canonicalize_dtype(new_dtype)</span>
  <span class="s2">if </span><span class="s1">(dtypes.issubdtype(old_dtype</span><span class="s2">, </span><span class="s1">np.bool_) </span><span class="s2">or</span>
      <span class="s1">dtypes.issubdtype(old_dtype</span><span class="s2">, </span><span class="s1">np.complexfloating) </span><span class="s2">or</span>
      <span class="s1">dtypes.issubdtype(new_dtype</span><span class="s2">, </span><span class="s1">np.bool_) </span><span class="s2">or</span>
      <span class="s1">dtypes.issubdtype(new_dtype</span><span class="s2">, </span><span class="s1">np.complexfloating)):</span>
    <span class="s2">if </span><span class="s1">old_dtype != new_dtype:</span>
      <span class="s2">raise </span><span class="s1">TypeError(</span><span class="s3">&quot;lax.bitcast_convert_type does not support bool or complex values &quot;</span>
                      <span class="s3">&quot;unless the operand and destination types match. &quot;</span>
                      <span class="s3">f&quot;Got operand dtype=</span><span class="s2">{</span><span class="s1">old_dtype</span><span class="s2">}</span><span class="s3">, </span><span class="s2">{</span><span class="s1">new_dtype=</span><span class="s2">}</span><span class="s3">. &quot;</span>
                      <span class="s3">&quot;Consider using the arr.view() method instead.&quot;</span><span class="s1">)</span>
  <span class="s2">return </span><span class="s1">new_dtype</span>

<span class="s1">bitcast_convert_type_p = standard_primitive(</span>
    <span class="s1">_bitcast_convert_type_shape_rule</span><span class="s2">, </span><span class="s1">_bitcast_convert_type_dtype_rule</span><span class="s2">,</span>
    <span class="s3">'bitcast_convert_type'</span><span class="s2">, </span><span class="s1">weak_type_rule=_strip_weak_type)</span>
<span class="s1">ad.defjvp_zero(bitcast_convert_type_p)</span>
<span class="s1">batching.defvectorized(bitcast_convert_type_p)</span>

<span class="s2">def </span><span class="s1">_bitcast_convert_type_lower(ctx</span><span class="s2">, </span><span class="s1">operand</span><span class="s2">, </span><span class="s1">*</span><span class="s2">, </span><span class="s1">new_dtype):</span>
  <span class="s1">aval_out</span><span class="s2">, </span><span class="s1">= ctx.avals_out</span>
  <span class="s2">return </span><span class="s1">hlo.BitcastConvertOp(mlir.aval_to_ir_type(aval_out)</span><span class="s2">, </span><span class="s1">operand).results</span>

<span class="s1">mlir.register_lowering(bitcast_convert_type_p</span><span class="s2">, </span><span class="s1">_bitcast_convert_type_lower)</span>


<span class="s2">def </span><span class="s1">_validate_preferred_element_type(input_dtype</span><span class="s2">, </span><span class="s1">preferred_element_type):</span>

  <span class="s2">if </span><span class="s1">dtypes.issubdtype(input_dtype</span><span class="s2">, </span><span class="s1">np.integer) </span><span class="s2">and </span><span class="s1">dtypes.issubdtype(preferred_element_type</span><span class="s2">, </span><span class="s1">np.floating):</span>
    <span class="s0"># Special-case integer-&gt;float multiply. This is allowed, and also allows</span>
    <span class="s0"># different signedness between input and output.</span>
    <span class="s2">pass</span>
  <span class="s2">else</span><span class="s1">:</span>
    <span class="s1">allowed_types = (np.integer</span><span class="s2">, </span><span class="s1">np.floating</span><span class="s2">, </span><span class="s1">np.complexfloating)</span>
    <span class="s2">if </span><span class="s1">any(dtypes.issubdtype(input_dtype</span><span class="s2">, </span><span class="s1">t) </span><span class="s2">and not </span><span class="s1">dtypes.issubdtype(preferred_element_type</span><span class="s2">, </span><span class="s1">t) </span><span class="s2">for </span><span class="s1">t </span><span class="s2">in </span><span class="s1">allowed_types):</span>
      <span class="s2">raise </span><span class="s1">TypeError(</span><span class="s3">&quot;Input type is incompatible with `preferred_element_type`. The compatible combinations of &quot;</span>
                      <span class="s3">&quot;(input_type, preferred_element_type) are (integral, integral), (integral, floating), &quot;</span>
                      <span class="s3">&quot;(floating, floating), (complex, complex.&quot;</span><span class="s1">)</span>
    <span class="s2">if </span><span class="s1">dtypes.issubdtype(input_dtype</span><span class="s2">, </span><span class="s1">np.signedinteger) </span><span class="s2">and not </span><span class="s1">dtypes.issubdtype(preferred_element_type</span><span class="s2">, </span><span class="s1">np.signedinteger):</span>
      <span class="s2">raise </span><span class="s1">TypeError(</span><span class="s3">&quot;`preferred_element_type` must have the same signedness as the original type.&quot;</span><span class="s1">)</span>
  <span class="s1">input_bitwidth = np.dtype(input_dtype).itemsize</span>
  <span class="s1">preferred_bitwidth = np.dtype(preferred_element_type).itemsize</span>
  <span class="s2">if </span><span class="s1">preferred_bitwidth &lt; input_bitwidth:</span>
    <span class="s2">raise </span><span class="s1">TypeError(</span><span class="s3">&quot;`preferred_element_type` must not be narrower than the original type.&quot;</span><span class="s1">)</span>

<span class="s2">def </span><span class="s1">_precision_config(precision):</span>
  <span class="s2">if </span><span class="s1">precision </span><span class="s2">is not None</span><span class="s1">:</span>
    <span class="s1">config = xla_client.PrecisionConfig()</span>
    <span class="s2">if </span><span class="s1">isinstance(precision</span><span class="s2">, </span><span class="s1">tuple):</span>
      <span class="s1">config.operand_precision.extend(precision)</span>
    <span class="s2">else</span><span class="s1">:</span>
      <span class="s1">config.operand_precision.extend((precision</span><span class="s2">, </span><span class="s1">precision))</span>
    <span class="s2">return </span><span class="s1">config</span>
  <span class="s2">return None</span>


<span class="s2">def </span><span class="s1">_dot_general_shape_rule(lhs</span><span class="s2">, </span><span class="s1">rhs</span><span class="s2">, </span><span class="s1">*</span><span class="s2">, </span><span class="s1">dimension_numbers</span><span class="s2">, </span><span class="s1">precision</span><span class="s2">,</span>
                            <span class="s1">preferred_element_type: Optional[DTypeLike]):</span>
  <span class="s1">(lhs_contracting</span><span class="s2">, </span><span class="s1">rhs_contracting)</span><span class="s2">, </span><span class="s1">(lhs_batch</span><span class="s2">, </span><span class="s1">rhs_batch) = dimension_numbers</span>
  <span class="s2">if not </span><span class="s1">all(np.all(np.greater_equal(d</span><span class="s2">, </span><span class="s4">0</span><span class="s1">)) </span><span class="s2">and </span><span class="s1">np.all(np.less(d</span><span class="s2">, </span><span class="s1">lhs.ndim))</span>
             <span class="s2">for </span><span class="s1">d </span><span class="s2">in </span><span class="s1">(lhs_contracting</span><span class="s2">, </span><span class="s1">lhs_batch)):</span>
    <span class="s1">msg = (</span><span class="s3">&quot;dot_general requires lhs dimension numbers to be nonnegative and &quot;</span>
           <span class="s3">&quot;less than the number of axes of the lhs value, got &quot;</span>
           <span class="s3">f&quot;lhs_batch of </span><span class="s2">{</span><span class="s1">lhs_batch</span><span class="s2">} </span><span class="s3">and lhs_contracting of </span><span class="s2">{</span><span class="s1">lhs_contracting</span><span class="s2">} </span><span class="s3">&quot;</span>
           <span class="s3">f&quot;for lhs of rank </span><span class="s2">{</span><span class="s1">lhs.ndim</span><span class="s2">}</span><span class="s3">&quot;</span><span class="s1">)</span>
    <span class="s2">raise </span><span class="s1">TypeError(msg)</span>
  <span class="s2">if not </span><span class="s1">all(np.all(np.greater_equal(d</span><span class="s2">, </span><span class="s4">0</span><span class="s1">)) </span><span class="s2">and </span><span class="s1">np.all(np.less(d</span><span class="s2">, </span><span class="s1">rhs.ndim))</span>
             <span class="s2">for </span><span class="s1">d </span><span class="s2">in </span><span class="s1">(rhs_contracting</span><span class="s2">, </span><span class="s1">rhs_batch)):</span>
    <span class="s1">msg = (</span><span class="s3">&quot;dot_general requires rhs dimension numbers to be nonnegative and &quot;</span>
           <span class="s3">&quot;less than the number of axes of the rhs value, got &quot;</span>
           <span class="s3">f&quot;rhs_batch of </span><span class="s2">{</span><span class="s1">rhs_batch</span><span class="s2">} </span><span class="s3">and rhs_contracting of </span><span class="s2">{</span><span class="s1">rhs_contracting</span><span class="s2">} </span><span class="s3">&quot;</span>
           <span class="s3">f&quot;for rhs of rank </span><span class="s2">{</span><span class="s1">rhs.ndim</span><span class="s2">}</span><span class="s3">&quot;</span><span class="s1">)</span>
    <span class="s2">raise </span><span class="s1">TypeError(msg)</span>
  <span class="s2">if </span><span class="s1">len(lhs_batch) != len(rhs_batch):</span>
    <span class="s1">msg = (</span><span class="s3">&quot;dot_general requires equal numbers of lhs_batch and rhs_batch &quot;</span>
           <span class="s3">&quot;dimensions, got lhs_batch {} and rhs_batch {}.&quot;</span><span class="s1">)</span>
    <span class="s2">raise </span><span class="s1">TypeError(msg.format(lhs_batch</span><span class="s2">, </span><span class="s1">rhs_batch))</span>
  <span class="s1">lhs_contracting_set</span><span class="s2">, </span><span class="s1">lhs_batch_set = set(lhs_contracting)</span><span class="s2">, </span><span class="s1">set(lhs_batch)</span>
  <span class="s1">rhs_contracting_set</span><span class="s2">, </span><span class="s1">rhs_batch_set = set(rhs_contracting)</span><span class="s2">, </span><span class="s1">set(rhs_batch)</span>
  <span class="s2">if </span><span class="s1">len(lhs_batch_set) != len(lhs_batch):</span>
    <span class="s1">msg = (</span><span class="s3">&quot;dot_general requires lhs batch dimensions to be distinct, got &quot;</span>
           <span class="s3">f&quot;lhs_batch </span><span class="s2">{</span><span class="s1">lhs_batch</span><span class="s2">}</span><span class="s3">.&quot;</span><span class="s1">)</span>
    <span class="s2">raise </span><span class="s1">TypeError(msg)</span>
  <span class="s2">if </span><span class="s1">len(rhs_batch_set) != len(rhs_batch):</span>
    <span class="s1">msg = (</span><span class="s3">&quot;dot_general requires rhs batch dimensions to be distinct, got &quot;</span>
           <span class="s3">f&quot;rhs_batch </span><span class="s2">{</span><span class="s1">rhs_batch</span><span class="s2">}</span><span class="s3">.&quot;</span><span class="s1">)</span>
    <span class="s2">raise </span><span class="s1">TypeError(msg)</span>
  <span class="s2">if </span><span class="s1">len(lhs_contracting_set) != len(lhs_contracting):</span>
    <span class="s1">msg = (</span><span class="s3">&quot;dot_general requires lhs contracting dimensions to be distinct, &quot;</span>
           <span class="s3">f&quot;got lhs_contracting </span><span class="s2">{</span><span class="s1">lhs_contracting</span><span class="s2">}</span><span class="s3">.&quot;</span><span class="s1">)</span>
    <span class="s2">raise </span><span class="s1">TypeError(msg)</span>
  <span class="s2">if </span><span class="s1">len(rhs_contracting_set) != len(rhs_contracting):</span>
    <span class="s1">msg = (</span><span class="s3">&quot;dot_general requires rhs contracting dimensions to be distinct, &quot;</span>
           <span class="s3">f&quot;got rhs_contracting </span><span class="s2">{</span><span class="s1">rhs_contracting</span><span class="s2">}</span><span class="s3">.&quot;</span><span class="s1">)</span>
    <span class="s2">raise </span><span class="s1">TypeError(msg)</span>
  <span class="s2">if </span><span class="s1">lhs_contracting_set &amp; lhs_batch_set:</span>
    <span class="s1">msg = (</span><span class="s3">&quot;dot_general requires lhs batch dimensions to be disjoint from &quot;</span>
           <span class="s3">&quot;contracting dimensions, got lhs_batch {} and lhs_contracting {}.&quot;</span><span class="s1">)</span>
    <span class="s2">raise </span><span class="s1">TypeError(msg.format(lhs_batch</span><span class="s2">, </span><span class="s1">lhs_contracting))</span>
  <span class="s2">if </span><span class="s1">rhs_contracting_set &amp; rhs_batch_set:</span>
    <span class="s1">msg = (</span><span class="s3">&quot;dot_general requires rhs batch dimensions to be disjoint from &quot;</span>
           <span class="s3">&quot;contracting dimensions, got rhs_batch {} and rhs_contracting {}.&quot;</span><span class="s1">)</span>
    <span class="s2">raise </span><span class="s1">TypeError(msg.format(rhs_batch</span><span class="s2">, </span><span class="s1">rhs_contracting))</span>
  <span class="s1">lhs_batch_shape = tuple(lhs.shape[i] </span><span class="s2">for </span><span class="s1">i </span><span class="s2">in </span><span class="s1">lhs_batch)</span>
  <span class="s1">rhs_batch_shape = tuple(rhs.shape[i] </span><span class="s2">for </span><span class="s1">i </span><span class="s2">in </span><span class="s1">rhs_batch)</span>
  <span class="s2">if not </span><span class="s1">core.symbolic_equal_shape(lhs_batch_shape</span><span class="s2">, </span><span class="s1">rhs_batch_shape):</span>
    <span class="s1">msg = (</span><span class="s3">&quot;dot_general requires lhs batch dimensions and rhs batch dimensions &quot;</span>
           <span class="s3">&quot;to have the same shape, got {} and {}.&quot;</span><span class="s1">)</span>
    <span class="s2">raise </span><span class="s1">TypeError(msg.format(lhs_batch_shape</span><span class="s2">, </span><span class="s1">rhs_batch_shape))</span>
  <span class="s1">lhs_contracting_shape = tuple(lhs.shape[i] </span><span class="s2">for </span><span class="s1">i </span><span class="s2">in </span><span class="s1">lhs_contracting)</span>
  <span class="s1">rhs_contracting_shape = tuple(rhs.shape[i] </span><span class="s2">for </span><span class="s1">i </span><span class="s2">in </span><span class="s1">rhs_contracting)</span>
  <span class="s2">if not </span><span class="s1">core.symbolic_equal_shape(lhs_contracting_shape</span><span class="s2">, </span><span class="s1">rhs_contracting_shape):</span>
    <span class="s1">msg = (</span><span class="s3">&quot;dot_general requires contracting dimensions to have the same &quot;</span>
           <span class="s3">&quot;shape, got {} and {}.&quot;</span><span class="s1">)</span>
    <span class="s2">raise </span><span class="s1">TypeError(msg.format(lhs_contracting_shape</span><span class="s2">, </span><span class="s1">rhs_contracting_shape))</span>

  <span class="s2">return </span><span class="s1">_dot_general_shape_computation(lhs.shape</span><span class="s2">, </span><span class="s1">rhs.shape</span><span class="s2">, </span><span class="s1">dimension_numbers)</span>

<span class="s2">def </span><span class="s1">_dot_general_shape_computation(lhs_shape</span><span class="s2">, </span><span class="s1">rhs_shape</span><span class="s2">, </span><span class="s1">dimension_numbers):</span>
  <span class="s1">(lhs_contracting</span><span class="s2">, </span><span class="s1">rhs_contracting)</span><span class="s2">, </span><span class="s1">(lhs_batch</span><span class="s2">, </span><span class="s1">rhs_batch) = dimension_numbers</span>
  <span class="s1">batch_shape = tuple(lhs_shape[i] </span><span class="s2">for </span><span class="s1">i </span><span class="s2">in </span><span class="s1">lhs_batch)</span>
  <span class="s1">lhs_contract_or_batch = tuple(sorted(tuple(lhs_contracting) + tuple(lhs_batch)))</span>
  <span class="s1">lhs_tensored_shape = tuple_delete(lhs_shape</span><span class="s2">, </span><span class="s1">lhs_contract_or_batch)</span>
  <span class="s1">rhs_contract_or_batch = tuple(sorted(tuple(rhs_contracting) + tuple(rhs_batch)))</span>
  <span class="s1">rhs_tensored_shape = tuple_delete(rhs_shape</span><span class="s2">, </span><span class="s1">rhs_contract_or_batch)</span>
  <span class="s2">return </span><span class="s1">batch_shape + lhs_tensored_shape + rhs_tensored_shape</span>

<span class="s2">def </span><span class="s1">tuple_delete(tup</span><span class="s2">, </span><span class="s1">idx):</span>
  <span class="s1">idx_ = set(idx)</span>
  <span class="s2">return </span><span class="s1">tuple(tup[i] </span><span class="s2">for </span><span class="s1">i </span><span class="s2">in </span><span class="s1">range(len(tup)) </span><span class="s2">if </span><span class="s1">i </span><span class="s2">not in </span><span class="s1">idx_)</span>


<span class="s2">def </span><span class="s1">_dot_general_dtype_rule(lhs</span><span class="s2">, </span><span class="s1">rhs</span><span class="s2">, </span><span class="s1">*</span><span class="s2">, </span><span class="s1">dimension_numbers</span><span class="s2">, </span><span class="s1">precision</span><span class="s2">,</span>
                            <span class="s1">preferred_element_type: Optional[DTypeLike]):</span>
  <span class="s1">input_dtype = naryop_dtype_rule(_input_dtype</span><span class="s2">, </span><span class="s1">[_any</span><span class="s2">, </span><span class="s1">_any]</span><span class="s2">, </span><span class="s3">'dot_general'</span><span class="s2">, </span><span class="s1">lhs</span><span class="s2">, </span><span class="s1">rhs)</span>
  <span class="s2">if </span><span class="s1">preferred_element_type </span><span class="s2">is None</span><span class="s1">:</span>
    <span class="s2">return </span><span class="s1">input_dtype</span>
  <span class="s1">_validate_preferred_element_type(input_dtype</span><span class="s2">, </span><span class="s1">preferred_element_type)</span>
  <span class="s2">return </span><span class="s1">preferred_element_type</span>

<span class="s2">def </span><span class="s1">_dot_general_transpose_lhs(g</span><span class="s2">, </span><span class="s1">x</span><span class="s2">, </span><span class="s1">y</span><span class="s2">, </span><span class="s1">*</span><span class="s2">, </span><span class="s1">dimension_numbers</span><span class="s2">, </span><span class="s1">precision</span><span class="s2">,</span>
                               <span class="s1">preferred_element_type: Optional[DTypeLike]</span><span class="s2">,</span>
                               <span class="s1">swap_ans=</span><span class="s2">False</span><span class="s1">):</span>
  <span class="s1">(x_contract</span><span class="s2">, </span><span class="s1">y_contract)</span><span class="s2">, </span><span class="s1">(x_batch</span><span class="s2">, </span><span class="s1">y_batch) = dimension_numbers</span>
  <span class="s1">x_ndim = x.aval.ndim</span>
  <span class="s1">x_kept = remaining(range(x_ndim)</span><span class="s2">, </span><span class="s1">x_contract</span><span class="s2">, </span><span class="s1">x_batch)</span>
  <span class="s1">y_kept = remaining(range(y.ndim)</span><span class="s2">, </span><span class="s1">y_contract</span><span class="s2">, </span><span class="s1">y_batch)</span>
  <span class="s2">if </span><span class="s1">swap_ans:</span>
    <span class="s1">ans_batch</span><span class="s2">, </span><span class="s1">ans_y</span><span class="s2">, </span><span class="s1">_ = ranges_like(x_batch</span><span class="s2">, </span><span class="s1">y_kept</span><span class="s2">, </span><span class="s1">x_kept)</span>
  <span class="s2">else</span><span class="s1">:</span>
    <span class="s1">ans_batch</span><span class="s2">, </span><span class="s1">_</span><span class="s2">, </span><span class="s1">ans_y = ranges_like(x_batch</span><span class="s2">, </span><span class="s1">x_kept</span><span class="s2">, </span><span class="s1">y_kept)</span>
  <span class="s1">dims = ((ans_y</span><span class="s2">, </span><span class="s1">y_kept)</span><span class="s2">, </span><span class="s1">(ans_batch</span><span class="s2">, </span><span class="s1">y_batch))</span>
  <span class="s1">x_contract_sorted_by_y = list(np.take(x_contract</span><span class="s2">, </span><span class="s1">np.argsort(y_contract)))  </span><span class="s0"># type: ignore[arg-type]</span>
  <span class="s1">out_axes = np.argsort(list(x_batch) + x_kept + x_contract_sorted_by_y)</span>
  <span class="s2">return </span><span class="s1">transpose(dot_general(g</span><span class="s2">, </span><span class="s1">y</span><span class="s2">, </span><span class="s1">dims</span><span class="s2">, </span><span class="s1">precision=precision</span><span class="s2">,</span>
                               <span class="s1">preferred_element_type=preferred_element_type)</span><span class="s2">,</span>
                   <span class="s1">tuple(out_axes))</span>

<span class="s2">def </span><span class="s1">_dot_general_transpose_rhs(g</span><span class="s2">, </span><span class="s1">x</span><span class="s2">, </span><span class="s1">y</span><span class="s2">, </span><span class="s1">*</span><span class="s2">, </span><span class="s1">dimension_numbers</span><span class="s2">, </span><span class="s1">precision</span><span class="s2">,</span>
                               <span class="s1">preferred_element_type: Optional[DTypeLike]):</span>
  <span class="s1">(x_contract</span><span class="s2">, </span><span class="s1">y_contract)</span><span class="s2">, </span><span class="s1">(x_batch</span><span class="s2">, </span><span class="s1">y_batch) = dimension_numbers</span>
  <span class="s1">swapped_dimension_numbers = ((y_contract</span><span class="s2">, </span><span class="s1">x_contract)</span><span class="s2">, </span><span class="s1">(y_batch</span><span class="s2">, </span><span class="s1">x_batch))</span>
  <span class="s2">return </span><span class="s1">_dot_general_transpose_lhs(</span>
    <span class="s1">g</span><span class="s2">, </span><span class="s1">y</span><span class="s2">, </span><span class="s1">x</span><span class="s2">, </span><span class="s1">dimension_numbers=swapped_dimension_numbers</span><span class="s2">, </span><span class="s1">precision=precision</span><span class="s2">,</span>
    <span class="s1">preferred_element_type=preferred_element_type</span><span class="s2">,</span>
    <span class="s1">swap_ans=</span><span class="s2">True</span><span class="s1">)</span>


<span class="s2">def </span><span class="s1">_dot_general_batch_rule(batched_args</span><span class="s2">, </span><span class="s1">batch_dims</span><span class="s2">, </span><span class="s1">*</span><span class="s2">, </span><span class="s1">dimension_numbers</span><span class="s2">,</span>
                            <span class="s1">precision</span><span class="s2">,</span>
                            <span class="s1">preferred_element_type: Optional[DTypeLike]):</span>
  <span class="s1">lhs</span><span class="s2">, </span><span class="s1">rhs = batched_args</span>
  <span class="s1">lbd</span><span class="s2">, </span><span class="s1">rbd = batch_dims</span>
  <span class="s1">(lhs_contract</span><span class="s2">, </span><span class="s1">rhs_contract)</span><span class="s2">, </span><span class="s1">(lhs_batch</span><span class="s2">, </span><span class="s1">rhs_batch) = dimension_numbers</span>
  <span class="s2">if </span><span class="s1">(type(lbd) </span><span class="s2">is </span><span class="s1">type(rbd) </span><span class="s2">is </span><span class="s1">ConcatAxis </span><span class="s2">and</span>
      <span class="s1">lbd.axis </span><span class="s2">in </span><span class="s1">lhs_contract </span><span class="s2">and </span><span class="s1">rbd.axis </span><span class="s2">in </span><span class="s1">rhs_contract):</span>
    <span class="s0"># first handle any other part of the dot with these as batch dims</span>
    <span class="s1">lhs_contract_ = [d </span><span class="s2">for </span><span class="s1">d </span><span class="s2">in </span><span class="s1">lhs_contract </span><span class="s2">if </span><span class="s1">d != lbd.axis]</span>
    <span class="s1">rhs_contract_ = [d </span><span class="s2">for </span><span class="s1">d </span><span class="s2">in </span><span class="s1">rhs_contract </span><span class="s2">if </span><span class="s1">d != rbd.axis]</span>
    <span class="s1">lhs_batch_ = (lbd.axis</span><span class="s2">, </span><span class="s1">*lhs_batch)</span>
    <span class="s1">rhs_batch_ = (rbd.axis</span><span class="s2">, </span><span class="s1">*rhs_batch)</span>
    <span class="s1">new_dnums = ((lhs_contract_</span><span class="s2">, </span><span class="s1">rhs_contract_)</span><span class="s2">, </span><span class="s1">(lhs_batch_</span><span class="s2">, </span><span class="s1">rhs_batch_))</span>
    <span class="s1">out = dot_general(lhs</span><span class="s2">, </span><span class="s1">rhs</span><span class="s2">, </span><span class="s1">new_dnums</span><span class="s2">, </span><span class="s1">precision=precision</span><span class="s2">,</span>
                      <span class="s1">preferred_element_type=preferred_element_type)</span>
    <span class="s0"># now a segment sum along that batch axis</span>
    <span class="s2">return </span><span class="s1">batching.segment_sum(out</span><span class="s2">, </span><span class="s1">lbd.segment_lengths)</span><span class="s2">, </span><span class="s4">0</span>

  <span class="s1">new_dimension_numbers</span><span class="s2">, </span><span class="s1">result_batch_dim = _dot_general_batch_dim_nums(</span>
      <span class="s1">(lhs.ndim</span><span class="s2">, </span><span class="s1">rhs.ndim)</span><span class="s2">, </span><span class="s1">batch_dims</span><span class="s2">, </span><span class="s1">dimension_numbers)</span>
  <span class="s1">batched_out = dot_general(lhs</span><span class="s2">, </span><span class="s1">rhs</span><span class="s2">, </span><span class="s1">new_dimension_numbers</span><span class="s2">,</span>
                            <span class="s1">precision=precision</span><span class="s2">,</span>
                            <span class="s1">preferred_element_type=preferred_element_type)</span>
  <span class="s2">return </span><span class="s1">batched_out</span><span class="s2">, </span><span class="s1">result_batch_dim</span>

<span class="s2">def </span><span class="s1">_dot_general_batch_dim_nums(ndims</span><span class="s2">, </span><span class="s1">batch_dims</span><span class="s2">, </span><span class="s1">dimension_numbers):</span>
  <span class="s0"># there are three kinds of dimensions in a dot_general:</span>
  <span class="s0"># - contraction dimensions appear in lhs and rhs but not the result</span>
  <span class="s0"># - batch dimensions appear in lhs, rhs, and result</span>
  <span class="s0"># - tensor product dimensions appear in the result and one of lhs or rhs</span>
  <span class="s1">lhs_ndim</span><span class="s2">, </span><span class="s1">rhs_ndim = ndims</span>
  <span class="s1">lbd</span><span class="s2">, </span><span class="s1">rbd = batch_dims</span>
  <span class="s2">assert </span><span class="s1">lbd </span><span class="s2">is not None or </span><span class="s1">rbd </span><span class="s2">is not None</span>
  <span class="s1">(lhs_contract</span><span class="s2">, </span><span class="s1">rhs_contract)</span><span class="s2">, </span><span class="s1">(lhs_batch</span><span class="s2">, </span><span class="s1">rhs_batch) = dimension_numbers</span>

  <span class="s2">def </span><span class="s1">bump_dims(dims</span><span class="s2">, </span><span class="s1">b):</span>
    <span class="s2">return </span><span class="s1">tuple(np.add(dims</span><span class="s2">, </span><span class="s1">np.greater_equal(dims</span><span class="s2">, </span><span class="s1">b)))</span>

  <span class="s2">if </span><span class="s1">type(lbd) </span><span class="s2">is </span><span class="s1">type(rbd) </span><span class="s2">is </span><span class="s1">int:</span>
    <span class="s0"># adding a batch dimension</span>
    <span class="s1">lhs_batch = (lbd</span><span class="s2">,</span><span class="s1">) + bump_dims(lhs_batch</span><span class="s2">, </span><span class="s1">lbd)</span>
    <span class="s1">rhs_batch = (rbd</span><span class="s2">,</span><span class="s1">) + bump_dims(rhs_batch</span><span class="s2">, </span><span class="s1">rbd)</span>
    <span class="s1">lhs_contract = bump_dims(lhs_contract</span><span class="s2">, </span><span class="s1">lbd)</span>
    <span class="s1">rhs_contract = bump_dims(rhs_contract</span><span class="s2">, </span><span class="s1">rbd)</span>
    <span class="s1">result_batch_dim = </span><span class="s4">0</span>
  <span class="s2">elif </span><span class="s1">rbd </span><span class="s2">is None and </span><span class="s1">type(lbd) </span><span class="s2">is </span><span class="s1">ConcatAxis </span><span class="s2">and </span><span class="s1">lbd.axis </span><span class="s2">not in </span><span class="s1">lhs_contract:</span>
    <span class="s2">if </span><span class="s1">lbd.axis </span><span class="s2">in </span><span class="s1">lhs_batch:</span>
      <span class="s1">axis = int(np.sum(np.less(lhs_batch</span><span class="s2">, </span><span class="s1">lbd.axis)))</span>
    <span class="s2">else</span><span class="s1">:</span>
      <span class="s1">lhs_tensor = [d </span><span class="s2">for </span><span class="s1">d </span><span class="s2">in </span><span class="s1">range(lhs_ndim)</span>
                    <span class="s2">if </span><span class="s1">d </span><span class="s2">not in </span><span class="s1">lhs_batch </span><span class="s2">and </span><span class="s1">d </span><span class="s2">not in </span><span class="s1">lhs_contract]</span>
      <span class="s1">axis = len(lhs_batch) + int(np.sum(np.less(lhs_tensor</span><span class="s2">, </span><span class="s1">lbd.axis)))</span>
    <span class="s1">result_batch_dim = ConcatAxis(axis</span><span class="s2">, </span><span class="s1">lbd.segment_lengths)</span>
  <span class="s2">elif </span><span class="s1">lbd </span><span class="s2">is None and </span><span class="s1">type(rbd) </span><span class="s2">is </span><span class="s1">ConcatAxis </span><span class="s2">and </span><span class="s1">rbd.axis </span><span class="s2">not in </span><span class="s1">rhs_contract:</span>
    <span class="s2">if </span><span class="s1">rbd.axis </span><span class="s2">in </span><span class="s1">rhs_batch:</span>
      <span class="s1">axis = int(np.sum(np.less(rhs_batch</span><span class="s2">, </span><span class="s1">rbd.axis)))</span>
    <span class="s2">else</span><span class="s1">:</span>
      <span class="s1">rhs_tensor = [d </span><span class="s2">for </span><span class="s1">d </span><span class="s2">in </span><span class="s1">range(rhs_ndim)</span>
                    <span class="s2">if </span><span class="s1">d </span><span class="s2">not in </span><span class="s1">rhs_batch </span><span class="s2">and </span><span class="s1">d </span><span class="s2">not in </span><span class="s1">rhs_contract]</span>
      <span class="s1">axis = (lhs_ndim - len(lhs_contract) +</span>
              <span class="s1">int(sum(np.less(rhs_tensor</span><span class="s2">, </span><span class="s1">rbd.axis))))</span>
    <span class="s1">result_batch_dim = ConcatAxis(axis</span><span class="s2">, </span><span class="s1">rbd.segment_lengths)</span>
  <span class="s2">elif </span><span class="s1">(type(lbd) </span><span class="s2">is </span><span class="s1">int </span><span class="s2">and</span>
        <span class="s1">(rbd </span><span class="s2">is None or </span><span class="s1">type(rbd) </span><span class="s2">is </span><span class="s1">ConcatAxis </span><span class="s2">and</span>
         <span class="s1">rbd.axis </span><span class="s2">not in </span><span class="s1">rhs_contract)):</span>
    <span class="s1">lhs_tensor = [d </span><span class="s2">for </span><span class="s1">d </span><span class="s2">in </span><span class="s1">range(lhs_ndim)</span>
                  <span class="s2">if </span><span class="s1">d </span><span class="s2">not in </span><span class="s1">lhs_batch </span><span class="s2">and </span><span class="s1">d </span><span class="s2">not in </span><span class="s1">lhs_contract]</span>
    <span class="s1">result_batch_dim = len(lhs_batch) + int(sum(np.less(lhs_tensor</span><span class="s2">, </span><span class="s1">lbd)))</span>
    <span class="s1">lhs_batch = bump_dims(lhs_batch</span><span class="s2">, </span><span class="s1">lbd)</span>
    <span class="s1">lhs_contract = bump_dims(lhs_contract</span><span class="s2">, </span><span class="s1">lbd)</span>
  <span class="s2">elif </span><span class="s1">(type(rbd) </span><span class="s2">is </span><span class="s1">int </span><span class="s2">and</span>
        <span class="s1">(lbd </span><span class="s2">is None or </span><span class="s1">type(lbd) </span><span class="s2">is </span><span class="s1">ConcatAxis </span><span class="s2">and</span>
         <span class="s1">lbd.axis </span><span class="s2">not in </span><span class="s1">lhs_contract)):</span>
    <span class="s1">rhs_tensor = [d </span><span class="s2">for </span><span class="s1">d </span><span class="s2">in </span><span class="s1">range(rhs_ndim)</span>
                  <span class="s2">if </span><span class="s1">d </span><span class="s2">not in </span><span class="s1">rhs_batch </span><span class="s2">and </span><span class="s1">d </span><span class="s2">not in </span><span class="s1">rhs_contract]</span>
    <span class="s1">result_batch_dim = (lhs_ndim - len(lhs_contract) +</span>
                        <span class="s1">int(sum(np.less(rhs_tensor</span><span class="s2">, </span><span class="s1">rbd))))</span>
    <span class="s1">rhs_batch = bump_dims(rhs_batch</span><span class="s2">, </span><span class="s1">rbd)</span>
    <span class="s1">rhs_contract = bump_dims(rhs_contract</span><span class="s2">, </span><span class="s1">rbd)</span>
  <span class="s2">else</span><span class="s1">:</span>
    <span class="s2">assert False</span>

  <span class="s1">new_dimension_numbers = ((lhs_contract</span><span class="s2">, </span><span class="s1">rhs_contract)</span><span class="s2">, </span><span class="s1">(lhs_batch</span><span class="s2">, </span><span class="s1">rhs_batch))</span>
  <span class="s2">return </span><span class="s1">new_dimension_numbers</span><span class="s2">, </span><span class="s1">result_batch_dim</span>

<span class="s2">def </span><span class="s1">_dot_general_padding_rule(in_avals</span><span class="s2">, </span><span class="s1">out_avals</span><span class="s2">, </span><span class="s1">lhs</span><span class="s2">, </span><span class="s1">rhs</span><span class="s2">, </span><span class="s1">*</span><span class="s2">,</span>
                              <span class="s1">dimension_numbers</span><span class="s2">, </span><span class="s1">**params):</span>
  <span class="s1">lhs_aval</span><span class="s2">, </span><span class="s1">_ = in_avals</span>
  <span class="s1">(lhs_contract</span><span class="s2">, </span><span class="s1">_)</span><span class="s2">, </span><span class="s1">_ = dimension_numbers</span>
  <span class="s1">padded_axes = [(i</span><span class="s2">, </span><span class="s1">lhs_aval.shape[i].val) </span><span class="s2">for </span><span class="s1">i </span><span class="s2">in </span><span class="s1">lhs_contract</span>
                 <span class="s2">if </span><span class="s1">isinstance(lhs_aval.shape[i]</span><span class="s2">, </span><span class="s1">pe.BoundedAxisSize)]</span>
  <span class="s1">lhs_ = _replace_masked_values(lhs</span><span class="s2">, </span><span class="s4">0</span><span class="s2">, </span><span class="s1">padded_axes)</span>
  <span class="s2">return </span><span class="s1">[dot_general(lhs_</span><span class="s2">, </span><span class="s1">rhs</span><span class="s2">, </span><span class="s1">dimension_numbers=dimension_numbers</span><span class="s2">, </span><span class="s1">**params)]</span>

<span class="s2">def </span><span class="s1">_dot_general_pp_rule(eqn</span><span class="s2">, </span><span class="s1">context</span><span class="s2">, </span><span class="s1">settings) -&gt; pp.Doc:</span>
  <span class="s0"># * suppress printing precision or preferred_element_type when None.</span>
  <span class="s0"># * print dimension_numbers as list-of-lists to be shorter.</span>
  <span class="s1">printed_params = {k: v </span><span class="s2">for </span><span class="s1">k</span><span class="s2">, </span><span class="s1">v </span><span class="s2">in </span><span class="s1">eqn.params.items() </span><span class="s2">if </span><span class="s1">v </span><span class="s2">is not None</span><span class="s1">}</span>
  <span class="s1">(lhs_cont</span><span class="s2">, </span><span class="s1">rhs_cont)</span><span class="s2">, </span><span class="s1">(lhs_batch</span><span class="s2">, </span><span class="s1">rhs_batch) = eqn.params[</span><span class="s3">'dimension_numbers'</span><span class="s1">]</span>
  <span class="s1">printed_params[</span><span class="s3">'dimension_numbers'</span><span class="s1">] = (</span>
      <span class="s1">(list(lhs_cont)</span><span class="s2">, </span><span class="s1">list(rhs_cont))</span><span class="s2">, </span><span class="s1">(list(lhs_batch)</span><span class="s2">, </span><span class="s1">list(rhs_batch)))</span>
  <span class="s2">return </span><span class="s1">core._pp_eqn(eqn.replace(params=printed_params)</span><span class="s2">, </span><span class="s1">context</span><span class="s2">, </span><span class="s1">settings)</span>

<span class="s1">dot_general_p = standard_primitive(_dot_general_shape_rule</span><span class="s2">,</span>
                                   <span class="s1">_dot_general_dtype_rule</span><span class="s2">, </span><span class="s3">'dot_general'</span><span class="s1">)</span>
<span class="s1">ad.defbilinear(dot_general_p</span><span class="s2">,</span>
               <span class="s1">_dot_general_transpose_lhs</span><span class="s2">, </span><span class="s1">_dot_general_transpose_rhs)</span>
<span class="s1">batching.primitive_batchers[dot_general_p] = _dot_general_batch_rule</span>
<span class="s1">pe.padding_rules[dot_general_p] = _dot_general_padding_rule</span>
<span class="s1">core.pp_eqn_rules[dot_general_p] = _dot_general_pp_rule</span>

<span class="s2">def </span><span class="s1">precision_attr(precision: PrecisionType) -&gt; ir.ArrayAttr:</span>
  <span class="s2">if </span><span class="s1">precision </span><span class="s2">is None</span><span class="s1">:</span>
    <span class="s1">full_precision = (Precision.DEFAULT</span><span class="s2">, </span><span class="s1">Precision.DEFAULT)</span>
  <span class="s2">elif not </span><span class="s1">isinstance(precision</span><span class="s2">, </span><span class="s1">tuple):</span>
    <span class="s1">full_precision = (precision</span><span class="s2">, </span><span class="s1">precision)</span>
  <span class="s2">else</span><span class="s1">:</span>
    <span class="s1">full_precision = precision</span>
  <span class="s2">return </span><span class="s1">ir.ArrayAttr.get(</span>
      <span class="s1">[hlo.PrecisionAttr.get(str(p)) </span><span class="s2">for </span><span class="s1">p </span><span class="s2">in </span><span class="s1">full_precision])</span>


<span class="s2">def </span><span class="s1">_dot_general_lower(ctx</span><span class="s2">, </span><span class="s1">lhs</span><span class="s2">, </span><span class="s1">rhs</span><span class="s2">, </span><span class="s1">*</span><span class="s2">, </span><span class="s1">dimension_numbers</span><span class="s2">,</span>
                       <span class="s1">precision</span><span class="s2">, </span><span class="s1">preferred_element_type: Optional[np.dtype]):</span>
  <span class="s2">del </span><span class="s1">preferred_element_type  </span><span class="s0"># Implied by the output aval</span>
  <span class="s1">lhs_aval</span><span class="s2">, </span><span class="s1">rhs_aval = ctx.avals_in</span>
  <span class="s1">aval_out</span><span class="s2">, </span><span class="s1">= ctx.avals_out</span>
  <span class="s1">(lhs_contracting</span><span class="s2">, </span><span class="s1">rhs_contracting)</span><span class="s2">, </span><span class="s1">(lhs_batch</span><span class="s2">, </span><span class="s1">rhs_batch) = dimension_numbers</span>

  <span class="s0"># TODO(b/195364460): Work around slow XLA/CPU implementation of float16 matmul</span>
  <span class="s2">if </span><span class="s1">ctx.module_context.platform == </span><span class="s3">&quot;cpu&quot;</span><span class="s1">:</span>
    <span class="s2">if </span><span class="s1">lhs_aval.dtype == np.float16:</span>
      <span class="s1">f32 = mlir.dtype_to_ir_type(np.dtype(np.float32))</span>
      <span class="s1">lhs = hlo.ConvertOp(ir.RankedTensorType.get(lhs_aval.shape</span><span class="s2">, </span><span class="s1">f32)</span><span class="s2">,</span>
                          <span class="s1">lhs).result</span>
    <span class="s2">if </span><span class="s1">rhs_aval.dtype == np.float16:</span>
      <span class="s1">f32 = mlir.dtype_to_ir_type(np.dtype(np.float32))</span>
      <span class="s1">rhs = hlo.ConvertOp(ir.RankedTensorType.get(rhs_aval.shape</span><span class="s2">, </span><span class="s1">f32)</span><span class="s2">,</span>
                          <span class="s1">rhs).result</span>
  <span class="s1">dot_dnums = hlo.DotDimensionNumbers.get(</span>
      <span class="s1">lhs_batching_dimensions=list(lhs_batch)</span><span class="s2">,</span>
      <span class="s1">rhs_batching_dimensions=list(rhs_batch)</span><span class="s2">,</span>
      <span class="s1">lhs_contracting_dimensions=list(lhs_contracting)</span><span class="s2">,</span>
      <span class="s1">rhs_contracting_dimensions=list(rhs_contracting))</span>
  <span class="s2">return </span><span class="s1">[</span>
      <span class="s1">hlo.DotGeneralOp(</span>
          <span class="s1">mlir.aval_to_ir_type(aval_out)</span><span class="s2">,</span>
          <span class="s1">lhs</span><span class="s2">,</span>
          <span class="s1">rhs</span><span class="s2">,</span>
          <span class="s1">dot_dnums</span><span class="s2">,</span>
          <span class="s1">precision_config=precision_attr(precision)).result</span>
  <span class="s1">]</span>

<span class="s1">mlir.register_lowering(dot_general_p</span><span class="s2">, </span><span class="s1">_dot_general_lower)</span>


<span class="s2">def </span><span class="s1">_broadcast_in_dim_shape_rule(operand</span><span class="s2">, </span><span class="s1">*</span><span class="s2">, </span><span class="s1">shape</span><span class="s2">, </span><span class="s1">broadcast_dimensions):</span>
  <span class="s1">_check_shapelike(</span><span class="s3">'broadcast_in_dim'</span><span class="s2">, </span><span class="s3">'shape'</span><span class="s2">, </span><span class="s1">shape)</span>
  <span class="s1">_check_shapelike(</span><span class="s3">'broadcast_in_dim'</span><span class="s2">, </span><span class="s3">'broadcast_dimensions'</span><span class="s2">,</span>
                   <span class="s1">broadcast_dimensions)</span>
  <span class="s1">operand_ndim = np.ndim(operand)</span>
  <span class="s2">if </span><span class="s1">operand_ndim != len(broadcast_dimensions):</span>
    <span class="s1">msg = (</span><span class="s3">'broadcast_in_dim broadcast_dimensions must have length equal to '</span>
           <span class="s3">'operand ndim; got broadcast_dimensions {} for operand ndim {}.'</span><span class="s1">)</span>
    <span class="s2">raise </span><span class="s1">TypeError(msg.format(broadcast_dimensions</span><span class="s2">, </span><span class="s1">operand_ndim))</span>
  <span class="s2">if </span><span class="s1">len(shape) &lt; operand_ndim:</span>
    <span class="s1">msg = (</span><span class="s3">'broadcast_in_dim target broadcast shape must have equal or higher rank '</span>
           <span class="s3">'to the operand shape; got operand ndim {} and target broadcast ndim {}.'</span><span class="s1">)</span>
    <span class="s2">raise </span><span class="s1">TypeError(msg.format(operand_ndim</span><span class="s2">, </span><span class="s1">len(shape)))</span>
  <span class="s2">if not </span><span class="s1">set(broadcast_dimensions).issubset(set(range(len(shape)))):</span>
    <span class="s1">msg = (</span><span class="s3">'broadcast_in_dim broadcast_dimensions must be a subset of output '</span>
           <span class="s3">'dimensions, got {} for operand ndim {} and shape {}.'</span><span class="s1">)</span>
    <span class="s2">raise </span><span class="s1">TypeError(msg.format(broadcast_dimensions</span><span class="s2">, </span><span class="s1">operand_ndim</span><span class="s2">, </span><span class="s1">shape))</span>
  <span class="s2">if not </span><span class="s1">all(core.symbolic_equal_one_of_dim(operand.shape[i]</span><span class="s2">,</span>
                                            <span class="s1">[</span><span class="s4">1</span><span class="s2">, </span><span class="s1">shape[broadcast_dimensions[i]]])</span>
             <span class="s2">for </span><span class="s1">i </span><span class="s2">in </span><span class="s1">range(operand_ndim)):</span>
    <span class="s1">msg = (</span>
        <span class="s3">&quot;broadcast_in_dim operand dimension sizes must either be 1, or be &quot;</span>
        <span class="s3">&quot;equal to their corresponding dimensions in the target broadcast &quot;</span>
        <span class="s3">&quot;shape; got operand of shape {}, target broadcast shape {}, &quot;</span>
        <span class="s3">&quot;broadcast_dimensions {} &quot;</span><span class="s1">)</span>
    <span class="s2">raise </span><span class="s1">TypeError(msg.format(operand.shape</span><span class="s2">, </span><span class="s1">shape</span><span class="s2">, </span><span class="s1">broadcast_dimensions))</span>
  <span class="s2">if </span><span class="s1">(len(broadcast_dimensions) != len(set(broadcast_dimensions)) </span><span class="s2">or</span>
      <span class="s1">tuple(broadcast_dimensions) != tuple(sorted(broadcast_dimensions))):</span>
    <span class="s1">msg = (</span><span class="s3">&quot;broadcast_in_dim broadcast_dimensions must be strictly increasing; &quot;</span>
           <span class="s3">&quot;got broadcast_dimensions {}&quot;</span><span class="s1">)</span>
    <span class="s2">raise </span><span class="s1">TypeError(msg.format(broadcast_dimensions))</span>

  <span class="s2">return </span><span class="s1">shape</span>

<span class="s2">def </span><span class="s1">_broadcast_in_dim_typecheck_rule(</span>
    <span class="s1">_</span><span class="s2">, </span><span class="s1">operand</span><span class="s2">, </span><span class="s1">*dyn_shape</span><span class="s2">, </span><span class="s1">shape</span><span class="s2">, </span><span class="s1">broadcast_dimensions):</span>
  <span class="s2">if not </span><span class="s1">dyn_shape:</span>
    <span class="s1">out_aval</span><span class="s2">, </span><span class="s1">effects = broadcast_in_dim_p.abstract_eval(</span>
        <span class="s1">operand.aval</span><span class="s2">, </span><span class="s1">shape=shape</span><span class="s2">, </span><span class="s1">broadcast_dimensions=broadcast_dimensions)</span>
    <span class="s2">return </span><span class="s1">[out_aval]</span><span class="s2">, </span><span class="s1">effects</span>
  <span class="s2">else</span><span class="s1">:</span>
    <span class="s0"># TODO(mattjj): perform more checks like _broadcast_in_dim_shape_rule</span>
    <span class="s1">out_shape = _merge_dyn_shape(shape</span><span class="s2">, </span><span class="s1">dyn_shape)</span>
    <span class="s1">out_shape = [x.val </span><span class="s2">if </span><span class="s1">type(x) </span><span class="s2">is </span><span class="s1">core.Literal </span><span class="s2">else </span><span class="s1">x </span><span class="s2">for </span><span class="s1">x </span><span class="s2">in </span><span class="s1">out_shape]  </span><span class="s0"># pytype: disable=attribute-error</span>
    <span class="s1">out_aval = core.DShapedArray(tuple(out_shape)</span><span class="s2">, </span><span class="s1">operand.aval.dtype</span><span class="s2">,</span>
                                 <span class="s1">operand.aval.weak_type)</span>
    <span class="s2">return </span><span class="s1">[out_aval]</span><span class="s2">, </span><span class="s1">core.no_effects</span>

<span class="s2">def </span><span class="s1">_broadcast_in_dim_transpose_rule(ct</span><span class="s2">, </span><span class="s1">operand</span><span class="s2">, </span><span class="s1">*dyn_shape</span><span class="s2">,</span>
                                     <span class="s1">shape</span><span class="s2">, </span><span class="s1">broadcast_dimensions):</span>
  <span class="s2">if </span><span class="s1">type(ct) </span><span class="s2">is </span><span class="s1">ad_util.Zero:</span>
    <span class="s2">return </span><span class="s1">[ad_util.Zero(operand.aval)]</span>
  <span class="s1">unit_dims = [i </span><span class="s2">for </span><span class="s1">i</span><span class="s2">, </span><span class="s1">s </span><span class="s2">in </span><span class="s1">enumerate(operand.aval.shape)</span>
               <span class="s2">if </span><span class="s1">core.symbolic_equal_dim(s</span><span class="s2">,  </span><span class="s4">1</span><span class="s1">)]</span>
  <span class="s1">bdims = tuple(np.delete(broadcast_dimensions</span><span class="s2">, </span><span class="s1">unit_dims))</span>
  <span class="s1">axes = tuple(np.delete(range(len(shape))</span><span class="s2">, </span><span class="s1">bdims))</span>
  <span class="s2">return </span><span class="s1">([expand_dims(_reduce_sum(ct</span><span class="s2">, </span><span class="s1">axes)</span><span class="s2">, </span><span class="s1">unit_dims)] +</span>
          <span class="s1">[</span><span class="s2">None</span><span class="s1">] * len(dyn_shape))</span>

<span class="s2">def </span><span class="s1">_broadcast_in_dim_batch_rule(batched_args</span><span class="s2">, </span><span class="s1">batch_dims</span><span class="s2">, </span><span class="s1">shape</span><span class="s2">,</span>
                                 <span class="s1">broadcast_dimensions):</span>
  <span class="s1">operand</span><span class="s2">, </span><span class="s1">*dyn_shape = batched_args</span>
  <span class="s1">operand_bdim</span><span class="s2">, </span><span class="s1">*dyn_shape_bdims = batch_dims</span>
  <span class="s2">if </span><span class="s1">len(dyn_shape) &gt; </span><span class="s4">1</span><span class="s1">: </span><span class="s2">raise </span><span class="s1">NotImplementedError</span>
  <span class="s2">if </span><span class="s1">(operand_bdim </span><span class="s2">is not None and</span>
      <span class="s1">(</span><span class="s2">not </span><span class="s1">dyn_shape_bdims </span><span class="s2">or </span><span class="s1">dyn_shape_bdims[</span><span class="s4">0</span><span class="s1">] </span><span class="s2">is None</span><span class="s1">)):</span>
    <span class="s1">new_operand = batching.moveaxis(operand</span><span class="s2">, </span><span class="s1">operand_bdim</span><span class="s2">, </span><span class="s4">0</span><span class="s1">)</span>
    <span class="s1">new_shape = (operand.shape[operand_bdim]</span><span class="s2">,</span><span class="s1">) + _merge_dyn_shape(shape</span><span class="s2">, </span><span class="s1">dyn_shape)</span>
    <span class="s1">new_broadcast_dimensions = (</span><span class="s4">0</span><span class="s2">,</span><span class="s1">) + tuple(np.add(</span><span class="s4">1</span><span class="s2">, </span><span class="s1">broadcast_dimensions))</span>
    <span class="s2">return </span><span class="s1">broadcast_in_dim(new_operand</span><span class="s2">, </span><span class="s1">new_shape</span><span class="s2">, </span><span class="s1">new_broadcast_dimensions)</span><span class="s2">, </span><span class="s4">0</span>
  <span class="s2">elif </span><span class="s1">(operand_bdim </span><span class="s2">is None and </span><span class="s1">dyn_shape_bdims </span><span class="s2">and</span>
        <span class="s1">dyn_shape_bdims[</span><span class="s4">0</span><span class="s1">] </span><span class="s2">is not None</span><span class="s1">):</span>
    <span class="s1">(d</span><span class="s2">,</span><span class="s1">)</span><span class="s2">, </span><span class="s1">(d_bdim</span><span class="s2">,</span><span class="s1">) = dyn_shape</span><span class="s2">, </span><span class="s1">dyn_shape_bdims  </span><span class="s0"># NotImplementedError above</span>
    <span class="s2">assert </span><span class="s1">d_bdim == </span><span class="s4">0  </span><span class="s0"># must be scalar in the program to be batched</span>
    <span class="s1">new_shape = _merge_dyn_shape(shape</span><span class="s2">, </span><span class="s1">(int(d.sum())</span><span class="s2">,</span><span class="s1">))</span>
    <span class="s1">out = broadcast_in_dim(operand</span><span class="s2">, </span><span class="s1">new_shape</span><span class="s2">, </span><span class="s1">broadcast_dimensions)</span>
    <span class="s1">idx</span><span class="s2">, </span><span class="s1">= (i </span><span class="s2">for </span><span class="s1">i</span><span class="s2">, </span><span class="s1">s </span><span class="s2">in </span><span class="s1">enumerate(shape) </span><span class="s2">if </span><span class="s1">s </span><span class="s2">is None</span><span class="s1">)</span>
    <span class="s2">return </span><span class="s1">out</span><span class="s2">, </span><span class="s1">batching.ConcatAxis(idx</span><span class="s2">, </span><span class="s1">d)</span>
  <span class="s2">else</span><span class="s1">:</span>
    <span class="s2">raise </span><span class="s1">NotImplementedError  </span><span class="s0"># TODO(mattjj)</span>

<span class="s2">def </span><span class="s1">_broadcast_in_dim_fwd_rule(eqn):</span>
  <span class="s1">v</span><span class="s2">, </span><span class="s1">*dyn = eqn.invars</span>
  <span class="s2">if not </span><span class="s1">dyn </span><span class="s2">and </span><span class="s1">core.symbolic_equal_shape(eqn.params[</span><span class="s3">'shape'</span><span class="s1">]</span><span class="s2">, </span><span class="s1">v.aval.shape):</span>
    <span class="s2">return </span><span class="s1">[v]</span><span class="s2">, None</span>
  <span class="s2">else</span><span class="s1">:</span>
    <span class="s2">return </span><span class="s1">[</span><span class="s2">None</span><span class="s1">]</span><span class="s2">, </span><span class="s1">eqn</span>

<span class="s2">def </span><span class="s1">_broadcast_in_dim_staging_rule(</span>
    <span class="s1">trace</span><span class="s2">, </span><span class="s1">x</span><span class="s2">, </span><span class="s1">*dyn</span><span class="s2">, </span><span class="s1">shape</span><span class="s2">, </span><span class="s1">broadcast_dimensions):</span>
  <span class="s1">params = dict(shape=shape</span><span class="s2">, </span><span class="s1">broadcast_dimensions=broadcast_dimensions)</span>
  <span class="s2">if not </span><span class="s1">dyn:</span>
    <span class="s2">return </span><span class="s1">trace.default_process_primitive(broadcast_in_dim_p</span><span class="s2">, </span><span class="s1">(x</span><span class="s2">,</span><span class="s1">)</span><span class="s2">, </span><span class="s1">params)</span>
  <span class="s1">aval = core.DShapedArray(_merge_dyn_shape(shape</span><span class="s2">, </span><span class="s1">dyn)</span><span class="s2">, </span><span class="s1">x.dtype</span><span class="s2">, </span><span class="s1">x.weak_type)</span>
  <span class="s2">return </span><span class="s1">_dyn_shape_staging_rule(trace</span><span class="s2">, </span><span class="s1">broadcast_in_dim_p</span><span class="s2">, </span><span class="s1">aval</span><span class="s2">, </span><span class="s1">x</span><span class="s2">, </span><span class="s1">*dyn</span><span class="s2">,</span>
                                 <span class="s1">**params)</span>

<span class="s2">def </span><span class="s1">_broadcast_in_dim_padding_rule(in_avals</span><span class="s2">, </span><span class="s1">out_avals</span><span class="s2">, </span><span class="s1">x</span><span class="s2">, </span><span class="s1">*dyn_shape</span><span class="s2">,</span>
                                   <span class="s1">shape</span><span class="s2">, </span><span class="s1">broadcast_dimensions):</span>
  <span class="s2">del </span><span class="s1">in_avals</span><span class="s2">, </span><span class="s1">dyn_shape</span>
  <span class="s1">out_aval</span><span class="s2">, </span><span class="s1">= out_avals</span>
  <span class="s1">new_shape = []</span>
  <span class="s1">new_dyn_shape = []</span>
  <span class="s2">for </span><span class="s1">d </span><span class="s2">in </span><span class="s1">out_aval.shape:</span>
    <span class="s2">if </span><span class="s1">type(d) </span><span class="s2">is </span><span class="s1">pe.BoundedAxisSize:</span>
      <span class="s1">new_shape.append(d.bound)</span>
    <span class="s2">elif </span><span class="s1">type(d) </span><span class="s2">is </span><span class="s1">int:</span>
      <span class="s1">new_shape.append(d)</span>
    <span class="s2">else</span><span class="s1">:</span>
      <span class="s2">assert </span><span class="s1">isinstance(d</span><span class="s2">, </span><span class="s1">core.Tracer)</span>
      <span class="s1">new_shape.append(</span><span class="s2">None</span><span class="s1">)</span>
      <span class="s1">new_dyn_shape.append(d)</span>
  <span class="s2">return </span><span class="s1">[broadcast_in_dim_p.bind(x</span><span class="s2">, </span><span class="s1">*new_dyn_shape</span><span class="s2">, </span><span class="s1">shape=tuple(new_shape)</span><span class="s2">,</span>
                                  <span class="s1">broadcast_dimensions=broadcast_dimensions)]</span>

<span class="s2">def </span><span class="s1">_broadcast_in_dim_jvp_rule(primals</span><span class="s2">, </span><span class="s1">tangents</span><span class="s2">, </span><span class="s1">*</span><span class="s2">, </span><span class="s1">shape</span><span class="s2">, </span><span class="s1">broadcast_dimensions):</span>
  <span class="s1">operand</span><span class="s2">, </span><span class="s1">*dyn_shape = primals</span>
  <span class="s1">operand_dot</span><span class="s2">, </span><span class="s1">*_ = tangents</span>
  <span class="s1">y = broadcast_in_dim_p.bind(operand</span><span class="s2">, </span><span class="s1">*dyn_shape</span><span class="s2">, </span><span class="s1">shape=shape</span><span class="s2">,</span>
                              <span class="s1">broadcast_dimensions=broadcast_dimensions)</span>
  <span class="s2">if </span><span class="s1">type(operand_dot) </span><span class="s2">is </span><span class="s1">ad_util.Zero:</span>
    <span class="s1">y_dot = ad_util.Zero.from_value(y)</span>
  <span class="s2">else</span><span class="s1">:</span>
    <span class="s1">y_dot = broadcast_in_dim_p.bind(operand_dot</span><span class="s2">, </span><span class="s1">*dyn_shape</span><span class="s2">, </span><span class="s1">shape=shape</span><span class="s2">,</span>
                                    <span class="s1">broadcast_dimensions=broadcast_dimensions)</span>
  <span class="s2">return </span><span class="s1">y</span><span class="s2">, </span><span class="s1">y_dot</span>

<span class="s2">def </span><span class="s1">_broadcast_in_dim_partial_eval(</span>
    <span class="s1">trace</span><span class="s2">, </span><span class="s1">operand</span><span class="s2">, </span><span class="s1">*dyn_shape</span><span class="s2">, </span><span class="s1">shape</span><span class="s2">, </span><span class="s1">broadcast_dimensions):</span>
  <span class="s2">if not </span><span class="s1">dyn_shape:</span>
    <span class="s2">return </span><span class="s1">trace.default_process_primitive(</span>
        <span class="s1">broadcast_in_dim_p</span><span class="s2">, </span><span class="s1">(operand</span><span class="s2">, </span><span class="s1">*dyn_shape)</span><span class="s2">,</span>
        <span class="s1">dict(shape=shape</span><span class="s2">, </span><span class="s1">broadcast_dimensions=broadcast_dimensions))</span>
  <span class="s2">assert </span><span class="s1">all(t.pval.is_known() </span><span class="s2">for </span><span class="s1">t </span><span class="s2">in </span><span class="s1">dyn_shape)</span>
  <span class="s1">operand_tracer = trace.instantiate_const(operand)</span>
  <span class="s1">dyn_shape_tracers = map(trace.instantiate_const</span><span class="s2">, </span><span class="s1">dyn_shape)</span>
  <span class="s1">dyn_shape_tracers_ = iter(dyn_shape_tracers)</span>
  <span class="s1">shape_ = [next(dyn_shape_tracers_) </span><span class="s2">if </span><span class="s1">d </span><span class="s2">is None else </span><span class="s1">d </span><span class="s2">for </span><span class="s1">d </span><span class="s2">in </span><span class="s1">shape]</span>
  <span class="s1">out_aval = core.DShapedArray(tuple(shape_)</span><span class="s2">, </span><span class="s1">operand.dtype</span><span class="s2">, </span><span class="s1">operand.weak_type)</span>
  <span class="s1">out_tracer = pe.JaxprTracer(trace</span><span class="s2">, </span><span class="s1">pe.PartialVal.unknown(out_aval)</span><span class="s2">, None</span><span class="s1">)</span>
  <span class="s1">eqn = pe.new_eqn_recipe(</span>
      <span class="s1">[operand_tracer</span><span class="s2">, </span><span class="s1">*dyn_shape_tracers]</span><span class="s2">, </span><span class="s1">[out_tracer]</span><span class="s2">, </span><span class="s1">broadcast_in_dim_p</span><span class="s2">,</span>
      <span class="s1">dict(shape=shape</span><span class="s2">, </span><span class="s1">broadcast_dimensions=broadcast_dimensions)</span><span class="s2">,</span>
      <span class="s1">core.no_effects</span><span class="s2">, </span><span class="s1">source_info_util.current())</span>
  <span class="s1">out_tracer.recipe = eqn</span>
  <span class="s2">return </span><span class="s1">out_tracer</span>

<span class="s2">def </span><span class="s1">_broadcast_in_dim_lower(ctx</span><span class="s2">, </span><span class="s1">x</span><span class="s2">, </span><span class="s1">*dyn_shape</span><span class="s2">, </span><span class="s1">shape</span><span class="s2">, </span><span class="s1">broadcast_dimensions) -&gt; Sequence[ir.Value]:</span>
  <span class="s1">aval_out</span><span class="s2">, </span><span class="s1">= ctx.avals_out</span>
  <span class="s2">if </span><span class="s1">dyn_shape:</span>
    <span class="s1">aval_out = aval_out.update(shape=_merge_dyn_shape(shape</span><span class="s2">, </span><span class="s1">dyn_shape))</span>


  <span class="s2">return </span><span class="s1">[mlir.broadcast_in_dim(ctx</span><span class="s2">, </span><span class="s1">x</span><span class="s2">, </span><span class="s1">aval_out</span><span class="s2">,</span>
                                <span class="s1">broadcast_dimensions=broadcast_dimensions)]</span>

<span class="s2">def </span><span class="s1">_broadcast_in_dim_pp_rule(eqn</span><span class="s2">, </span><span class="s1">context</span><span class="s2">, </span><span class="s1">settings):</span>
  <span class="s0"># Don't print shape or trivial broadcast_dimensions in params, since it can be</span>
  <span class="s0"># inferred from the let-binder's type annotation.</span>
  <span class="s1">printed_params = {}</span>
  <span class="s2">if </span><span class="s1">eqn.params[</span><span class="s3">'broadcast_dimensions'</span><span class="s1">]:</span>
    <span class="s1">printed_params[</span><span class="s3">'broadcast_dimensions'</span><span class="s1">] = eqn.params[</span><span class="s3">'broadcast_dimensions'</span><span class="s1">]</span>
  <span class="s1">new_eqn = eqn.replpace(params=printed_params</span><span class="s2">, </span><span class="s1">invars=eqn.invars[:</span><span class="s4">1</span><span class="s1">])</span>
  <span class="s2">return </span><span class="s1">core._pp_eqn(new_eqn</span><span class="s2">, </span><span class="s1">context</span><span class="s2">, </span><span class="s1">settings)</span>

<span class="s2">def </span><span class="s1">_broadcast_in_dim_abstract_eval(x</span><span class="s2">, </span><span class="s1">*dyn_shape</span><span class="s2">, </span><span class="s1">shape</span><span class="s2">, </span><span class="s1">broadcast_dimensions):</span>
  <span class="s2">if </span><span class="s1">dyn_shape: </span><span class="s2">raise </span><span class="s1">NotImplementedError</span>
  <span class="s2">assert not </span><span class="s1">any(d </span><span class="s2">is None for </span><span class="s1">d </span><span class="s2">in </span><span class="s1">shape)  </span><span class="s0"># not implemented</span>
  <span class="s2">del </span><span class="s1">dyn_shape</span>
  <span class="s2">if not </span><span class="s1">any(isinstance(d</span><span class="s2">, </span><span class="s1">core.DArray) </span><span class="s2">and</span>
             <span class="s1">type(core.get_aval(d).dtype) </span><span class="s2">is </span><span class="s1">core.bint </span><span class="s2">for </span><span class="s1">d </span><span class="s2">in </span><span class="s1">shape):</span>
    <span class="s1">shape = _broadcast_in_dim_shape_rule(  </span><span class="s0"># error checking</span>
        <span class="s1">x</span><span class="s2">, </span><span class="s1">shape=shape</span><span class="s2">, </span><span class="s1">broadcast_dimensions=broadcast_dimensions)</span>
    <span class="s2">return </span><span class="s1">core.ShapedArray(shape</span><span class="s2">, </span><span class="s1">x.dtype</span><span class="s2">, </span><span class="s1">x.weak_type</span><span class="s2">, </span><span class="s1">x.named_shape)</span>
  <span class="s0"># If any BInts in shape, produce a DShapedArray (even if x is a ShapedArray)</span>
  <span class="s0"># TODO(mattjj): unify DShapedArray with ShapedArray, and remove this code</span>
  <span class="s2">return </span><span class="s1">core.DShapedArray(shape</span><span class="s2">, </span><span class="s1">x.dtype</span><span class="s2">, </span><span class="s1">x.weak_type)</span>

<span class="s1">broadcast_in_dim_p = standard_primitive(</span>
    <span class="s1">_broadcast_in_dim_shape_rule</span><span class="s2">, </span><span class="s1">_input_dtype</span><span class="s2">, </span><span class="s3">'broadcast_in_dim'</span><span class="s1">)</span>
<span class="s1">broadcast_in_dim_p.def_abstract_eval(_broadcast_in_dim_abstract_eval)</span>
<span class="s1">ad.primitive_jvps[broadcast_in_dim_p] = _broadcast_in_dim_jvp_rule</span>
<span class="s1">ad.primitive_transposes[broadcast_in_dim_p] = _broadcast_in_dim_transpose_rule</span>
<span class="s1">batching.primitive_batchers[broadcast_in_dim_p] = _broadcast_in_dim_batch_rule</span>
<span class="s1">pe.forwarding_rules[broadcast_in_dim_p] = _broadcast_in_dim_fwd_rule</span>
<span class="s1">pe.custom_partial_eval_rules[broadcast_in_dim_p] = _broadcast_in_dim_partial_eval</span>
<span class="s1">pe.custom_staging_rules[broadcast_in_dim_p] = _broadcast_in_dim_staging_rule</span>
<span class="s1">pe.padding_rules[broadcast_in_dim_p] = _broadcast_in_dim_padding_rule</span>
<span class="s1">core.custom_typechecks[broadcast_in_dim_p] = _broadcast_in_dim_typecheck_rule</span>
<span class="s1">mlir.register_lowering(broadcast_in_dim_p</span><span class="s2">, </span><span class="s1">_broadcast_in_dim_lower)</span>
<span class="s0"># TODO(mattjj): un-comment the next line</span>
<span class="s0"># core.pp_eqn_rules[broadcast_in_dim_p] = _broadcast_in_dim_pp_rule</span>


<span class="s2">def </span><span class="s1">_clamp_shape_rule(min</span><span class="s2">, </span><span class="s1">operand</span><span class="s2">, </span><span class="s1">max):</span>
  <span class="s2">if </span><span class="s1">min.shape </span><span class="s2">and </span><span class="s1">min.shape != operand.shape:</span>
    <span class="s2">raise </span><span class="s1">TypeError(</span><span class="s3">&quot;clamp requires min.shape == operand.shape or min.shape == &quot;</span>
                    <span class="s3">f&quot;(), got min.shape=</span><span class="s2">{</span><span class="s1">min.shape</span><span class="s2">}</span><span class="s3">, </span><span class="s2">{</span><span class="s1">operand.shape=</span><span class="s2">}</span><span class="s3">.&quot;</span><span class="s1">)</span>
  <span class="s2">if </span><span class="s1">max.shape </span><span class="s2">and </span><span class="s1">max.shape != operand.shape:</span>
    <span class="s2">raise </span><span class="s1">TypeError(</span><span class="s3">&quot;clamp requires max.shape == operand.shape or max.shape == &quot;</span>
                    <span class="s3">f&quot;(), got max.shape=</span><span class="s2">{</span><span class="s1">max.shape</span><span class="s2">}</span><span class="s3">, </span><span class="s2">{</span><span class="s1">operand.shape=</span><span class="s2">}</span><span class="s3">.&quot;</span><span class="s1">)</span>
  <span class="s2">return </span><span class="s1">operand.shape</span>

<span class="s1">_clamp_dtype_rule = partial(naryop_dtype_rule</span><span class="s2">, </span><span class="s1">_input_dtype</span><span class="s2">, </span><span class="s1">[_any</span><span class="s2">, </span><span class="s1">_any</span><span class="s2">, </span><span class="s1">_any]</span><span class="s2">,</span>
                            <span class="s3">'clamp'</span><span class="s1">)</span>

<span class="s2">def </span><span class="s1">_clamp_batch_rule(batched_args</span><span class="s2">, </span><span class="s1">batch_dims</span><span class="s2">, </span><span class="s1">**params):</span>
  <span class="s1">min</span><span class="s2">, </span><span class="s1">x</span><span class="s2">, </span><span class="s1">max = batched_args</span>
  <span class="s1">min_bdim</span><span class="s2">, </span><span class="s1">x_bdim</span><span class="s2">, </span><span class="s1">max_bdim = batch_dims</span>
  <span class="s1">size = next(x.shape[i] </span><span class="s2">for </span><span class="s1">x</span><span class="s2">, </span><span class="s1">i </span><span class="s2">in </span><span class="s1">zip(batched_args</span><span class="s2">, </span><span class="s1">batch_dims)</span>
              <span class="s2">if </span><span class="s1">i </span><span class="s2">is not None</span><span class="s1">)</span>

  <span class="s0"># avoid transposes and some broadcasts in special cases</span>
  <span class="s2">if </span><span class="s1">min_bdim == x_bdim == max_bdim:</span>
    <span class="s2">if </span><span class="s1">np.shape(min) == np.shape(x) == np.shape(max):</span>
      <span class="s2">return </span><span class="s1">clamp_p.bind(min</span><span class="s2">, </span><span class="s1">x</span><span class="s2">, </span><span class="s1">max)</span><span class="s2">, </span><span class="s1">x_bdim</span>
    <span class="s2">elif </span><span class="s1">np.ndim(min) == np.ndim(max) == </span><span class="s4">0</span><span class="s1">:</span>
      <span class="s2">return </span><span class="s1">clamp_p.bind(min</span><span class="s2">, </span><span class="s1">x</span><span class="s2">, </span><span class="s1">max)</span><span class="s2">, </span><span class="s1">x_bdim</span>
    <span class="s2">elif </span><span class="s1">np.ndim(min) == np.ndim(max) == </span><span class="s4">1</span><span class="s1">:</span>
      <span class="s1">min = broadcast_in_dim(min</span><span class="s2">, </span><span class="s1">x.shape</span><span class="s2">, </span><span class="s1">[min_bdim])</span>
      <span class="s1">max = broadcast_in_dim(max</span><span class="s2">, </span><span class="s1">x.shape</span><span class="s2">, </span><span class="s1">[max_bdim])</span>
      <span class="s2">return </span><span class="s1">clamp_p.bind(min</span><span class="s2">, </span><span class="s1">x</span><span class="s2">, </span><span class="s1">max)</span><span class="s2">, </span><span class="s1">x_bdim</span>
  <span class="s2">elif </span><span class="s1">np.ndim(min) == </span><span class="s4">0 </span><span class="s2">and </span><span class="s1">np.ndim(max) == </span><span class="s4">0 </span><span class="s2">and </span><span class="s1">x_bdim </span><span class="s2">is not None</span><span class="s1">:</span>
    <span class="s2">return </span><span class="s1">clamp_p.bind(min</span><span class="s2">, </span><span class="s1">x</span><span class="s2">, </span><span class="s1">max)</span><span class="s2">, </span><span class="s1">x_bdim</span>

  <span class="s1">min = batching.bdim_at_front(min</span><span class="s2">, </span><span class="s1">min_bdim</span><span class="s2">, </span><span class="s1">size) </span><span class="s2">if </span><span class="s1">np.shape(min) </span><span class="s2">else </span><span class="s1">min</span>
  <span class="s1">max = batching.bdim_at_front(max</span><span class="s2">, </span><span class="s1">max_bdim</span><span class="s2">, </span><span class="s1">size) </span><span class="s2">if </span><span class="s1">np.shape(max) </span><span class="s2">else </span><span class="s1">max</span>
  <span class="s1">x = batching.bdim_at_front(x</span><span class="s2">, </span><span class="s1">x_bdim</span><span class="s2">, </span><span class="s1">size) </span><span class="s2">if </span><span class="s1">np.shape(x) </span><span class="s2">else </span><span class="s1">x</span>
  <span class="s2">if </span><span class="s1">np.ndim(min) == </span><span class="s4">0 </span><span class="s2">and </span><span class="s1">np.ndim(x) &gt; </span><span class="s4">0</span><span class="s1">:</span>
    <span class="s1">min = broadcast(min</span><span class="s2">, </span><span class="s1">x.shape)</span>
  <span class="s2">if </span><span class="s1">np.ndim(max) == </span><span class="s4">0 </span><span class="s2">and </span><span class="s1">np.ndim(x) &gt; </span><span class="s4">0</span><span class="s1">:</span>
    <span class="s1">max = broadcast(max</span><span class="s2">, </span><span class="s1">x.shape)</span>
  <span class="s2">if </span><span class="s4">0 </span><span class="s1">&lt; np.ndim(min) &lt; np.ndim(x):</span>
    <span class="s2">assert </span><span class="s1">np.ndim(min) == </span><span class="s4">1</span><span class="s2">, </span><span class="s1">np.ndim(min)</span>
    <span class="s1">min = broadcast_in_dim(min</span><span class="s2">, </span><span class="s1">x.shape</span><span class="s2">, </span><span class="s1">[</span><span class="s4">0</span><span class="s1">])</span>
  <span class="s2">if </span><span class="s4">0 </span><span class="s1">&lt; np.ndim(max) &lt; np.ndim(x):</span>
    <span class="s2">assert </span><span class="s1">np.ndim(max) == </span><span class="s4">1</span><span class="s2">, </span><span class="s1">np.ndim(max)</span>
    <span class="s1">max = broadcast_in_dim(max</span><span class="s2">, </span><span class="s1">x.shape</span><span class="s2">, </span><span class="s1">[</span><span class="s4">0</span><span class="s1">])</span>
  <span class="s2">if </span><span class="s1">np.ndim(min) &gt; np.ndim(x):</span>
    <span class="s2">assert </span><span class="s1">np.ndim(x) == </span><span class="s4">0</span><span class="s2">, </span><span class="s1">np.ndim(x)</span>
    <span class="s1">x = broadcast(x</span><span class="s2">, </span><span class="s1">min.shape)</span>
  <span class="s2">return </span><span class="s1">clamp_p.bind(min</span><span class="s2">, </span><span class="s1">x</span><span class="s2">, </span><span class="s1">max)</span><span class="s2">, </span><span class="s4">0</span>

<span class="s1">clamp_p = standard_primitive(_clamp_shape_rule</span><span class="s2">, </span><span class="s1">_clamp_dtype_rule</span><span class="s2">, </span><span class="s3">'clamp'</span><span class="s1">)</span>
<span class="s1">ad.defjvp(clamp_p</span><span class="s2">,</span>
          <span class="s2">lambda </span><span class="s1">g</span><span class="s2">, </span><span class="s1">min</span><span class="s2">, </span><span class="s1">operand</span><span class="s2">, </span><span class="s1">max:</span>
          <span class="s1">select(bitwise_and(gt(min</span><span class="s2">, </span><span class="s1">operand)</span><span class="s2">, </span><span class="s1">lt(min</span><span class="s2">, </span><span class="s1">max))</span><span class="s2">,</span>
                 <span class="s1">g</span><span class="s2">, </span><span class="s1">_zeros(operand))</span><span class="s2">,</span>
          <span class="s2">lambda </span><span class="s1">g</span><span class="s2">, </span><span class="s1">min</span><span class="s2">, </span><span class="s1">operand</span><span class="s2">, </span><span class="s1">max:</span>
          <span class="s1">select(bitwise_and(gt(operand</span><span class="s2">, </span><span class="s1">min)</span><span class="s2">, </span><span class="s1">lt(operand</span><span class="s2">, </span><span class="s1">max))</span><span class="s2">,</span>
                 <span class="s1">g</span><span class="s2">, </span><span class="s1">_zeros(operand))</span><span class="s2">,</span>
          <span class="s2">lambda </span><span class="s1">g</span><span class="s2">, </span><span class="s1">min</span><span class="s2">, </span><span class="s1">operand</span><span class="s2">, </span><span class="s1">max:</span>
          <span class="s1">select(lt(max</span><span class="s2">, </span><span class="s1">operand)</span><span class="s2">, </span><span class="s1">g</span><span class="s2">, </span><span class="s1">_zeros(operand)))</span>
<span class="s1">batching.primitive_batchers[clamp_p] = _clamp_batch_rule</span>
<span class="s1">mlir.register_lowering(</span>
    <span class="s1">clamp_p</span><span class="s2">, </span><span class="s1">partial(_nary_lower_hlo</span><span class="s2">, </span><span class="s1">hlo.ClampOp))</span>
<span class="s1">pe.def_trivial_padding(clamp_p)</span>

<span class="s2">def </span><span class="s1">_concatenate_shape_rule(*operands</span><span class="s2">, </span><span class="s1">**kwargs):</span>
  <span class="s1">dimension = kwargs.pop(</span><span class="s3">'dimension'</span><span class="s1">)</span>
  <span class="s2">if not </span><span class="s1">operands:</span>
    <span class="s1">msg = </span><span class="s3">&quot;concatenate expects at least one operand, got 0.&quot;</span>
    <span class="s2">raise </span><span class="s1">TypeError(msg)</span>
  <span class="s2">if not </span><span class="s1">all(isinstance(operand</span><span class="s2">, </span><span class="s1">UnshapedArray) </span><span class="s2">for </span><span class="s1">operand </span><span class="s2">in </span><span class="s1">operands):</span>
    <span class="s1">msg = </span><span class="s3">&quot;All objects to concatenate must be arrays, got {}.&quot;</span>
    <span class="s1">op = next(op </span><span class="s2">for </span><span class="s1">op </span><span class="s2">in </span><span class="s1">operands </span><span class="s2">if not </span><span class="s1">isinstance(op</span><span class="s2">, </span><span class="s1">UnshapedArray))</span>
    <span class="s2">raise </span><span class="s1">TypeError(msg.format(type(op)))</span>
  <span class="s2">if </span><span class="s1">len({operand.ndim </span><span class="s2">for </span><span class="s1">operand </span><span class="s2">in </span><span class="s1">operands}) != </span><span class="s4">1</span><span class="s1">:</span>
    <span class="s1">msg = </span><span class="s3">&quot;Cannot concatenate arrays with different numbers of dimensions: got {}.&quot;</span>
    <span class="s2">raise </span><span class="s1">TypeError(msg.format(</span><span class="s3">&quot;, &quot;</span><span class="s1">.join(str(o.shape) </span><span class="s2">for </span><span class="s1">o </span><span class="s2">in </span><span class="s1">operands)))</span>
  <span class="s2">if not </span><span class="s4">0 </span><span class="s1">&lt;= dimension &lt; operands[</span><span class="s4">0</span><span class="s1">].ndim:</span>
    <span class="s1">msg = </span><span class="s3">&quot;concatenate dimension out of bounds: dimension {} for shapes {}.&quot;</span>
    <span class="s2">raise </span><span class="s1">TypeError(msg.format(dimension</span><span class="s2">, </span><span class="s3">&quot;, &quot;</span><span class="s1">.join([str(o.shape) </span><span class="s2">for </span><span class="s1">o </span><span class="s2">in </span><span class="s1">operands])))</span>
  <span class="s1">shapes = [operand.shape[:dimension] + operand.shape[dimension+</span><span class="s4">1</span><span class="s1">:]</span>
            <span class="s2">for </span><span class="s1">operand </span><span class="s2">in </span><span class="s1">operands]</span>
  <span class="s2">if not </span><span class="s1">shapes[:-</span><span class="s4">1</span><span class="s1">] == shapes[</span><span class="s4">1</span><span class="s1">:]:</span>
    <span class="s1">msg = (</span><span class="s3">&quot;Cannot concatenate arrays with shapes that differ in dimensions &quot;</span>
           <span class="s3">&quot;other than the one being concatenated: concatenating along &quot;</span>
           <span class="s3">&quot;dimension {} for shapes {}.&quot;</span><span class="s1">)</span>
    <span class="s1">shapes = [operand.shape </span><span class="s2">for </span><span class="s1">operand </span><span class="s2">in </span><span class="s1">operands]</span>
    <span class="s2">raise </span><span class="s1">TypeError(msg.format(dimension</span><span class="s2">, </span><span class="s3">&quot;, &quot;</span><span class="s1">.join(map(str</span><span class="s2">, </span><span class="s1">shapes))))</span>

  <span class="s1">concat_size = sum(o.shape[dimension] </span><span class="s2">for </span><span class="s1">o </span><span class="s2">in </span><span class="s1">operands)</span>
  <span class="s1">ex_shape = operands[</span><span class="s4">0</span><span class="s1">].shape</span>
  <span class="s2">return </span><span class="s1">ex_shape[:dimension] + (concat_size</span><span class="s2">,</span><span class="s1">) + ex_shape[dimension+</span><span class="s4">1</span><span class="s1">:]</span>

<span class="s2">def </span><span class="s1">_concatenate_dtype_rule(*operands</span><span class="s2">, </span><span class="s1">**kwargs):</span>
  <span class="s1">_check_same_dtypes(</span><span class="s3">'concatenate'</span><span class="s2">, False, </span><span class="s1">*(o.dtype </span><span class="s2">for </span><span class="s1">o </span><span class="s2">in </span><span class="s1">operands))</span>
  <span class="s2">return </span><span class="s1">operands[</span><span class="s4">0</span><span class="s1">].dtype</span>

<span class="s2">def </span><span class="s1">_concatenate_transpose_rule(t</span><span class="s2">, </span><span class="s1">*operands</span><span class="s2">, </span><span class="s1">dimension):</span>
  <span class="s1">operand_shapes = [o.aval.shape </span><span class="s2">if </span><span class="s1">ad.is_undefined_primal(o) </span><span class="s2">else </span><span class="s1">o.shape</span>
                    <span class="s2">for </span><span class="s1">o </span><span class="s2">in </span><span class="s1">operands]</span>
  <span class="s2">if </span><span class="s1">type(t) </span><span class="s2">is </span><span class="s1">ad_util.Zero:</span>
    <span class="s2">return </span><span class="s1">[ad_util.Zero(o.aval) </span><span class="s2">if </span><span class="s1">ad.is_undefined_primal(o) </span><span class="s2">else None</span>
            <span class="s2">for </span><span class="s1">o </span><span class="s2">in </span><span class="s1">operands]</span>
  <span class="s2">else</span><span class="s1">:</span>
    <span class="s1">limit_points = np.cumsum(</span>
        <span class="s1">[shape[dimension] </span><span class="s2">for </span><span class="s1">shape </span><span class="s2">in </span><span class="s1">operand_shapes]).tolist()</span>
    <span class="s1">starts = np.zeros((len(operands)</span><span class="s2">, </span><span class="s1">t.ndim)</span><span class="s2">, </span><span class="s1">dtype=int).tolist()</span>
    <span class="s1">limits = np.tile(t.shape</span><span class="s2">, </span><span class="s1">(len(operands)</span><span class="s2">, </span><span class="s4">1</span><span class="s1">)).tolist()</span>

    <span class="s2">for </span><span class="s1">i</span><span class="s2">, </span><span class="s1">s </span><span class="s2">in </span><span class="s1">enumerate(starts[</span><span class="s4">1</span><span class="s1">:]):</span>
      <span class="s1">s[dimension] = limit_points[:-</span><span class="s4">1</span><span class="s1">][i]</span>
    <span class="s2">for </span><span class="s1">i</span><span class="s2">, </span><span class="s1">l </span><span class="s2">in </span><span class="s1">enumerate(limits):</span>
      <span class="s1">l[dimension] = limit_points[i]</span>

    <span class="s2">return </span><span class="s1">[slicing.slice(t</span><span class="s2">, </span><span class="s1">start</span><span class="s2">, </span><span class="s1">limit) </span><span class="s2">if </span><span class="s1">ad.is_undefined_primal(o)</span>
            <span class="s2">else None for </span><span class="s1">o</span><span class="s2">, </span><span class="s1">start</span><span class="s2">, </span><span class="s1">limit </span><span class="s2">in </span><span class="s1">zip(operands</span><span class="s2">, </span><span class="s1">starts</span><span class="s2">, </span><span class="s1">limits)]</span>

<span class="s2">def </span><span class="s1">_concatenate_batch_rule(batched_args</span><span class="s2">, </span><span class="s1">batch_dims</span><span class="s2">, </span><span class="s1">*</span><span class="s2">, </span><span class="s1">dimension):</span>
  <span class="s1">size = next(op.shape[bdim] </span><span class="s2">for </span><span class="s1">op</span><span class="s2">, </span><span class="s1">bdim </span><span class="s2">in </span><span class="s1">zip(batched_args</span><span class="s2">, </span><span class="s1">batch_dims)</span>
              <span class="s2">if </span><span class="s1">bdim </span><span class="s2">is not None</span><span class="s1">)</span>
  <span class="s1">operands = [batching.moveaxis(op</span><span class="s2">, </span><span class="s1">bdim</span><span class="s2">, </span><span class="s4">0</span><span class="s1">) </span><span class="s2">if </span><span class="s1">bdim </span><span class="s2">is not None</span>
              <span class="s2">else </span><span class="s1">broadcast(op</span><span class="s2">, </span><span class="s1">(size</span><span class="s2">,</span><span class="s1">))</span>
              <span class="s2">for </span><span class="s1">op</span><span class="s2">, </span><span class="s1">bdim </span><span class="s2">in </span><span class="s1">zip(batched_args</span><span class="s2">, </span><span class="s1">batch_dims)]</span>
  <span class="s2">return </span><span class="s1">concatenate(operands</span><span class="s2">, </span><span class="s1">dimension + </span><span class="s4">1</span><span class="s1">)</span><span class="s2">, </span><span class="s4">0</span>

<span class="s2">def </span><span class="s1">_concatenate_pad_rule(in_avals</span><span class="s2">, </span><span class="s1">out_avals</span><span class="s2">, </span><span class="s1">*operands</span><span class="s2">, </span><span class="s1">dimension):</span>
  <span class="s2">if </span><span class="s1">all(isinstance(a.shape[dimension]</span><span class="s2">, </span><span class="s1">(int</span><span class="s2">, </span><span class="s1">np.integer))</span>
         <span class="s2">for </span><span class="s1">a </span><span class="s2">in </span><span class="s1">in_avals):</span>
    <span class="s2">return </span><span class="s1">[concatenate(operands</span><span class="s2">, </span><span class="s1">dimension)]</span>
  <span class="s2">else</span><span class="s1">:</span>
    <span class="s2">raise </span><span class="s1">NotImplementedError  </span><span class="s0"># TODO(mattjj)</span>

<span class="s1">concatenate_p = standard_primitive(</span>
    <span class="s1">_concatenate_shape_rule</span><span class="s2">, </span><span class="s1">_concatenate_dtype_rule</span><span class="s2">, </span><span class="s3">'concatenate'</span><span class="s1">)</span>
<span class="s1">ad.deflinear2(concatenate_p</span><span class="s2">, </span><span class="s1">_concatenate_transpose_rule)</span>
<span class="s1">ad.primitive_transposes[concatenate_p] = _concatenate_transpose_rule</span>
<span class="s1">batching.primitive_batchers[concatenate_p] = _concatenate_batch_rule</span>
<span class="s1">pe.padding_rules[concatenate_p] = _concatenate_pad_rule</span>

<span class="s2">def </span><span class="s1">_concatenate_lower(ctx</span><span class="s2">, </span><span class="s1">*xs</span><span class="s2">, </span><span class="s1">dimension):</span>
  <span class="s2">return </span><span class="s1">hlo.ConcatenateOp(xs</span><span class="s2">, </span><span class="s1">mlir.i64_attr(dimension)).results</span>
<span class="s1">mlir.register_lowering(concatenate_p</span><span class="s2">, </span><span class="s1">_concatenate_lower)</span>


<span class="s2">def </span><span class="s1">_pad_dtype_rule(operand</span><span class="s2">, </span><span class="s1">padding_value</span><span class="s2">, </span><span class="s1">*</span><span class="s2">, </span><span class="s1">padding_config):</span>
  <span class="s2">if </span><span class="s1">operand.dtype != padding_value.dtype:</span>
    <span class="s1">msg = </span><span class="s3">&quot;pad operand and padding_value must be same dtype: got {} and {}.&quot;</span>
    <span class="s2">raise </span><span class="s1">TypeError(msg.format(operand.dtype</span><span class="s2">, </span><span class="s1">padding_value.dtype))</span>

  <span class="s2">return </span><span class="s1">_input_dtype(operand</span><span class="s2">, </span><span class="s1">padding_value)</span>

<span class="s2">def </span><span class="s1">_pad_shape_rule(operand</span><span class="s2">, </span><span class="s1">padding_value</span><span class="s2">, </span><span class="s1">*</span><span class="s2">, </span><span class="s1">padding_config):</span>
  <span class="s2">del </span><span class="s1">padding_value</span>
  <span class="s1">op_shape = np.shape(operand)</span>
  <span class="s2">if not </span><span class="s1">len(padding_config) == np.ndim(operand):</span>
    <span class="s2">raise </span><span class="s1">ValueError(</span><span class="s3">&quot;length of padding_config must equal the number of axes &quot;</span>
                     <span class="s3">f&quot;of operand, got padding_config </span><span class="s2">{</span><span class="s1">padding_config</span><span class="s2">} </span><span class="s3">&quot;</span>
                     <span class="s3">f&quot;for operand shape </span><span class="s2">{</span><span class="s1">op_shape</span><span class="s2">}</span><span class="s3">&quot;</span><span class="s1">)</span>
  <span class="s2">if not </span><span class="s1">all(i &gt;= </span><span class="s4">0 </span><span class="s2">for </span><span class="s1">_</span><span class="s2">, </span><span class="s1">_</span><span class="s2">, </span><span class="s1">i </span><span class="s2">in </span><span class="s1">padding_config):</span>
    <span class="s2">raise </span><span class="s1">ValueError(</span><span class="s3">&quot;interior padding in padding_config must be nonnegative, &quot;</span>
                     <span class="s3">f&quot;got padding_config </span><span class="s2">{</span><span class="s1">padding_config</span><span class="s2">}</span><span class="s3">&quot;</span><span class="s1">)</span>
  <span class="s1">result = tuple(core.sum_dim(l</span><span class="s2">, </span><span class="s1">h</span><span class="s2">, </span><span class="s1">core.dilate_dim(d</span><span class="s2">, </span><span class="s1">i + </span><span class="s4">1</span><span class="s1">))</span>
                 <span class="s2">for </span><span class="s1">(l</span><span class="s2">, </span><span class="s1">h</span><span class="s2">, </span><span class="s1">i)</span><span class="s2">, </span><span class="s1">d </span><span class="s2">in </span><span class="s1">zip(padding_config</span><span class="s2">, </span><span class="s1">op_shape))</span>
  <span class="s2">if not </span><span class="s1">all(core.greater_equal_dim(d</span><span class="s2">, </span><span class="s4">0</span><span class="s1">) </span><span class="s2">for </span><span class="s1">d </span><span class="s2">in </span><span class="s1">result):</span>
    <span class="s1">msg = (</span><span class="s3">f&quot;Dimension size after padding is not at least 0, &quot;</span>
           <span class="s3">f&quot;got result shape </span><span class="s2">{</span><span class="s1">result</span><span class="s2">}</span><span class="s3">, for padding_config </span><span class="s2">{</span><span class="s1">padding_config</span><span class="s2">}</span><span class="s3">&quot;</span>
           <span class="s3">f&quot; and operand shape </span><span class="s2">{</span><span class="s1">op_shape</span><span class="s2">}</span><span class="s3">&quot;</span><span class="s1">)</span>
    <span class="s2">raise </span><span class="s1">ValueError(msg)</span>
  <span class="s2">return </span><span class="s1">result</span>

<span class="s2">def </span><span class="s1">_pad_transpose(t</span><span class="s2">, </span><span class="s1">operand</span><span class="s2">, </span><span class="s1">padding_value</span><span class="s2">, </span><span class="s1">*</span><span class="s2">, </span><span class="s1">padding_config):</span>
  <span class="s2">if </span><span class="s1">type(t) </span><span class="s2">is </span><span class="s1">ad_util.Zero:</span>
    <span class="s1">t_operand = ad_util.Zero(operand.aval) </span><span class="s2">if </span><span class="s1">ad.is_undefined_primal(operand) </span><span class="s2">else None</span>
    <span class="s1">t_padv = ad_util.Zero(padding_value.aval) </span><span class="s2">if </span><span class="s1">ad.is_undefined_primal(padding_value) </span><span class="s2">else None</span>
  <span class="s2">else</span><span class="s1">:</span>
    <span class="s1">lo</span><span class="s2">, </span><span class="s1">hi</span><span class="s2">, </span><span class="s1">interior = util.unzip3(padding_config)</span>
    <span class="s1">total = </span><span class="s2">lambda </span><span class="s1">x: _reduce_sum(x</span><span class="s2">, </span><span class="s1">list(range(t.ndim)))</span>

    <span class="s2">def </span><span class="s1">t_op():</span>
      <span class="s1">unpad_config = safe_zip(np.negative(lo)</span><span class="s2">, </span><span class="s1">np.negative(hi)</span><span class="s2">,</span>
                              <span class="s1">np.zeros_like(interior))</span>
      <span class="s1">unpadded = pad(t</span><span class="s2">, </span><span class="s1">np.array(</span><span class="s4">0.</span><span class="s2">, </span><span class="s1">t.dtype)</span><span class="s2">, </span><span class="s1">unpad_config)</span>
      <span class="s2">return </span><span class="s1">slicing.slice(unpadded</span><span class="s2">, </span><span class="s1">np.zeros_like(lo)</span><span class="s2">, </span><span class="s1">unpadded.shape</span><span class="s2">,</span>
                           <span class="s1">np.add(interior</span><span class="s2">, </span><span class="s4">1</span><span class="s1">))</span>

    <span class="s1">t_operand = t_op() </span><span class="s2">if </span><span class="s1">ad.is_undefined_primal(operand) </span><span class="s2">else None</span>
    <span class="s1">t_padv = sub(total(t)</span><span class="s2">, </span><span class="s1">total(t_operand)) </span><span class="s2">if </span><span class="s1">ad.is_undefined_primal(padding_value) </span><span class="s2">else None</span>
  <span class="s2">return </span><span class="s1">[t_operand</span><span class="s2">, </span><span class="s1">t_padv]</span>

<span class="s2">def </span><span class="s1">_pad_batch_rule(batched_args</span><span class="s2">, </span><span class="s1">batch_dims</span><span class="s2">, </span><span class="s1">*</span><span class="s2">, </span><span class="s1">padding_config):</span>
  <span class="s1">operand</span><span class="s2">, </span><span class="s1">padding_value = batched_args</span>
  <span class="s1">operand_bdim</span><span class="s2">, </span><span class="s1">padding_value_bdim = batch_dims</span>
  <span class="s2">if </span><span class="s1">operand_bdim </span><span class="s2">is None</span><span class="s1">:</span>
    <span class="s1">operand_bdim = </span><span class="s4">0</span>
    <span class="s1">operand = broadcast(operand</span><span class="s2">, </span><span class="s1">(padding_value.shape[padding_value_bdim]</span><span class="s2">,</span><span class="s1">))</span>

  <span class="s1">padding_config = list(padding_config)</span>
  <span class="s1">padding_config.insert(operand_bdim</span><span class="s2">, </span><span class="s1">(</span><span class="s4">0</span><span class="s2">, </span><span class="s4">0</span><span class="s2">, </span><span class="s4">0</span><span class="s1">))</span>
  <span class="s2">if </span><span class="s1">padding_value_bdim </span><span class="s2">is None</span><span class="s1">:</span>
    <span class="s2">return </span><span class="s1">pad(operand</span><span class="s2">, </span><span class="s1">padding_value</span><span class="s2">, </span><span class="s1">padding_config)</span><span class="s2">, </span><span class="s1">operand_bdim</span>

  <span class="s2">assert </span><span class="s1">padding_value_bdim == </span><span class="s4">0</span><span class="s2">, </span><span class="s1">padding_value_bdim</span>

  <span class="s1">x = pad(operand</span><span class="s2">, </span><span class="s1">_zero(operand)</span><span class="s2">, </span><span class="s1">padding_config)</span>
  <span class="s1">mask = pad(full_like(operand</span><span class="s2">, True, </span><span class="s1">np.bool_)</span><span class="s2">, False, </span><span class="s1">padding_config)</span>
  <span class="s1">broadcasted_padding = broadcast_in_dim(padding_value</span><span class="s2">, </span><span class="s1">x.shape</span><span class="s2">,</span>
                                         <span class="s1">(operand_bdim</span><span class="s2">,</span><span class="s1">))</span>
  <span class="s2">return </span><span class="s1">select(mask</span><span class="s2">, </span><span class="s1">x</span><span class="s2">, </span><span class="s1">broadcasted_padding)</span><span class="s2">, </span><span class="s1">operand_bdim</span>

<span class="s1">pad_p = standard_primitive(_pad_shape_rule</span><span class="s2">, </span><span class="s1">_pad_dtype_rule</span><span class="s2">, </span><span class="s3">'pad'</span><span class="s1">)</span>
<span class="s1">ad.deflinear2(pad_p</span><span class="s2">, </span><span class="s1">_pad_transpose)</span>
<span class="s1">batching.primitive_batchers[pad_p] = _pad_batch_rule</span>

<span class="s2">def </span><span class="s1">_pad_lower(ctx</span><span class="s2">, </span><span class="s1">x</span><span class="s2">, </span><span class="s1">padding_value</span><span class="s2">, </span><span class="s1">*</span><span class="s2">, </span><span class="s1">padding_config):</span>
  <span class="s1">aval_out</span><span class="s2">, </span><span class="s1">= ctx.avals_out</span>
  <span class="s1">low</span><span class="s2">, </span><span class="s1">high</span><span class="s2">, </span><span class="s1">interior = util.unzip3(padding_config)</span>
  <span class="s2">return </span><span class="s1">[mlir.pad(ctx</span><span class="s2">, </span><span class="s1">aval_out</span><span class="s2">, </span><span class="s1">x</span><span class="s2">, </span><span class="s1">padding_value</span><span class="s2">, </span><span class="s1">low</span><span class="s2">, </span><span class="s1">high</span><span class="s2">, </span><span class="s1">interior)]</span>

<span class="s1">mlir.register_lowering(pad_p</span><span class="s2">, </span><span class="s1">_pad_lower)</span>


<span class="s0"># The squeeze primitive exists for the benefit of masking and other</span>
<span class="s0"># transformations that need to keep track of axis identity.</span>
<span class="s0"># For example, consider reshaping a 2D array with shape (1, N) into a 1D array</span>
<span class="s0"># with shape (N,). This results in the following JAXpr:</span>
<span class="s0">#   reshape[ dimension=None new_sizes=(N,) ]</span>
<span class="s0"># For N &gt; 1, we can match up the output array axis with the second axis of the</span>
<span class="s0"># input. But for N = 1, it is not clear how axes match up: all we know from the</span>
<span class="s0"># JAXpr is that we are reshaping from (1, 1) to (1,).</span>
<span class="s0"># In constrast, squeeze[ dimensions=(0,) ] is unambiguous.</span>


<span class="s2">def </span><span class="s1">_squeeze_dtype_rule(operand</span><span class="s2">, </span><span class="s1">*</span><span class="s2">, </span><span class="s1">dimensions):</span>
  <span class="s2">return </span><span class="s1">operand.dtype</span>

<span class="s2">def </span><span class="s1">_squeeze_shape_rule(operand</span><span class="s2">, </span><span class="s1">*</span><span class="s2">, </span><span class="s1">dimensions):</span>
  <span class="s2">return </span><span class="s1">_compute_squeeze_shape(np.shape(operand)</span><span class="s2">, </span><span class="s1">dimensions)</span>

<span class="s2">def </span><span class="s1">_compute_squeeze_shape(shape</span><span class="s2">, </span><span class="s1">dimensions):</span>
  <span class="s1">dims_set = set(dimensions)</span>
  <span class="s2">if </span><span class="s1">len(dims_set) != len(dimensions):</span>
    <span class="s2">raise </span><span class="s1">ValueError(</span><span class="s3">f&quot;dimensions are not unique: </span><span class="s2">{</span><span class="s1">dimensions</span><span class="s2">}</span><span class="s3">&quot;</span><span class="s1">)</span>
  <span class="s2">if not </span><span class="s1">all(</span><span class="s4">0 </span><span class="s1">&lt;= d &lt; len(shape) </span><span class="s2">for </span><span class="s1">d </span><span class="s2">in </span><span class="s1">dims_set):</span>
    <span class="s2">raise </span><span class="s1">ValueError(</span><span class="s3">f&quot;dimensions outside range [0, ndim): </span><span class="s2">{</span><span class="s1">dimensions</span><span class="s2">}</span><span class="s3">&quot;</span><span class="s1">)</span>
  <span class="s2">if </span><span class="s1">any(</span><span class="s2">not </span><span class="s1">core.symbolic_equal_dim(shape[d]</span><span class="s2">, </span><span class="s4">1</span><span class="s1">) </span><span class="s2">for </span><span class="s1">d </span><span class="s2">in </span><span class="s1">dimensions):</span>
    <span class="s2">raise </span><span class="s1">ValueError(</span>
        <span class="s3">&quot;cannot select an axis to squeeze out which has size not equal to &quot;</span>
        <span class="s3">f&quot;one, got </span><span class="s2">{</span><span class="s1">shape=</span><span class="s2">} </span><span class="s3">and </span><span class="s2">{</span><span class="s1">dimensions=</span><span class="s2">}</span><span class="s3">&quot;</span><span class="s1">)</span>
  <span class="s2">return </span><span class="s1">tuple(s </span><span class="s2">for </span><span class="s1">i</span><span class="s2">, </span><span class="s1">s </span><span class="s2">in </span><span class="s1">enumerate(shape) </span><span class="s2">if </span><span class="s1">i </span><span class="s2">not in </span><span class="s1">dims_set)</span>

<span class="s2">def </span><span class="s1">_squeeze_transpose_rule(t</span><span class="s2">, </span><span class="s1">operand</span><span class="s2">, </span><span class="s1">*</span><span class="s2">, </span><span class="s1">dimensions):</span>
  <span class="s2">assert </span><span class="s1">ad.is_undefined_primal(operand)</span>
  <span class="s2">return </span><span class="s1">[expand_dims(t</span><span class="s2">, </span><span class="s1">dimensions)]</span>

<span class="s2">def </span><span class="s1">_squeeze_batch_rule(batched_args</span><span class="s2">, </span><span class="s1">batch_dims</span><span class="s2">, </span><span class="s1">*</span><span class="s2">, </span><span class="s1">dimensions):</span>
  <span class="s1">operand</span><span class="s2">, </span><span class="s1">= batched_args</span>
  <span class="s1">bdim</span><span class="s2">, </span><span class="s1">= batch_dims</span>
  <span class="s1">operand = batching.moveaxis(operand</span><span class="s2">, </span><span class="s1">bdim</span><span class="s2">, </span><span class="s4">0</span><span class="s1">)</span>
  <span class="s1">dimensions = tuple(np.add(</span><span class="s4">1</span><span class="s2">, </span><span class="s1">dimensions))</span>
  <span class="s2">return </span><span class="s1">squeeze(operand</span><span class="s2">, </span><span class="s1">dimensions=dimensions)</span><span class="s2">, </span><span class="s4">0</span>

<span class="s1">squeeze_p = standard_primitive(_squeeze_shape_rule</span><span class="s2">, </span><span class="s1">_squeeze_dtype_rule</span><span class="s2">,</span>
                               <span class="s3">'squeeze'</span><span class="s1">)</span>
<span class="s1">ad.deflinear2(squeeze_p</span><span class="s2">, </span><span class="s1">_squeeze_transpose_rule)</span>
<span class="s1">batching.primitive_batchers[squeeze_p] = _squeeze_batch_rule</span>
<span class="s1">pe.def_trivial_padding(squeeze_p)</span>

<span class="s2">def </span><span class="s1">_squeeze_lower(ctx</span><span class="s2">, </span><span class="s1">operand</span><span class="s2">, </span><span class="s1">*</span><span class="s2">, </span><span class="s1">dimensions):</span>
  <span class="s2">del </span><span class="s1">dimensions  </span><span class="s0"># Implied by the output aval.</span>
  <span class="s2">return </span><span class="s1">[mlir.reshape(ctx</span><span class="s2">, </span><span class="s1">operand</span><span class="s2">, </span><span class="s1">ctx.avals_out[</span><span class="s4">0</span><span class="s1">])]</span>

<span class="s1">mlir.register_lowering(squeeze_p</span><span class="s2">, </span><span class="s1">_squeeze_lower)</span>


<span class="s2">def </span><span class="s1">shape_as_value(shape: core.Shape):</span>
  <span class="s5">&quot;&quot;&quot;Converts a shape that may contain Poly values into a JAX value.&quot;&quot;&quot;</span>
  <span class="s2">if </span><span class="s1">len(shape) == </span><span class="s4">0</span><span class="s1">:</span>
    <span class="s2">return </span><span class="s1">full((</span><span class="s4">0</span><span class="s2">,</span><span class="s1">)</span><span class="s2">, </span><span class="s1">np.array(</span><span class="s4">0</span><span class="s2">, </span><span class="s1">np.int64))</span>
  <span class="s1">dims = [</span>
      <span class="s1">expand_dims(convert_element_type(core.dimension_as_value(d)</span><span class="s2">, </span><span class="s1">np.int64)</span><span class="s2">,</span>
                  <span class="s1">(</span><span class="s4">0</span><span class="s2">,</span><span class="s1">))</span>
      <span class="s2">for </span><span class="s1">d </span><span class="s2">in </span><span class="s1">shape</span>
  <span class="s1">]</span>
  <span class="s2">return </span><span class="s1">concatenate(dims</span><span class="s2">, </span><span class="s1">dimension=</span><span class="s4">0</span><span class="s1">)</span>

<span class="s2">def </span><span class="s1">_reshape_shape_rule(operand</span><span class="s2">, </span><span class="s1">*</span><span class="s2">, </span><span class="s1">new_sizes</span><span class="s2">, </span><span class="s1">dimensions):</span>
  <span class="s2">if not </span><span class="s1">all(core.greater_equal_dim(d</span><span class="s2">, </span><span class="s4">0</span><span class="s1">) </span><span class="s2">for </span><span class="s1">d </span><span class="s2">in </span><span class="s1">new_sizes):</span>
    <span class="s1">msg = </span><span class="s3">'reshape new_sizes must all be positive, got {}.'</span>
    <span class="s2">raise </span><span class="s1">TypeError(msg.format(new_sizes))</span>
  <span class="s0"># TODO(necula): re-enable this check</span>
  <span class="s2">if </span><span class="s1">(</span><span class="s2">not </span><span class="s1">config.jax_dynamic_shapes </span><span class="s2">and</span>
      <span class="s2">not </span><span class="s1">core.same_shape_sizes(np.shape(operand)</span><span class="s2">, </span><span class="s1">new_sizes)):</span>
    <span class="s1">msg = </span><span class="s3">'reshape total size must be unchanged, got new_sizes {} for shape {}.'</span>
    <span class="s2">raise </span><span class="s1">TypeError(msg.format(new_sizes</span><span class="s2">, </span><span class="s1">np.shape(operand)))</span>
  <span class="s2">if </span><span class="s1">dimensions </span><span class="s2">is not None</span><span class="s1">:</span>
    <span class="s2">if </span><span class="s1">set(dimensions) != set(range(np.ndim(operand))):</span>
      <span class="s1">msg = (</span><span class="s3">'reshape dimensions must be a permutation of operand dimensions, '</span>
             <span class="s3">'got dimensions {} for shape {}.'</span><span class="s1">)</span>
      <span class="s2">raise </span><span class="s1">TypeError(msg.format(dimensions</span><span class="s2">, </span><span class="s1">np.shape(operand)))</span>
  <span class="s2">return </span><span class="s1">tuple(new_sizes)</span>

<span class="s2">def </span><span class="s1">_reshape_typecheck_rule(_</span><span class="s2">, </span><span class="s1">operand</span><span class="s2">, </span><span class="s1">*dyn_shape</span><span class="s2">, </span><span class="s1">new_sizes</span><span class="s2">, </span><span class="s1">dimensions):</span>
  <span class="s2">if not </span><span class="s1">dyn_shape:</span>
    <span class="s1">out_aval</span><span class="s2">, </span><span class="s1">effects = reshape_p.abstract_eval(</span>
        <span class="s1">operand.aval</span><span class="s2">, </span><span class="s1">new_sizes=new_sizes</span><span class="s2">, </span><span class="s1">dimensions=dimensions)</span>
    <span class="s2">return </span><span class="s1">[out_aval]</span><span class="s2">, </span><span class="s1">effects</span>
  <span class="s2">else</span><span class="s1">:</span>
    <span class="s0"># TODO(mattjj, necula): perform more checks like _reshape_shape_rule</span>
    <span class="s1">out_shape = _merge_dyn_shape(new_sizes</span><span class="s2">, </span><span class="s1">dyn_shape)</span>
    <span class="s1">out_shape = [x.val </span><span class="s2">if </span><span class="s1">type(x) </span><span class="s2">is </span><span class="s1">core.Literal </span><span class="s2">else </span><span class="s1">x </span><span class="s2">for </span><span class="s1">x </span><span class="s2">in </span><span class="s1">out_shape]  </span><span class="s0"># pytype: disable=attribute-error</span>
    <span class="s1">out_aval = core.DShapedArray(tuple(out_shape)</span><span class="s2">, </span><span class="s1">operand.aval.dtype</span><span class="s2">,</span>
                                 <span class="s1">operand.aval.weak_type)</span>
    <span class="s2">return </span><span class="s1">[out_aval]</span><span class="s2">, </span><span class="s1">core.no_effects</span>


<span class="s2">def </span><span class="s1">_reshape_dtype_rule(operand</span><span class="s2">, </span><span class="s1">*</span><span class="s2">, </span><span class="s1">new_sizes</span><span class="s2">, </span><span class="s1">dimensions):</span>
  <span class="s2">return </span><span class="s1">operand.dtype</span>

<span class="s2">def </span><span class="s1">_reshape_transpose_rule(t</span><span class="s2">, </span><span class="s1">operand</span><span class="s2">, </span><span class="s1">*</span><span class="s2">, </span><span class="s1">new_sizes</span><span class="s2">, </span><span class="s1">dimensions):</span>
  <span class="s2">assert </span><span class="s1">ad.is_undefined_primal(operand)</span>
  <span class="s2">if </span><span class="s1">dimensions </span><span class="s2">is None</span><span class="s1">:</span>
    <span class="s2">return </span><span class="s1">[reshape(t</span><span class="s2">, </span><span class="s1">operand.aval.shape)]</span>
  <span class="s2">else</span><span class="s1">:</span>
    <span class="s2">return </span><span class="s1">[transpose(reshape(t</span><span class="s2">, </span><span class="s1">np.take(operand.aval.shape</span><span class="s2">, </span><span class="s1">dimensions))</span><span class="s2">,</span>
                      <span class="s1">np.argsort(dimensions))]</span>

<span class="s2">def </span><span class="s1">_reshape_batch_rule(batched_args</span><span class="s2">, </span><span class="s1">batch_dims</span><span class="s2">, </span><span class="s1">*</span><span class="s2">, </span><span class="s1">new_sizes</span><span class="s2">, </span><span class="s1">dimensions):</span>
  <span class="s1">operand</span><span class="s2">, </span><span class="s1">= batched_args</span>
  <span class="s1">bdim</span><span class="s2">, </span><span class="s1">= batch_dims</span>
  <span class="s1">operand = batching.moveaxis(operand</span><span class="s2">, </span><span class="s1">bdim</span><span class="s2">, </span><span class="s4">0</span><span class="s1">)</span>
  <span class="s2">if </span><span class="s1">dimensions </span><span class="s2">is not None</span><span class="s1">:</span>
    <span class="s1">dimensions = (</span><span class="s4">0</span><span class="s2">,</span><span class="s1">) + tuple(np.add(</span><span class="s4">1</span><span class="s2">, </span><span class="s1">dimensions))</span>
  <span class="s2">return </span><span class="s1">reshape(operand</span><span class="s2">, </span><span class="s1">operand.shape[:</span><span class="s4">1</span><span class="s1">] + new_sizes</span><span class="s2">, </span><span class="s1">dimensions)</span><span class="s2">, </span><span class="s4">0</span>


<span class="s2">def </span><span class="s1">_reshape_lower(ctx</span><span class="s2">, </span><span class="s1">x</span><span class="s2">, </span><span class="s1">*dyn_shape</span><span class="s2">, </span><span class="s1">new_sizes</span><span class="s2">, </span><span class="s1">dimensions):</span>
  <span class="s1">aval_out</span><span class="s2">, </span><span class="s1">= ctx.avals_out</span>
  <span class="s2">if </span><span class="s1">dimensions </span><span class="s2">is not None</span><span class="s1">:</span>
    <span class="s1">x = hlo.TransposeOp(x</span><span class="s2">, </span><span class="s1">mlir.dense_int_elements(dimensions)).result</span>
  <span class="s2">if </span><span class="s1">dyn_shape:</span>
    <span class="s1">aval_out = aval_out.update(shape=_merge_dyn_shape(new_sizes</span><span class="s2">, </span><span class="s1">dyn_shape))</span>
  <span class="s2">return </span><span class="s1">[mlir.reshape(ctx</span><span class="s2">, </span><span class="s1">x</span><span class="s2">, </span><span class="s1">aval_out)]</span>

<span class="s2">def </span><span class="s1">_reshape_staging_rule(</span>
    <span class="s1">trace</span><span class="s2">, </span><span class="s1">x</span><span class="s2">, </span><span class="s1">*dyn</span><span class="s2">, </span><span class="s1">new_sizes</span><span class="s2">, </span><span class="s1">dimensions):</span>
  <span class="s1">params = dict(new_sizes=new_sizes</span><span class="s2">, </span><span class="s1">dimensions=dimensions)</span>
  <span class="s2">if not </span><span class="s1">dyn:</span>
    <span class="s2">return </span><span class="s1">trace.default_process_primitive(reshape_p</span><span class="s2">, </span><span class="s1">(x</span><span class="s2">,</span><span class="s1">)</span><span class="s2">, </span><span class="s1">params)</span>
  <span class="s1">av = core.DShapedArray(_merge_dyn_shape(new_sizes</span><span class="s2">, </span><span class="s1">dyn)</span><span class="s2">, </span><span class="s1">x.dtype</span><span class="s2">, </span><span class="s1">x.weak_type)</span>
  <span class="s2">return </span><span class="s1">_dyn_shape_staging_rule(trace</span><span class="s2">, </span><span class="s1">reshape_p</span><span class="s2">, </span><span class="s1">av</span><span class="s2">, </span><span class="s1">x</span><span class="s2">, </span><span class="s1">*dyn</span><span class="s2">, </span><span class="s1">**params)</span>

<span class="s1">reshape_p = standard_primitive(_reshape_shape_rule</span><span class="s2">, </span><span class="s1">_reshape_dtype_rule</span><span class="s2">,</span>
                               <span class="s3">'reshape'</span><span class="s1">)</span>
<span class="s1">ad.deflinear2(reshape_p</span><span class="s2">, </span><span class="s1">_reshape_transpose_rule)</span>
<span class="s1">batching.primitive_batchers[reshape_p] = _reshape_batch_rule</span>
<span class="s1">mlir.register_lowering(reshape_p</span><span class="s2">, </span><span class="s1">_reshape_lower)</span>
<span class="s1">core.custom_typechecks[reshape_p] = _reshape_typecheck_rule</span>
<span class="s1">pe.custom_staging_rules[reshape_p] = _reshape_staging_rule</span>


<span class="s2">def </span><span class="s1">_rev_shape_rule(operand</span><span class="s2">, </span><span class="s1">*</span><span class="s2">, </span><span class="s1">dimensions):</span>
  <span class="s1">_check_shapelike(</span><span class="s3">'rev'</span><span class="s2">, </span><span class="s3">'dimensions'</span><span class="s2">, </span><span class="s1">dimensions)</span>
  <span class="s2">if </span><span class="s1">len(set(dimensions)) != len(dimensions):</span>
    <span class="s1">msg = </span><span class="s3">'rev dimensions must be unique, got {}.'</span>
    <span class="s2">raise </span><span class="s1">TypeError(msg.format(dimensions))</span>
  <span class="s2">if </span><span class="s1">dimensions </span><span class="s2">and not </span><span class="s1">_max(dimensions) &lt; operand.ndim:</span>
    <span class="s1">msg = (</span><span class="s3">'rev dimensions must all be less than operand ndim, got dimensions '</span>
           <span class="s3">'{} for operand ndim {}.'</span><span class="s1">)</span>
    <span class="s2">raise </span><span class="s1">TypeError(msg.format(dimensions</span><span class="s2">, </span><span class="s1">operand.ndim))</span>
  <span class="s2">return </span><span class="s1">operand.shape</span>

<span class="s2">def </span><span class="s1">_rev_batch_rule(batched_args</span><span class="s2">, </span><span class="s1">batch_dims</span><span class="s2">, </span><span class="s1">*</span><span class="s2">, </span><span class="s1">dimensions):</span>
  <span class="s1">operand</span><span class="s2">, </span><span class="s1">= batched_args</span>
  <span class="s1">bdim</span><span class="s2">, </span><span class="s1">= batch_dims</span>
  <span class="s1">new_dimensions = [i + </span><span class="s4">1 </span><span class="s2">if </span><span class="s1">i &gt;= bdim </span><span class="s2">else </span><span class="s1">i </span><span class="s2">for </span><span class="s1">i </span><span class="s2">in </span><span class="s1">dimensions]</span>
  <span class="s2">return </span><span class="s1">rev(operand</span><span class="s2">, </span><span class="s1">new_dimensions)</span><span class="s2">, </span><span class="s1">bdim</span>

<span class="s1">rev_p = standard_primitive(_rev_shape_rule</span><span class="s2">, </span><span class="s1">_input_dtype</span><span class="s2">, </span><span class="s3">'rev'</span><span class="s1">)</span>
<span class="s1">ad.deflinear2(rev_p</span><span class="s2">, lambda </span><span class="s1">t</span><span class="s2">, </span><span class="s1">_</span><span class="s2">, </span><span class="s1">dimensions: [rev(t</span><span class="s2">, </span><span class="s1">dimensions)])</span>
<span class="s1">batching.primitive_batchers[rev_p] = _rev_batch_rule</span>

<span class="s2">def </span><span class="s1">_rev_lower(ctx</span><span class="s2">, </span><span class="s1">x</span><span class="s2">, </span><span class="s1">*</span><span class="s2">, </span><span class="s1">dimensions):</span>
  <span class="s2">return </span><span class="s1">hlo.ReverseOp(x</span><span class="s2">, </span><span class="s1">mlir.dense_int_elements(dimensions)).results</span>
<span class="s1">mlir.register_lowering(rev_p</span><span class="s2">, </span><span class="s1">_rev_lower)</span>


<span class="s2">def </span><span class="s1">_transpose_shape_rule(operand</span><span class="s2">, </span><span class="s1">*</span><span class="s2">, </span><span class="s1">permutation):</span>
  <span class="s2">if not </span><span class="s1">isinstance(permutation</span><span class="s2">, </span><span class="s1">(tuple</span><span class="s2">, </span><span class="s1">list</span><span class="s2">, </span><span class="s1">np.ndarray)):</span>
    <span class="s1">msg = </span><span class="s3">&quot;transpose permutation must be a tuple/list/ndarray, got {}.&quot;</span>
    <span class="s2">raise </span><span class="s1">TypeError(msg.format(type(permutation)))</span>
  <span class="s2">if </span><span class="s1">tuple(sorted(permutation)) != tuple(range(operand.ndim)):</span>
    <span class="s1">msg = (</span><span class="s3">&quot;transpose permutation isn't a permutation of operand dimensions, &quot;</span>
           <span class="s3">&quot;got permutation {} for operand shape {}.&quot;</span><span class="s1">)</span>
    <span class="s2">raise </span><span class="s1">TypeError(msg.format(permutation</span><span class="s2">, </span><span class="s1">operand.shape))</span>
  <span class="s2">return </span><span class="s1">tuple(operand.shape[old_idx] </span><span class="s2">for </span><span class="s1">old_idx </span><span class="s2">in </span><span class="s1">permutation)</span>

<span class="s2">def </span><span class="s1">_transpose_batch_rule(batched_args</span><span class="s2">, </span><span class="s1">batch_dims</span><span class="s2">, </span><span class="s1">*</span><span class="s2">, </span><span class="s1">permutation):</span>
  <span class="s1">operand</span><span class="s2">, </span><span class="s1">= batched_args</span>
  <span class="s1">bdim</span><span class="s2">, </span><span class="s1">= batch_dims</span>
  <span class="s1">perm = (bdim</span><span class="s2">,</span><span class="s1">) + tuple(i </span><span class="s2">if </span><span class="s1">i &lt; bdim </span><span class="s2">else </span><span class="s1">i+</span><span class="s4">1 </span><span class="s2">for </span><span class="s1">i </span><span class="s2">in </span><span class="s1">permutation)</span>
  <span class="s2">return </span><span class="s1">transpose(operand</span><span class="s2">, </span><span class="s1">perm)</span><span class="s2">, </span><span class="s4">0</span>

<span class="s2">def </span><span class="s1">_transpose_lower(ctx</span><span class="s2">, </span><span class="s1">x</span><span class="s2">, </span><span class="s1">*</span><span class="s2">, </span><span class="s1">permutation):</span>
  <span class="s1">aval_out</span><span class="s2">, </span><span class="s1">= ctx.avals_out</span>
  <span class="s2">if </span><span class="s1">core.is_opaque_dtype(aval_out.dtype):</span>
    <span class="s2">return </span><span class="s1">[aval_out.dtype._rules.transpose_mlir(ctx</span><span class="s2">, </span><span class="s1">aval_out</span><span class="s2">, </span><span class="s1">x</span><span class="s2">, </span><span class="s1">permutation=permutation)]</span>
  <span class="s2">return </span><span class="s1">hlo.TransposeOp(x</span><span class="s2">, </span><span class="s1">mlir.dense_int_elements(permutation)).results</span>

<span class="s1">transpose_p = standard_primitive(_transpose_shape_rule</span><span class="s2">, </span><span class="s1">_input_dtype</span><span class="s2">,</span>
                                 <span class="s3">'transpose'</span><span class="s1">)</span>
<span class="s1">ad.deflinear2(transpose_p</span><span class="s2">,</span>
              <span class="s2">lambda </span><span class="s1">t</span><span class="s2">, </span><span class="s1">_</span><span class="s2">, </span><span class="s1">permutation: [transpose(t</span><span class="s2">, </span><span class="s1">np.argsort(permutation))])  </span><span class="s0"># type: ignore[arg-type]</span>
<span class="s1">batching.primitive_batchers[transpose_p] = _transpose_batch_rule</span>
<span class="s1">mlir.register_lowering(transpose_p</span><span class="s2">, </span><span class="s1">_transpose_lower)</span>
<span class="s1">pe.def_trivial_padding(transpose_p)</span>


<span class="s2">def </span><span class="s1">_select_shape_rule(which</span><span class="s2">, </span><span class="s1">*cases):</span>
  <span class="s2">if </span><span class="s1">len(cases) == </span><span class="s4">0</span><span class="s1">:</span>
    <span class="s2">raise </span><span class="s1">TypeError(</span><span class="s3">&quot;select must have at least one case&quot;</span><span class="s1">)</span>
  <span class="s2">if </span><span class="s1">any(case.shape != cases[</span><span class="s4">0</span><span class="s1">].shape </span><span class="s2">for </span><span class="s1">case </span><span class="s2">in </span><span class="s1">cases[</span><span class="s4">1</span><span class="s1">:]):</span>
    <span class="s1">msg = </span><span class="s3">&quot;select cases must have the same shapes, got [{}].&quot;</span>
    <span class="s2">raise </span><span class="s1">TypeError(msg.format(</span><span class="s3">&quot;, &quot;</span><span class="s1">.join([str(c.shape) </span><span class="s2">for </span><span class="s1">c </span><span class="s2">in </span><span class="s1">cases])))</span>
  <span class="s2">if </span><span class="s1">which.shape </span><span class="s2">and </span><span class="s1">which.shape != cases[</span><span class="s4">0</span><span class="s1">].shape:</span>
    <span class="s1">msg = (</span><span class="s3">&quot;select `which` must be scalar or have the same shape as cases, &quot;</span>
           <span class="s3">&quot;got `which` shape {} but case shape {}.&quot;</span><span class="s1">)</span>
    <span class="s2">raise </span><span class="s1">TypeError(msg.format(which.shape</span><span class="s2">, </span><span class="s1">cases[</span><span class="s4">0</span><span class="s1">].shape))</span>
  <span class="s2">return </span><span class="s1">cases[</span><span class="s4">0</span><span class="s1">].shape</span>

<span class="s2">def </span><span class="s1">_select_dtype_rule(which</span><span class="s2">, </span><span class="s1">*cases):</span>
  <span class="s1">_check_same_dtypes(</span><span class="s3">&quot;select&quot;</span><span class="s2">, False, </span><span class="s1">*(c.dtype </span><span class="s2">for </span><span class="s1">c </span><span class="s2">in </span><span class="s1">cases))</span>
  <span class="s2">if </span><span class="s1">(</span><span class="s2">not </span><span class="s1">dtypes.issubdtype(which.dtype</span><span class="s2">, </span><span class="s1">np.bool_) </span><span class="s2">and</span>
      <span class="s2">not </span><span class="s1">dtypes.issubdtype(which.dtype</span><span class="s2">, </span><span class="s1">np.integer)):</span>
    <span class="s2">raise </span><span class="s1">TypeError(</span><span class="s3">&quot;select `which` must be boolean or integer type, got &quot;</span>
                    <span class="s3">f&quot;</span><span class="s2">{</span><span class="s1">which.dtype</span><span class="s2">}</span><span class="s3">.&quot;</span><span class="s1">)</span>
  <span class="s2">if </span><span class="s1">dtypes.issubdtype(which.dtype</span><span class="s2">, </span><span class="s1">np.bool_) </span><span class="s2">and </span><span class="s1">len(cases) &gt; </span><span class="s4">2</span><span class="s1">:</span>
    <span class="s2">raise </span><span class="s1">TypeError(</span><span class="s3">&quot;select with boolean `which` cannot have &gt; 2 cases.&quot;</span><span class="s1">)</span>
  <span class="s2">return </span><span class="s1">cases[</span><span class="s4">0</span><span class="s1">].dtype</span>

<span class="s2">def </span><span class="s1">_select_weak_type_rule(which</span><span class="s2">, </span><span class="s1">*cases):</span>
  <span class="s2">return </span><span class="s1">all(c.weak_type </span><span class="s2">for </span><span class="s1">c </span><span class="s2">in </span><span class="s1">cases)</span>

<span class="s2">def </span><span class="s1">_select_transpose_rule(t</span><span class="s2">, </span><span class="s1">which</span><span class="s2">, </span><span class="s1">*cases):</span>
  <span class="s2">assert not </span><span class="s1">ad.is_undefined_primal(which)</span>
  <span class="s2">if </span><span class="s1">type(t) </span><span class="s2">is </span><span class="s1">ad_util.Zero:</span>
    <span class="s2">return </span><span class="s1">[</span><span class="s2">None</span><span class="s1">] + [ad_util.Zero(c.aval) </span><span class="s2">if </span><span class="s1">ad.is_undefined_primal(c) </span><span class="s2">else None</span>
                     <span class="s2">for </span><span class="s1">c </span><span class="s2">in </span><span class="s1">cases]</span>
  <span class="s2">else</span><span class="s1">:</span>
    <span class="s1">zeros = full_like(t</span><span class="s2">, </span><span class="s4">0</span><span class="s1">)</span>
    <span class="s2">return </span><span class="s1">[</span><span class="s2">None</span><span class="s1">] + [</span>
        <span class="s1">select(eq(which</span><span class="s2">, </span><span class="s1">_const(which</span><span class="s2">, </span><span class="s1">i))</span><span class="s2">, </span><span class="s1">t</span><span class="s2">, </span><span class="s1">zeros)</span>
        <span class="s2">if </span><span class="s1">ad.is_undefined_primal(case) </span><span class="s2">else None</span>
        <span class="s2">for </span><span class="s1">i</span><span class="s2">, </span><span class="s1">case </span><span class="s2">in </span><span class="s1">enumerate(cases)</span>
    <span class="s1">]</span>

<span class="s2">def </span><span class="s1">_select_batch_rule(batched_args</span><span class="s2">, </span><span class="s1">batch_dims</span><span class="s2">, </span><span class="s1">**unused_kwargs):</span>
  <span class="s1">which</span><span class="s2">, </span><span class="s1">*cases = batched_args</span>
  <span class="s1">which_bdim</span><span class="s2">, </span><span class="s1">*case_bdims = batch_dims</span>
  <span class="s1">size = next(x.shape[i] </span><span class="s2">for </span><span class="s1">x</span><span class="s2">, </span><span class="s1">i </span><span class="s2">in </span><span class="s1">zip(batched_args</span><span class="s2">, </span><span class="s1">batch_dims)</span>
              <span class="s2">if </span><span class="s1">i </span><span class="s2">is not None</span><span class="s1">)</span>

  <span class="s0"># avoid transposes and some broadcasts in special cases</span>
  <span class="s2">if </span><span class="s1">all(which_bdim == bdim </span><span class="s2">for </span><span class="s1">bdim </span><span class="s2">in </span><span class="s1">case_bdims):</span>
    <span class="s2">if </span><span class="s1">np.shape(which) == np.shape(cases[</span><span class="s4">0</span><span class="s1">]):</span>
      <span class="s2">return </span><span class="s1">select_n(which</span><span class="s2">, </span><span class="s1">*cases)</span><span class="s2">, </span><span class="s1">which_bdim</span>
    <span class="s2">else</span><span class="s1">:</span>
      <span class="s0"># vmapped function had a scalar which with nonscalar args</span>
      <span class="s2">assert </span><span class="s1">np.ndim(which) == </span><span class="s4">1</span>
      <span class="s1">which = broadcast_in_dim(which</span><span class="s2">, </span><span class="s1">cases[</span><span class="s4">0</span><span class="s1">].shape</span><span class="s2">, </span><span class="s1">[which_bdim])</span>
      <span class="s2">return </span><span class="s1">select_n(which</span><span class="s2">, </span><span class="s1">*cases)</span><span class="s2">, </span><span class="s1">which_bdim</span>
  <span class="s2">elif </span><span class="s1">np.ndim(which) == </span><span class="s4">0 </span><span class="s2">and </span><span class="s1">all(bdim </span><span class="s2">is not None for </span><span class="s1">bdim </span><span class="s2">in </span><span class="s1">case_bdims):</span>
    <span class="s2">if </span><span class="s1">all(case_bdims[</span><span class="s4">0</span><span class="s1">] == bdim </span><span class="s2">for </span><span class="s1">bdim </span><span class="s2">in </span><span class="s1">case_bdims[</span><span class="s4">1</span><span class="s1">:]):</span>
      <span class="s2">return </span><span class="s1">select_n(which</span><span class="s2">, </span><span class="s1">*cases)</span><span class="s2">, </span><span class="s1">case_bdims[</span><span class="s4">0</span><span class="s1">]</span>
    <span class="s2">elif </span><span class="s1">all(np.shape(cases[</span><span class="s4">0</span><span class="s1">]) == np.shape(c) </span><span class="s2">for </span><span class="s1">c </span><span class="s2">in </span><span class="s1">cases):</span>
      <span class="s1">bdim = case_bdims[</span><span class="s4">0</span><span class="s1">]</span>
      <span class="s1">other_cases = [batching.moveaxis(c</span><span class="s2">, </span><span class="s1">c_bdim</span><span class="s2">, </span><span class="s1">bdim)</span>
                     <span class="s2">for </span><span class="s1">c</span><span class="s2">, </span><span class="s1">c_bdim </span><span class="s2">in </span><span class="s1">zip(cases[</span><span class="s4">1</span><span class="s1">:]</span><span class="s2">, </span><span class="s1">case_bdims[</span><span class="s4">1</span><span class="s1">:])]</span>
      <span class="s2">return </span><span class="s1">select_n(which</span><span class="s2">, </span><span class="s1">cases[</span><span class="s4">0</span><span class="s1">]</span><span class="s2">, </span><span class="s1">*other_cases)</span><span class="s2">, </span><span class="s1">bdim</span>

  <span class="s1">which = (batching.bdim_at_front(which</span><span class="s2">, </span><span class="s1">which_bdim</span><span class="s2">, </span><span class="s1">size) </span><span class="s2">if </span><span class="s1">np.shape(which)</span>
           <span class="s2">else </span><span class="s1">which)</span>
  <span class="s2">if not </span><span class="s1">all(() == np.shape(c) </span><span class="s2">for </span><span class="s1">c </span><span class="s2">in </span><span class="s1">cases):</span>
    <span class="s1">cases = [batching.bdim_at_front(c</span><span class="s2">, </span><span class="s1">bdim</span><span class="s2">, </span><span class="s1">size)</span>
             <span class="s2">for </span><span class="s1">c</span><span class="s2">, </span><span class="s1">bdim </span><span class="s2">in </span><span class="s1">zip(cases</span><span class="s2">, </span><span class="s1">case_bdims)]</span>
  <span class="s2">assert </span><span class="s1">all(np.shape(cases[</span><span class="s4">0</span><span class="s1">]) == np.shape(c) </span><span class="s2">for </span><span class="s1">c </span><span class="s2">in </span><span class="s1">cases[</span><span class="s4">1</span><span class="s1">:])</span>
  <span class="s2">if </span><span class="s4">0 </span><span class="s1">&lt; np.ndim(which) &lt; np.ndim(cases[</span><span class="s4">0</span><span class="s1">]):</span>
    <span class="s0"># vmapped function had a scalar which with nonscalar args</span>
    <span class="s2">assert </span><span class="s1">np.ndim(which) == </span><span class="s4">1</span>
    <span class="s1">which = broadcast_in_dim(which</span><span class="s2">, </span><span class="s1">cases[</span><span class="s4">0</span><span class="s1">].shape</span><span class="s2">, </span><span class="s1">[</span><span class="s4">0</span><span class="s1">])</span>
  <span class="s2">if </span><span class="s1">np.ndim(which) &gt; np.ndim(cases[</span><span class="s4">0</span><span class="s1">]):</span>
    <span class="s2">assert </span><span class="s1">np.ndim(cases[</span><span class="s4">0</span><span class="s1">]) == </span><span class="s4">0</span>
    <span class="s1">cases = [broadcast(c</span><span class="s2">, </span><span class="s1">which.shape) </span><span class="s2">for </span><span class="s1">c </span><span class="s2">in </span><span class="s1">cases]</span>
  <span class="s2">return </span><span class="s1">select_n(which</span><span class="s2">, </span><span class="s1">*cases)</span><span class="s2">, </span><span class="s4">0</span>

<span class="s2">def </span><span class="s1">_select_jvp(primals</span><span class="s2">, </span><span class="s1">tangents):</span>
  <span class="s1">which</span><span class="s2">, </span><span class="s1">*case_primals = primals</span>
  <span class="s1">case_tangents = tangents[</span><span class="s4">1</span><span class="s1">:]</span>
  <span class="s1">out = select_n(which</span><span class="s2">, </span><span class="s1">*case_primals)</span>
  <span class="s2">if </span><span class="s1">all(type(t) </span><span class="s2">is </span><span class="s1">ad_util.Zero </span><span class="s2">for </span><span class="s1">t </span><span class="s2">in </span><span class="s1">case_tangents):</span>
    <span class="s1">out_dot = ad_util.Zero(case_tangents[</span><span class="s4">0</span><span class="s1">].aval)</span>
  <span class="s2">else</span><span class="s1">:</span>
    <span class="s1">z = _zeros(next(t </span><span class="s2">for </span><span class="s1">t </span><span class="s2">in </span><span class="s1">case_tangents </span><span class="s2">if </span><span class="s1">type(t) </span><span class="s2">is not </span><span class="s1">ad_util.Zero))</span>
    <span class="s1">case_tangents = [z </span><span class="s2">if </span><span class="s1">type(t) </span><span class="s2">is </span><span class="s1">ad_util.Zero </span><span class="s2">else </span><span class="s1">t </span><span class="s2">for </span><span class="s1">t </span><span class="s2">in </span><span class="s1">case_tangents]</span>
    <span class="s1">out_dot = select_n(which</span><span class="s2">, </span><span class="s1">*case_tangents)</span>
  <span class="s2">return </span><span class="s1">out</span><span class="s2">, </span><span class="s1">out_dot</span>

<span class="s2">def </span><span class="s1">_select_hlo_lowering(ctx</span><span class="s2">, </span><span class="s1">which</span><span class="s2">, </span><span class="s1">*cases):</span>
  <span class="s1">which_aval = ctx.avals_in[</span><span class="s4">0</span><span class="s1">]</span>
  <span class="s2">if </span><span class="s1">which_aval.dtype == np.dtype(np.bool_):</span>
    <span class="s2">assert </span><span class="s1">len(cases) &lt;= </span><span class="s4">2</span>
    <span class="s2">if </span><span class="s1">len(cases) == </span><span class="s4">1</span><span class="s1">: </span><span class="s2">return </span><span class="s1">cases</span>
    <span class="s2">return </span><span class="s1">hlo.SelectOp(which</span><span class="s2">, </span><span class="s1">cases[</span><span class="s4">1</span><span class="s1">]</span><span class="s2">, </span><span class="s1">cases[</span><span class="s4">0</span><span class="s1">]).results</span>

  <span class="s2">if </span><span class="s1">dtypes.issubdtype(which_aval.dtype</span><span class="s2">, </span><span class="s1">np.signedinteger):</span>
    <span class="s1">compare_type = </span><span class="s3">'SIGNED'</span>
  <span class="s2">else</span><span class="s1">:</span>
    <span class="s1">compare_type = </span><span class="s3">'UNSIGNED'</span>
  <span class="s1">lt = </span><span class="s3">'LT'</span>

  <span class="s2">def </span><span class="s1">_select(offset</span><span class="s2">, </span><span class="s1">cases):</span>
    <span class="s2">assert </span><span class="s1">len(cases) &gt; </span><span class="s4">0</span>
    <span class="s2">if </span><span class="s1">len(cases) == </span><span class="s4">1</span><span class="s1">:</span>
      <span class="s2">return </span><span class="s1">cases[</span><span class="s4">0</span><span class="s1">]</span>
    <span class="s1">mid = len(cases) // </span><span class="s4">2</span>
    <span class="s1">pred = mlir.compare_hlo(which</span><span class="s2">,</span>
                            <span class="s1">mlir.full_like_aval(ctx</span><span class="s2">, </span><span class="s1">offset + mid</span><span class="s2">, </span><span class="s1">which_aval)</span><span class="s2">,</span>
                            <span class="s1">lt</span><span class="s2">, </span><span class="s1">compare_type)</span>
    <span class="s2">return </span><span class="s1">hlo.SelectOp(pred</span><span class="s2">, </span><span class="s1">_select(offset</span><span class="s2">, </span><span class="s1">cases[:mid])</span><span class="s2">,</span>
                        <span class="s1">_select(offset + mid</span><span class="s2">, </span><span class="s1">cases[mid:])).result</span>

  <span class="s2">return </span><span class="s1">[_select(</span><span class="s4">0</span><span class="s2">, </span><span class="s1">cases)]</span>

<span class="s1">select_n_p = standard_primitive(</span>
    <span class="s1">_select_shape_rule</span><span class="s2">, </span><span class="s1">_select_dtype_rule</span><span class="s2">, </span><span class="s3">'select_n'</span><span class="s2">,</span>
    <span class="s1">weak_type_rule=_select_weak_type_rule)</span>
<span class="s1">ad.primitive_jvps[select_n_p] = _select_jvp</span>
<span class="s1">ad.primitive_transposes[select_n_p] = _select_transpose_rule</span>
<span class="s1">batching.primitive_batchers[select_n_p] = _select_batch_rule</span>
<span class="s1">mlir.register_lowering(select_n_p</span><span class="s2">, </span><span class="s1">_select_hlo_lowering)</span>
<span class="s1">pe.def_trivial_padding(select_n_p)</span>


<span class="s2">def </span><span class="s1">_reduce_shape_rule(*avals</span><span class="s2">, </span><span class="s1">computation</span><span class="s2">, </span><span class="s1">jaxpr</span><span class="s2">, </span><span class="s1">consts</span><span class="s2">, </span><span class="s1">dimensions):</span>
  <span class="s1">operand_avals</span><span class="s2">, </span><span class="s1">init_val_avals = split_list(avals</span><span class="s2">, </span><span class="s1">[len(avals) // </span><span class="s4">2</span><span class="s1">])</span>
  <span class="s2">if </span><span class="s1">any(arg.shape != () </span><span class="s2">for </span><span class="s1">arg </span><span class="s2">in </span><span class="s1">init_val_avals):</span>
    <span class="s1">init_val_shapes = [a.shape </span><span class="s2">for </span><span class="s1">a </span><span class="s2">in </span><span class="s1">init_val_avals]</span>
    <span class="s2">raise </span><span class="s1">ValueError(</span><span class="s3">f'reduce found non-scalar initial value: </span><span class="s2">{</span><span class="s1">init_val_shapes</span><span class="s2">}</span><span class="s3">'</span><span class="s1">)</span>
  <span class="s2">return </span><span class="s1">[tuple(np.delete(op.shape</span><span class="s2">, </span><span class="s1">dimensions)) </span><span class="s2">for </span><span class="s1">op </span><span class="s2">in </span><span class="s1">operand_avals]</span>

<span class="s2">def </span><span class="s1">_reduce_dtype_rule(*avals</span><span class="s2">, </span><span class="s1">computation</span><span class="s2">, </span><span class="s1">jaxpr</span><span class="s2">, </span><span class="s1">consts</span><span class="s2">, </span><span class="s1">dimensions):</span>
  <span class="s1">operand_avals</span><span class="s2">, </span><span class="s1">init_val_avals = split_list(avals</span><span class="s2">, </span><span class="s1">[len(avals) // </span><span class="s4">2</span><span class="s1">])</span>
  <span class="s1">operand_dtypes = [dtypes.canonicalize_dtype(op.dtype) </span><span class="s2">for </span><span class="s1">op </span><span class="s2">in </span><span class="s1">operand_avals]</span>
  <span class="s1">init_val_dtypes = [dtypes.canonicalize_dtype(init.dtype) </span><span class="s2">for </span><span class="s1">init </span><span class="s2">in </span><span class="s1">init_val_avals]</span>
  <span class="s2">if </span><span class="s1">operand_dtypes != init_val_dtypes:</span>
    <span class="s2">raise </span><span class="s1">TypeError(</span>
        <span class="s3">&quot;reduce operand dtypes should match corresponding initial value dtypes, &quot;</span>
        <span class="s3">f&quot;got operands=</span><span class="s2">{</span><span class="s1">operand_avals</span><span class="s2">} </span><span class="s3">and initial_values=</span><span class="s2">{</span><span class="s1">init_val_avals</span><span class="s2">}</span><span class="s3">&quot;</span><span class="s1">)</span>
  <span class="s2">return </span><span class="s1">operand_dtypes</span>

<span class="s2">def </span><span class="s1">_reduce_weak_type_rule(*avals</span><span class="s2">, </span><span class="s1">computation</span><span class="s2">, </span><span class="s1">jaxpr</span><span class="s2">, </span><span class="s1">consts</span><span class="s2">, </span><span class="s1">dimensions):</span>
  <span class="s1">operand_avals</span><span class="s2">, </span><span class="s1">init_val_avals = split_list(avals</span><span class="s2">, </span><span class="s1">[len(avals) // </span><span class="s4">2</span><span class="s1">])</span>
  <span class="s2">return </span><span class="s1">[op.weak_type </span><span class="s2">and </span><span class="s1">init_val.weak_type</span>
          <span class="s2">for </span><span class="s1">op</span><span class="s2">, </span><span class="s1">init_val </span><span class="s2">in </span><span class="s1">safe_zip(operand_avals</span><span class="s2">, </span><span class="s1">init_val_avals)]</span>

<span class="s2">def </span><span class="s1">_reduce_batch_rule(batched_args</span><span class="s2">, </span><span class="s1">batch_dims</span><span class="s2">, </span><span class="s1">*</span><span class="s2">, </span><span class="s1">computation</span><span class="s2">, </span><span class="s1">jaxpr</span><span class="s2">,</span>
                       <span class="s1">consts</span><span class="s2">, </span><span class="s1">dimensions):</span>
  <span class="s0"># TODO(mattjj,frostig): use batch_jaxpr, delete computation (assumes poly??)</span>
  <span class="s1">num_operands = len(batched_args) // </span><span class="s4">2</span>
  <span class="s1">operands</span><span class="s2">, </span><span class="s1">init_values = split_list(batched_args</span><span class="s2">, </span><span class="s1">[num_operands])</span>
  <span class="s1">operand_bdims</span><span class="s2">, </span><span class="s1">init_value_bdims = split_list(batch_dims</span><span class="s2">, </span><span class="s1">[num_operands])</span>
  <span class="s2">if </span><span class="s1">all(init_value_bdim </span><span class="s2">is </span><span class="s1">batching.not_mapped</span>
         <span class="s2">for </span><span class="s1">init_value_bdim </span><span class="s2">in </span><span class="s1">init_value_bdims):</span>
    <span class="s1">size = next(x.shape[ax] </span><span class="s2">for </span><span class="s1">x</span><span class="s2">, </span><span class="s1">ax </span><span class="s2">in </span><span class="s1">zip(batched_args</span><span class="s2">, </span><span class="s1">batch_dims)</span>
                <span class="s2">if </span><span class="s1">ax </span><span class="s2">is not None</span><span class="s1">)</span>
    <span class="s1">operands = [batching.bdim_at_front(arg</span><span class="s2">, </span><span class="s1">bdim</span><span class="s2">, </span><span class="s1">size)</span>
                <span class="s2">for </span><span class="s1">arg</span><span class="s2">, </span><span class="s1">bdim </span><span class="s2">in </span><span class="s1">zip(operands</span><span class="s2">, </span><span class="s1">operand_bdims)]</span>
    <span class="s1">new_dimensions = [d + </span><span class="s4">1 </span><span class="s2">for </span><span class="s1">d </span><span class="s2">in </span><span class="s1">dimensions]</span>
    <span class="s1">new_operand_bdims = [</span><span class="s4">0</span><span class="s1">] * num_operands</span>
    <span class="s2">return </span><span class="s1">reduce_p.bind(*(operands + init_values)</span><span class="s2">,</span>
                         <span class="s1">computation=computation</span><span class="s2">,</span>
                         <span class="s1">dimensions=tuple(new_dimensions)</span><span class="s2">,</span>
                         <span class="s1">consts=consts</span><span class="s2">,</span>
                         <span class="s1">jaxpr=jaxpr)</span><span class="s2">, </span><span class="s1">new_operand_bdims</span>
  <span class="s2">else</span><span class="s1">:</span>
    <span class="s2">raise </span><span class="s1">NotImplementedError  </span><span class="s0"># loop and stack</span>

<span class="s2">def </span><span class="s1">_reduce_jvp(reducer</span><span class="s2">, </span><span class="s1">init_values</span><span class="s2">, </span><span class="s1">primals</span><span class="s2">, </span><span class="s1">tangents</span><span class="s2">, </span><span class="s1">axes):</span>
  <span class="s1">input_shape = np.array(primals[</span><span class="s4">0</span><span class="s1">].shape</span><span class="s2">, </span><span class="s1">dtype=np.int_)</span>

  <span class="s1">n = np.prod(input_shape[list(axes)])</span>
  <span class="s1">non_axes = np.delete(np.arange(len(input_shape))</span><span class="s2">, </span><span class="s1">axes)</span>

  <span class="s0"># Move the reduced axes to the front, and flatten them to 1D.</span>
  <span class="s1">permutation = axes + tuple(non_axes)</span>
  <span class="s1">new_shape = (n</span><span class="s2">,</span><span class="s1">) + tuple(input_shape[non_axes])</span>
  <span class="s1">primals = tuple(reshape(x</span><span class="s2">, </span><span class="s1">new_shape</span><span class="s2">, </span><span class="s1">permutation) </span><span class="s2">for </span><span class="s1">x </span><span class="s2">in </span><span class="s1">primals)</span>
  <span class="s1">tangents = tuple(reshape(t</span><span class="s2">, </span><span class="s1">new_shape</span><span class="s2">, </span><span class="s1">permutation) </span><span class="s2">for </span><span class="s1">t </span><span class="s2">in </span><span class="s1">tangents)</span>

  <span class="s2">for </span><span class="s1">d </span><span class="s2">in </span><span class="s1">range(len(non_axes) + </span><span class="s4">1</span><span class="s1">):</span>
    <span class="s1">reducer = api.vmap(reducer)</span>
  <span class="s2">def </span><span class="s1">_reduce_tree(*xs</span><span class="s2">, </span><span class="s1">axis=</span><span class="s4">0</span><span class="s1">):</span>
    <span class="s5">&quot;&quot;&quot;Reduce by repeatedly splitting the array and multiplying.&quot;&quot;&quot;</span>
    <span class="s2">while </span><span class="s1">xs[</span><span class="s4">0</span><span class="s1">].shape[axis] &gt; </span><span class="s4">1</span><span class="s1">:</span>
      <span class="s1">n = xs[</span><span class="s4">0</span><span class="s1">].shape[axis]</span>
      <span class="s1">n1 = (n + </span><span class="s4">1</span><span class="s1">) // </span><span class="s4">2</span>
      <span class="s1">n2 = n - n1</span>
      <span class="s1">xs1 = [slicing.slice_in_dim(x</span><span class="s2">, </span><span class="s4">0</span><span class="s2">, </span><span class="s1">n1) </span><span class="s2">for </span><span class="s1">x </span><span class="s2">in </span><span class="s1">xs]</span>
      <span class="s1">xs2 = [slicing.slice_in_dim(x</span><span class="s2">, </span><span class="s1">n1</span><span class="s2">, None</span><span class="s1">) </span><span class="s2">for </span><span class="s1">x </span><span class="s2">in </span><span class="s1">xs]</span>
      <span class="s2">if </span><span class="s1">n2 != n1:</span>
        <span class="s1">paddings = [(</span><span class="s4">0</span><span class="s2">, </span><span class="s4">0</span><span class="s2">, </span><span class="s4">0</span><span class="s1">)] * len(xs[</span><span class="s4">0</span><span class="s1">].shape)</span>
        <span class="s1">paddings[axis] = (</span><span class="s4">0</span><span class="s2">, </span><span class="s4">1</span><span class="s2">, </span><span class="s4">0</span><span class="s1">)</span>
        <span class="s1">xs2 = [pad(x2</span><span class="s2">, </span><span class="s1">i</span><span class="s2">, </span><span class="s1">paddings) </span><span class="s2">for </span><span class="s1">x2</span><span class="s2">, </span><span class="s1">i </span><span class="s2">in </span><span class="s1">zip(xs2</span><span class="s2">, </span><span class="s1">init_values)]</span>
      <span class="s1">xs = reducer(*(xs1 + xs2))</span>
    <span class="s2">if </span><span class="s1">xs[</span><span class="s4">0</span><span class="s1">].shape[axis] == </span><span class="s4">0</span><span class="s1">:</span>
      <span class="s2">return </span><span class="s1">[full(input_shape[non_axes]</span><span class="s2">, </span><span class="s1">i) </span><span class="s2">for </span><span class="s1">i </span><span class="s2">in </span><span class="s1">init_values]</span>
    <span class="s2">return </span><span class="s1">tuple(squeeze(x</span><span class="s2">, </span><span class="s1">(axis</span><span class="s2">,</span><span class="s1">)) </span><span class="s2">for </span><span class="s1">x </span><span class="s2">in </span><span class="s1">xs)</span>

  <span class="s2">return </span><span class="s1">api.jvp(_reduce_tree</span><span class="s2">, </span><span class="s1">primals</span><span class="s2">, </span><span class="s1">tangents)</span>

<span class="s2">def </span><span class="s1">_reduce_jvp_rule(primals</span><span class="s2">, </span><span class="s1">tangents</span><span class="s2">, </span><span class="s1">*</span><span class="s2">, </span><span class="s1">computation</span><span class="s2">, </span><span class="s1">jaxpr</span><span class="s2">,</span>
                     <span class="s1">consts</span><span class="s2">, </span><span class="s1">dimensions):</span>
  <span class="s1">primal_xs</span><span class="s2">, </span><span class="s1">init_values = split_list(primals</span><span class="s2">, </span><span class="s1">[len(primals) // </span><span class="s4">2</span><span class="s1">])</span>
  <span class="s1">tangent_xs</span><span class="s2">, </span><span class="s1">tangent_init = split_list(tangents</span><span class="s2">, </span><span class="s1">[len(tangents) // </span><span class="s4">2</span><span class="s1">])</span>
  <span class="s0"># This test may be too strict, if a value is actually zero but we cannot prove</span>
  <span class="s0"># it is symbolically zero.</span>
  <span class="s2">if </span><span class="s1">any(type(t) </span><span class="s2">is not </span><span class="s1">ad_util.Zero </span><span class="s2">for </span><span class="s1">t </span><span class="s2">in </span><span class="s1">tangent_init):</span>
    <span class="s2">raise </span><span class="s1">NotImplementedError(</span>
      <span class="s3">&quot;Gradient of general lax.reduce with non-zero tangents for &quot;</span>
      <span class="s3">&quot;initial values to reduction not implemented&quot;</span><span class="s1">)</span>
  <span class="s1">reducer = core.jaxpr_as_fun(core.ClosedJaxpr(jaxpr</span><span class="s2">, </span><span class="s1">consts))</span>
  <span class="s2">return </span><span class="s1">_reduce_jvp(reducer</span><span class="s2">, </span><span class="s1">init_values</span><span class="s2">, </span><span class="s1">primal_xs</span><span class="s2">, </span><span class="s1">tangent_xs</span><span class="s2">, </span><span class="s1">dimensions)</span>

<span class="s2">def </span><span class="s1">_reduce_named_shape_rule(*avals</span><span class="s2">, </span><span class="s1">computation</span><span class="s2">, </span><span class="s1">jaxpr</span><span class="s2">, </span><span class="s1">consts</span><span class="s2">, </span><span class="s1">dimensions):</span>
  <span class="s0"># TODO(mattjj,frostig): see the TODOs noting limitations/assumptions in</span>
  <span class="s0"># _reduce_batching_rule. We're making the same assumptions here for now.</span>
  <span class="s1">num_operands = len(avals) // </span><span class="s4">2</span>
  <span class="s1">operand_avals</span><span class="s2">, </span><span class="s1">init_avals = split_list(avals</span><span class="s2">, </span><span class="s1">[num_operands])</span>
  <span class="s2">if </span><span class="s1">any(a.named_shape </span><span class="s2">for </span><span class="s1">a </span><span class="s2">in </span><span class="s1">init_avals):</span>
    <span class="s2">raise </span><span class="s1">NotImplementedError</span>
  <span class="s1">named_shapes = [a.named_shape </span><span class="s2">for </span><span class="s1">a </span><span class="s2">in </span><span class="s1">operand_avals]</span>
  <span class="s1">join = core.join_named_shapes(*(a.named_shape </span><span class="s2">for </span><span class="s1">a </span><span class="s2">in </span><span class="s1">operand_avals))</span>
  <span class="s2">return </span><span class="s1">[join] * len(named_shapes)</span>


<span class="s1">reduce_p = core.Primitive(</span><span class="s3">'reduce'</span><span class="s1">)</span>
<span class="s1">reduce_p.multiple_results = </span><span class="s2">True</span>
<span class="s1">reduce_p.def_impl(partial(dispatch.apply_primitive</span><span class="s2">, </span><span class="s1">reduce_p))</span>
<span class="s1">reduce_p.def_abstract_eval(</span>
    <span class="s1">partial(standard_multi_result_abstract_eval</span><span class="s2">, </span><span class="s1">reduce_p</span><span class="s2">, </span><span class="s1">_reduce_shape_rule</span><span class="s2">,</span>
            <span class="s1">_reduce_dtype_rule</span><span class="s2">, </span><span class="s1">_reduce_weak_type_rule</span><span class="s2">,</span>
            <span class="s1">_reduce_named_shape_rule))</span>
<span class="s1">batching.primitive_batchers[reduce_p] = _reduce_batch_rule</span>
<span class="s1">ad.primitive_jvps[reduce_p] = _reduce_jvp_rule</span>

<span class="s2">def </span><span class="s1">_reduce_lower(ctx</span><span class="s2">, </span><span class="s1">*values</span><span class="s2">, </span><span class="s1">computation</span><span class="s2">, </span><span class="s1">jaxpr</span><span class="s2">, </span><span class="s1">consts</span><span class="s2">, </span><span class="s1">dimensions):</span>
  <span class="s2">assert </span><span class="s1">all(isinstance(x</span><span class="s2">, </span><span class="s1">core.ShapedArray) </span><span class="s2">for </span><span class="s1">x </span><span class="s2">in </span><span class="s1">ctx.avals_in)</span><span class="s2">, </span><span class="s1">ctx.avals_in</span>
  <span class="s1">operands</span><span class="s2">, </span><span class="s1">init_values = util.split_list(values</span><span class="s2">, </span><span class="s1">[len(values) // </span><span class="s4">2</span><span class="s1">])</span>
  <span class="s1">init_value_avals = ctx.avals_in[len(values) // </span><span class="s4">2</span><span class="s1">:]</span>
  <span class="s1">op = hlo.ReduceOp([mlir.aval_to_ir_type(aval) </span><span class="s2">for </span><span class="s1">aval </span><span class="s2">in </span><span class="s1">ctx.avals_out]</span><span class="s2">,</span>
                    <span class="s1">operands</span><span class="s2">, </span><span class="s1">init_values</span><span class="s2">, </span><span class="s1">mlir.dense_int_elements(dimensions))</span>
  <span class="s1">ir_types = [mlir.aval_to_ir_type(aval) </span><span class="s2">for </span><span class="s1">aval </span><span class="s2">in </span><span class="s1">init_value_avals]</span>
  <span class="s1">reducer = op.regions[</span><span class="s4">0</span><span class="s1">].blocks.append(*(ir_types + ir_types))</span>
  <span class="s2">with </span><span class="s1">ir.InsertionPoint(reducer):</span>
    <span class="s1">reducer_ctx = ctx.module_context.replace(</span>
        <span class="s1">name_stack=source_info_util.new_name_stack())</span>
    <span class="s2">if </span><span class="s1">jaxpr.effects:</span>
      <span class="s2">raise </span><span class="s1">NotImplementedError(</span><span class="s3">'Cannot lower effectful `reduce`.'</span><span class="s1">)</span>
    <span class="s1">out_nodes</span><span class="s2">, </span><span class="s1">_ = mlir.jaxpr_subcomp(reducer_ctx</span><span class="s2">, </span><span class="s1">jaxpr</span><span class="s2">, </span><span class="s1">mlir.TokenSet()</span><span class="s2">, </span><span class="s1">consts</span><span class="s2">,</span>
                                      <span class="s1">*([a] </span><span class="s2">for </span><span class="s1">a </span><span class="s2">in </span><span class="s1">reducer.arguments)</span><span class="s2">,</span>
                                      <span class="s1">dim_var_values=ctx.dim_var_values)</span>
    <span class="s1">hlo.ReturnOp(util.flatten(out_nodes))</span>
  <span class="s2">return </span><span class="s1">op.results</span>

<span class="s1">mlir.register_lowering(reduce_p</span><span class="s2">, </span><span class="s1">_reduce_lower)</span>


<span class="s2">def </span><span class="s1">_reduce_number_dtype_rule(name</span><span class="s2">, </span><span class="s1">operand</span><span class="s2">, </span><span class="s1">*args</span><span class="s2">, </span><span class="s1">**kw):</span>
  <span class="s2">if not </span><span class="s1">dtypes.issubdtype(operand.dtype</span><span class="s2">, </span><span class="s1">np.number):</span>
    <span class="s2">raise </span><span class="s1">TypeError(</span><span class="s3">&quot;{} does not accept dtype {}. Accepted dtypes are subtypes &quot;</span>
                    <span class="s3">&quot;of number.&quot;</span><span class="s1">.format(name</span><span class="s2">, </span><span class="s1">np.dtype(operand.dtype).name))</span>
  <span class="s2">return </span><span class="s1">dtypes.canonicalize_dtype(operand.dtype)</span>

<span class="s2">def </span><span class="s1">_reduce_sum_shape_rule(operand</span><span class="s2">, </span><span class="s1">*</span><span class="s2">, </span><span class="s1">axes):</span>
  <span class="s2">return </span><span class="s1">_reduce_op_shape_rule(operand</span><span class="s2">, </span><span class="s1">axes=axes)</span>

<span class="s2">def </span><span class="s1">_reduce_sum_transpose_rule(cotangent</span><span class="s2">, </span><span class="s1">operand</span><span class="s2">, </span><span class="s1">*</span><span class="s2">, </span><span class="s1">axes):</span>
  <span class="s2">assert </span><span class="s1">ad.is_undefined_primal(operand)</span>
  <span class="s1">input_shape = operand.aval.shape</span>
  <span class="s1">broadcast_dimensions = tuple(np.delete(np.arange(len(input_shape))</span><span class="s2">, </span><span class="s1">axes))</span>
  <span class="s1">result = broadcast_in_dim(cotangent</span><span class="s2">, </span><span class="s1">input_shape</span><span class="s2">, </span><span class="s1">broadcast_dimensions)</span>
  <span class="s2">assert </span><span class="s1">result.shape == input_shape</span>
  <span class="s2">return </span><span class="s1">[result]</span>

<span class="s2">def </span><span class="s1">_reducer_padding(traceable</span><span class="s2">, </span><span class="s1">ident</span><span class="s2">, </span><span class="s1">in_avals</span><span class="s2">, </span><span class="s1">out_avals</span><span class="s2">, </span><span class="s1">operand</span><span class="s2">, </span><span class="s1">*</span><span class="s2">, </span><span class="s1">axes):</span>
  <span class="s2">del </span><span class="s1">out_avals</span>
  <span class="s1">aval</span><span class="s2">, </span><span class="s1">= in_avals</span>
  <span class="s1">padded_axes = [(i</span><span class="s2">, </span><span class="s1">d.val) </span><span class="s2">for </span><span class="s1">i</span><span class="s2">, </span><span class="s1">d </span><span class="s2">in </span><span class="s1">enumerate(aval.shape)</span>
                 <span class="s2">if </span><span class="s1">isinstance(d</span><span class="s2">, </span><span class="s1">pe.BoundedAxisSize)]</span>
  <span class="s1">operand_ = _replace_masked_values(operand</span><span class="s2">, </span><span class="s1">ident(aval.dtype)</span><span class="s2">, </span><span class="s1">padded_axes)</span>
  <span class="s2">return </span><span class="s1">[traceable(operand_</span><span class="s2">, </span><span class="s1">axes)]</span>

<span class="s2">def </span><span class="s1">_replace_masked_values(x</span><span class="s2">, </span><span class="s1">val</span><span class="s2">, </span><span class="s1">padded_axes):</span>
  <span class="s2">if not </span><span class="s1">padded_axes: </span><span class="s2">return </span><span class="s1">x</span>
  <span class="s1">dtype = dtypes._scalar_type_to_dtype(int)</span>
  <span class="s1">masks = [broadcasted_iota(dtype</span><span class="s2">, </span><span class="s1">x.shape</span><span class="s2">, </span><span class="s1">i) &lt; d </span><span class="s2">for </span><span class="s1">i</span><span class="s2">, </span><span class="s1">d </span><span class="s2">in </span><span class="s1">padded_axes]</span>
  <span class="s2">return </span><span class="s1">select(_reduce(operator.and_</span><span class="s2">, </span><span class="s1">masks)</span><span class="s2">, </span><span class="s1">x</span><span class="s2">, </span><span class="s1">full_like(x</span><span class="s2">, </span><span class="s1">val))</span>


<span class="s1">reduce_sum_p = standard_primitive(</span>
  <span class="s1">_reduce_sum_shape_rule</span><span class="s2">, </span><span class="s1">partial(_reduce_number_dtype_rule</span><span class="s2">, </span><span class="s3">'reduce_sum'</span><span class="s1">)</span><span class="s2">,</span>
  <span class="s3">'reduce_sum'</span><span class="s1">)</span>
<span class="s1">ad.deflinear2(reduce_sum_p</span><span class="s2">, </span><span class="s1">_reduce_sum_transpose_rule)</span>
<span class="s1">batching.defreducer(reduce_sum_p)</span>
<span class="s1">pe.padding_rules[reduce_sum_p] = partial(_reducer_padding</span><span class="s2">, </span><span class="s1">_reduce_sum</span><span class="s2">,</span>
                                         <span class="s1">_get_sum_identity)</span>


<span class="s2">def </span><span class="s1">_reduce_op_shape_rule(operand</span><span class="s2">, </span><span class="s1">*</span><span class="s2">, </span><span class="s1">axes</span><span class="s2">, </span><span class="s1">input_shape=</span><span class="s2">None</span><span class="s1">):</span>
  <span class="s2">del </span><span class="s1">input_shape  </span><span class="s0"># Unused.</span>
  <span class="s2">if </span><span class="s1">len(axes) != len(set(axes)):</span>
    <span class="s2">raise </span><span class="s1">ValueError(</span><span class="s3">f&quot;duplicate value in 'axes' of reduction: </span><span class="s2">{</span><span class="s1">axes</span><span class="s2">}</span><span class="s3">&quot;</span><span class="s1">)</span>
  <span class="s2">if not </span><span class="s1">all(</span><span class="s4">0 </span><span class="s1">&lt;= a &lt; operand.ndim </span><span class="s2">for </span><span class="s1">a </span><span class="s2">in </span><span class="s1">axes):</span>
    <span class="s2">raise </span><span class="s1">ValueError(</span><span class="s3">f&quot;reduction axes </span><span class="s2">{</span><span class="s1">axes</span><span class="s2">} </span><span class="s3">contains out-of-bounds indices for </span><span class="s2">{</span><span class="s1">operand</span><span class="s2">}</span><span class="s3">.&quot;</span><span class="s1">)</span>
  <span class="s1">axes = frozenset(axes)</span>
  <span class="s2">return </span><span class="s1">tuple(d </span><span class="s2">for </span><span class="s1">i</span><span class="s2">, </span><span class="s1">d </span><span class="s2">in </span><span class="s1">enumerate(operand.shape) </span><span class="s2">if </span><span class="s1">i </span><span class="s2">not in </span><span class="s1">axes)</span>

<span class="s2">def </span><span class="s1">_reduce_prod_jvp_rule(primals</span><span class="s2">, </span><span class="s1">tangents</span><span class="s2">, </span><span class="s1">*</span><span class="s2">, </span><span class="s1">axes):</span>
  <span class="s1">reducer = </span><span class="s2">lambda </span><span class="s1">x</span><span class="s2">, </span><span class="s1">y: [mul(x</span><span class="s2">, </span><span class="s1">y)]</span>
  <span class="s1">primals_out</span><span class="s2">, </span><span class="s1">tangents_out = _reduce_jvp(reducer</span><span class="s2">, </span><span class="s1">[_const(primals[</span><span class="s4">0</span><span class="s1">]</span><span class="s2">, </span><span class="s4">1</span><span class="s1">)]</span><span class="s2">,</span>
                                          <span class="s1">primals</span><span class="s2">, </span><span class="s1">tangents</span><span class="s2">, </span><span class="s1">axes)</span>
  <span class="s2">return </span><span class="s1">primals_out[</span><span class="s4">0</span><span class="s1">]</span><span class="s2">, </span><span class="s1">tangents_out[</span><span class="s4">0</span><span class="s1">]</span>

<span class="s1">reduce_prod_p = standard_primitive(</span>
  <span class="s1">_reduce_op_shape_rule</span><span class="s2">, </span><span class="s1">partial(_reduce_number_dtype_rule</span><span class="s2">, </span><span class="s3">'reduce_prod'</span><span class="s1">)</span><span class="s2">,</span>
  <span class="s3">'reduce_prod'</span><span class="s1">)</span>
<span class="s1">ad.primitive_jvps[reduce_prod_p] = _reduce_prod_jvp_rule</span>
<span class="s1">batching.defreducer(reduce_prod_p)</span>
<span class="s1">pe.padding_rules[reduce_prod_p] = partial(_reducer_padding</span><span class="s2">, </span><span class="s1">_reduce_prod</span><span class="s2">,</span>
                                          <span class="s1">_get_prod_identity)</span>


<span class="s2">def </span><span class="s1">_reduce_chooser_shape_rule(operand</span><span class="s2">, </span><span class="s1">*</span><span class="s2">, </span><span class="s1">axes):</span>
  <span class="s2">return </span><span class="s1">tuple(np.delete(operand.shape</span><span class="s2">, </span><span class="s1">axes))</span>

<span class="s2">def </span><span class="s1">_reduce_chooser_jvp_rule(g</span><span class="s2">, </span><span class="s1">ans</span><span class="s2">, </span><span class="s1">operand</span><span class="s2">, </span><span class="s1">*</span><span class="s2">, </span><span class="s1">axes):</span>
  <span class="s0"># TODO(mattjj): an alternative is to use variadic reduce to compute the chosen</span>
  <span class="s0"># locations in a single pass (rather than comparing equality) and use a</span>
  <span class="s0"># gather, and/or even push along the chosen elements of g (b/112040122)</span>
  <span class="s1">shape = [</span><span class="s4">1 </span><span class="s2">if </span><span class="s1">i </span><span class="s2">in </span><span class="s1">axes </span><span class="s2">else </span><span class="s1">d </span><span class="s2">for </span><span class="s1">i</span><span class="s2">, </span><span class="s1">d </span><span class="s2">in </span><span class="s1">enumerate(operand.shape)]</span>
  <span class="s1">location_indicators = convert_element_type(</span>
      <span class="s1">_eq_meet(operand</span><span class="s2">, </span><span class="s1">reshape(ans</span><span class="s2">, </span><span class="s1">shape))</span><span class="s2">, </span><span class="s1">g.dtype)</span>
  <span class="s1">counts = _reduce_sum(location_indicators</span><span class="s2">, </span><span class="s1">axes)</span>
  <span class="s2">return </span><span class="s1">div(_reduce_sum(mul(g</span><span class="s2">, </span><span class="s1">location_indicators)</span><span class="s2">, </span><span class="s1">axes)</span><span class="s2">, </span><span class="s1">counts)</span>


<span class="s1">reduce_max_p = standard_primitive(_reduce_op_shape_rule</span><span class="s2">, </span><span class="s1">_input_dtype</span><span class="s2">,</span>
                                  <span class="s3">'reduce_max'</span><span class="s1">)</span>
<span class="s1">ad.defjvp2(reduce_max_p</span><span class="s2">, </span><span class="s1">_reduce_chooser_jvp_rule)</span>
<span class="s1">batching.defreducer(reduce_max_p)</span>
<span class="s1">pe.padding_rules[reduce_max_p] = partial(_reducer_padding</span><span class="s2">, </span><span class="s1">_reduce_max</span><span class="s2">,</span>
                                         <span class="s1">_get_max_identity)</span>


<span class="s1">reduce_min_p = standard_primitive(_reduce_op_shape_rule</span><span class="s2">, </span><span class="s1">_input_dtype</span><span class="s2">,</span>
                                  <span class="s3">'reduce_min'</span><span class="s1">)</span>
<span class="s1">ad.defjvp2(reduce_min_p</span><span class="s2">, </span><span class="s1">_reduce_chooser_jvp_rule)</span>
<span class="s1">batching.defreducer(reduce_min_p)</span>
<span class="s1">pe.padding_rules[reduce_min_p] = partial(_reducer_padding</span><span class="s2">, </span><span class="s1">_reduce_min</span><span class="s2">,</span>
                                         <span class="s1">_get_min_identity)</span>


<span class="s2">def </span><span class="s1">_argminmax_shape_rule(operand</span><span class="s2">, </span><span class="s1">*</span><span class="s2">, </span><span class="s1">axes</span><span class="s2">, </span><span class="s1">index_dtype):</span>
  <span class="s1">axis</span><span class="s2">, </span><span class="s1">= axes</span>
  <span class="s2">if not </span><span class="s1">(</span><span class="s4">0 </span><span class="s1">&lt;= axis &lt; len(operand.shape)):</span>
    <span class="s2">raise </span><span class="s1">ValueError(</span><span class="s3">f&quot;Invalid axis </span><span class="s2">{</span><span class="s1">axis</span><span class="s2">} </span><span class="s3">for operand shape </span><span class="s2">{</span><span class="s1">operand.shape</span><span class="s2">}</span><span class="s3">&quot;</span><span class="s1">)</span>
  <span class="s2">if not </span><span class="s1">core.greater_equal_dim(operand.shape[axis]</span><span class="s2">, </span><span class="s4">1</span><span class="s1">):</span>
    <span class="s2">raise </span><span class="s1">ValueError(</span><span class="s3">&quot;argmin and argmax require non-empty reduced dimension. &quot;</span>
                     <span class="s3">f&quot;operand.shape=</span><span class="s2">{</span><span class="s1">operand.shape</span><span class="s2">} {</span><span class="s1">axis=</span><span class="s2">}</span><span class="s3">&quot;</span><span class="s1">)</span>
  <span class="s2">return </span><span class="s1">tuple(np.delete(operand.shape</span><span class="s2">, </span><span class="s1">axis))</span>

<span class="s2">def </span><span class="s1">_argminmax_dtype_rule(operand</span><span class="s2">, </span><span class="s1">*</span><span class="s2">, </span><span class="s1">axes</span><span class="s2">, </span><span class="s1">index_dtype):</span>
  <span class="s2">if not </span><span class="s1">dtypes.issubdtype(index_dtype</span><span class="s2">, </span><span class="s1">np.integer):</span>
    <span class="s2">raise </span><span class="s1">TypeError(</span><span class="s3">&quot;index_dtype must be an integer type, but got {}&quot;</span>
                    <span class="s1">.format(np.dtype(index_dtype).name))</span>
  <span class="s2">return </span><span class="s1">index_dtype</span>

<span class="s2">def </span><span class="s1">_compute_argminmax(value_comparator</span><span class="s2">, </span><span class="s1">get_identity</span><span class="s2">,</span>
                       <span class="s1">operand</span><span class="s2">, </span><span class="s1">*</span><span class="s2">, </span><span class="s1">index_dtype</span><span class="s2">, </span><span class="s1">axes):</span>
  <span class="s0"># value_comparator is either lax.lt (for argmin) or lax.gt</span>
  <span class="s0"># get_identity(operand.dtype) is inf for argmin or -inf for argmax</span>
  <span class="s1">axis</span><span class="s2">, </span><span class="s1">= axes</span>
  <span class="s1">indices = broadcasted_iota(index_dtype</span><span class="s2">, </span><span class="s1">np.shape(operand)</span><span class="s2">, </span><span class="s1">axis)</span>
  <span class="s2">def </span><span class="s1">reducer_fn(op_val_index</span><span class="s2">, </span><span class="s1">acc_val_index):</span>
    <span class="s1">op_val</span><span class="s2">, </span><span class="s1">op_index = op_val_index</span>
    <span class="s1">acc_val</span><span class="s2">, </span><span class="s1">acc_index = acc_val_index</span>
    <span class="s0"># Pick op_val if Lt (for argmin) or if NaN</span>
    <span class="s1">pick_op_val = bitwise_or(value_comparator(op_val</span><span class="s2">, </span><span class="s1">acc_val)</span><span class="s2">,</span>
                             <span class="s1">ne(op_val</span><span class="s2">, </span><span class="s1">op_val))</span>
    <span class="s0"># If x and y are not NaN and x = y, then pick the first</span>
    <span class="s1">pick_op_index = bitwise_or(pick_op_val</span><span class="s2">,</span>
                               <span class="s1">bitwise_and(eq(op_val</span><span class="s2">, </span><span class="s1">acc_val)</span><span class="s2">,</span>
                                           <span class="s1">lt(op_index</span><span class="s2">, </span><span class="s1">acc_index)))</span>
    <span class="s2">return </span><span class="s1">(select(pick_op_val</span><span class="s2">, </span><span class="s1">op_val</span><span class="s2">, </span><span class="s1">acc_val)</span><span class="s2">,</span>
            <span class="s1">select(pick_op_index</span><span class="s2">, </span><span class="s1">op_index</span><span class="s2">, </span><span class="s1">acc_index))</span>
  <span class="s1">res = reduce([operand</span><span class="s2">, </span><span class="s1">indices]</span><span class="s2">,</span>
               <span class="s1">[get_identity(operand.dtype)</span><span class="s2">, </span><span class="s1">np.array(</span><span class="s4">0</span><span class="s2">, </span><span class="s1">index_dtype)]</span><span class="s2">,</span>
               <span class="s1">reducer_fn</span><span class="s2">,</span>
               <span class="s1">axes)</span>
  <span class="s2">return </span><span class="s1">res[</span><span class="s4">1</span><span class="s1">]</span>

<span class="s1">argmin_p = standard_primitive(_argminmax_shape_rule</span><span class="s2">, </span><span class="s1">_argminmax_dtype_rule</span><span class="s2">,</span>
                              <span class="s3">'argmin'</span><span class="s2">, </span><span class="s1">weak_type_rule=_strip_weak_type)</span>
<span class="s1">batching.defreducer(argmin_p)</span>
<span class="s1">ad.defjvp_zero(argmin_p)</span>

<span class="s1">argmax_p = standard_primitive(_argminmax_shape_rule</span><span class="s2">, </span><span class="s1">_argminmax_dtype_rule</span><span class="s2">,</span>
                              <span class="s3">'argmax'</span><span class="s2">, </span><span class="s1">weak_type_rule=_strip_weak_type)</span>
<span class="s1">batching.defreducer(argmax_p)</span>
<span class="s1">ad.defjvp_zero(argmax_p)</span>

<span class="s1">mlir.register_lowering(argmin_p</span><span class="s2">, </span><span class="s1">mlir.cache_lowering(mlir.lower_fun(</span>
  <span class="s1">partial(_compute_argminmax</span><span class="s2">, </span><span class="s1">lt</span><span class="s2">, </span><span class="s1">_get_min_identity)</span><span class="s2">,</span>
  <span class="s1">multiple_results=</span><span class="s2">False</span><span class="s1">)))</span>

<span class="s1">mlir.register_lowering(argmax_p</span><span class="s2">, </span><span class="s1">mlir.cache_lowering(mlir.lower_fun(</span>
  <span class="s1">partial(_compute_argminmax</span><span class="s2">, </span><span class="s1">gt</span><span class="s2">, </span><span class="s1">_get_max_identity)</span><span class="s2">,</span>
  <span class="s1">multiple_results=</span><span class="s2">False</span><span class="s1">)))</span>


<span class="s2">def </span><span class="s1">_reduce_logical_shape_rule(operand</span><span class="s2">, </span><span class="s1">*</span><span class="s2">, </span><span class="s1">axes):</span>
  <span class="s2">if </span><span class="s1">operand.dtype != np.bool_ </span><span class="s2">and not </span><span class="s1">np.issubdtype(operand.dtype</span><span class="s2">, </span><span class="s1">np.integer):</span>
    <span class="s2">raise </span><span class="s1">TypeError(</span><span class="s3">f&quot;logical reduction requires operand dtype bool or int, got </span><span class="s2">{</span><span class="s1">operand.dtype</span><span class="s2">}</span><span class="s3">.&quot;</span><span class="s1">)</span>
  <span class="s2">return </span><span class="s1">tuple(np.delete(operand.shape</span><span class="s2">, </span><span class="s1">axes))</span>

<span class="s1">reduce_or_p = standard_primitive(</span>
    <span class="s1">_reduce_logical_shape_rule</span><span class="s2">, </span><span class="s1">_input_dtype</span><span class="s2">, </span><span class="s3">'reduce_or'</span><span class="s2">,</span>
    <span class="s1">weak_type_rule=_strip_weak_type)</span>
<span class="s1">batching.defreducer(reduce_or_p)</span>


<span class="s1">reduce_and_p = standard_primitive(</span>
    <span class="s1">_reduce_logical_shape_rule</span><span class="s2">, </span><span class="s1">_input_dtype</span><span class="s2">, </span><span class="s3">'reduce_and'</span><span class="s2">,</span>
    <span class="s1">weak_type_rule=_strip_weak_type)</span>
<span class="s1">batching.defreducer(reduce_and_p)</span>


<span class="s1">reduce_xor_p = standard_primitive(</span>
    <span class="s1">_reduce_logical_shape_rule</span><span class="s2">, </span><span class="s1">_input_dtype</span><span class="s2">, </span><span class="s3">'reduce_xor'</span><span class="s2">,</span>
    <span class="s1">weak_type_rule=_strip_weak_type)</span>
<span class="s1">batching.defreducer(reduce_xor_p)</span>


<span class="s2">def </span><span class="s1">_unary_reduce_lower(reducer</span><span class="s2">, </span><span class="s1">unit_factory</span><span class="s2">, </span><span class="s1">ctx</span><span class="s2">, </span><span class="s1">x</span><span class="s2">, </span><span class="s1">*</span><span class="s2">, </span><span class="s1">axes):</span>
  <span class="s1">aval_out</span><span class="s2">, </span><span class="s1">= ctx.avals_out</span>
  <span class="s1">dtype = aval_out.dtype</span>
  <span class="s1">op = hlo.ReduceOp([mlir.aval_to_ir_type(aval_out)]</span><span class="s2">, </span><span class="s1">[x]</span><span class="s2">,</span>
                    <span class="s1">mlir.ir_constants(unit_factory(aval_out.dtype))</span><span class="s2">,</span>
                    <span class="s1">mlir.dense_int_elements(axes))</span>
  <span class="s1">scalar_type = mlir.aval_to_ir_type(core.ShapedArray(()</span><span class="s2">, </span><span class="s1">dtype))</span>
  <span class="s1">reducer_region = op.regions[</span><span class="s4">0</span><span class="s1">].blocks.append(scalar_type</span><span class="s2">, </span><span class="s1">scalar_type)</span>
  <span class="s2">with </span><span class="s1">ir.InsertionPoint(reducer_region):</span>
    <span class="s1">add = reducer(*reducer_region.arguments)</span>
    <span class="s1">hlo.ReturnOp(add.results)</span>
  <span class="s2">return </span><span class="s1">op.results</span>

<span class="s1">mlir.register_lowering(reduce_sum_p</span><span class="s2">, </span><span class="s1">partial(_unary_reduce_lower</span><span class="s2">, </span><span class="s1">hlo.AddOp</span><span class="s2">,</span>
                                             <span class="s1">_get_sum_identity))</span>
<span class="s1">mlir.register_lowering(reduce_prod_p</span><span class="s2">, </span><span class="s1">partial(_unary_reduce_lower</span><span class="s2">, </span><span class="s1">hlo.MulOp</span><span class="s2">,</span>
                                              <span class="s1">_get_prod_identity))</span>
<span class="s1">mlir.register_lowering(reduce_or_p</span><span class="s2">, </span><span class="s1">partial(_unary_reduce_lower</span><span class="s2">, </span><span class="s1">hlo.OrOp</span><span class="s2">,</span>
                                            <span class="s1">_get_bitwise_or_identity))</span>
<span class="s1">mlir.register_lowering(reduce_and_p</span><span class="s2">, </span><span class="s1">partial(_unary_reduce_lower</span><span class="s2">, </span><span class="s1">hlo.AndOp</span><span class="s2">,</span>
                                             <span class="s1">_get_bitwise_and_identity))</span>
<span class="s1">mlir.register_lowering(reduce_xor_p</span><span class="s2">, </span><span class="s1">partial(_unary_reduce_lower</span><span class="s2">, </span><span class="s1">hlo.XorOp</span><span class="s2">,</span>
                                             <span class="s1">_get_bitwise_or_identity))</span>
<span class="s1">mlir.register_lowering(reduce_min_p</span><span class="s2">, </span><span class="s1">partial(_unary_reduce_lower</span><span class="s2">, </span><span class="s1">mlir.min_hlo</span><span class="s2">,</span>
                                             <span class="s1">_get_min_identity))</span>
<span class="s1">mlir.register_lowering(reduce_max_p</span><span class="s2">, </span><span class="s1">partial(_unary_reduce_lower</span><span class="s2">, </span><span class="s1">mlir.max_hlo</span><span class="s2">,</span>
                                             <span class="s1">_get_max_identity))</span>


<span class="s2">def </span><span class="s1">_reduce_precision_shape_rule(operand</span><span class="s2">, </span><span class="s1">*</span><span class="s2">, </span><span class="s1">exponent_bits</span><span class="s2">, </span><span class="s1">mantissa_bits):</span>
  <span class="s1">exponent_bits = operator.index(exponent_bits)</span>
  <span class="s1">mantissa_bits = operator.index(mantissa_bits)</span>
  <span class="s2">if </span><span class="s1">exponent_bits &lt; </span><span class="s4">1</span><span class="s1">:</span>
    <span class="s2">raise </span><span class="s1">ValueError(</span><span class="s3">f&quot;reduce_precision: exponent_bits must be positive; got </span><span class="s2">{</span><span class="s1">exponent_bits</span><span class="s2">}</span><span class="s3">&quot;</span><span class="s1">)</span>
  <span class="s2">if </span><span class="s1">mantissa_bits &lt; </span><span class="s4">0</span><span class="s1">:</span>
    <span class="s2">raise </span><span class="s1">ValueError(</span><span class="s3">f&quot;reduce_precision: mantissa_bits must be non-negative; got </span><span class="s2">{</span><span class="s1">mantissa_bits</span><span class="s2">}</span><span class="s3">&quot;</span><span class="s1">)</span>
  <span class="s2">return </span><span class="s1">operand.shape</span>


<span class="s1">reduce_precision_p = standard_primitive(</span>
    <span class="s1">_reduce_precision_shape_rule</span><span class="s2">,</span>
    <span class="s1">partial(unop_dtype_rule</span><span class="s2">, </span><span class="s1">_identity</span><span class="s2">, </span><span class="s1">_float</span><span class="s2">, </span><span class="s3">'reduce_precision'</span><span class="s1">)</span><span class="s2">,</span>
    <span class="s1">name=</span><span class="s3">'reduce_precision'</span><span class="s1">)</span>
<span class="s1">batching.defvectorized(reduce_precision_p)</span>

<span class="s2">def </span><span class="s1">_reduce_precision_lower(ctx</span><span class="s2">, </span><span class="s1">operand</span><span class="s2">, </span><span class="s1">*</span><span class="s2">, </span><span class="s1">exponent_bits</span><span class="s2">, </span><span class="s1">mantissa_bits):</span>
  <span class="s1">aval_out</span><span class="s2">, </span><span class="s1">= ctx.avals_out</span>
  <span class="s2">return </span><span class="s1">hlo.ReducePrecisionOp(operand</span><span class="s2">, </span><span class="s1">mlir.i32_attr(exponent_bits)</span><span class="s2">,</span>
                               <span class="s1">mlir.i32_attr(mantissa_bits)).results</span>

<span class="s1">mlir.register_lowering(reduce_precision_p</span><span class="s2">, </span><span class="s1">_reduce_precision_lower)</span>


<span class="s1">_UINT_DTYPES = {</span>
  <span class="s4">16</span><span class="s1">: np.dtype(np.uint16)</span><span class="s2">,</span>
  <span class="s4">32</span><span class="s1">: np.dtype(np.uint32)</span><span class="s2">,</span>
  <span class="s4">64</span><span class="s1">: np.dtype(np.uint64)</span><span class="s2">,</span>
<span class="s1">}</span>

<span class="s1">_INT_DTYPES = {</span>
  <span class="s4">16</span><span class="s1">: np.dtype(np.int16)</span><span class="s2">,</span>
  <span class="s4">32</span><span class="s1">: np.dtype(np.int32)</span><span class="s2">,</span>
  <span class="s4">64</span><span class="s1">: np.dtype(np.int64)</span><span class="s2">,</span>
<span class="s1">}</span>


<span class="s2">def </span><span class="s1">_sort_abstract_eval(*args</span><span class="s2">, </span><span class="s1">**kwargs):</span>
  <span class="s1">args = tuple(raise_to_shaped(arg) </span><span class="s2">for </span><span class="s1">arg </span><span class="s2">in </span><span class="s1">args)</span>
  <span class="s2">if </span><span class="s1">any(arg.shape != args[</span><span class="s4">0</span><span class="s1">].shape </span><span class="s2">for </span><span class="s1">arg </span><span class="s2">in </span><span class="s1">args[</span><span class="s4">1</span><span class="s1">:]):</span>
    <span class="s1">shapes = </span><span class="s3">&quot; &quot;</span><span class="s1">.join(str(a.shape) </span><span class="s2">for </span><span class="s1">a </span><span class="s2">in </span><span class="s1">args)</span>
    <span class="s2">raise </span><span class="s1">TypeError(</span><span class="s3">f&quot;Arguments to sort must have equal shapes, got: </span><span class="s2">{</span><span class="s1">shapes</span><span class="s2">}</span><span class="s3">&quot;</span><span class="s1">)</span>
  <span class="s2">return </span><span class="s1">args</span>


<span class="s2">def </span><span class="s1">_float_to_int_for_sort(x):</span>
  <span class="s0"># Switch from a floating point value to a integer value in such a way that</span>
  <span class="s0"># when using the integer value to compare, we get the same result for normal</span>
  <span class="s0"># values, and -nan is treated as the smallest value, and nan is treated as</span>
  <span class="s0"># the largest value.</span>
  <span class="s0"># If f is a float, and</span>
  <span class="s0"># x = bit_cast&lt;int32&gt;(f);</span>
  <span class="s0"># y = x &lt; 0 ? int32_max - x : x;</span>
  <span class="s0"># then y is ordered as an int32 such that finite values have the obvious</span>
  <span class="s0"># order. In this scheme, -0 would be before 0, and -NaN and NaN appear at</span>
  <span class="s0"># the beginning and end of the ordering. This causes issues for stable</span>
  <span class="s0"># sorts, so we avoid this by standardizing the representation of zeros</span>
  <span class="s0"># and NaNs in the output.</span>
  <span class="s0"># Note that in order to avoid -x to overflow, we calculate</span>
  <span class="s0"># int32_max - x as unsigned, and then convert back to signed.</span>
  <span class="s2">if </span><span class="s1">x.dtype == dtypes.bfloat16:</span>
    <span class="s1">x = convert_element_type(x</span><span class="s2">, </span><span class="s1">np.float32)</span>
  <span class="s1">nbits = np.finfo(x).bits</span>
  <span class="s1">signed_dtype = _INT_DTYPES[nbits]</span>
  <span class="s1">unsigned_dtype = _UINT_DTYPES[nbits]</span>

  <span class="s1">signed = bitcast_convert_type(x</span><span class="s2">, </span><span class="s1">signed_dtype)</span>
  <span class="s1">unsigned = bitcast_convert_type(x</span><span class="s2">, </span><span class="s1">unsigned_dtype)</span>

  <span class="s0"># We cannot standardize zeros in x because XLA elides this is some cases.</span>
  <span class="s0"># We cannot standardize NaNs in x because it triggers jax.debug_nans</span>
  <span class="s0"># So instead we do these replacements in the signed integer representation.</span>

  <span class="s0"># Standardize zeros:</span>
  <span class="s1">signed = select(eq(x</span><span class="s2">, </span><span class="s1">_zero(x))</span><span class="s2">, </span><span class="s1">_zeros(signed)</span><span class="s2">, </span><span class="s1">signed)</span>
  <span class="s0"># Standardize nans:</span>
  <span class="s1">signed_nan = x.dtype.type(np.nan).view(signed_dtype)</span>
  <span class="s1">signed = select(_isnan(x)</span><span class="s2">, </span><span class="s1">full_like(signed</span><span class="s2">, </span><span class="s1">signed_nan)</span><span class="s2">, </span><span class="s1">signed)</span>

  <span class="s1">flipped = bitcast_convert_type(</span>
    <span class="s1">sub(unsigned_dtype.type(np.iinfo(signed_dtype).max)</span><span class="s2">, </span><span class="s1">unsigned)</span><span class="s2">, </span><span class="s1">signed_dtype)</span>
  <span class="s2">return </span><span class="s1">select(lt(signed</span><span class="s2">, </span><span class="s1">_zero(signed))</span><span class="s2">, </span><span class="s1">flipped</span><span class="s2">, </span><span class="s1">signed)</span>

<span class="s0"># Default comparator that sorts the operands lexicographically on the</span>
<span class="s0"># first `num_keys` arguments.</span>
<span class="s0"># For floating point types, a total order is created where</span>
<span class="s0"># -infinity &lt; ... &lt; 0 &lt; ... &lt; infinity &lt; NaN.</span>
<span class="s0"># 0.0 and -0.0 are treated as equivalent, as are all NaN representations.</span>
<span class="s0"># For complex types, the (real, imag) pairs are sorted lexicographically</span>
<span class="s0"># (following NumPy's semantics).</span>
<span class="s0"># This code adds complex-number support and lexicographic ordering to the algorithm from:</span>
<span class="s0"># https://github.com/tensorflow/tensorflow/blob/ba43780830f09da72081fe5061c436f1c6203a92/tensorflow/compiler/xla/client/lib/comparators.h#L33</span>
<span class="s2">def </span><span class="s1">_sort_lt_comparator(*operands</span><span class="s2">, </span><span class="s1">num_keys=</span><span class="s4">1</span><span class="s1">):</span>
  <span class="s1">x_keys</span><span class="s2">, </span><span class="s1">y_keys = _operands_to_keys(*operands</span><span class="s2">, </span><span class="s1">num_keys=num_keys)</span>
  <span class="s1">p = </span><span class="s2">None</span>
  <span class="s2">for </span><span class="s1">xk</span><span class="s2">, </span><span class="s1">yk </span><span class="s2">in </span><span class="s1">zip(x_keys[::-</span><span class="s4">1</span><span class="s1">]</span><span class="s2">, </span><span class="s1">y_keys[::-</span><span class="s4">1</span><span class="s1">]):</span>
    <span class="s1">p = (bitwise_or(lt(xk</span><span class="s2">, </span><span class="s1">yk)</span><span class="s2">, </span><span class="s1">bitwise_and(eq(xk</span><span class="s2">, </span><span class="s1">yk)</span><span class="s2">, </span><span class="s1">p)) </span><span class="s2">if </span><span class="s1">p </span><span class="s2">is not None</span>
         <span class="s2">else </span><span class="s1">lt(xk</span><span class="s2">, </span><span class="s1">yk))</span>
  <span class="s2">return </span><span class="s1">p</span>

<span class="s0"># Similar to sort_lt_comparator, but implements less than or equal. Used by</span>
<span class="s0"># the searchsorted() implementation.</span>
<span class="s2">def </span><span class="s1">_sort_le_comparator(*operands</span><span class="s2">, </span><span class="s1">num_keys=</span><span class="s4">1</span><span class="s1">):</span>
  <span class="s1">x_keys</span><span class="s2">, </span><span class="s1">y_keys = _operands_to_keys(*operands</span><span class="s2">, </span><span class="s1">num_keys=num_keys)</span>
  <span class="s1">p = </span><span class="s2">None</span>
  <span class="s2">for </span><span class="s1">xk</span><span class="s2">, </span><span class="s1">yk </span><span class="s2">in </span><span class="s1">zip(x_keys[::-</span><span class="s4">1</span><span class="s1">]</span><span class="s2">, </span><span class="s1">y_keys[::-</span><span class="s4">1</span><span class="s1">]):</span>
    <span class="s1">p = (bitwise_or(lt(xk</span><span class="s2">, </span><span class="s1">yk)</span><span class="s2">, </span><span class="s1">bitwise_and(eq(xk</span><span class="s2">, </span><span class="s1">yk)</span><span class="s2">, </span><span class="s1">p)) </span><span class="s2">if </span><span class="s1">p </span><span class="s2">is not None</span>
         <span class="s2">else </span><span class="s1">le(xk</span><span class="s2">, </span><span class="s1">yk))</span>
  <span class="s2">return </span><span class="s1">p</span>

<span class="s2">def </span><span class="s1">_operands_to_keys(*operands</span><span class="s2">, </span><span class="s1">num_keys=</span><span class="s4">1</span><span class="s1">):</span>
  <span class="s2">assert </span><span class="s1">len(operands) &gt;= </span><span class="s4">2 </span><span class="s2">and </span><span class="s1">len(operands) % </span><span class="s4">2 </span><span class="s1">== </span><span class="s4">0</span><span class="s2">, </span><span class="s1">operands</span>
  <span class="s2">assert </span><span class="s1">len(operands) // </span><span class="s4">2 </span><span class="s1">&gt;= num_keys</span><span class="s2">, </span><span class="s1">(operands</span><span class="s2">, </span><span class="s1">num_keys)</span>
  <span class="s1">x_keys</span><span class="s2">, </span><span class="s1">y_keys = []</span><span class="s2">, </span><span class="s1">[]</span>
  <span class="s2">for </span><span class="s1">x</span><span class="s2">, </span><span class="s1">y </span><span class="s2">in </span><span class="s1">zip(operands[:</span><span class="s4">2</span><span class="s1">*num_keys:</span><span class="s4">2</span><span class="s1">]</span><span class="s2">, </span><span class="s1">operands[</span><span class="s4">1</span><span class="s1">:</span><span class="s4">2</span><span class="s1">*num_keys:</span><span class="s4">2</span><span class="s1">]):</span>
    <span class="s2">assert </span><span class="s1">x.dtype == y.dtype</span><span class="s2">, </span><span class="s1">(x.dtype</span><span class="s2">, </span><span class="s1">y.dtype)</span>
    <span class="s2">if </span><span class="s1">dtypes.issubdtype(x.dtype</span><span class="s2">, </span><span class="s1">np.complexfloating):</span>
      <span class="s1">x_keys.extend([_float_to_int_for_sort(real(x))</span><span class="s2">, </span><span class="s1">_float_to_int_for_sort(imag(x))])</span>
      <span class="s1">y_keys.extend([_float_to_int_for_sort(real(y))</span><span class="s2">, </span><span class="s1">_float_to_int_for_sort(imag(y))])</span>
    <span class="s2">elif </span><span class="s1">dtypes.issubdtype(x.dtype</span><span class="s2">, </span><span class="s1">np.floating):</span>
      <span class="s1">x_keys.append(_float_to_int_for_sort(x))</span>
      <span class="s1">y_keys.append(_float_to_int_for_sort(y))</span>
    <span class="s2">else</span><span class="s1">:</span>
      <span class="s1">x_keys.append(x)</span>
      <span class="s1">y_keys.append(y)</span>
  <span class="s2">return </span><span class="s1">x_keys</span><span class="s2">, </span><span class="s1">y_keys</span>


<span class="s2">def </span><span class="s1">_sort_jvp(primals</span><span class="s2">, </span><span class="s1">tangents</span><span class="s2">, </span><span class="s1">*</span><span class="s2">, </span><span class="s1">dimension</span><span class="s2">, </span><span class="s1">is_stable</span><span class="s2">, </span><span class="s1">num_keys):</span>
  <span class="s1">shape = primals[</span><span class="s4">0</span><span class="s1">].shape</span>
  <span class="s1">iotas = []</span>
  <span class="s2">for </span><span class="s1">dim</span><span class="s2">, </span><span class="s1">size </span><span class="s2">in </span><span class="s1">enumerate(shape):</span>
    <span class="s1">dtype = np.int32 </span><span class="s2">if </span><span class="s1">size &lt; np.iinfo(np.int32).max </span><span class="s2">else </span><span class="s1">np.int64</span>
    <span class="s1">iotas.append(broadcasted_iota(dtype</span><span class="s2">, </span><span class="s1">shape</span><span class="s2">, </span><span class="s1">dim))</span>
  <span class="s1">primals = sort_p.bind(*(primals + (iotas[dimension]</span><span class="s2">,</span><span class="s1">))</span><span class="s2">, </span><span class="s1">dimension=dimension</span><span class="s2">,</span>
                        <span class="s1">is_stable=is_stable</span><span class="s2">, </span><span class="s1">num_keys=num_keys)</span>
  <span class="s1">idx = tuple(primals[-</span><span class="s4">1</span><span class="s1">] </span><span class="s2">if </span><span class="s1">i == dimension </span><span class="s2">else </span><span class="s1">iotas[i]</span>
              <span class="s2">for </span><span class="s1">i </span><span class="s2">in </span><span class="s1">range(len(shape)))</span>
  <span class="s1">tangents_out = tuple(t </span><span class="s2">if </span><span class="s1">type(t) </span><span class="s2">is </span><span class="s1">ad_util.Zero </span><span class="s2">else </span><span class="s1">t[idx] </span><span class="s2">for </span><span class="s1">t </span><span class="s2">in </span><span class="s1">tangents)</span>
  <span class="s2">return </span><span class="s1">tuple(primals[:-</span><span class="s4">1</span><span class="s1">])</span><span class="s2">, </span><span class="s1">tangents_out</span>

<span class="s2">def </span><span class="s1">_sort_batch_rule(batched_args</span><span class="s2">, </span><span class="s1">batch_dims</span><span class="s2">, </span><span class="s1">*</span><span class="s2">, </span><span class="s1">dimension</span><span class="s2">, </span><span class="s1">is_stable</span><span class="s2">, </span><span class="s1">num_keys):</span>
  <span class="s1">prototype_arg</span><span class="s2">, </span><span class="s1">new_bdim = next(</span>
    <span class="s1">(a</span><span class="s2">, </span><span class="s1">b) </span><span class="s2">for </span><span class="s1">a</span><span class="s2">, </span><span class="s1">b </span><span class="s2">in </span><span class="s1">zip(batched_args</span><span class="s2">, </span><span class="s1">batch_dims) </span><span class="s2">if </span><span class="s1">b </span><span class="s2">is not None</span><span class="s1">)</span>
  <span class="s1">new_args = []</span>
  <span class="s2">for </span><span class="s1">arg</span><span class="s2">, </span><span class="s1">bdim </span><span class="s2">in </span><span class="s1">zip(batched_args</span><span class="s2">, </span><span class="s1">batch_dims):</span>
    <span class="s2">if </span><span class="s1">bdim </span><span class="s2">is None</span><span class="s1">:</span>
      <span class="s1">dims = np.delete(np.arange(prototype_arg.ndim)</span><span class="s2">, </span><span class="s1">new_bdim)</span>
      <span class="s1">new_args.append(broadcast_in_dim(arg</span><span class="s2">, </span><span class="s1">prototype_arg.shape</span><span class="s2">, </span><span class="s1">dims))</span>
    <span class="s2">else</span><span class="s1">:</span>
      <span class="s1">new_args.append(batching.moveaxis(arg</span><span class="s2">, </span><span class="s1">bdim</span><span class="s2">, </span><span class="s1">new_bdim))</span>
  <span class="s1">new_dimension = dimension + (new_bdim &lt;= dimension)</span>
  <span class="s1">bdims = (new_bdim</span><span class="s2">,</span><span class="s1">) * len(new_args)</span>
  <span class="s2">return </span><span class="s1">(sort_p.bind(*new_args</span><span class="s2">, </span><span class="s1">dimension=new_dimension</span><span class="s2">, </span><span class="s1">is_stable=is_stable</span><span class="s2">, </span><span class="s1">num_keys=num_keys)</span><span class="s2">,</span>
          <span class="s1">bdims)</span>


<span class="s1">sort_p = Primitive(</span><span class="s3">'sort'</span><span class="s1">)</span>
<span class="s1">sort_p.multiple_results = </span><span class="s2">True</span>
<span class="s1">sort_p.def_impl(partial(dispatch.apply_primitive</span><span class="s2">, </span><span class="s1">sort_p))</span>
<span class="s1">sort_p.def_abstract_eval(_sort_abstract_eval)</span>
<span class="s1">ad.primitive_jvps[sort_p] = _sort_jvp</span>
<span class="s1">batching.primitive_batchers[sort_p] = _sort_batch_rule</span>


<span class="s2">def </span><span class="s1">_sort_lower(ctx</span><span class="s2">, </span><span class="s1">*operands</span><span class="s2">, </span><span class="s1">dimension</span><span class="s2">, </span><span class="s1">is_stable</span><span class="s2">, </span><span class="s1">num_keys):</span>
  <span class="s2">assert </span><span class="s1">all(isinstance(x</span><span class="s2">, </span><span class="s1">core.ShapedArray) </span><span class="s2">for </span><span class="s1">x </span><span class="s2">in </span><span class="s1">ctx.avals_in)</span><span class="s2">, </span><span class="s1">ctx.avals_in</span>
  <span class="s1">sort = hlo.SortOp([mlir.aval_to_ir_type(aval) </span><span class="s2">for </span><span class="s1">aval </span><span class="s2">in </span><span class="s1">ctx.avals_out]</span><span class="s2">,</span>
                    <span class="s1">mlir.flatten_lowering_ir_args(operands)</span><span class="s2">,</span>
                    <span class="s1">dimension=mlir.i64_attr(dimension)</span><span class="s2">,</span>
                    <span class="s1">is_stable=ir.BoolAttr.get(is_stable))</span>
  <span class="s1">scalar_avals = [aval.update(shape=()) </span><span class="s2">for </span><span class="s1">aval </span><span class="s2">in </span><span class="s1">ctx.avals_in]</span>
  <span class="s1">scalar_types = safe_map(mlir.aval_to_ir_type</span><span class="s2">, </span><span class="s1">scalar_avals)</span>
  <span class="s1">comparator = sort.comparator.blocks.append(</span>
      <span class="s1">*util.flatten(zip(scalar_types</span><span class="s2">, </span><span class="s1">scalar_types)))</span>
  <span class="s2">with </span><span class="s1">ir.InsertionPoint(comparator):</span>
    <span class="s1">lower_comparator = mlir.lower_fun(partial(_sort_lt_comparator)</span><span class="s2">,</span>
                                      <span class="s1">multiple_results=</span><span class="s2">False</span><span class="s1">)</span>
    <span class="s1">sub_ctx = ctx.replace(primitive=</span><span class="s2">None,</span>
                          <span class="s1">avals_in=util.flatten(zip(scalar_avals</span><span class="s2">, </span><span class="s1">scalar_avals))</span><span class="s2">,</span>
                          <span class="s1">avals_out=[core.ShapedArray(()</span><span class="s2">, </span><span class="s1">np.bool_)])</span>

    <span class="s1">out = lower_comparator(sub_ctx</span><span class="s2">, </span><span class="s1">*[[a] </span><span class="s2">for </span><span class="s1">a </span><span class="s2">in </span><span class="s1">comparator.arguments]</span><span class="s2">,</span>
                           <span class="s1">num_keys=num_keys)</span>
    <span class="s1">hlo.ReturnOp(util.flatten(out))</span>
  <span class="s2">return </span><span class="s1">sort.results</span>

<span class="s1">mlir.register_lowering(sort_p</span><span class="s2">, </span><span class="s1">_sort_lower)</span>


<span class="s2">def </span><span class="s1">_top_k_abstract_eval(operand</span><span class="s2">, </span><span class="s1">*</span><span class="s2">, </span><span class="s1">k):</span>
  <span class="s2">if </span><span class="s1">dtypes.issubdtype(operand.dtype</span><span class="s2">, </span><span class="s1">np.complexfloating):</span>
    <span class="s2">raise </span><span class="s1">ValueError(</span><span class="s3">&quot;top_k is not compatible with complex inputs.&quot;</span><span class="s1">)</span>
  <span class="s2">if </span><span class="s1">k &lt; </span><span class="s4">0</span><span class="s1">:</span>
    <span class="s2">raise </span><span class="s1">ValueError(</span><span class="s3">f&quot;k argument to top_k must be nonnegative, got </span><span class="s2">{</span><span class="s1">k</span><span class="s2">}</span><span class="s3">&quot;</span><span class="s1">)</span>
  <span class="s2">if </span><span class="s1">len(operand.shape) == </span><span class="s4">0</span><span class="s1">:</span>
    <span class="s2">raise </span><span class="s1">TypeError(</span><span class="s3">&quot;top_k operand must have &gt;= 1 dimension, got {}&quot;</span>
                    <span class="s1">.format(operand.shape))</span>
  <span class="s1">shape = list(operand.shape)</span>
  <span class="s2">if </span><span class="s1">shape[-</span><span class="s4">1</span><span class="s1">] &lt; k:</span>
    <span class="s1">msg = </span><span class="s3">&quot;k argument to top_k must be no larger than minor dimension; {} vs {}&quot;</span>
    <span class="s2">raise </span><span class="s1">ValueError(msg.format(k</span><span class="s2">, </span><span class="s1">shape))</span>
  <span class="s1">shape[-</span><span class="s4">1</span><span class="s1">] = k</span>
  <span class="s2">return </span><span class="s1">(operand.update(shape=shape</span><span class="s2">, </span><span class="s1">dtype=operand.dtype</span><span class="s2">,</span>
                         <span class="s1">weak_type=operand.weak_type)</span><span class="s2">,</span>
          <span class="s1">operand.update(shape=shape</span><span class="s2">, </span><span class="s1">dtype=np.dtype(np.int32)))</span>

<span class="s2">def </span><span class="s1">_top_k_jvp(primals</span><span class="s2">, </span><span class="s1">tangents</span><span class="s2">, </span><span class="s1">*</span><span class="s2">, </span><span class="s1">k):</span>
  <span class="s1">operand</span><span class="s2">, </span><span class="s1">= primals</span>
  <span class="s1">tangent</span><span class="s2">, </span><span class="s1">= tangents</span>
  <span class="s1">primals_out = top_k(operand</span><span class="s2">, </span><span class="s1">k)</span>
  <span class="s2">if </span><span class="s1">type(tangent) </span><span class="s2">is </span><span class="s1">ad_util.Zero:</span>
    <span class="s1">tangent_out = ad_util.Zero.from_value(primals_out[</span><span class="s4">0</span><span class="s1">])</span>
  <span class="s2">else</span><span class="s1">:</span>
    <span class="s1">_</span><span class="s2">, </span><span class="s1">k_idxs = primals_out</span>
    <span class="s1">idx_shape = k_idxs.shape</span>
    <span class="s1">rank = len(idx_shape)</span>
    <span class="s1">gather_index_shape = idx_shape + (</span><span class="s4">1</span><span class="s2">,</span><span class="s1">)</span>
    <span class="s1">gather_indices = []</span>
    <span class="s2">for </span><span class="s1">i </span><span class="s2">in </span><span class="s1">range(rank-</span><span class="s4">1</span><span class="s1">):</span>
      <span class="s1">_iota = iota(k_idxs.dtype</span><span class="s2">, </span><span class="s1">idx_shape[i])</span>
      <span class="s1">_iota = broadcast_in_dim(_iota</span><span class="s2">, </span><span class="s1">gather_index_shape</span><span class="s2">, </span><span class="s1">(i</span><span class="s2">,</span><span class="s1">))</span>
      <span class="s1">gather_indices.append(_iota)</span>
    <span class="s1">gather_indices.append(reshape(k_idxs</span><span class="s2">, </span><span class="s1">gather_index_shape))</span>
    <span class="s1">gather_indices = concatenate(gather_indices</span><span class="s2">, </span><span class="s1">dimension=rank)</span>
    <span class="s1">slice_sizes = (</span><span class="s4">1</span><span class="s2">,</span><span class="s1">) * rank</span>
    <span class="s1">dnums = slicing.GatherDimensionNumbers(</span>
      <span class="s1">offset_dims=()</span><span class="s2">,</span>
      <span class="s1">collapsed_slice_dims=tuple(range(rank))</span><span class="s2">,</span>
      <span class="s1">start_index_map=tuple(range(rank)))</span>
    <span class="s1">tangent_out = slicing.gather(tangent</span><span class="s2">, </span><span class="s1">gather_indices</span><span class="s2">, </span><span class="s1">dnums</span><span class="s2">, </span><span class="s1">slice_sizes)</span>
  <span class="s2">return </span><span class="s1">primals_out</span><span class="s2">, </span><span class="s1">(tangent_out</span><span class="s2">, </span><span class="s1">ad_util.Zero.from_value(primals_out[</span><span class="s4">1</span><span class="s1">]))</span>

<span class="s2">def </span><span class="s1">_top_k_batch_rule(batched_args</span><span class="s2">, </span><span class="s1">batch_dims</span><span class="s2">, </span><span class="s1">*</span><span class="s2">, </span><span class="s1">k):</span>
  <span class="s1">operand</span><span class="s2">, </span><span class="s1">= batched_args</span>
  <span class="s1">bdim</span><span class="s2">, </span><span class="s1">= batch_dims</span>
  <span class="s2">if </span><span class="s1">bdim == operand.ndim-</span><span class="s4">1</span><span class="s1">:</span>
    <span class="s1">perm = np.arange(operand.ndim)</span>
    <span class="s1">perm[bdim-</span><span class="s4">1</span><span class="s1">]</span><span class="s2">, </span><span class="s1">perm[bdim] = perm[bdim]</span><span class="s2">, </span><span class="s1">perm[bdim-</span><span class="s4">1</span><span class="s1">]</span>
    <span class="s1">top_k_v</span><span class="s2">, </span><span class="s1">top_k_i = top_k(transpose(operand</span><span class="s2">, </span><span class="s1">perm)</span><span class="s2">, </span><span class="s1">k=k)</span>
    <span class="s2">return </span><span class="s1">(transpose(top_k_v</span><span class="s2">, </span><span class="s1">perm)</span><span class="s2">,</span>
            <span class="s1">transpose(top_k_i</span><span class="s2">, </span><span class="s1">perm))</span><span class="s2">, </span><span class="s1">(bdim</span><span class="s2">, </span><span class="s1">bdim)</span>
  <span class="s2">else</span><span class="s1">:</span>
    <span class="s2">return </span><span class="s1">top_k(operand</span><span class="s2">, </span><span class="s1">k=k)</span><span class="s2">, </span><span class="s1">(bdim</span><span class="s2">, </span><span class="s1">bdim)</span>

<span class="s2">def </span><span class="s1">_top_k_translation_rule(ctx</span><span class="s2">, </span><span class="s1">avals_in</span><span class="s2">, </span><span class="s1">avals_out</span><span class="s2">, </span><span class="s1">x</span><span class="s2">, </span><span class="s1">*</span><span class="s2">, </span><span class="s1">k):</span>
  <span class="s2">return </span><span class="s1">xla.xla_destructure(ctx.builder</span><span class="s2">, </span><span class="s1">xops.TopK(x</span><span class="s2">, </span><span class="s1">k))</span>

<span class="s1">top_k_p = Primitive(</span><span class="s3">'top_k'</span><span class="s1">)</span>
<span class="s1">top_k_p.multiple_results = </span><span class="s2">True</span>
<span class="s1">top_k_p.def_impl(partial(dispatch.apply_primitive</span><span class="s2">, </span><span class="s1">top_k_p))</span>
<span class="s1">top_k_p.def_abstract_eval(_top_k_abstract_eval)</span>
<span class="s2">def </span><span class="s1">_top_k_lower(ctx</span><span class="s2">, </span><span class="s1">operand</span><span class="s2">, </span><span class="s1">k):</span>
  <span class="s2">return </span><span class="s1">chlo.TopKOp(operand</span><span class="s2">, </span><span class="s1">mlir.i64_attr(k)).results</span>
<span class="s1">mlir.register_lowering(top_k_p</span><span class="s2">, </span><span class="s1">_top_k_lower)</span>
<span class="s1">ad.primitive_jvps[top_k_p] = _top_k_jvp</span>
<span class="s1">batching.primitive_batchers[top_k_p] = _top_k_batch_rule</span>

<span class="s2">def </span><span class="s1">_stop_gradient_jvp_rule(primals</span><span class="s2">, </span><span class="s1">tangents):</span>
  <span class="s0"># if we don't call stop_gradient here, we'd only peel off one autodiff tracer</span>
  <span class="s1">x</span><span class="s2">, </span><span class="s1">= primals</span>
  <span class="s2">return </span><span class="s1">stop_gradient(x)</span><span class="s2">, </span><span class="s1">ad_util.Zero.from_value(x)</span>

<span class="s2">def </span><span class="s1">_stop_gradient_batch_rule(batched_args</span><span class="s2">, </span><span class="s1">batch_dims):</span>
  <span class="s1">x</span><span class="s2">, </span><span class="s1">= batched_args</span>
  <span class="s1">dim</span><span class="s2">, </span><span class="s1">= batch_dims</span>
  <span class="s2">return </span><span class="s1">stop_gradient(x)</span><span class="s2">, </span><span class="s1">dim</span>

<span class="s1">ad.primitive_jvps[ad_util.stop_gradient_p] = _stop_gradient_jvp_rule</span>
<span class="s1">batching.primitive_batchers[ad_util.stop_gradient_p] = _stop_gradient_batch_rule</span>
<span class="s1">pe.def_trivial_padding(ad_util.stop_gradient_p)</span>


<span class="s2">def </span><span class="s1">create_token(_=</span><span class="s2">None</span><span class="s1">):</span>
  <span class="s5">&quot;&quot;&quot;Creates an XLA token value with no preconditions for sequencing effects. 
 
  Experimental. 
 
  The argument is ignored. It exists for backward compatibility. 
  &quot;&quot;&quot;</span>
  <span class="s2">return </span><span class="s1">create_token_p.bind()</span>

<span class="s1">create_token_p = Primitive(</span><span class="s3">&quot;create_token&quot;</span><span class="s1">)</span>
<span class="s1">create_token_p.def_impl(partial(dispatch.apply_primitive</span><span class="s2">, </span><span class="s1">create_token_p))</span>
<span class="s1">create_token_p.def_abstract_eval(</span><span class="s2">lambda </span><span class="s1">*_: abstract_token)</span>

<span class="s2">def </span><span class="s1">_create_token_lowering(ctx</span><span class="s2">, </span><span class="s1">*operands):</span>
  <span class="s1">aval_out</span><span class="s2">, </span><span class="s1">= ctx.avals_out</span>
  <span class="s2">return </span><span class="s1">hlo.CreateTokenOp().results</span>
<span class="s1">mlir.register_lowering(create_token_p</span><span class="s2">, </span><span class="s1">_create_token_lowering)</span>


<span class="s2">def </span><span class="s1">after_all(*operands):</span>
  <span class="s5">&quot;&quot;&quot;Merges one or more XLA token values. Experimental. 
 
  Wraps the XLA AfterAll operator.&quot;&quot;&quot;</span>
  <span class="s2">return </span><span class="s1">after_all_p.bind(*operands)</span>

<span class="s2">def </span><span class="s1">_after_all_abstract_eval(*operands):</span>
  <span class="s2">if </span><span class="s1">any(x </span><span class="s2">is not </span><span class="s1">abstract_token </span><span class="s2">for </span><span class="s1">x </span><span class="s2">in </span><span class="s1">operands):</span>
    <span class="s2">raise </span><span class="s1">TypeError(</span><span class="s3">&quot;Arguments to after_all must be tokens&quot;</span><span class="s1">)</span>
  <span class="s2">return </span><span class="s1">abstract_token</span>


<span class="s1">after_all_p = Primitive(</span><span class="s3">&quot;after_all&quot;</span><span class="s1">)</span>
<span class="s1">after_all_p.def_impl(partial(dispatch.apply_primitive</span><span class="s2">, </span><span class="s1">after_all_p))</span>
<span class="s1">after_all_p.def_abstract_eval(_after_all_abstract_eval)</span>

<span class="s2">def </span><span class="s1">_after_all_lowering(ctx</span><span class="s2">, </span><span class="s1">*operands):</span>
  <span class="s1">aval_out</span><span class="s2">, </span><span class="s1">= ctx.avals_out</span>
  <span class="s2">return </span><span class="s1">hlo.AfterAllOp(operands).results</span>
<span class="s1">mlir.register_lowering(after_all_p</span><span class="s2">, </span><span class="s1">_after_all_lowering)</span>


<span class="s2">class </span><span class="s1">InOutFeedEffect(effects.Effect):</span>
  <span class="s2">pass</span>
<span class="s1">infeed_effect = InOutFeedEffect()</span>
<span class="s1">outfeed_effect = InOutFeedEffect()</span>


<span class="s2">def </span><span class="s1">infeed(token</span><span class="s2">, </span><span class="s1">shape=</span><span class="s2">None, </span><span class="s1">partitions=</span><span class="s2">None</span><span class="s1">):</span>
  <span class="s5">&quot;&quot;&quot;Consumes an infeed value of `shape` from the host. Experimental. 
 
  `token` is used to sequence infeed and outfeed effects. 
  `partitions` may be specified inside a `sharded_jit` function. 
  &quot;&quot;&quot;</span>
  <span class="s1">flat_shapes</span><span class="s2">, </span><span class="s1">treedef = pytree.flatten(shape)</span>
  <span class="s2">for </span><span class="s1">shape </span><span class="s2">in </span><span class="s1">flat_shapes:</span>
    <span class="s2">if not </span><span class="s1">isinstance(shape</span><span class="s2">, </span><span class="s1">ShapedArray):</span>
      <span class="s2">raise </span><span class="s1">TypeError(</span><span class="s3">&quot;shape argument to infeed must be a pytree of &quot;</span>
                      <span class="s3">&quot;ShapedArray values, got {}&quot;</span><span class="s1">.format(shape))</span>
  <span class="s2">if </span><span class="s1">partitions </span><span class="s2">is not None</span><span class="s1">:</span>
    <span class="s0"># Always replicate token.</span>
    <span class="s0"># We specifically use type() to raise an error for PartitionSpecs.</span>
    <span class="s2">if </span><span class="s1">type(partitions) != tuple:  </span><span class="s0"># pylint: disable=unidiomatic-typecheck</span>
      <span class="s2">raise </span><span class="s1">ValueError(</span><span class="s3">f&quot;'partitions' argument to infeed should be a tuple, &quot;</span>
                       <span class="s3">f&quot;got </span><span class="s2">{</span><span class="s1">partitions</span><span class="s2">}</span><span class="s3">&quot;</span><span class="s1">)</span>
    <span class="s1">partitions = partitions + (</span><span class="s2">None,</span><span class="s1">)</span>
  <span class="s1">xs_and_token = infeed_p.bind(token</span><span class="s2">, </span><span class="s1">shapes=tuple(flat_shapes)</span><span class="s2">,</span>
                               <span class="s1">partitions=partitions)</span>
  <span class="s2">return </span><span class="s1">(treedef.unflatten(xs_and_token[:-</span><span class="s4">1</span><span class="s1">])</span><span class="s2">, </span><span class="s1">xs_and_token[-</span><span class="s4">1</span><span class="s1">])</span>

<span class="s2">def </span><span class="s1">_infeed_abstract_eval(token</span><span class="s2">, </span><span class="s1">*</span><span class="s2">, </span><span class="s1">shapes</span><span class="s2">, </span><span class="s1">partitions):</span>
  <span class="s2">if </span><span class="s1">token </span><span class="s2">is not </span><span class="s1">abstract_token:</span>
    <span class="s2">raise </span><span class="s1">TypeError(</span><span class="s3">&quot;First argument to infeed must be a token&quot;</span><span class="s1">)</span>
  <span class="s2">return </span><span class="s1">(*shapes</span><span class="s2">, </span><span class="s1">abstract_token)</span><span class="s2">, </span><span class="s1">{infeed_effect}</span>


<span class="s1">infeed_p = Primitive(</span><span class="s3">&quot;infeed&quot;</span><span class="s1">)</span>
<span class="s1">infeed_p.multiple_results = </span><span class="s2">True</span>
<span class="s1">infeed_p.def_impl(partial(dispatch.apply_primitive</span><span class="s2">, </span><span class="s1">infeed_p))</span>
<span class="s1">infeed_p.def_effectful_abstract_eval(_infeed_abstract_eval)</span>
<span class="s1">mlir.lowerable_effects.add_type(InOutFeedEffect)</span>


<span class="s2">def </span><span class="s1">_infeed_lowering(ctx</span><span class="s2">, </span><span class="s1">token</span><span class="s2">, </span><span class="s1">*</span><span class="s2">, </span><span class="s1">shapes</span><span class="s2">, </span><span class="s1">partitions):</span>
  <span class="s1">output_types = safe_map(mlir.aval_to_ir_types</span><span class="s2">, </span><span class="s1">ctx.avals_out[:-</span><span class="s4">1</span><span class="s1">])</span>
  <span class="s1">flat_output_types = util.flatten(output_types)</span>
  <span class="s0"># TODO(phawkins): verify `shapes` have a major-to-minor layout.</span>
  <span class="s1">layouts = ir.ArrayAttr.get([</span>
      <span class="s1">ir.ArrayAttr.get(</span>
          <span class="s1">[mlir.i64_attr(i)</span>
           <span class="s2">for </span><span class="s1">i </span><span class="s2">in </span><span class="s1">range(len(aval.shape) - </span><span class="s4">1</span><span class="s2">, </span><span class="s1">-</span><span class="s4">1</span><span class="s2">, </span><span class="s1">-</span><span class="s4">1</span><span class="s1">)])</span>
      <span class="s2">for </span><span class="s1">aval </span><span class="s2">in </span><span class="s1">shapes</span>
  <span class="s1">])</span>
  <span class="s1">infeed = hlo.InfeedOp(</span>
      <span class="s1">flat_output_types + [hlo.TokenType.get()]</span><span class="s2">,</span>
      <span class="s1">token</span><span class="s2">,</span>
      <span class="s1">infeed_config=ir.StringAttr.get(</span><span class="s3">''</span><span class="s1">)</span><span class="s2">,</span>
      <span class="s1">layout=layouts)</span>
  <span class="s2">if </span><span class="s1">partitions </span><span class="s2">is not None</span><span class="s1">:</span>
    <span class="s1">mlir.set_sharding(infeed</span><span class="s2">, </span><span class="s1">xla.sharding_to_proto(partitions))</span>
  <span class="s1">token = infeed.results[-</span><span class="s4">1</span><span class="s1">]</span>
  <span class="s1">outs = infeed.results[:-</span><span class="s4">1</span><span class="s1">]</span>
  <span class="s2">return </span><span class="s1">util.unflatten(outs</span><span class="s2">, </span><span class="s1">safe_map(len</span><span class="s2">, </span><span class="s1">output_types)) + [[</span>
      <span class="s1">token</span><span class="s2">,</span>
  <span class="s1">]]</span>

<span class="s1">mlir.register_lowering(infeed_p</span><span class="s2">, </span><span class="s1">_infeed_lowering)</span>


<span class="s2">def </span><span class="s1">outfeed(token</span><span class="s2">, </span><span class="s1">xs</span><span class="s2">, </span><span class="s1">partitions = </span><span class="s2">None</span><span class="s1">):</span>
  <span class="s5">&quot;&quot;&quot;Outfeeds value `xs` to the host. Experimental. 
 
  `token` is used to sequence infeed and outfeed effects. 
  `partitions` may be specified inside a `sharded_jit` or `pjit` function. 
  &quot;&quot;&quot;</span>
  <span class="s2">if </span><span class="s1">partitions </span><span class="s2">is not None</span><span class="s1">:</span>
    <span class="s0"># We specifically use type() to raise an error for PartitionSpecs.</span>
    <span class="s2">if </span><span class="s1">type(partitions) != tuple:  </span><span class="s0"># pylint: disable=unidiomatic-typecheck</span>
      <span class="s2">raise </span><span class="s1">ValueError(</span><span class="s3">f&quot;'partitions' argument to outfeed should be a tuple, &quot;</span>
                       <span class="s3">f&quot;got </span><span class="s2">{</span><span class="s1">partitions</span><span class="s2">}</span><span class="s3">&quot;</span><span class="s1">)</span>
  <span class="s1">flat_xs</span><span class="s2">, </span><span class="s1">_ = pytree.flatten(xs)</span>
  <span class="s2">return </span><span class="s1">outfeed_p.bind(token</span><span class="s2">, </span><span class="s1">*flat_xs</span><span class="s2">, </span><span class="s1">partitions=partitions)</span>

<span class="s2">def </span><span class="s1">_outfeed_abstract_eval(token</span><span class="s2">, </span><span class="s1">*xs</span><span class="s2">, </span><span class="s1">partitions):</span>
  <span class="s2">if </span><span class="s1">token </span><span class="s2">is not </span><span class="s1">abstract_token:</span>
    <span class="s2">raise </span><span class="s1">TypeError(</span><span class="s3">&quot;First argument to outfeed must be a token&quot;</span><span class="s1">)</span>
  <span class="s2">return </span><span class="s1">abstract_token</span><span class="s2">, </span><span class="s1">{outfeed_effect}</span>

<span class="s1">outfeed_p = Primitive(</span><span class="s3">&quot;outfeed&quot;</span><span class="s1">)</span>
<span class="s1">outfeed_p.def_impl(partial(dispatch.apply_primitive</span><span class="s2">, </span><span class="s1">outfeed_p))</span>
<span class="s1">outfeed_p.def_effectful_abstract_eval(_outfeed_abstract_eval)</span>
<span class="s1">mlir.lowerable_effects.add_type(InOutFeedEffect)</span>


<span class="s2">def </span><span class="s1">_outfeed_lowering(ctx</span><span class="s2">, </span><span class="s1">token</span><span class="s2">, </span><span class="s1">*xs</span><span class="s2">, </span><span class="s1">partitions):</span>
  <span class="s1">outfeed = hlo.OutfeedOp(</span>
      <span class="s1">mlir.flatten_lowering_ir_args(xs)</span><span class="s2">,</span>
      <span class="s1">token</span><span class="s2">,</span>
      <span class="s1">outfeed_config=ir.StringAttr.get(</span><span class="s3">''</span><span class="s1">))</span>
  <span class="s2">if </span><span class="s1">partitions </span><span class="s2">is not None</span><span class="s1">:</span>
    <span class="s1">mlir.set_sharding(outfeed</span><span class="s2">, </span><span class="s1">xla.sharding_to_proto(partitions))</span>
  <span class="s2">return </span><span class="s1">outfeed.results</span>

<span class="s1">mlir.register_lowering(outfeed_p</span><span class="s2">, </span><span class="s1">_outfeed_lowering)</span>


<span class="s2">def </span><span class="s1">rng_uniform(a</span><span class="s2">, </span><span class="s1">b</span><span class="s2">, </span><span class="s1">shape):</span>
  <span class="s5">&quot;&quot;&quot;Stateful PRNG generator. Experimental and its use is discouraged. 
 
  Returns uniformly distributed random numbers in the range [a, b) 
 
  You should use jax.random for most purposes; this function exists only for 
  niche use cases with special performance requirements. 
 
  This API may be removed at any time. 
  &quot;&quot;&quot;</span>
  <span class="s2">return </span><span class="s1">rng_uniform_p.bind(a</span><span class="s2">, </span><span class="s1">b</span><span class="s2">, </span><span class="s1">shape=tuple(shape))</span>

<span class="s2">def </span><span class="s1">_rng_uniform_abstract_eval(a</span><span class="s2">, </span><span class="s1">b</span><span class="s2">, </span><span class="s1">*</span><span class="s2">, </span><span class="s1">shape):</span>
  <span class="s2">if </span><span class="s1">a.dtype != b.dtype:</span>
    <span class="s2">raise </span><span class="s1">ValueError(</span>
      <span class="s3">&quot;Arguments to rng_uniform must have identical dtypes, got {} &quot;</span>
      <span class="s3">&quot;and {}.&quot;</span><span class="s1">.format(a.dtype</span><span class="s2">, </span><span class="s1">b.dtype))</span>
  <span class="s2">if </span><span class="s1">a.shape != () </span><span class="s2">or </span><span class="s1">b.shape != ():</span>
    <span class="s2">raise </span><span class="s1">ValueError(</span>
      <span class="s3">&quot;Arguments to rng_uniform must be scalars; got shapes {} and {}.&quot;</span>
      <span class="s1">.format(a.shape</span><span class="s2">, </span><span class="s1">b.shape))</span>
  <span class="s2">return </span><span class="s1">a.update(shape=shape</span><span class="s2">, </span><span class="s1">dtype=a.dtype</span><span class="s2">,</span>
                  <span class="s1">weak_type=(a.weak_type </span><span class="s2">and </span><span class="s1">b.weak_type))</span>

<span class="s1">rng_uniform_p = Primitive(</span><span class="s3">&quot;rng_uniform&quot;</span><span class="s1">)</span>
<span class="s1">rng_uniform_p.def_impl(partial(dispatch.apply_primitive</span><span class="s2">, </span><span class="s1">rng_uniform_p))</span>
<span class="s1">rng_uniform_p.def_abstract_eval(_rng_uniform_abstract_eval)</span>

<span class="s2">def </span><span class="s1">_rng_uniform_lowering(ctx</span><span class="s2">, </span><span class="s1">a</span><span class="s2">, </span><span class="s1">b</span><span class="s2">, </span><span class="s1">*</span><span class="s2">, </span><span class="s1">shape):</span>
  <span class="s1">aval_out</span><span class="s2">, </span><span class="s1">= ctx.avals_out</span>
  <span class="s1">shape</span><span class="s2">, </span><span class="s1">= mlir.ir_constants(np.array(aval_out.shape</span><span class="s2">, </span><span class="s1">np.int64)</span><span class="s2">,</span>
                             <span class="s1">canonicalize_types=</span><span class="s2">False</span><span class="s1">)</span>
  <span class="s2">return </span><span class="s1">hlo.RngOp(a</span><span class="s2">, </span><span class="s1">b</span><span class="s2">, </span><span class="s1">shape</span><span class="s2">,</span>
                   <span class="s1">hlo.RngDistributionAttr.get(</span><span class="s3">'UNIFORM'</span><span class="s1">)).results</span>

<span class="s1">mlir.register_lowering(rng_uniform_p</span><span class="s2">, </span><span class="s1">_rng_uniform_lowering)</span>


<span class="s2">def </span><span class="s1">_rng_bit_generator_shape_rule(key</span><span class="s2">, </span><span class="s1">*</span><span class="s2">, </span><span class="s1">shape</span><span class="s2">, </span><span class="s1">dtype</span><span class="s2">, </span><span class="s1">algorithm):</span>
  <span class="s2">del </span><span class="s1">dtype</span><span class="s2">, </span><span class="s1">algorithm</span>
  <span class="s2">return </span><span class="s1">(key.shape</span><span class="s2">, </span><span class="s1">tuple(shape))</span>

<span class="s2">def </span><span class="s1">_rng_bit_generator_dtype_rule(key</span><span class="s2">, </span><span class="s1">*</span><span class="s2">, </span><span class="s1">shape</span><span class="s2">, </span><span class="s1">dtype</span><span class="s2">, </span><span class="s1">algorithm):</span>
  <span class="s2">del </span><span class="s1">shape</span><span class="s2">, </span><span class="s1">algorithm</span>
  <span class="s2">return </span><span class="s1">(key.dtype</span><span class="s2">, </span><span class="s1">dtype)</span>

<span class="s2">def </span><span class="s1">_rng_bit_generator_weak_type_rule(key</span><span class="s2">, </span><span class="s1">*</span><span class="s2">, </span><span class="s1">shape</span><span class="s2">, </span><span class="s1">dtype</span><span class="s2">, </span><span class="s1">algorithm):</span>
  <span class="s2">del </span><span class="s1">shape</span><span class="s2">, </span><span class="s1">dtype</span><span class="s2">, </span><span class="s1">algorithm</span>
  <span class="s2">return </span><span class="s1">(key.weak_type</span><span class="s2">, False</span><span class="s1">)</span>

<span class="s1">RandomAlgorithm = xops.RandomAlgorithm</span>
<span class="s1">RandomAlgorithm.__str__ = </span><span class="s2">lambda </span><span class="s1">algorithm: algorithm.name  </span><span class="s0"># type: ignore[assignment]</span>

<span class="s2">def </span><span class="s1">_rng_algorithm(algorithm: RandomAlgorithm):</span>
  <span class="s2">if </span><span class="s1">algorithm == RandomAlgorithm.RNG_THREE_FRY:</span>
    <span class="s2">return </span><span class="s1">hlo.RngAlgorithmAttr.get(</span><span class="s3">&quot;THREE_FRY&quot;</span><span class="s1">)</span>
  <span class="s2">elif </span><span class="s1">algorithm == RandomAlgorithm.RNG_PHILOX:</span>
    <span class="s2">return </span><span class="s1">hlo.RngAlgorithmAttr.get(</span><span class="s3">&quot;PHILOX&quot;</span><span class="s1">)</span>
  <span class="s2">elif </span><span class="s1">algorithm == RandomAlgorithm.RNG_DEFAULT:</span>
    <span class="s2">return </span><span class="s1">hlo.RngAlgorithmAttr.get(</span><span class="s3">&quot;DEFAULT&quot;</span><span class="s1">)</span>
  <span class="s2">else</span><span class="s1">:</span>
    <span class="s2">assert False</span>

<span class="s2">def </span><span class="s1">_rng_bit_generator_lowering(</span>
    <span class="s1">ctx</span><span class="s2">, </span><span class="s1">key</span><span class="s2">, </span><span class="s1">*</span><span class="s2">, </span><span class="s1">shape</span><span class="s2">, </span><span class="s1">dtype</span><span class="s2">, </span><span class="s1">algorithm):</span>
  <span class="s1">key_type = ir.RankedTensorType(key.type)</span>
  <span class="s1">key_shape</span><span class="s2">, </span><span class="s1">key_etype = key_type.shape</span><span class="s2">, </span><span class="s1">key_type.element_type</span>
  <span class="s0"># While the RngBitGenerator HLO accepts a u64[2] key on all backends, we</span>
  <span class="s0"># typically represent the key argument to this primitive as a u32[4] so as to</span>
  <span class="s0"># sidestep issues with the jax_enable_x64=False configuration. As a result, we</span>
  <span class="s0"># need to convert u32[4] -&gt; u64[2] here in the translation rule. However, we</span>
  <span class="s0"># also polymorphically allow a u64[2] for backward compatibility.</span>
  <span class="s0">#</span>
  <span class="s0"># Separately, xops.RngBitGenerator doesn't support generating u8 or</span>
  <span class="s0"># u16, so we request u32 and truncate in that case.</span>
  <span class="s1">u32_type = ir.IntegerType.get_unsigned(</span><span class="s4">32</span><span class="s1">)</span>
  <span class="s1">u64_type = ir.IntegerType.get_unsigned(</span><span class="s4">64</span><span class="s1">)</span>
  <span class="s2">assert </span><span class="s1">((key_shape == [</span><span class="s4">4</span><span class="s1">] </span><span class="s2">and </span><span class="s1">key_etype == u32_type) </span><span class="s2">or</span>
          <span class="s1">(key_shape == [</span><span class="s4">2</span><span class="s1">] </span><span class="s2">and </span><span class="s1">key_etype == u64_type))</span><span class="s2">, </span><span class="s1">(key_shape</span><span class="s2">, </span><span class="s1">key_etype)</span>
  <span class="s1">dtype = np.dtype(dtype)</span>
  <span class="s1">etype = mlir.dtype_to_ir_type(dtype)</span>
  <span class="s2">if </span><span class="s1">dtype </span><span class="s2">in </span><span class="s1">(np.dtype(</span><span class="s3">'uint8'</span><span class="s1">)</span><span class="s2">, </span><span class="s1">np.dtype(</span><span class="s3">'uint16'</span><span class="s1">)</span><span class="s2">, </span><span class="s1">np.dtype(</span><span class="s3">'uint32'</span><span class="s1">)</span><span class="s2">,</span>
               <span class="s1">np.dtype(</span><span class="s3">'uint64'</span><span class="s1">)):</span>
    <span class="s1">rbg_etype = etype</span>
  <span class="s2">else</span><span class="s1">:</span>
    <span class="s1">rbg_etype = u32_type</span>
  <span class="s2">if </span><span class="s1">key_etype == u32_type:</span>
    <span class="s1">key = hlo.BitcastConvertOp(</span>
        <span class="s1">ir.RankedTensorType.get([</span><span class="s4">2</span><span class="s1">]</span><span class="s2">, </span><span class="s1">u64_type)</span><span class="s2">,</span>
        <span class="s1">hlo.ReshapeOp(ir.RankedTensorType.get([</span><span class="s4">2</span><span class="s2">, </span><span class="s4">2</span><span class="s1">]</span><span class="s2">, </span><span class="s1">u32_type)</span><span class="s2">, </span><span class="s1">key)).result</span>
  <span class="s1">algorithm_attr = _rng_algorithm(algorithm)</span>
  <span class="s1">out_key</span><span class="s2">, </span><span class="s1">out_vals = hlo.RngBitGeneratorOp(</span>
      <span class="s1">key.type</span><span class="s2">,</span>
      <span class="s1">ir.RankedTensorType.get(shape</span><span class="s2">, </span><span class="s1">rbg_etype)</span><span class="s2">,</span>
      <span class="s1">algorithm_attr</span><span class="s2">, </span><span class="s1">key).results</span>
  <span class="s2">if </span><span class="s1">key_etype == u32_type:</span>
    <span class="s1">out_key = hlo.ReshapeOp(</span>
        <span class="s1">ir.RankedTensorType.get([</span><span class="s4">4</span><span class="s1">]</span><span class="s2">, </span><span class="s1">u32_type)</span><span class="s2">,</span>
        <span class="s1">hlo.BitcastConvertOp(</span>
            <span class="s1">ir.RankedTensorType.get([</span><span class="s4">2</span><span class="s2">, </span><span class="s4">2</span><span class="s1">]</span><span class="s2">, </span><span class="s1">u32_type)</span><span class="s2">, </span><span class="s1">out_key)).result</span>
  <span class="s2">if </span><span class="s1">rbg_etype != etype:</span>
    <span class="s1">out_vals = hlo.ConvertOp(</span>
      <span class="s1">ir.RankedTensorType.get(ir.RankedTensorType(out_vals.type).shape</span><span class="s2">, </span><span class="s1">etype)</span><span class="s2">,</span>
      <span class="s1">out_vals).result</span>
  <span class="s2">return </span><span class="s1">[out_key</span><span class="s2">, </span><span class="s1">out_vals]</span>


<span class="s2">def </span><span class="s1">_rng_bit_generator_named_shape_rule(key</span><span class="s2">, </span><span class="s1">*</span><span class="s2">, </span><span class="s1">shape</span><span class="s2">, </span><span class="s1">dtype</span><span class="s2">, </span><span class="s1">algorithm):</span>
  <span class="s2">return </span><span class="s1">[key.named_shape</span><span class="s2">, </span><span class="s1">key.named_shape]</span>

<span class="s1">rng_bit_generator_p = Primitive(</span><span class="s3">&quot;rng_bit_generator&quot;</span><span class="s1">)</span>
<span class="s1">rng_bit_generator_p.multiple_results = </span><span class="s2">True</span>
<span class="s1">rng_bit_generator_p.def_impl(</span>
    <span class="s1">partial(dispatch.apply_primitive</span><span class="s2">, </span><span class="s1">rng_bit_generator_p))</span>
<span class="s1">rng_bit_generator_p.def_abstract_eval(</span>
    <span class="s1">partial(standard_multi_result_abstract_eval</span><span class="s2">, </span><span class="s1">rng_bit_generator_p</span><span class="s2">,</span>
            <span class="s1">_rng_bit_generator_shape_rule</span><span class="s2">, </span><span class="s1">_rng_bit_generator_dtype_rule</span><span class="s2">,</span>
            <span class="s1">_rng_bit_generator_weak_type_rule</span><span class="s2">,</span>
            <span class="s1">_rng_bit_generator_named_shape_rule))</span>
<span class="s1">mlir.register_lowering(rng_bit_generator_p</span><span class="s2">,</span>
                       <span class="s1">_rng_bit_generator_lowering)</span>


<span class="s2">def </span><span class="s1">_array_copy(arr: ArrayLike) -&gt; Array:</span>
  <span class="s2">return </span><span class="s1">copy_p.bind(arr)</span>


<span class="s2">def </span><span class="s1">_which_dim_sharded(s: PmapSharding) -&gt; Optional[int]:</span>
  <span class="s1">sharded_dim = </span><span class="s2">None</span>
  <span class="s2">for </span><span class="s1">i</span><span class="s2">, </span><span class="s1">s </span><span class="s2">in </span><span class="s1">enumerate(s.sharding_spec.sharding):</span>
    <span class="s2">if </span><span class="s1">isinstance(s</span><span class="s2">, </span><span class="s1">pxla.Unstacked):</span>
      <span class="s1">sharded_dim = i</span>
      <span class="s2">break</span>
  <span class="s2">return </span><span class="s1">sharded_dim</span>


<span class="s2">def </span><span class="s1">_identity_fn(x): </span><span class="s2">return </span><span class="s1">x</span>


<span class="s2">def </span><span class="s1">_copy_impl_pmap_sharding(sharded_dim</span><span class="s2">, </span><span class="s1">*args</span><span class="s2">, </span><span class="s1">**kwargs):</span>
  <span class="s1">axis_name</span><span class="s2">, </span><span class="s1">static_broadcasted_tuple</span><span class="s2">, </span><span class="s1">donate_tuple = api._shared_code_pmap(</span>
    <span class="s1">_identity_fn</span><span class="s2">, None, </span><span class="s1">()</span><span class="s2">, </span><span class="s1">()</span><span class="s2">, </span><span class="s1">sharded_dim</span><span class="s2">, </span><span class="s1">sharded_dim)</span>
  <span class="s1">p = api._prepare_pmap(</span>
      <span class="s1">_identity_fn</span><span class="s2">, </span><span class="s1">sharded_dim</span><span class="s2">, </span><span class="s1">sharded_dim</span><span class="s2">, </span><span class="s1">static_broadcasted_tuple</span><span class="s2">,</span>
      <span class="s1">donate_tuple</span><span class="s2">, None, None, None, </span><span class="s1">args</span><span class="s2">, </span><span class="s1">kwargs)</span>
  <span class="s1">out_flat =  pxla.xla_pmap_impl(</span>
      <span class="s1">p.flat_fun</span><span class="s2">, </span><span class="s1">*p.flat_args</span><span class="s2">, </span><span class="s1">backend=</span><span class="s2">None, </span><span class="s1">axis_name=axis_name</span><span class="s2">,</span>
      <span class="s1">axis_size=p.local_axis_size</span><span class="s2">, </span><span class="s1">global_axis_size=p.global_axis_size</span><span class="s2">,</span>
      <span class="s1">devices=p.devices</span><span class="s2">, </span><span class="s1">in_axes=p.in_axes_flat</span><span class="s2">,</span>
      <span class="s1">out_axes_thunk=p.out_axes_thunk</span><span class="s2">, </span><span class="s1">name=p.flat_fun.__name__</span><span class="s2">,</span>
      <span class="s1">donated_invars=p.donated_invars</span><span class="s2">,</span>
      <span class="s1">is_explicit_global_axis_size=p.is_explicit_global_axis_size</span><span class="s2">,</span>
  <span class="s1">)</span>
  <span class="s2">return </span><span class="s1">tree_util.tree_unflatten(p.out_tree()</span><span class="s2">, </span><span class="s1">out_flat)</span>


<span class="s0"># TODO(https://github.com/google/jax/issues/13552): Look into making this a</span>
<span class="s0"># method on jax.Array so that we can bypass the XLA compilation here.</span>
<span class="s2">def </span><span class="s1">_copy_impl(prim</span><span class="s2">, </span><span class="s1">*args</span><span class="s2">, </span><span class="s1">**kwargs):</span>
  <span class="s1">a</span><span class="s2">, </span><span class="s1">= args</span>
  <span class="s2">if </span><span class="s1">isinstance(a</span><span class="s2">, </span><span class="s1">jax.Array) </span><span class="s2">and </span><span class="s1">isinstance(a.sharding</span><span class="s2">, </span><span class="s1">PmapSharding):</span>
    <span class="s1">sharded_dim = _which_dim_sharded(a.sharding)</span>
    <span class="s2">return </span><span class="s1">_copy_impl_pmap_sharding(sharded_dim</span><span class="s2">, </span><span class="s1">*args</span><span class="s2">, </span><span class="s1">**kwargs)</span>
  <span class="s2">return </span><span class="s1">dispatch.apply_primitive(prim</span><span class="s2">, </span><span class="s1">*args</span><span class="s2">, </span><span class="s1">**kwargs)</span>

<span class="s0"># The copy_p primitive exists for expressing making copies of runtime arrays.</span>
<span class="s0"># For that reason we don't simplify it out of jaxprs (e.g. for jit invariance).</span>
<span class="s0"># It's used in jnp.array(x, copy=True), which is the user-facing API.</span>
<span class="s1">copy_p = core.Primitive(</span><span class="s3">'copy'</span><span class="s1">)</span>
<span class="s1">copy_p.def_impl(partial(_copy_impl</span><span class="s2">, </span><span class="s1">copy_p))</span>
<span class="s1">copy_p.def_abstract_eval(</span><span class="s2">lambda </span><span class="s1">x: x)</span>
<span class="s1">mlir.register_lowering(copy_p</span><span class="s2">, lambda </span><span class="s1">ctx</span><span class="s2">, </span><span class="s1">x: [x])</span>
<span class="s1">ad.deflinear(copy_p</span><span class="s2">, lambda </span><span class="s1">t: [copy_p.bind(t)])</span>
<span class="s1">pe.def_trivial_padding(copy_p)</span>
<span class="s1">batching.defvectorized(copy_p)</span>


<span class="s2">def </span><span class="s1">rng_bit_generator(key</span><span class="s2">, </span><span class="s1">shape</span><span class="s2">, </span><span class="s1">dtype=np.uint32</span><span class="s2">,</span>
                      <span class="s1">algorithm=RandomAlgorithm.RNG_DEFAULT):</span>
  <span class="s5">&quot;&quot;&quot;Stateless PRNG bit generator. Experimental and its use is discouraged. 
 
  Returns uniformly distributed random bits with the specified shape and dtype 
  (what is required to be an integer type) using the platform specific 
  default algorithm or the one specified. 
 
  It provides direct access to the RngBitGenerator primitive exposed by XLA 
  (https://www.tensorflow.org/xla/operation_semantics#rngbitgenerator) for low 
  level API access. 
 
  Most users should use `jax.random` instead for a stable and more user 
  friendly API. 
  &quot;&quot;&quot;</span>
  <span class="s1">shape = core.canonicalize_shape(shape)</span>
  <span class="s1">dtype = dtypes.canonicalize_dtype(dtype)</span>
  <span class="s2">if </span><span class="s1">np.dtype(dtype) </span><span class="s2">not in </span><span class="s1">{np.dtype(</span><span class="s3">'uint8'</span><span class="s1">)</span><span class="s2">, </span><span class="s1">np.dtype(</span><span class="s3">'uint16'</span><span class="s1">)</span><span class="s2">,</span>
                             <span class="s1">np.dtype(</span><span class="s3">'uint32'</span><span class="s1">)</span><span class="s2">, </span><span class="s1">np.dtype(</span><span class="s3">'uint64'</span><span class="s1">)}:</span>
    <span class="s2">raise </span><span class="s1">TypeError(</span><span class="s3">f'rng_bit_generator: unsupported dtype </span><span class="s2">{</span><span class="s1">dtype</span><span class="s2">}</span><span class="s3">'</span><span class="s1">)</span>
  <span class="s2">return </span><span class="s1">tuple(</span>
      <span class="s1">rng_bit_generator_p.bind(</span>
          <span class="s1">key</span><span class="s2">, </span><span class="s1">shape=shape</span><span class="s2">, </span><span class="s1">dtype=dtype</span><span class="s2">, </span><span class="s1">algorithm=algorithm))</span>


<span class="s2">def </span><span class="s1">_iota_abstract_eval(*</span><span class="s2">, </span><span class="s1">dtype</span><span class="s2">, </span><span class="s1">shape</span><span class="s2">, </span><span class="s1">dimension):</span>
  <span class="s1">_check_shapelike(</span><span class="s3">&quot;iota&quot;</span><span class="s2">, </span><span class="s3">&quot;shape&quot;</span><span class="s2">, </span><span class="s1">shape)</span>
  <span class="s2">if not </span><span class="s1">any(dtypes.issubdtype(dtype</span><span class="s2">, </span><span class="s1">t) </span><span class="s2">for </span><span class="s1">t </span><span class="s2">in </span><span class="s1">_num):</span>
    <span class="s1">msg = </span><span class="s3">'iota does not accept dtype {}. Accepted dtypes are subtypes of {}.'</span>
    <span class="s1">typename = str(np.dtype(dtype).name)</span>
    <span class="s1">accepted_typenames = (t.__name__ </span><span class="s2">for </span><span class="s1">t </span><span class="s2">in </span><span class="s1">_num)</span>
    <span class="s2">raise </span><span class="s1">TypeError(msg.format(typename</span><span class="s2">, </span><span class="s3">', '</span><span class="s1">.join(accepted_typenames)))</span>
  <span class="s2">if not </span><span class="s4">0 </span><span class="s1">&lt;= dimension &lt; len(shape):</span>
    <span class="s2">raise </span><span class="s1">ValueError(</span><span class="s3">&quot;iota dimension must be between 0 and len(shape), got &quot;</span>
                     <span class="s3">f&quot;</span><span class="s2">{</span><span class="s1">dimension=</span><span class="s2">} </span><span class="s3">for </span><span class="s2">{</span><span class="s1">shape=</span><span class="s2">}</span><span class="s3">&quot;</span><span class="s1">)</span>
  <span class="s2">if not </span><span class="s1">any(isinstance(d</span><span class="s2">, </span><span class="s1">core.DArray) </span><span class="s2">and</span>
             <span class="s1">type(core.get_aval(d).dtype) </span><span class="s2">is </span><span class="s1">core.bint </span><span class="s2">for </span><span class="s1">d </span><span class="s2">in </span><span class="s1">shape):</span>
    <span class="s2">return </span><span class="s1">ShapedArray(shape</span><span class="s2">, </span><span class="s1">dtype)</span>
  <span class="s0"># TODO(mattjj): unify DShapedArray with ShapedArray, and remove this code</span>
  <span class="s2">return </span><span class="s1">core.DShapedArray(shape</span><span class="s2">, </span><span class="s1">dtype</span><span class="s2">, False</span><span class="s1">)</span>

<span class="s1">iota_p = Primitive(</span><span class="s3">'iota'</span><span class="s1">)</span>
<span class="s1">iota_p.def_impl(partial(dispatch.apply_primitive</span><span class="s2">, </span><span class="s1">iota_p))</span>
<span class="s1">iota_p.def_abstract_eval(_iota_abstract_eval)</span>

<span class="s2">def </span><span class="s1">_iota_staging_rule(trace</span><span class="s2">, </span><span class="s1">*dyn_shape</span><span class="s2">, </span><span class="s1">dtype</span><span class="s2">, </span><span class="s1">shape</span><span class="s2">, </span><span class="s1">dimension):</span>
  <span class="s1">params = dict(dtype=dtype</span><span class="s2">, </span><span class="s1">shape=shape</span><span class="s2">, </span><span class="s1">dimension=dimension)</span>
  <span class="s2">if not </span><span class="s1">dyn_shape:</span>
    <span class="s2">return </span><span class="s1">trace.default_process_primitive(iota_p</span><span class="s2">, </span><span class="s1">()</span><span class="s2">, </span><span class="s1">params)</span>
  <span class="s1">aval = core.DShapedArray(_merge_dyn_shape(shape</span><span class="s2">, </span><span class="s1">dyn_shape)</span><span class="s2">, </span><span class="s1">dtype</span><span class="s2">, False</span><span class="s1">)</span>
  <span class="s2">return </span><span class="s1">_dyn_shape_staging_rule(trace</span><span class="s2">, </span><span class="s1">iota_p</span><span class="s2">, </span><span class="s1">aval</span><span class="s2">, </span><span class="s1">*dyn_shape</span><span class="s2">, </span><span class="s1">**params)</span>
<span class="s1">pe.custom_staging_rules[iota_p] = _iota_staging_rule</span>

<span class="s2">def </span><span class="s1">_iota_typecheck_rule(_</span><span class="s2">, </span><span class="s1">*dyn_shape</span><span class="s2">, </span><span class="s1">dtype</span><span class="s2">, </span><span class="s1">shape</span><span class="s2">, </span><span class="s1">dimension):</span>
  <span class="s2">if not </span><span class="s1">dyn_shape:</span>
    <span class="s1">out_aval</span><span class="s2">, </span><span class="s1">effects = iota_p.abstract_eval(</span>
        <span class="s1">dtype=dtype</span><span class="s2">, </span><span class="s1">shape=shape</span><span class="s2">, </span><span class="s1">dimension=dimension)</span>
    <span class="s2">return </span><span class="s1">[out_aval]</span><span class="s2">, </span><span class="s1">effects</span>
  <span class="s2">else</span><span class="s1">:</span>
    <span class="s1">out_shape = _merge_dyn_shape(shape</span><span class="s2">, </span><span class="s1">dyn_shape)</span>
    <span class="s1">out_shape = [x.val </span><span class="s2">if </span><span class="s1">type(x) </span><span class="s2">is </span><span class="s1">core.Literal </span><span class="s2">else </span><span class="s1">x </span><span class="s2">for </span><span class="s1">x </span><span class="s2">in </span><span class="s1">out_shape]  </span><span class="s0"># pytype: disable=attribute-error</span>
    <span class="s1">out_aval = core.DShapedArray(tuple(out_shape)</span><span class="s2">, </span><span class="s1">dtype</span><span class="s2">, False</span><span class="s1">)</span>
    <span class="s2">return </span><span class="s1">[out_aval]</span><span class="s2">, </span><span class="s1">core.no_effects</span>
<span class="s1">core.custom_typechecks[iota_p] = _iota_typecheck_rule</span>

<span class="s2">def </span><span class="s1">_iota_lower(ctx</span><span class="s2">, </span><span class="s1">*dyn_shape</span><span class="s2">, </span><span class="s1">dtype</span><span class="s2">, </span><span class="s1">shape</span><span class="s2">, </span><span class="s1">dimension):</span>
  <span class="s2">del </span><span class="s1">dtype</span>
  <span class="s1">aval_out</span><span class="s2">, </span><span class="s1">= ctx.avals_out</span>
  <span class="s2">if </span><span class="s1">dyn_shape:</span>
    <span class="s1">aval_out = aval_out.update(shape=_merge_dyn_shape(shape</span><span class="s2">, </span><span class="s1">dyn_shape))</span>
  <span class="s2">if not </span><span class="s1">core.is_constant_shape(aval_out.shape):</span>
    <span class="s1">shape = mlir.eval_dynamic_shape(ctx</span><span class="s2">, </span><span class="s1">aval_out.shape)</span>
    <span class="s2">return </span><span class="s1">hlo.DynamicIotaOp(</span>
        <span class="s1">mlir.aval_to_ir_type(aval_out)</span><span class="s2">,</span>
        <span class="s1">mlir.shape_tensor(shape)</span><span class="s2">,</span>
        <span class="s1">mlir.i64_attr(dimension)</span><span class="s2">,</span>
    <span class="s1">).results</span>
  <span class="s2">else</span><span class="s1">:</span>
    <span class="s2">return </span><span class="s1">hlo.IotaOp(mlir.aval_to_ir_type(aval_out)</span><span class="s2">,</span>
                      <span class="s1">mlir.i64_attr(dimension)).results</span>
<span class="s1">mlir.register_lowering(iota_p</span><span class="s2">, </span><span class="s1">_iota_lower)</span>

<span class="s2">def </span><span class="s1">_iota_batching_rule(in_vals</span><span class="s2">, </span><span class="s1">in_dims</span><span class="s2">, </span><span class="s1">*</span><span class="s2">, </span><span class="s1">dtype</span><span class="s2">, </span><span class="s1">shape</span><span class="s2">, </span><span class="s1">dimension):</span>
  <span class="s1">(segment_lengths</span><span class="s2">,</span><span class="s1">)</span><span class="s2">, </span><span class="s1">(ax</span><span class="s2">,</span><span class="s1">) = in_vals</span><span class="s2">, </span><span class="s1">in_dims</span>
  <span class="s1">shapes = [_merge_dyn_shape(shape</span><span class="s2">, </span><span class="s1">(d</span><span class="s2">,</span><span class="s1">)) </span><span class="s2">for </span><span class="s1">d </span><span class="s2">in </span><span class="s1">segment_lengths]</span>
  <span class="s1">iotas = [broadcasted_iota(dtype</span><span class="s2">, </span><span class="s1">s</span><span class="s2">, </span><span class="s1">dimension) </span><span class="s2">for </span><span class="s1">s </span><span class="s2">in </span><span class="s1">shapes]</span>
  <span class="s2">return </span><span class="s1">concatenate(iotas</span><span class="s2">, </span><span class="s1">dimension)</span><span class="s2">, </span><span class="s1">batching.ConcatAxis(ax</span><span class="s2">, </span><span class="s1">segment_lengths)</span>
<span class="s1">batching.primitive_batchers[iota_p] = _iota_batching_rule</span>

<span class="s2">def </span><span class="s1">_iota_pp_rule(eqn</span><span class="s2">, </span><span class="s1">context</span><span class="s2">, </span><span class="s1">settings):</span>
  <span class="s1">printed_params = {}</span>
  <span class="s2">if </span><span class="s1">len(eqn.params[</span><span class="s3">'shape'</span><span class="s1">]) &gt; </span><span class="s4">1</span><span class="s1">:</span>
    <span class="s1">printed_params[</span><span class="s3">'dimension'</span><span class="s1">] = eqn.params[</span><span class="s3">'dimension'</span><span class="s1">]</span>
  <span class="s2">return </span><span class="s1">core._pp_eqn(eqn.replace(params=printed_params)</span><span class="s2">, </span><span class="s1">context</span><span class="s2">, </span><span class="s1">settings)</span>
<span class="s0"># core.pp_eqn_rules[iota_p] = _iota_pp_rule</span>

<span class="s2">def </span><span class="s1">_iota_padding_rule(in_avals</span><span class="s2">, </span><span class="s1">out_avals</span><span class="s2">, </span><span class="s1">*dyn_shape</span><span class="s2">, </span><span class="s1">dtype</span><span class="s2">, </span><span class="s1">shape</span><span class="s2">, </span><span class="s1">dimension):</span>
  <span class="s1">out_aval</span><span class="s2">, </span><span class="s1">= out_avals</span>
  <span class="s1">new_shape = []</span>
  <span class="s1">new_dyn_shape = []</span>
  <span class="s2">for </span><span class="s1">d </span><span class="s2">in </span><span class="s1">out_aval.shape:</span>
    <span class="s2">if </span><span class="s1">type(d) </span><span class="s2">is </span><span class="s1">pe.BoundedAxisSize:</span>
      <span class="s1">new_shape.append(d.bound)</span>
    <span class="s2">elif </span><span class="s1">type(d) </span><span class="s2">is </span><span class="s1">int:</span>
      <span class="s1">new_shape.append(d)</span>
    <span class="s2">else</span><span class="s1">:</span>
      <span class="s2">assert </span><span class="s1">isinstance(d</span><span class="s2">, </span><span class="s1">core.Tracer)</span>
      <span class="s1">new_shape.append(</span><span class="s2">None</span><span class="s1">)</span>
      <span class="s1">new_dyn_shape.append(d)</span>
  <span class="s2">return </span><span class="s1">[iota_p.bind(*new_dyn_shape</span><span class="s2">, </span><span class="s1">shape=tuple(new_shape)</span><span class="s2">,</span>
                      <span class="s1">dtype=dtype</span><span class="s2">, </span><span class="s1">dimension=dimension)]</span>
<span class="s1">pe.padding_rules[iota_p] = _iota_padding_rule</span>


<span class="s0">### util</span>

<span class="s1">_ndim = np.ndim</span>


<span class="s2">def </span><span class="s1">_dilate_shape(shape</span><span class="s2">, </span><span class="s1">dilation):</span>
  <span class="s5">&quot;&quot;&quot;Utility function for computing the shape resulting from a dilation.&quot;&quot;&quot;</span>
  <span class="s2">if not </span><span class="s1">np.all(np.greater(dilation</span><span class="s2">, </span><span class="s4">0</span><span class="s1">)):</span>
    <span class="s1">msg = </span><span class="s3">&quot;All dilations must be positive, got {}.&quot;</span>
    <span class="s2">raise </span><span class="s1">TypeError(msg.format(dilation))</span>
  <span class="s1">dilation = (</span><span class="s4">1</span><span class="s2">,</span><span class="s1">) * (len(shape) - len(dilation)) + tuple(dilation)</span>
  <span class="s2">return </span><span class="s1">core.dilate_shape(shape</span><span class="s2">, </span><span class="s1">dilation)</span>

<span class="s2">def </span><span class="s1">_ceil_divide(x1</span><span class="s2">, </span><span class="s1">x2):</span>
  <span class="s2">return </span><span class="s1">-np.floor_divide(np.negative(x1)</span><span class="s2">, </span><span class="s1">x2)</span>


<span class="s2">class </span><span class="s1">PaddingType(enum.Enum):</span>
  <span class="s1">VALID = </span><span class="s4">1</span>
  <span class="s1">SAME = </span><span class="s4">2</span>
  <span class="s1">SAME_LOWER = </span><span class="s4">3</span>


<span class="s2">def </span><span class="s1">padtype_to_pads(in_shape</span><span class="s2">, </span><span class="s1">window_shape</span><span class="s2">, </span><span class="s1">window_strides</span><span class="s2">, </span><span class="s1">padding):</span>
  <span class="s5">&quot;&quot;&quot;Convert padding string to list of pairs of pad values.&quot;&quot;&quot;</span>

  <span class="s2">if </span><span class="s1">isinstance(padding</span><span class="s2">, </span><span class="s1">str):</span>
    <span class="s1">mapping = {</span>
        <span class="s3">'VALID'</span><span class="s1">: PaddingType.VALID</span><span class="s2">,</span>
        <span class="s3">'SAME'</span><span class="s1">: PaddingType.SAME</span><span class="s2">,</span>
        <span class="s3">'SAME_LOWER'</span><span class="s1">: PaddingType.SAME_LOWER</span><span class="s2">,</span>
    <span class="s1">}</span>
    <span class="s2">try</span><span class="s1">:</span>
      <span class="s1">padding = mapping[padding.upper()]</span>
    <span class="s2">except </span><span class="s1">KeyError </span><span class="s2">as </span><span class="s1">err:</span>
      <span class="s1">msg = </span><span class="s3">&quot;Unrecognized padding type: expected 'VALID' or 'SAME', got {}.&quot;</span>
      <span class="s2">raise </span><span class="s1">RuntimeError(msg.format(padding)) </span><span class="s2">from </span><span class="s1">err</span>

  <span class="s2">if </span><span class="s1">padding == PaddingType.SAME </span><span class="s2">or </span><span class="s1">padding == PaddingType.SAME_LOWER:</span>
    <span class="s1">out_shape = _ceil_divide(in_shape</span><span class="s2">, </span><span class="s1">window_strides)</span>
    <span class="s1">pad_sizes = np.maximum(</span><span class="s4">0</span><span class="s2">, </span><span class="s1">(out_shape - </span><span class="s4">1</span><span class="s1">) * window_strides +</span>
                                <span class="s1">window_shape - in_shape)</span>
    <span class="s2">if </span><span class="s1">padding == PaddingType.SAME:</span>
      <span class="s2">return </span><span class="s1">[</span>
          <span class="s1">(pad_size // </span><span class="s4">2</span><span class="s2">, </span><span class="s1">pad_size - pad_size // </span><span class="s4">2</span><span class="s1">) </span><span class="s2">for </span><span class="s1">pad_size </span><span class="s2">in </span><span class="s1">pad_sizes</span>
      <span class="s1">]</span>
    <span class="s2">else</span><span class="s1">:</span>
      <span class="s2">return </span><span class="s1">[</span>
          <span class="s1">(pad_size - pad_size // </span><span class="s4">2</span><span class="s2">, </span><span class="s1">pad_size // </span><span class="s4">2</span><span class="s1">) </span><span class="s2">for </span><span class="s1">pad_size </span><span class="s2">in </span><span class="s1">pad_sizes</span>
      <span class="s1">]</span>
  <span class="s2">elif </span><span class="s1">padding == PaddingType.VALID:</span>
    <span class="s2">return </span><span class="s1">[(</span><span class="s4">0</span><span class="s2">, </span><span class="s4">0</span><span class="s1">)] * len(in_shape)</span>
  <span class="s2">else</span><span class="s1">:</span>
    <span class="s1">msg = </span><span class="s3">&quot;Unknown padding type: {}.&quot;</span>
    <span class="s2">raise </span><span class="s1">TypeError(msg.format(padding))</span>


<span class="s0"># Map of lax function to equivalent jax.numpy function for use in error string below.</span>
<span class="s1">_JNP_FUNCTION_EQUIVALENTS = {</span>
  <span class="s3">'abs'</span><span class="s1">: </span><span class="s3">'fabs'</span><span class="s2">,</span>
  <span class="s3">'acos'</span><span class="s1">: </span><span class="s3">'arccos'</span><span class="s2">,</span>
  <span class="s3">'acosh'</span><span class="s1">: </span><span class="s3">'arccosh'</span><span class="s2">,</span>
  <span class="s3">'add'</span><span class="s1">: </span><span class="s3">'add'</span><span class="s2">,</span>
  <span class="s3">'asin'</span><span class="s1">: </span><span class="s3">'arcsin'</span><span class="s2">,</span>
  <span class="s3">'asinh'</span><span class="s1">: </span><span class="s3">'arcsinh'</span><span class="s2">,</span>
  <span class="s3">'atan'</span><span class="s1">: </span><span class="s3">'arctan'</span><span class="s2">,</span>
  <span class="s3">'atan2'</span><span class="s1">: </span><span class="s3">'arctan2'</span><span class="s2">,</span>
  <span class="s3">'atanh'</span><span class="s1">: </span><span class="s3">'arctanh'</span><span class="s2">,</span>
  <span class="s3">'bitwise_and'</span><span class="s1">: </span><span class="s3">'bitwise_and'</span><span class="s2">,</span>
  <span class="s3">'bitwise_not'</span><span class="s1">: </span><span class="s3">'bitwise_not'</span><span class="s2">,</span>
  <span class="s3">'bitwise_or'</span><span class="s1">: </span><span class="s3">'bitwise_or'</span><span class="s2">,</span>
  <span class="s3">'bitwise_xor'</span><span class="s1">: </span><span class="s3">'bitwise_xor'</span><span class="s2">,</span>
  <span class="s3">'cbrt'</span><span class="s1">: </span><span class="s3">'cbrt'</span><span class="s2">,</span>
  <span class="s3">'ceil'</span><span class="s1">: </span><span class="s3">'ceil'</span><span class="s2">,</span>
  <span class="s3">'concatenate'</span><span class="s1">: </span><span class="s3">'concatenate'</span><span class="s2">,</span>
  <span class="s3">'cos'</span><span class="s1">: </span><span class="s3">'cos'</span><span class="s2">,</span>
  <span class="s3">'cosh'</span><span class="s1">: </span><span class="s3">'cosh'</span><span class="s2">,</span>
  <span class="s3">'div'</span><span class="s1">: </span><span class="s3">'divide'</span><span class="s2">,</span>
  <span class="s3">'eq'</span><span class="s1">: </span><span class="s3">'equal'</span><span class="s2">,</span>
  <span class="s3">'exp'</span><span class="s1">: </span><span class="s3">'exp'</span><span class="s2">,</span>
  <span class="s3">'expm1'</span><span class="s1">: </span><span class="s3">'expm1'</span><span class="s2">,</span>
  <span class="s3">'floor'</span><span class="s1">: </span><span class="s3">'floor'</span><span class="s2">,</span>
  <span class="s3">'greater'</span><span class="s1">: </span><span class="s3">'greater'</span><span class="s2">,</span>
  <span class="s3">'greater_equal'</span><span class="s1">: </span><span class="s3">'greater_equal'</span><span class="s2">,</span>
  <span class="s3">'less'</span><span class="s1">: </span><span class="s3">'less'</span><span class="s2">,</span>
  <span class="s3">'less_equal'</span><span class="s1">: </span><span class="s3">'less_equal'</span><span class="s2">,</span>
  <span class="s3">'log'</span><span class="s1">: </span><span class="s3">'log'</span><span class="s2">,</span>
  <span class="s3">'logical_and'</span><span class="s1">: </span><span class="s3">'logical_and'</span><span class="s2">,</span>
  <span class="s3">'logical_not'</span><span class="s1">: </span><span class="s3">'logical_not'</span><span class="s2">,</span>
  <span class="s3">'logical_or'</span><span class="s1">: </span><span class="s3">'logical_or'</span><span class="s2">,</span>
  <span class="s3">'logical_xor'</span><span class="s1">: </span><span class="s3">'logical_xor'</span><span class="s2">,</span>
  <span class="s3">'log1p'</span><span class="s1">: </span><span class="s3">'log1p'</span><span class="s2">,</span>
  <span class="s3">'max'</span><span class="s1">: </span><span class="s3">'maximum'</span><span class="s2">,</span>
  <span class="s3">'min'</span><span class="s1">: </span><span class="s3">'minimum'</span><span class="s2">,</span>
  <span class="s3">'mul'</span><span class="s1">: </span><span class="s3">'multiply'</span><span class="s2">,</span>
  <span class="s3">'ne'</span><span class="s1">: </span><span class="s3">'not_equal'</span><span class="s2">,</span>
  <span class="s3">'neg'</span><span class="s1">: </span><span class="s3">'negative'</span><span class="s2">,</span>
  <span class="s3">'nextafter'</span><span class="s1">: </span><span class="s3">'nextafter'</span><span class="s2">,</span>
  <span class="s3">'pow'</span><span class="s1">: </span><span class="s3">'float_power'</span><span class="s2">,</span>
  <span class="s3">'rount'</span><span class="s1">: </span><span class="s3">'rount'</span><span class="s2">,</span>
  <span class="s3">'select'</span><span class="s1">: </span><span class="s3">'where'</span><span class="s2">,</span>
  <span class="s3">'shift_left'</span><span class="s1">: </span><span class="s3">'left_shift'</span><span class="s2">,</span>
  <span class="s3">'shift_right_logical'</span><span class="s1">: </span><span class="s3">'right_shift'</span><span class="s2">,</span>
  <span class="s3">'shift_right_arithmetic'</span><span class="s1">: </span><span class="s3">'right_shift'</span><span class="s2">,</span>
  <span class="s3">'sign'</span><span class="s1">: </span><span class="s3">'sign'</span><span class="s2">,</span>
  <span class="s3">'sin'</span><span class="s1">: </span><span class="s3">'sin'</span><span class="s2">,</span>
  <span class="s3">'sinh'</span><span class="s1">: </span><span class="s3">'sinh'</span><span class="s2">,</span>
  <span class="s3">'sqrt'</span><span class="s1">: </span><span class="s3">'sqrt'</span><span class="s2">,</span>
  <span class="s3">'sub'</span><span class="s1">: </span><span class="s3">'subtract'</span><span class="s2">,</span>
  <span class="s3">'tan'</span><span class="s1">: </span><span class="s3">'tan'</span><span class="s2">,</span>
  <span class="s3">'tanh'</span><span class="s1">: </span><span class="s3">'tanh'</span>
<span class="s1">}</span>

<span class="s2">def </span><span class="s1">_check_same_dtypes(name</span><span class="s2">, </span><span class="s1">ignore_fp_precision</span><span class="s2">, </span><span class="s1">*ttypes):</span>
  <span class="s5">&quot;&quot;&quot;Check that dtypes agree, possibly ignoring float precision.&quot;&quot;&quot;</span>
  <span class="s0"># the `ignore_fp_precision` flag exists because the XLA shape inference logic</span>
  <span class="s0"># allows mixed floating point precision, but the HLO verifier often rejects it</span>
  <span class="s2">if </span><span class="s1">any(core.is_opaque_dtype(t) </span><span class="s2">for </span><span class="s1">t </span><span class="s2">in </span><span class="s1">ttypes):</span>
    <span class="s2">return  </span><span class="s0"># TODO(mattjj,frostig): do some checking, friend</span>
  <span class="s1">types = map(np.dtype</span><span class="s2">, </span><span class="s1">ttypes)  </span><span class="s0"># canonicalize</span>
  <span class="s2">if </span><span class="s1">ignore_fp_precision:</span>
    <span class="s1">types = [</span>
        <span class="s1">np.floating </span><span class="s2">if </span><span class="s1">dtypes.issubdtype(dtype</span><span class="s2">, </span><span class="s1">np.floating)</span>
        <span class="s2">else </span><span class="s1">np.complexfloating </span><span class="s2">if </span><span class="s1">dtypes.issubdtype(dtype</span><span class="s2">, </span><span class="s1">np.complexfloating)</span>
        <span class="s2">else </span><span class="s1">dtype </span><span class="s2">for </span><span class="s1">dtype </span><span class="s2">in </span><span class="s1">types]</span>
  <span class="s2">if </span><span class="s1">len({dtypes.canonicalize_dtype(t) </span><span class="s2">for </span><span class="s1">t </span><span class="s2">in </span><span class="s1">types}) != </span><span class="s4">1</span><span class="s1">:</span>
    <span class="s2">if </span><span class="s1">ignore_fp_precision:</span>
      <span class="s1">msg = (</span><span class="s3">&quot;lax.{} requires arguments to have same dtypes up to floating point &quot;</span>
             <span class="s3">&quot;precision, got {}.&quot;</span><span class="s1">)</span>
    <span class="s2">else</span><span class="s1">:</span>
      <span class="s1">msg = </span><span class="s3">&quot;lax.{} requires arguments to have the same dtypes, got {}.&quot;</span>
    <span class="s2">if </span><span class="s1">name </span><span class="s2">in </span><span class="s1">_JNP_FUNCTION_EQUIVALENTS:</span>
      <span class="s1">equiv = _JNP_FUNCTION_EQUIVALENTS[name]</span>
      <span class="s1">msg += </span><span class="s3">f&quot; (Tip: jnp.</span><span class="s2">{</span><span class="s1">equiv</span><span class="s2">} </span><span class="s3">is a similar function that does automatic type promotion on inputs).&quot;</span>
    <span class="s2">raise </span><span class="s1">TypeError(msg.format(name</span><span class="s2">, </span><span class="s3">&quot;, &quot;</span><span class="s1">.join(map(str</span><span class="s2">, </span><span class="s1">types))))</span>


<span class="s2">def </span><span class="s1">_check_shapelike(fun_name</span><span class="s2">, </span><span class="s1">arg_name</span><span class="s2">, </span><span class="s1">obj</span><span class="s2">, </span><span class="s1">non_zero_shape=</span><span class="s2">False</span><span class="s1">):</span>
  <span class="s5">&quot;&quot;&quot;Check that `obj` is a shape-like value (e.g. tuple of nonnegative ints).&quot;&quot;&quot;</span>
  <span class="s2">if not </span><span class="s1">isinstance(obj</span><span class="s2">, </span><span class="s1">(tuple</span><span class="s2">, </span><span class="s1">list</span><span class="s2">, </span><span class="s1">np.ndarray)):</span>
    <span class="s1">msg = </span><span class="s3">&quot;{} {} must be of type tuple/list/ndarray, got {}.&quot;</span>
    <span class="s2">raise </span><span class="s1">TypeError(msg.format(fun_name</span><span class="s2">, </span><span class="s1">arg_name</span><span class="s2">, </span><span class="s1">type(obj)))</span>
  <span class="s0"># bool(obj) for an ndarray raises an error, so we check len</span>
  <span class="s2">if not </span><span class="s1">len(obj):  </span><span class="s0"># pylint: disable=g-explicit-length-test</span>
    <span class="s2">return</span>
  <span class="s2">if </span><span class="s1">(config.jax_dynamic_shapes </span><span class="s2">and </span><span class="s1">isinstance(obj</span><span class="s2">, </span><span class="s1">(tuple</span><span class="s2">, </span><span class="s1">list)) </span><span class="s2">and</span>
      <span class="s1">any(isinstance(d</span><span class="s2">, </span><span class="s1">(core.Tracer</span><span class="s2">, </span><span class="s1">core.DArray)) </span><span class="s2">for </span><span class="s1">d </span><span class="s2">in </span><span class="s1">obj)):</span>
    <span class="s2">return  </span><span class="s0"># TODO(mattjj): handle more checks in the dynamic shape case</span>
  <span class="s1">obj_arr = np.array(obj)</span>
  <span class="s2">if </span><span class="s1">obj_arr.ndim != </span><span class="s4">1</span><span class="s1">:</span>
    <span class="s1">msg = </span><span class="s3">&quot;{} {} must be 1-dimensional, got {}.&quot;</span>
    <span class="s2">raise </span><span class="s1">TypeError(msg.format(obj_arr.ndim))</span>
  <span class="s2">try</span><span class="s1">:</span>
    <span class="s1">canonicalize_shape(obj_arr)</span>
  <span class="s2">except </span><span class="s1">TypeError </span><span class="s2">as </span><span class="s1">err:</span>
    <span class="s1">msg = </span><span class="s3">&quot;{} {} must have every element be an integer type, got {}.&quot;</span>
    <span class="s2">raise </span><span class="s1">TypeError(msg.format(fun_name</span><span class="s2">, </span><span class="s1">arg_name</span><span class="s2">, </span><span class="s1">tuple(map(type</span><span class="s2">, </span><span class="s1">obj)))) </span><span class="s2">from </span><span class="s1">err</span>
  <span class="s1">lower_bound</span><span class="s2">, </span><span class="s1">bound_error = (</span>
      <span class="s1">(</span><span class="s4">1</span><span class="s2">, </span><span class="s3">&quot;strictly positive&quot;</span><span class="s1">) </span><span class="s2">if </span><span class="s1">non_zero_shape </span><span class="s2">else </span><span class="s1">(</span><span class="s4">0</span><span class="s2">, </span><span class="s3">&quot;nonnegative&quot;</span><span class="s1">))</span>
  <span class="s2">if not </span><span class="s1">all(core.greater_equal_dim(d</span><span class="s2">, </span><span class="s1">lower_bound) </span><span class="s2">for </span><span class="s1">d </span><span class="s2">in </span><span class="s1">obj_arr):</span>
    <span class="s1">msg = </span><span class="s3">&quot;{} {} must have every element be {}, got {}.&quot;</span>
    <span class="s2">raise </span><span class="s1">TypeError(msg.format(fun_name</span><span class="s2">, </span><span class="s1">arg_name</span><span class="s2">, </span><span class="s1">bound_error</span><span class="s2">, </span><span class="s1">obj))</span>


<span class="s2">def </span><span class="s1">_const(example</span><span class="s2">, </span><span class="s1">val):</span>
  <span class="s1">dtype = _dtype(example)</span>
  <span class="s2">if </span><span class="s1">dtypes.is_python_scalar(example):</span>
    <span class="s1">val = dtypes.scalar_type_of(example)(val)</span>
    <span class="s2">return </span><span class="s1">val </span><span class="s2">if </span><span class="s1">dtype == _dtype(val) </span><span class="s2">else </span><span class="s1">np.array(val</span><span class="s2">, </span><span class="s1">dtype)</span>
  <span class="s2">return </span><span class="s1">np.array(val</span><span class="s2">, </span><span class="s1">dtype)</span>

<span class="s1">_zeros: Callable = partial(full_like</span><span class="s2">, </span><span class="s1">fill_value=</span><span class="s4">0</span><span class="s1">)</span>
<span class="s1">_zero: Callable = partial(full_like</span><span class="s2">, </span><span class="s1">shape=()</span><span class="s2">, </span><span class="s1">fill_value=</span><span class="s4">0</span><span class="s1">)</span>
<span class="s1">_ones: Callable = partial(full_like</span><span class="s2">, </span><span class="s1">fill_value=</span><span class="s4">1</span><span class="s1">)</span>
<span class="s1">_one: Callable = partial(full_like</span><span class="s2">, </span><span class="s1">shape=()</span><span class="s2">, </span><span class="s1">fill_value=</span><span class="s4">1</span><span class="s1">)</span>
<span class="s1">_twos: Callable = partial(full_like</span><span class="s2">, </span><span class="s1">fill_value=</span><span class="s4">2</span><span class="s1">)</span>
<span class="s1">_two: Callable = partial(full_like</span><span class="s2">, </span><span class="s1">shape=()</span><span class="s2">, </span><span class="s1">fill_value=</span><span class="s4">2</span><span class="s1">)</span>

<span class="s1">dtype: Callable = partial(dtypes.dtype</span><span class="s2">, </span><span class="s1">canonicalize=</span><span class="s2">True</span><span class="s1">)</span>
<span class="s1">_dtype: Callable = partial(dtypes.dtype</span><span class="s2">, </span><span class="s1">canonicalize=</span><span class="s2">True</span><span class="s1">)</span>

<span class="s2">def </span><span class="s1">_isnan(x: ArrayLike) -&gt; Array:</span>
  <span class="s2">return </span><span class="s1">ne(x</span><span class="s2">, </span><span class="s1">x)</span>

<span class="s2">def </span><span class="s1">_iscomplex(x) -&gt; bool:</span>
  <span class="s2">return </span><span class="s1">dtypes.issubdtype(_dtype(x)</span><span class="s2">, </span><span class="s1">np.complexfloating)</span>


<span class="s2">def </span><span class="s1">ranges_like(*xs):</span>
  <span class="s1">start = </span><span class="s4">0</span>
  <span class="s2">for </span><span class="s1">x </span><span class="s2">in </span><span class="s1">xs:</span>
    <span class="s1">x_len = len(x)</span>
    <span class="s2">yield </span><span class="s1">range(start</span><span class="s2">, </span><span class="s1">start + x_len)</span>
    <span class="s1">start += x_len</span>


<span class="s2">def </span><span class="s1">remaining(original</span><span class="s2">, </span><span class="s1">*removed_lists):</span>
  <span class="s1">removed = set(itertools.chain(*removed_lists))</span>
  <span class="s2">return </span><span class="s1">[i </span><span class="s2">for </span><span class="s1">i </span><span class="s2">in </span><span class="s1">original </span><span class="s2">if </span><span class="s1">i </span><span class="s2">not in </span><span class="s1">removed]</span>


<span class="s2">def </span><span class="s1">canonicalize_precision(precision: PrecisionLike) -&gt; Optional[Tuple[PrecisionType</span><span class="s2">, </span><span class="s1">PrecisionType]]:</span>
  <span class="s5">&quot;&quot;&quot;Turns an API precision specification, into a pair of enumeration values. 
 
  The API can take the precision as a string, or int, and either as a single 
  value to apply to both operands, or as a sequence of two values. 
  &quot;&quot;&quot;</span>
  <span class="s2">if </span><span class="s1">precision </span><span class="s2">is None</span><span class="s1">:</span>
    <span class="s2">if </span><span class="s1">config.jax_default_matmul_precision </span><span class="s2">is None</span><span class="s1">:</span>
      <span class="s2">return None</span>
    <span class="s2">try</span><span class="s1">:</span>
      <span class="s2">return </span><span class="s1">type_cast(</span>
          <span class="s1">Tuple[PrecisionType</span><span class="s2">, </span><span class="s1">PrecisionType]</span><span class="s2">,</span>
          <span class="s1">(Precision(config.jax_default_matmul_precision)</span><span class="s2">,</span>
           <span class="s1">Precision(config.jax_default_matmul_precision)))</span>
    <span class="s2">except </span><span class="s1">TypeError:</span>
      <span class="s2">raise </span><span class="s1">ValueError(</span>
          <span class="s3">&quot;jax_default_matmul_precision flag must be set to None or a value in &quot;</span>
          <span class="s3">f&quot;</span><span class="s2">{</span><span class="s1">list(Precision._strings)</span><span class="s2">}</span><span class="s3">, but got </span><span class="s2">{</span><span class="s1">config.jax_default_matmul_precision</span><span class="s2">}</span><span class="s3">&quot;</span>
      <span class="s1">) </span><span class="s2">from None</span>
  <span class="s2">elif </span><span class="s1">isinstance(precision</span><span class="s2">, </span><span class="s1">str) </span><span class="s2">and </span><span class="s1">precision </span><span class="s2">in </span><span class="s1">Precision._strings:</span>
    <span class="s2">return </span><span class="s1">type_cast(Tuple[PrecisionType</span><span class="s2">, </span><span class="s1">PrecisionType]</span><span class="s2">,</span>
                     <span class="s1">(Precision(precision)</span><span class="s2">, </span><span class="s1">Precision(precision)))</span>
  <span class="s2">elif </span><span class="s1">isinstance(precision</span><span class="s2">, </span><span class="s1">xla_client.PrecisionConfig.Precision):</span>
    <span class="s2">return </span><span class="s1">type_cast(Tuple[PrecisionType</span><span class="s2">, </span><span class="s1">PrecisionType]</span><span class="s2">, </span><span class="s1">(precision</span><span class="s2">, </span><span class="s1">precision))</span>
  <span class="s2">elif </span><span class="s1">(isinstance(precision</span><span class="s2">, </span><span class="s1">(list</span><span class="s2">, </span><span class="s1">tuple)) </span><span class="s2">and </span><span class="s1">len(precision) == </span><span class="s4">2 </span><span class="s2">and</span>
        <span class="s1">all(isinstance(p</span><span class="s2">, </span><span class="s1">xla_client.PrecisionConfig.Precision) </span><span class="s2">for </span><span class="s1">p </span><span class="s2">in </span><span class="s1">precision)):</span>
    <span class="s2">return </span><span class="s1">type_cast(Tuple[PrecisionType</span><span class="s2">, </span><span class="s1">PrecisionType]</span><span class="s2">, </span><span class="s1">precision)</span>
  <span class="s2">elif </span><span class="s1">(isinstance(precision</span><span class="s2">, </span><span class="s1">(list</span><span class="s2">, </span><span class="s1">tuple)) </span><span class="s2">and </span><span class="s1">len(precision) == </span><span class="s4">2 </span><span class="s2">and</span>
        <span class="s1">all(isinstance(s</span><span class="s2">, </span><span class="s1">str) </span><span class="s2">for </span><span class="s1">s </span><span class="s2">in </span><span class="s1">precision)):</span>
    <span class="s1">s1</span><span class="s2">, </span><span class="s1">s2 = precision</span>
    <span class="s1">p1 = type_cast(Tuple[PrecisionType</span><span class="s2">, </span><span class="s1">PrecisionType]</span><span class="s2">, </span><span class="s1">canonicalize_precision(s1))[</span><span class="s4">0</span><span class="s1">]</span>
    <span class="s1">p2 = type_cast(Tuple[PrecisionType</span><span class="s2">, </span><span class="s1">PrecisionType]</span><span class="s2">, </span><span class="s1">canonicalize_precision(s2))[</span><span class="s4">0</span><span class="s1">]</span>
    <span class="s2">return </span><span class="s1">(p1</span><span class="s2">, </span><span class="s1">p2)</span>
  <span class="s2">else</span><span class="s1">:</span>
    <span class="s2">raise </span><span class="s1">ValueError(</span>
        <span class="s3">f&quot;Precision argument must be None, a string in </span><span class="s2">{</span><span class="s1">list(Precision._strings)</span><span class="s2">}</span><span class="s3">, &quot;</span>
        <span class="s3">&quot;a lax.Precision value or a tuple of two lax.Precision values or &quot;</span>
        <span class="s3">f&quot;strings; got </span><span class="s2">{</span><span class="s1">precision</span><span class="s2">}</span><span class="s3">.&quot;</span><span class="s1">)</span>

<span class="s2">def </span><span class="s1">_balanced_eq(x</span><span class="s2">, </span><span class="s1">z</span><span class="s2">, </span><span class="s1">y):</span>
  <span class="s2">return </span><span class="s1">div(select(_eq_meet(x</span><span class="s2">, </span><span class="s1">z)</span><span class="s2">, </span><span class="s1">_ones(z)</span><span class="s2">, </span><span class="s1">_zeros(z))</span><span class="s2">,</span>
             <span class="s1">select(_eq_meet(y</span><span class="s2">, </span><span class="s1">z)</span><span class="s2">, </span><span class="s1">_twos(z)</span><span class="s2">, </span><span class="s1">_ones(z)))</span>


<span class="s2">def </span><span class="s1">_eq_meet(a</span><span class="s2">, </span><span class="s1">b):</span>
  <span class="s1">a_dtype</span><span class="s2">, </span><span class="s1">b_dtype = _dtype(a)</span><span class="s2">, </span><span class="s1">_dtype(b)</span>
  <span class="s2">if </span><span class="s1">a_dtype != b_dtype:</span>
    <span class="s1">higher_dtype = dtypes.promote_types(a_dtype</span><span class="s2">, </span><span class="s1">b_dtype)</span>
    <span class="s2">if </span><span class="s1">higher_dtype == a_dtype:</span>
      <span class="s1">a = convert_element_type(a</span><span class="s2">, </span><span class="s1">b_dtype)</span>
    <span class="s2">else</span><span class="s1">:</span>
      <span class="s1">b = convert_element_type(b</span><span class="s2">, </span><span class="s1">a_dtype)</span>
  <span class="s2">return </span><span class="s1">eq(a</span><span class="s2">, </span><span class="s1">b)</span>


<span class="s2">def </span><span class="s1">_abstractify(x):</span>
  <span class="s2">return </span><span class="s1">raise_to_shaped(core.get_aval(x))</span>


<span class="s2">def </span><span class="s1">empty(dtype):</span>
  <span class="s2">return </span><span class="s1">empty_p.bind(dtype=dtype)</span>
<span class="s1">empty_p = core.Primitive(</span><span class="s3">'empty'</span><span class="s1">)</span>
<span class="s1">empty_p.def_abstract_eval(</span><span class="s2">lambda </span><span class="s1">*</span><span class="s2">, </span><span class="s1">dtype: core.ShapedArray(()</span><span class="s2">, </span><span class="s1">dtype))</span>
<span class="s2">def </span><span class="s1">_empty_lower(ctx</span><span class="s2">, </span><span class="s1">*</span><span class="s2">, </span><span class="s1">dtype):</span>
  <span class="s2">if </span><span class="s1">core.is_opaque_dtype(dtype):</span>
    <span class="s2">return </span><span class="s1">dtype._rules.empty_mlir(ctx</span><span class="s2">, </span><span class="s1">ctx.avals_out[</span><span class="s4">0</span><span class="s1">])</span>
  <span class="s2">return </span><span class="s1">mlir.ir_constants(np.zeros(()</span><span class="s2">, </span><span class="s1">np.dtype(dtype)))</span>
<span class="s1">mlir.register_lowering(empty_p</span><span class="s2">, </span><span class="s1">_empty_lower)</span>


<span class="s2">class </span><span class="s1">BIntRules:</span>
  <span class="s1">@staticmethod</span>
  <span class="s2">def </span><span class="s1">physical_avals(aval) -&gt; Sequence[core.AbstractValue]:</span>
    <span class="s1">dtype = dtypes._scalar_type_to_dtype(int)</span>
    <span class="s2">return </span><span class="s1">[core.ShapedArray(aval.shape</span><span class="s2">, </span><span class="s1">dtype)]</span>

  <span class="s1">@staticmethod</span>
  <span class="s2">def </span><span class="s1">result_handler(sticky_device</span><span class="s2">, </span><span class="s1">aval):</span>
    <span class="s2">def </span><span class="s1">handler(_</span><span class="s2">, </span><span class="s1">buf):</span>
      <span class="s1">buf.aval = core.ShapedArray(buf.shape</span><span class="s2">, </span><span class="s1">buf.dtype)</span>
      <span class="s2">return </span><span class="s1">core.DArray(aval</span><span class="s2">, </span><span class="s1">buf)</span>
    <span class="s2">return </span><span class="s1">handler</span>

  <span class="s1">@staticmethod</span>
  <span class="s2">def </span><span class="s1">global_sharded_result_handler(aval</span><span class="s2">, </span><span class="s1">out_sharding</span><span class="s2">, </span><span class="s1">committed</span><span class="s2">,</span>
                                    <span class="s1">is_out_sharding_from_xla):</span>
    <span class="s1">phys_aval</span><span class="s2">, </span><span class="s1">= BIntRules.physical_avals(aval)</span>
    <span class="s1">phys_handler_maker = pxla.global_result_handlers[core.ShapedArray]</span>

    <span class="s2">if not </span><span class="s1">dispatch.is_single_device_sharding(out_sharding):</span>
      <span class="s2">raise </span><span class="s1">NotImplementedError  </span><span class="s0"># TODO(mattjj)</span>
    <span class="s2">else</span><span class="s1">:</span>
      <span class="s1">phys_sharding = out_sharding</span>
    <span class="s1">phys_handler = phys_handler_maker(phys_aval</span><span class="s2">, </span><span class="s1">phys_sharding</span><span class="s2">, </span><span class="s1">committed</span><span class="s2">,</span>
                                      <span class="s1">is_out_sharding_from_xla)</span>

    <span class="s2">def </span><span class="s1">handler(bufs):</span>
      <span class="s2">return </span><span class="s1">core.DArray(aval</span><span class="s2">, </span><span class="s1">phys_handler(bufs))</span>
    <span class="s2">return </span><span class="s1">handler</span>

<span class="s1">core.bint._rules = BIntRules</span>
</pre>
</body>
</html>