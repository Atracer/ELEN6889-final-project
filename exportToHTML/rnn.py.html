<html>
<head>
<title>rnn.py</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<style type="text/css">
.s0 { color: #808080;}
.s1 { color: #a9b7c6;}
.s2 { color: #629755; font-style: italic;}
.s3 { color: #cc7832;}
.s4 { color: #6897bb;}
.s5 { color: #6a8759;}
</style>
</head>
<body bgcolor="#2b2b2b">
<table CELLSPACING=0 CELLPADDING=5 COLS=1 WIDTH="100%" BGCOLOR="#606060" >
<tr><td><center>
<font face="Arial, Helvetica" color="#000000">
rnn.py</font>
</center></td></tr></table>
<pre><span class="s0"># Copyright 2022 The JAX Authors.</span>
<span class="s0">#</span>
<span class="s0"># Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);</span>
<span class="s0"># you may not use this file except in compliance with the License.</span>
<span class="s0"># You may obtain a copy of the License at</span>
<span class="s0">#</span>
<span class="s0">#     https://www.apache.org/licenses/LICENSE-2.0</span>
<span class="s0">#</span>
<span class="s0"># Unless required by applicable law or agreed to in writing, software</span>
<span class="s0"># distributed under the License is distributed on an &quot;AS IS&quot; BASIS,</span>
<span class="s0"># WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span>
<span class="s0"># See the License for the specific language governing permissions and</span>
<span class="s0"># limitations under the License.</span>
<span class="s2">&quot;&quot;&quot;`jax.experimental.rnn`: GPU accelerated RNN 
 
---------------------------------------------- 
 
This module provides experimental support to CUDNN-backed LSTM. 
 
Currrently, the only supported RNN flavor is LSTM with double-bias. We use 
notations and variable names similar to 
https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html#torch.nn.LSTM 
 
and CUDNN_LSTM entry in 
https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnRNNMode_t. 
 
Note that a bidirectional LSTM is treated as having twice the number of layers, 
where a forward layer i is followed by a reverse layer i. Each direction has 
its own associated weights. We use pseudo-layer to denote such layers 
following CUDNN documentation 
https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnGetRNNWeightParams. 
 
CUDNN takes an opaque 1D weight array that densely packs all the weight arrays 
in a sparsely documented layout. Through trial-and-error and testing, we believe 
the layout is the following. Assume 2-layer bi-LSTM with double-bias, so 4 
pseudo-layers in total (forward-0, reverse-0, forward-1, reverse-1). 
 
There are 4 kinds of weights: W_ih, W_hh, b_ih and b_hh, where 
 
W_ih = (W_ii, W_if, W_ig, W_io) concatenated on leading axis, 
W_hh = (W_hi, W_hf, W_hg, W_ho) concatenated on leading axis, 
b_ih = (b_ii, b_if, b_ig, b_io) concatenated on leading axis, 
b_hh = (b_hi, b_hf, b_hg, b_ho) concatenated on leading axis. 
 
Say W_ih^0 denotates W_ih from pseudo-layer 0. The linear weights are packed 
together from all pseudo-layers followed by bias weights from all pseudo-layers. 
In particular, for each layer, W_ih is followed by W_hh and b_ih by b_hh. 
 
(W_ih^0, W_hh^0, W_ih^1, W_hh^1, W_ih^2, W_hh^2, W_ih^3, W_hh^3, 
 b_ih^0, b_hh^0, b_ih^1, b_hh^1, b_ih^2, b_hh^2, b_ih^3, b_hh^3) 
 
See `get_params_shapes_in_lstm`. 
 
Example usage: 
``` 
  x = jax.random.normal( 
      k1, (batch_size, seq_len, input_size), dtype=jnp.float32) 
  h_0 = jax.random.normal( 
      k2, (num_directions * num_layers, batch_size, hidden_size), 
      dtype=jnp.float32) 
  c_0 = jax.random.normal( 
      k3, (num_directions * num_layers, batch_size, hidden_size), 
      dtype=jnp.float32) 
  seq_lengths = jnp.ones((batch_size,), dtype=jnp.int32) * seq_len 
  weights = rnn.init_lstm_weight(k4, input_size, hidden_size, num_layers, 
                                 bidirectional) 
  y, h_n, c_n = rnn.lstm( 
      x, 
      h_0, 
      c_0, 
      weights, 
      seq_lengths=seq_lengths, 
      input_size=input_size, 
      hidden_size=hidden_size, 
      num_layers=num_layers, 
      dropout=False, 
      bidirectional=bidirectional) 
``` 
 
TODO: 
  - Add support for input and weight dtypes other than float32. 
  - Support ragged inputs. 
  - Support RNNs other than LSTM. 
&quot;&quot;&quot;</span>
<span class="s3">from </span><span class="s1">functools </span><span class="s3">import </span><span class="s1">partial</span>
<span class="s3">import </span><span class="s1">math</span>
<span class="s3">from </span><span class="s1">typing </span><span class="s3">import </span><span class="s1">Any</span><span class="s3">, </span><span class="s1">Dict</span><span class="s3">, </span><span class="s1">List</span><span class="s3">, </span><span class="s1">Tuple</span>

<span class="s3">import </span><span class="s1">jax</span>
<span class="s3">import </span><span class="s1">numpy </span><span class="s3">as </span><span class="s1">np</span>
<span class="s3">from </span><span class="s1">jax._src </span><span class="s3">import </span><span class="s1">core</span>
<span class="s3">from </span><span class="s1">jax.interpreters </span><span class="s3">import </span><span class="s1">mlir</span>
<span class="s3">from </span><span class="s1">jax.interpreters </span><span class="s3">import </span><span class="s1">xla</span>
<span class="s3">from </span><span class="s1">jax._src.custom_derivatives </span><span class="s3">import </span><span class="s1">custom_vjp</span>
<span class="s3">from </span><span class="s1">jax._src.typing </span><span class="s3">import </span><span class="s1">Array</span><span class="s3">, </span><span class="s1">Shape</span>
<span class="s3">import </span><span class="s1">jax.numpy </span><span class="s3">as </span><span class="s1">jnp</span>
<span class="s3">try</span><span class="s1">:</span>
  <span class="s3">from </span><span class="s1">jax._src.lib </span><span class="s3">import </span><span class="s1">gpu_rnn</span>
<span class="s3">except </span><span class="s1">ImportError:</span>
  <span class="s1">gpu_rnn = </span><span class="s3">None  </span><span class="s0"># type: ignore[assignment]</span>

<span class="s1">PRNGKeyArray = Any</span>
<span class="s1">sigmoid = jax.nn.sigmoid</span>
<span class="s1">tanh = jax.nn.tanh</span>


<span class="s3">def </span><span class="s1">_W_ih_l(layer_i: int</span><span class="s3">, </span><span class="s1">input_size: int</span><span class="s3">, </span><span class="s1">hidden_size: int</span><span class="s3">,</span>
            <span class="s1">bidirectional: bool) -&gt; Shape:</span>
  <span class="s2">&quot;&quot;&quot;Shape of W_ii|W_if|W_ig|W_io. 
 
  Note that layer_i is an index of pseudo-layers. 
  &quot;&quot;&quot;</span>
  <span class="s3">if </span><span class="s1">layer_i == </span><span class="s4">0 </span><span class="s3">or </span><span class="s1">(layer_i == </span><span class="s4">1 </span><span class="s3">and </span><span class="s1">bidirectional):</span>
    <span class="s3">return </span><span class="s1">(</span><span class="s4">4 </span><span class="s1">* hidden_size</span><span class="s3">, </span><span class="s1">input_size)</span>
  <span class="s3">else</span><span class="s1">:</span>
    <span class="s1">num_directions = </span><span class="s4">2 </span><span class="s3">if </span><span class="s1">bidirectional </span><span class="s3">else </span><span class="s4">1</span>
    <span class="s3">return </span><span class="s1">(</span><span class="s4">4 </span><span class="s1">* hidden_size</span><span class="s3">, </span><span class="s1">num_directions * hidden_size)</span>


<span class="s3">def </span><span class="s1">_W_hh_l(layer_i: int</span><span class="s3">, </span><span class="s1">input_size: int</span><span class="s3">, </span><span class="s1">hidden_size: int</span><span class="s3">,</span>
            <span class="s1">bidirectional: bool) -&gt; Shape:</span>
  <span class="s2">&quot;&quot;&quot;Shape of W_hi|W_hf|W_hg|W_ho.&quot;&quot;&quot;</span>
  <span class="s3">return </span><span class="s1">(</span><span class="s4">4 </span><span class="s1">* hidden_size</span><span class="s3">, </span><span class="s1">hidden_size)</span>


<span class="s3">def </span><span class="s1">_b_ih_l(layer_i: int</span><span class="s3">, </span><span class="s1">input_size: int</span><span class="s3">, </span><span class="s1">hidden_size: int</span><span class="s3">,</span>
            <span class="s1">bidirectional: bool) -&gt; Shape:</span>
  <span class="s2">&quot;&quot;&quot;Shape of b_ii|b_if|b_ig|b_io.&quot;&quot;&quot;</span>
  <span class="s3">return </span><span class="s1">(</span><span class="s4">4 </span><span class="s1">* hidden_size</span><span class="s3">,</span><span class="s1">)</span>


<span class="s3">def </span><span class="s1">_b_hh_l(layer_i: int</span><span class="s3">, </span><span class="s1">input_size: int</span><span class="s3">, </span><span class="s1">hidden_size: int</span><span class="s3">,</span>
            <span class="s1">bidirectional: bool) -&gt; Shape:</span>
  <span class="s2">&quot;&quot;&quot;Shape of b_hi|b_hf|b_hg|b_ho.&quot;&quot;&quot;</span>
  <span class="s3">return </span><span class="s1">(</span><span class="s4">4 </span><span class="s1">* hidden_size</span><span class="s3">,</span><span class="s1">)</span>


<span class="s3">def </span><span class="s1">_get_params_shapes_in_lstm(input_size: int</span><span class="s3">, </span><span class="s1">hidden_size: int</span><span class="s3">,</span>
                               <span class="s1">num_layers: int</span><span class="s3">,</span>
                               <span class="s1">bidirectional: bool) -&gt; List[Shape]:</span>
  <span class="s2">&quot;&quot;&quot;Get flat param shapes in LSTM. See module docstring for layout.&quot;&quot;&quot;</span>
  <span class="s1">layer_shapes = []</span>
  <span class="s1">num_directions = </span><span class="s4">2 </span><span class="s3">if </span><span class="s1">bidirectional </span><span class="s3">else </span><span class="s4">1</span>
  <span class="s1">num_pseudo_layers = num_layers * num_directions</span>
  <span class="s1">linear_weights = [_W_ih_l</span><span class="s3">, </span><span class="s1">_W_hh_l]</span>
  <span class="s3">for </span><span class="s1">i </span><span class="s3">in </span><span class="s1">range(num_pseudo_layers):</span>
    <span class="s3">for </span><span class="s1">w_kind </span><span class="s3">in </span><span class="s1">linear_weights:</span>
      <span class="s1">layer_shape = w_kind(i</span><span class="s3">, </span><span class="s1">input_size</span><span class="s3">, </span><span class="s1">hidden_size</span><span class="s3">, </span><span class="s1">bidirectional)</span>
      <span class="s1">layer_shapes.append(layer_shape)</span>

  <span class="s1">bias_weights = [_b_ih_l</span><span class="s3">, </span><span class="s1">_b_hh_l]</span>
  <span class="s3">for </span><span class="s1">i </span><span class="s3">in </span><span class="s1">range(num_pseudo_layers):</span>
    <span class="s3">for </span><span class="s1">w_kind </span><span class="s3">in </span><span class="s1">bias_weights:</span>
      <span class="s1">layer_shape = w_kind(i</span><span class="s3">, </span><span class="s1">input_size</span><span class="s3">, </span><span class="s1">hidden_size</span><span class="s3">, </span><span class="s1">bidirectional)</span>
      <span class="s1">layer_shapes.append(layer_shape)</span>
  <span class="s3">return </span><span class="s1">layer_shapes</span>


<span class="s3">def </span><span class="s1">get_num_params_in_lstm(input_size: int</span><span class="s3">, </span><span class="s1">hidden_size: int</span><span class="s3">, </span><span class="s1">num_layers: int</span><span class="s3">,</span>
                           <span class="s1">bidirectional: bool) -&gt; int:</span>
  <span class="s2">&quot;&quot;&quot;Get param count in LSTM.&quot;&quot;&quot;</span>
  <span class="s1">layer_shapes = _get_params_shapes_in_lstm(input_size</span><span class="s3">, </span><span class="s1">hidden_size</span><span class="s3">, </span><span class="s1">num_layers</span><span class="s3">,</span>
                                            <span class="s1">bidirectional)</span>
  <span class="s1">param_count = sum([math.prod(shape) </span><span class="s3">for </span><span class="s1">shape </span><span class="s3">in </span><span class="s1">layer_shapes])</span>
  <span class="s3">return </span><span class="s1">param_count</span>


<span class="s3">def </span><span class="s1">init_lstm_weight(rng: PRNGKeyArray</span><span class="s3">, </span><span class="s1">input_size: int</span><span class="s3">, </span><span class="s1">hidden_size: int</span><span class="s3">,</span>
                     <span class="s1">num_layers: int</span><span class="s3">, </span><span class="s1">bidirectional: bool):</span>
  <span class="s2">&quot;&quot;&quot;Random initialize LSTM weights from U(-k, k), k=sqrt(1/hidden_size).&quot;&quot;&quot;</span>
  <span class="s1">param_count = get_num_params_in_lstm(input_size</span><span class="s3">, </span><span class="s1">hidden_size</span><span class="s3">, </span><span class="s1">num_layers</span><span class="s3">,</span>
                                       <span class="s1">bidirectional)</span>
  <span class="s1">k = np.sqrt(</span><span class="s4">1.0 </span><span class="s1">/ hidden_size)</span>
  <span class="s3">return </span><span class="s1">jax.random.uniform(</span>
      <span class="s1">rng</span><span class="s3">, </span><span class="s1">shape=(param_count</span><span class="s3">,</span><span class="s1">)</span><span class="s3">, </span><span class="s1">dtype=jnp.float32</span><span class="s3">, </span><span class="s1">minval=-k</span><span class="s3">, </span><span class="s1">maxval=k)</span>


<span class="s3">def </span><span class="s1">unpack_lstm_weights(</span>
    <span class="s1">weights: Array</span><span class="s3">, </span><span class="s1">input_size: int</span><span class="s3">, </span><span class="s1">hidden_size: int</span><span class="s3">, </span><span class="s1">num_layers: int</span><span class="s3">,</span>
    <span class="s1">bidirectional: bool</span>
<span class="s1">) -&gt; Tuple[Dict[int</span><span class="s3">, </span><span class="s1">Array]</span><span class="s3">, </span><span class="s1">Dict[int</span><span class="s3">, </span><span class="s1">Array]</span><span class="s3">, </span><span class="s1">Dict[int</span><span class="s3">, </span><span class="s1">Array]</span><span class="s3">, </span><span class="s1">Dict[int</span><span class="s3">,</span>
                                                                      <span class="s1">Array]]:</span>
  <span class="s2">&quot;&quot;&quot;Unpack cudnn LSTM weights into individual weights. 
 
  CUDNN LSTM weight layout: (num_layers, num_directions, W_ih, W_hh, b_ih, b_hh) 
  Returns W_ih, W_hh, b_ih, b_hh. e.g. W_ih[2][1] is the concat weights of 
  4 weights (W_ii, W_if, W_ig, W_io), each of shape (hidden_size, input_size) 
  at 2nd layer for the reverse direction. See notations from 
  https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html#torch.nn.LSTM. 
  &quot;&quot;&quot;</span>
  <span class="s1">flat_shapes = _get_params_shapes_in_lstm(input_size</span><span class="s3">, </span><span class="s1">hidden_size</span><span class="s3">, </span><span class="s1">num_layers</span><span class="s3">,</span>
                                           <span class="s1">bidirectional)</span>
  <span class="s1">flat_shapes_offset = </span><span class="s4">0</span>
  <span class="s1">w_offsets = </span><span class="s4">0</span>
  <span class="s1">num_directions = </span><span class="s4">2 </span><span class="s3">if </span><span class="s1">bidirectional </span><span class="s3">else </span><span class="s4">1</span>
  <span class="s1">num_pseudo_layers = num_layers * num_directions</span>

  <span class="s1">W_ih: Dict[int</span><span class="s3">, </span><span class="s1">Array] = {}</span>
  <span class="s1">W_hh: Dict[int</span><span class="s3">, </span><span class="s1">Array] = {}</span>
  <span class="s3">for </span><span class="s1">l </span><span class="s3">in </span><span class="s1">range(num_pseudo_layers):</span>
    <span class="s3">for </span><span class="s1">w_kind </span><span class="s3">in </span><span class="s1">[W_ih</span><span class="s3">, </span><span class="s1">W_hh]:</span>
      <span class="s1">shape = flat_shapes[flat_shapes_offset]</span>
      <span class="s1">flat_shapes_offset += </span><span class="s4">1</span>
      <span class="s1">num_elems = math.prod(shape)</span>
      <span class="s1">w_kind[l] = weights[w_offsets:w_offsets + num_elems].reshape(shape)</span>
      <span class="s1">w_offsets += num_elems</span>

  <span class="s1">b_ih: Dict[int</span><span class="s3">, </span><span class="s1">Array] = {}</span>
  <span class="s1">b_hh: Dict[int</span><span class="s3">, </span><span class="s1">Array] = {}</span>
  <span class="s3">for </span><span class="s1">l </span><span class="s3">in </span><span class="s1">range(num_pseudo_layers):</span>
    <span class="s3">for </span><span class="s1">w_kind </span><span class="s3">in </span><span class="s1">[b_ih</span><span class="s3">, </span><span class="s1">b_hh]:</span>
      <span class="s1">shape = flat_shapes[flat_shapes_offset]</span>
      <span class="s1">flat_shapes_offset += </span><span class="s4">1</span>
      <span class="s1">num_elems = math.prod(shape)</span>
      <span class="s1">w_kind[l] = weights[w_offsets:w_offsets + num_elems].reshape(shape)</span>
      <span class="s1">w_offsets += num_elems</span>
  <span class="s3">return </span><span class="s1">W_ih</span><span class="s3">, </span><span class="s1">W_hh</span><span class="s3">, </span><span class="s1">b_ih</span><span class="s3">, </span><span class="s1">b_hh</span>


<span class="s1">@partial(custom_vjp</span><span class="s3">, </span><span class="s1">nondiff_argnums=(</span><span class="s4">5</span><span class="s3">, </span><span class="s4">6</span><span class="s3">, </span><span class="s4">7</span><span class="s3">, </span><span class="s4">8</span><span class="s3">, </span><span class="s4">9</span><span class="s1">))</span>
<span class="s3">def </span><span class="s1">lstm(x: Array</span><span class="s3">, </span><span class="s1">h_0: Array</span><span class="s3">, </span><span class="s1">c_0: Array</span><span class="s3">, </span><span class="s1">weights: Array</span><span class="s3">, </span><span class="s1">seq_lengths: Array</span><span class="s3">,</span>
         <span class="s1">input_size: int</span><span class="s3">, </span><span class="s1">hidden_size: int</span><span class="s3">, </span><span class="s1">num_layers: int</span><span class="s3">, </span><span class="s1">dropout: float</span><span class="s3">,</span>
         <span class="s1">bidirectional: bool) -&gt; Tuple[Array</span><span class="s3">, </span><span class="s1">Array</span><span class="s3">, </span><span class="s1">Array]:</span>
  <span class="s2">&quot;&quot;&quot;LSTM via CuDNN or HIPDNN (not-yet-supported). 
 
  Assume batch-first inputs. 
 
  Arguments: 
    x: (batch_size, max_seq_length, input_size) 
    h_0: (num_directions * num_layers, batch_size, hidden_size) 
    c_0: (num_directions * num_layers, batch_size, hidden_size) 
    weights: (num_params,) where num_params = get_num_params_in_lstm(...) 
    seq_lengths: (batch_size,) 
  Returns: (y, h_n, c_n, workspace, reserve_space). 
    y: (batch_size, max_seq_length, hidden_size * num_directions) 
    h_n: (num_directions * num_layers, batch_size, hidden_size) 
    c_n: (num_directions * num_layers, batch_size, hidden_size) 
  &quot;&quot;&quot;</span>
  <span class="s1">(y</span><span class="s3">, </span><span class="s1">h_n</span><span class="s3">, </span><span class="s1">c_n)</span><span class="s3">, </span><span class="s1">_ = lstm_fwd(</span>
      <span class="s1">x</span><span class="s3">,</span>
      <span class="s1">h_0</span><span class="s3">,</span>
      <span class="s1">c_0</span><span class="s3">,</span>
      <span class="s1">weights</span><span class="s3">,</span>
      <span class="s1">seq_lengths</span><span class="s3">,</span>
      <span class="s1">input_size=input_size</span><span class="s3">,</span>
      <span class="s1">hidden_size=hidden_size</span><span class="s3">,</span>
      <span class="s1">num_layers=num_layers</span><span class="s3">,</span>
      <span class="s1">dropout=dropout</span><span class="s3">,</span>
      <span class="s1">bidirectional=bidirectional)</span>
  <span class="s3">return </span><span class="s1">y</span><span class="s3">, </span><span class="s1">h_n</span><span class="s3">, </span><span class="s1">c_n</span>


<span class="s1">@partial(jax.jit</span><span class="s3">, </span><span class="s1">static_argnums=(</span><span class="s4">8</span><span class="s3">, </span><span class="s4">9</span><span class="s3">, </span><span class="s4">10</span><span class="s3">, </span><span class="s4">11</span><span class="s3">, </span><span class="s4">12</span><span class="s1">))</span>
<span class="s3">def </span><span class="s1">lstm_ref(x: Array</span><span class="s3">, </span><span class="s1">h_0: Array</span><span class="s3">, </span><span class="s1">c_0: Array</span><span class="s3">, </span><span class="s1">W_ih: Dict[int</span><span class="s3">, </span><span class="s1">Array]</span><span class="s3">,</span>
             <span class="s1">W_hh: Dict[int</span><span class="s3">, </span><span class="s1">Array]</span><span class="s3">, </span><span class="s1">b_ih: Dict[int</span><span class="s3">, </span><span class="s1">Array]</span><span class="s3">,</span>
             <span class="s1">b_hh: Dict[int</span><span class="s3">, </span><span class="s1">Array]</span><span class="s3">, </span><span class="s1">seq_lengths: Array</span><span class="s3">, </span><span class="s1">input_size: int</span><span class="s3">,</span>
             <span class="s1">hidden_size: int</span><span class="s3">, </span><span class="s1">num_layers: int</span><span class="s3">, </span><span class="s1">dropout: float</span><span class="s3">,</span>
             <span class="s1">bidirectional: bool) -&gt; Tuple[Array</span><span class="s3">, </span><span class="s1">Array</span><span class="s3">, </span><span class="s1">Array]:</span>
  <span class="s2">&quot;&quot;&quot;Reference implementation of LSTM. 
 
  See https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html#lstm 
  https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnRNNMode_t 
  &quot;&quot;&quot;</span>
  <span class="s3">if </span><span class="s1">seq_lengths.dtype != jnp.dtype(</span><span class="s5">&quot;int32&quot;</span><span class="s1">):</span>
    <span class="s3">raise </span><span class="s1">NotImplementedError(</span><span class="s5">&quot;`seq_lengths` can only be int32.&quot;</span><span class="s1">)</span>
  <span class="s3">if </span><span class="s1">dropout != </span><span class="s4">0.0</span><span class="s1">:</span>
    <span class="s3">raise </span><span class="s1">NotImplementedError(</span>
        <span class="s5">'Dropout not supported in LSTM reference because we cannot determine CUDNN dropout mask.'</span>
    <span class="s1">)</span>

  <span class="s0"># TODO(zhangqiaorjc): Handle ragged seq_lengths.</span>
  <span class="s0"># batch_size, max_seq_length = x.shape[0], x.shape[1]</span>
  <span class="s0"># assert seq_lengths.shape == (batch_size,)</span>
  <span class="s0"># for i in range(batch_size):</span>
  <span class="s0">#   if int(seq_lengths[i]) != max_seq_length:</span>
  <span class="s0">#     raise NotImplementedError('Does not yet support ragged sequences.')</span>

  <span class="s3">def </span><span class="s1">lstm_cell(carry</span><span class="s3">, </span><span class="s1">x</span><span class="s3">, </span><span class="s1">*</span><span class="s3">, </span><span class="s1">W_ih</span><span class="s3">, </span><span class="s1">W_hh</span><span class="s3">, </span><span class="s1">b_ih</span><span class="s3">, </span><span class="s1">b_hh):</span>
    <span class="s1">h</span><span class="s3">, </span><span class="s1">c = carry</span>
    <span class="s1">W_ii</span><span class="s3">, </span><span class="s1">W_if</span><span class="s3">, </span><span class="s1">W_ig</span><span class="s3">, </span><span class="s1">W_io = jnp.split(W_ih</span><span class="s3">, </span><span class="s4">4</span><span class="s3">, </span><span class="s1">axis=</span><span class="s4">0</span><span class="s1">)</span>
    <span class="s1">W_hi</span><span class="s3">, </span><span class="s1">W_hf</span><span class="s3">, </span><span class="s1">W_hg</span><span class="s3">, </span><span class="s1">W_ho = jnp.split(W_hh</span><span class="s3">, </span><span class="s4">4</span><span class="s3">, </span><span class="s1">axis=</span><span class="s4">0</span><span class="s1">)</span>
    <span class="s1">b_ii</span><span class="s3">, </span><span class="s1">b_if</span><span class="s3">, </span><span class="s1">b_ig</span><span class="s3">, </span><span class="s1">b_io = jnp.split(b_ih</span><span class="s3">, </span><span class="s4">4</span><span class="s3">, </span><span class="s1">axis=</span><span class="s4">0</span><span class="s1">)</span>
    <span class="s1">b_hi</span><span class="s3">, </span><span class="s1">b_hf</span><span class="s3">, </span><span class="s1">b_hg</span><span class="s3">, </span><span class="s1">b_ho = jnp.split(b_hh</span><span class="s3">, </span><span class="s4">4</span><span class="s3">, </span><span class="s1">axis=</span><span class="s4">0</span><span class="s1">)</span>
    <span class="s1">i = sigmoid(x @ W_ii.T + b_ii[</span><span class="s3">None</span><span class="s1">] + h @ W_hi.T + b_hi[</span><span class="s3">None</span><span class="s1">])</span>
    <span class="s1">f = sigmoid(x @ W_if.T + b_if[</span><span class="s3">None</span><span class="s1">] + h @ W_hf.T + b_hf[</span><span class="s3">None</span><span class="s1">])</span>
    <span class="s1">g = tanh(x @ W_ig.T + b_ig[</span><span class="s3">None</span><span class="s1">] + h @ W_hg.T + b_hg[</span><span class="s3">None</span><span class="s1">])</span>
    <span class="s1">o = sigmoid(x @ W_io.T + b_io[</span><span class="s3">None</span><span class="s1">] + h @ W_ho.T + b_ho[</span><span class="s3">None</span><span class="s1">])</span>
    <span class="s1">c = f * c + i * g</span>
    <span class="s1">h = o * tanh(c)</span>
    <span class="s3">return </span><span class="s1">(h</span><span class="s3">, </span><span class="s1">c)</span><span class="s3">, </span><span class="s1">h</span>

  <span class="s1">seq_first_y = x.transpose(</span><span class="s4">1</span><span class="s3">, </span><span class="s4">0</span><span class="s3">, </span><span class="s4">2</span><span class="s1">)</span>
  <span class="s3">if not </span><span class="s1">bidirectional:</span>
    <span class="s1">final_h = []</span>
    <span class="s1">final_c = []</span>
    <span class="s3">for </span><span class="s1">l </span><span class="s3">in </span><span class="s1">range(num_layers):</span>
      <span class="s1">cell = partial(</span>
          <span class="s1">lstm_cell</span><span class="s3">, </span><span class="s1">W_ih=W_ih[l]</span><span class="s3">, </span><span class="s1">W_hh=W_hh[l]</span><span class="s3">, </span><span class="s1">b_ih=b_ih[l]</span><span class="s3">, </span><span class="s1">b_hh=b_hh[l])</span>
      <span class="s1">(h_t</span><span class="s3">, </span><span class="s1">c_t)</span><span class="s3">, </span><span class="s1">seq_first_y = jax.lax.scan(cell</span><span class="s3">, </span><span class="s1">(h_0[l]</span><span class="s3">, </span><span class="s1">c_0[l])</span><span class="s3">,</span>
                                             <span class="s1">seq_first_y)</span>
      <span class="s1">final_h.append(h_t)</span>
      <span class="s1">final_c.append(c_t)</span>
    <span class="s1">h_n = jnp.stack(final_h)</span>
    <span class="s1">c_n = jnp.stack(final_c)</span>
    <span class="s3">return </span><span class="s1">seq_first_y.transpose(</span><span class="s4">1</span><span class="s3">, </span><span class="s4">0</span><span class="s3">, </span><span class="s4">2</span><span class="s1">)</span><span class="s3">, </span><span class="s1">h_n</span><span class="s3">, </span><span class="s1">c_n</span>

  <span class="s0"># bidirectional</span>
  <span class="s1">final_h = []</span>
  <span class="s1">final_c = []</span>
  <span class="s3">for </span><span class="s1">l </span><span class="s3">in </span><span class="s1">range(num_layers * </span><span class="s4">2</span><span class="s1">):</span>
    <span class="s1">cell = partial(</span>
        <span class="s1">lstm_cell</span><span class="s3">, </span><span class="s1">W_ih=W_ih[l]</span><span class="s3">, </span><span class="s1">W_hh=W_hh[l]</span><span class="s3">, </span><span class="s1">b_ih=b_ih[l]</span><span class="s3">, </span><span class="s1">b_hh=b_hh[l])</span>
    <span class="s3">if </span><span class="s1">l % </span><span class="s4">2 </span><span class="s1">== </span><span class="s4">0</span><span class="s1">:</span>
      <span class="s1">(h_t</span><span class="s3">, </span><span class="s1">c_t)</span><span class="s3">, </span><span class="s1">seq_first_y_fwd = jax.lax.scan(cell</span><span class="s3">, </span><span class="s1">(h_0[l]</span><span class="s3">, </span><span class="s1">c_0[l])</span><span class="s3">,</span>
                                                 <span class="s1">seq_first_y)</span>
    <span class="s3">else</span><span class="s1">:</span>
      <span class="s1">(h_t</span><span class="s3">, </span><span class="s1">c_t)</span><span class="s3">, </span><span class="s1">seq_first_y_bwd = jax.lax.scan(</span>
          <span class="s1">cell</span><span class="s3">, </span><span class="s1">(h_0[l]</span><span class="s3">, </span><span class="s1">c_0[l])</span><span class="s3">, </span><span class="s1">seq_first_y</span><span class="s3">, </span><span class="s1">reverse=</span><span class="s3">True</span><span class="s1">)</span>
      <span class="s0"># Inputs to next layer are concat'ed from fwd and bwd.</span>
      <span class="s1">seq_first_y = jnp.concatenate([seq_first_y_fwd</span><span class="s3">, </span><span class="s1">seq_first_y_bwd]</span><span class="s3">, </span><span class="s1">axis=-</span><span class="s4">1</span><span class="s1">)  </span><span class="s0"># pytype: disable=name-error</span>
    <span class="s1">final_h.append(h_t)</span>
    <span class="s1">final_c.append(c_t)</span>
  <span class="s1">h_n = jnp.stack(final_h)</span>
  <span class="s1">c_n = jnp.stack(final_c)</span>
  <span class="s3">return </span><span class="s1">seq_first_y.transpose(</span><span class="s4">1</span><span class="s3">, </span><span class="s4">0</span><span class="s3">, </span><span class="s4">2</span><span class="s1">)</span><span class="s3">, </span><span class="s1">h_n</span><span class="s3">, </span><span class="s1">c_n</span>


<span class="s3">def </span><span class="s1">lstm_fwd(x: Array</span><span class="s3">, </span><span class="s1">h_0: Array</span><span class="s3">, </span><span class="s1">c_0: Array</span><span class="s3">, </span><span class="s1">w: Array</span><span class="s3">, </span><span class="s1">seq_lengths: Array</span><span class="s3">,</span>
             <span class="s1">input_size: int</span><span class="s3">, </span><span class="s1">hidden_size: int</span><span class="s3">, </span><span class="s1">num_layers: int</span><span class="s3">, </span><span class="s1">dropout: float</span><span class="s3">,</span>
             <span class="s1">bidirectional: bool):</span>
  <span class="s3">if </span><span class="s1">seq_lengths.dtype != jnp.dtype(</span><span class="s5">&quot;int32&quot;</span><span class="s1">):</span>
    <span class="s3">raise </span><span class="s1">NotImplementedError(</span><span class="s5">&quot;`seq_lengths` can only be int32.&quot;</span><span class="s1">)</span>
  <span class="s1">y</span><span class="s3">, </span><span class="s1">h_n</span><span class="s3">, </span><span class="s1">c_n</span><span class="s3">, </span><span class="s1">workspace</span><span class="s3">, </span><span class="s1">reserve_space = rnn_fwd_p.bind(</span>
      <span class="s1">x</span><span class="s3">,</span>
      <span class="s1">h_0</span><span class="s3">,</span>
      <span class="s1">c_0</span><span class="s3">,</span>
      <span class="s1">w</span><span class="s3">,</span>
      <span class="s1">seq_lengths</span><span class="s3">,</span>
      <span class="s1">input_size=input_size</span><span class="s3">,</span>
      <span class="s1">hidden_size=hidden_size</span><span class="s3">,</span>
      <span class="s1">num_layers=num_layers</span><span class="s3">,</span>
      <span class="s1">dropout=dropout</span><span class="s3">,</span>
      <span class="s1">bidirectional=bidirectional)</span>
  <span class="s3">return </span><span class="s1">(y</span><span class="s3">, </span><span class="s1">h_n</span><span class="s3">, </span><span class="s1">c_n)</span><span class="s3">, </span><span class="s1">(x</span><span class="s3">, </span><span class="s1">h_0</span><span class="s3">, </span><span class="s1">c_0</span><span class="s3">, </span><span class="s1">w</span><span class="s3">, </span><span class="s1">seq_lengths</span><span class="s3">, </span><span class="s1">y</span><span class="s3">, </span><span class="s1">workspace</span><span class="s3">,</span>
                         <span class="s1">reserve_space)</span>


<span class="s3">def </span><span class="s1">rnn_abstract_eval(x_aval</span><span class="s3">, </span><span class="s1">h_0_aval</span><span class="s3">, </span><span class="s1">c_0_aval</span><span class="s3">, </span><span class="s1">w_aval</span><span class="s3">, </span><span class="s1">seq_lengths_aval</span><span class="s3">,</span>
                      <span class="s1">input_size: int</span><span class="s3">, </span><span class="s1">hidden_size: int</span><span class="s3">, </span><span class="s1">num_layers: int</span><span class="s3">,</span>
                      <span class="s1">dropout: float</span><span class="s3">, </span><span class="s1">bidirectional: bool):</span>
  <span class="s1">batch_size</span><span class="s3">, </span><span class="s1">max_seq_length = x_aval.shape[</span><span class="s4">0</span><span class="s1">]</span><span class="s3">, </span><span class="s1">x_aval.shape[</span><span class="s4">1</span><span class="s1">]</span>
  <span class="s1">num_directions = </span><span class="s4">2 </span><span class="s3">if </span><span class="s1">bidirectional </span><span class="s3">else </span><span class="s4">1</span>
  <span class="s1">output_shape = (batch_size</span><span class="s3">, </span><span class="s1">max_seq_length</span><span class="s3">, </span><span class="s1">num_directions * hidden_size)</span>
  <span class="s1">output_aval = core.ShapedArray(output_shape</span><span class="s3">, </span><span class="s1">x_aval.dtype)</span>
  <span class="s1">workspace_size</span><span class="s3">, </span><span class="s1">reserve_space_size = (</span>
      <span class="s1">gpu_rnn.compute_rnn_workspace_reserve_space_sizes(  </span><span class="s0"># pytype: disable=attribute-error</span>
          <span class="s1">input_size</span><span class="s3">, </span><span class="s1">hidden_size</span><span class="s3">, </span><span class="s1">num_layers</span><span class="s3">, </span><span class="s1">batch_size</span><span class="s3">, </span><span class="s1">max_seq_length</span><span class="s3">,</span>
          <span class="s1">dropout</span><span class="s3">, </span><span class="s1">bidirectional))</span>
  <span class="s1">workspace_aval = core.ShapedArray((workspace_size</span><span class="s3">,</span><span class="s1">)</span><span class="s3">, </span><span class="s1">jnp.float32)</span>
  <span class="s1">reserve_space_aval = core.ShapedArray((reserve_space_size</span><span class="s3">,</span><span class="s1">)</span><span class="s3">, </span><span class="s1">jnp.float32)</span>
  <span class="s3">return </span><span class="s1">output_aval</span><span class="s3">, </span><span class="s1">h_0_aval</span><span class="s3">, </span><span class="s1">c_0_aval</span><span class="s3">, </span><span class="s1">workspace_aval</span><span class="s3">, </span><span class="s1">reserve_space_aval</span>


<span class="s1">rnn_fwd_p = core.Primitive(</span><span class="s5">'rnn_fwd'</span><span class="s1">)</span>
<span class="s1">rnn_fwd_p.multiple_results = </span><span class="s3">True</span>
<span class="s1">rnn_fwd_p.def_impl(partial(xla.apply_primitive</span><span class="s3">, </span><span class="s1">rnn_fwd_p))</span>
<span class="s1">rnn_fwd_p.def_abstract_eval(rnn_abstract_eval)</span>
<span class="s3">if </span><span class="s1">gpu_rnn:</span>
  <span class="s1">mlir.register_lowering(rnn_fwd_p</span><span class="s3">, </span><span class="s1">gpu_rnn.cudnn_rnn_lowering</span><span class="s3">, </span><span class="s1">platform=</span><span class="s5">'cuda'</span><span class="s1">)</span>


<span class="s3">def </span><span class="s1">lstm_bwd(input_size: int</span><span class="s3">, </span><span class="s1">hidden_size: int</span><span class="s3">, </span><span class="s1">num_layers: int</span><span class="s3">, </span><span class="s1">dropout: float</span><span class="s3">,</span>
             <span class="s1">bidirectional</span><span class="s3">, </span><span class="s1">residuals</span><span class="s3">, </span><span class="s1">gradients):</span>
  <span class="s1">x</span><span class="s3">, </span><span class="s1">h_0</span><span class="s3">, </span><span class="s1">c_0</span><span class="s3">, </span><span class="s1">w</span><span class="s3">, </span><span class="s1">seq_lengths</span><span class="s3">, </span><span class="s1">y</span><span class="s3">, </span><span class="s1">workspace</span><span class="s3">, </span><span class="s1">reserve_space = residuals</span>
  <span class="s1">dy</span><span class="s3">, </span><span class="s1">dh_n</span><span class="s3">, </span><span class="s1">dc_n = gradients</span>
  <span class="s1">dx</span><span class="s3">, </span><span class="s1">dh_0</span><span class="s3">, </span><span class="s1">dc_0</span><span class="s3">, </span><span class="s1">dw = rnn_bwd_p.bind(</span>
      <span class="s1">dy</span><span class="s3">,</span>
      <span class="s1">dh_n</span><span class="s3">,</span>
      <span class="s1">dc_n</span><span class="s3">,</span>
      <span class="s1">x</span><span class="s3">,</span>
      <span class="s1">h_0</span><span class="s3">,</span>
      <span class="s1">c_0</span><span class="s3">,</span>
      <span class="s1">w</span><span class="s3">,</span>
      <span class="s1">y</span><span class="s3">,</span>
      <span class="s1">workspace</span><span class="s3">,</span>
      <span class="s1">reserve_space</span><span class="s3">,</span>
      <span class="s1">seq_lengths</span><span class="s3">,</span>
      <span class="s1">input_size=input_size</span><span class="s3">,</span>
      <span class="s1">hidden_size=hidden_size</span><span class="s3">,</span>
      <span class="s1">num_layers=num_layers</span><span class="s3">,</span>
      <span class="s1">dropout=dropout</span><span class="s3">,</span>
      <span class="s1">bidirectional=bidirectional)</span>
  <span class="s3">return </span><span class="s1">(dx</span><span class="s3">, </span><span class="s1">dh_0</span><span class="s3">, </span><span class="s1">dc_0</span><span class="s3">, </span><span class="s1">dw</span><span class="s3">, </span><span class="s1">jnp.zeros_like(seq_lengths))</span>


<span class="s3">def </span><span class="s1">rnn_bwd_abstract_eval(dy_aval</span><span class="s3">, </span><span class="s1">dhn_aval</span><span class="s3">, </span><span class="s1">dcn_aval</span><span class="s3">, </span><span class="s1">x_aval</span><span class="s3">, </span><span class="s1">h0_aval</span><span class="s3">, </span><span class="s1">c0_aval</span><span class="s3">,</span>
                          <span class="s1">w_aval</span><span class="s3">, </span><span class="s1">y_aval</span><span class="s3">, </span><span class="s1">workspace_aval</span><span class="s3">, </span><span class="s1">reserve_space_aval</span><span class="s3">,</span>
                          <span class="s1">seq_lengths_aval</span><span class="s3">, </span><span class="s1">input_size: int</span><span class="s3">, </span><span class="s1">hidden_size: int</span><span class="s3">,</span>
                          <span class="s1">num_layers: int</span><span class="s3">, </span><span class="s1">dropout: float</span><span class="s3">, </span><span class="s1">bidirectional: bool):</span>
  <span class="s3">return </span><span class="s1">x_aval</span><span class="s3">, </span><span class="s1">h0_aval</span><span class="s3">, </span><span class="s1">c0_aval</span><span class="s3">, </span><span class="s1">w_aval</span>


<span class="s1">rnn_bwd_p = core.Primitive(</span><span class="s5">'rnn_bwd'</span><span class="s1">)</span>
<span class="s1">rnn_bwd_p.multiple_results = </span><span class="s3">True</span>
<span class="s1">rnn_bwd_p.def_impl(partial(xla.apply_primitive</span><span class="s3">, </span><span class="s1">rnn_bwd_p))</span>
<span class="s1">rnn_bwd_p.def_abstract_eval(rnn_bwd_abstract_eval)</span>
<span class="s3">if </span><span class="s1">gpu_rnn:</span>
  <span class="s1">mlir.register_lowering(</span>
      <span class="s1">rnn_bwd_p</span><span class="s3">, </span><span class="s1">gpu_rnn.cudnn_rnn_bwd_lowering</span><span class="s3">, </span><span class="s1">platform=</span><span class="s5">'cuda'</span><span class="s1">)</span>

<span class="s1">lstm.defvjp(lstm_fwd</span><span class="s3">, </span><span class="s1">lstm_bwd)</span>
</pre>
</body>
</html>