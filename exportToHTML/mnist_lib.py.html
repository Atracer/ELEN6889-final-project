<html>
<head>
<title>mnist_lib.py</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<style type="text/css">
.s0 { color: #808080;}
.s1 { color: #a9b7c6;}
.s2 { color: #629755; font-style: italic;}
.s3 { color: #cc7832;}
.s4 { color: #6a8759;}
.s5 { color: #6897bb;}
</style>
</head>
<body bgcolor="#2b2b2b">
<table CELLSPACING=0 CELLPADDING=5 COLS=1 WIDTH="100%" BGCOLOR="#606060" >
<tr><td><center>
<font face="Arial, Helvetica" color="#000000">
mnist_lib.py</font>
</center></td></tr></table>
<pre><span class="s0"># Copyright 2020 The JAX Authors.</span>
<span class="s0">#</span>
<span class="s0"># Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);</span>
<span class="s0"># you may not use this file except in compliance with the License.</span>
<span class="s0"># You may obtain a copy of the License at</span>
<span class="s0">#</span>
<span class="s0">#     https://www.apache.org/licenses/LICENSE-2.0</span>
<span class="s0">#</span>
<span class="s0"># Unless required by applicable law or agreed to in writing, software</span>
<span class="s0"># distributed under the License is distributed on an &quot;AS IS&quot; BASIS,</span>
<span class="s0"># WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span>
<span class="s0"># See the License for the specific language governing permissions and</span>
<span class="s0"># limitations under the License.</span>
<span class="s2">&quot;&quot;&quot;Definitions of two versions of MNIST (model and training code ). 
 
One definition uses pure JAX (for those who prefer an example with fewer 
moving parts, at the expense of code size), and another using Flax. 
 
See README.md for how these are used. 
&quot;&quot;&quot;</span>
<span class="s3">import </span><span class="s1">functools</span>
<span class="s3">import </span><span class="s1">logging</span>
<span class="s3">import </span><span class="s1">re</span>
<span class="s3">import </span><span class="s1">time</span>
<span class="s3">from </span><span class="s1">typing </span><span class="s3">import </span><span class="s1">Any</span><span class="s3">, </span><span class="s1">Callable</span><span class="s3">, </span><span class="s1">Optional</span><span class="s3">, </span><span class="s1">Sequence</span><span class="s3">, </span><span class="s1">Tuple</span>
<span class="s3">from </span><span class="s1">absl </span><span class="s3">import </span><span class="s1">flags</span>

<span class="s3">import </span><span class="s1">flax  </span><span class="s0"># type: ignore[import]</span>
<span class="s3">from </span><span class="s1">flax </span><span class="s3">import </span><span class="s1">linen </span><span class="s3">as </span><span class="s1">nn</span>

<span class="s3">import </span><span class="s1">jax</span>
<span class="s3">import </span><span class="s1">jax.numpy </span><span class="s3">as </span><span class="s1">jnp</span>

<span class="s3">from </span><span class="s1">matplotlib </span><span class="s3">import </span><span class="s1">pyplot </span><span class="s3">as </span><span class="s1">plt  </span><span class="s0"># type: ignore</span>
<span class="s3">import </span><span class="s1">numpy </span><span class="s3">as </span><span class="s1">np</span>
<span class="s3">import </span><span class="s1">optax</span>
<span class="s3">import </span><span class="s1">tensorflow </span><span class="s3">as </span><span class="s1">tf  </span><span class="s0"># type: ignore</span>
<span class="s3">import </span><span class="s1">tensorflow_datasets </span><span class="s3">as </span><span class="s1">tfds  </span><span class="s0"># type: ignore</span>

<span class="s1">flags.DEFINE_boolean(</span><span class="s4">&quot;mock_data&quot;</span><span class="s3">, False, </span><span class="s4">&quot;Use fake data, for testing.&quot;</span><span class="s1">)</span>
<span class="s1">FLAGS = flags.FLAGS</span>

<span class="s0">#### Model parameters</span>

<span class="s0"># For fun, let's use different batch sizes for training and for evaluation.</span>
<span class="s1">train_batch_size = </span><span class="s5">128</span>
<span class="s1">test_batch_size = </span><span class="s5">16</span>

<span class="s0"># Define common parameters for both the JAX and the Flax models.</span>
<span class="s1">input_shape = (</span><span class="s5">28</span><span class="s3">, </span><span class="s5">28</span><span class="s3">, </span><span class="s5">1</span><span class="s1">)  </span><span class="s0"># Excluding batch_size</span>
<span class="s1">layer_sizes = [</span><span class="s5">784</span><span class="s3">, </span><span class="s5">512</span><span class="s3">, </span><span class="s5">512</span><span class="s3">, </span><span class="s5">10</span><span class="s1">]  </span><span class="s0"># 10 is the number of classes</span>
<span class="s1">param_scale = </span><span class="s5">0.1</span>
<span class="s1">step_size = </span><span class="s5">0.001</span>


<span class="s3">def </span><span class="s1">load_mnist(split: tfds.Split</span><span class="s3">, </span><span class="s1">batch_size: int):</span>
  <span class="s2">&quot;&quot;&quot;Loads either training or test MNIST data. 
 
  Args: 
    split: either tfds.Split.TRAIN or tfds.Split.TEST. 
 
  Returns: 
    an iterator with pairs (images, labels). The images have shape 
    (B, 28, 28, 1) and the labels have shape (B, 10), where B is the batch_size. 
  &quot;&quot;&quot;</span>
  <span class="s3">if </span><span class="s1">FLAGS.mock_data:</span>
    <span class="s3">with </span><span class="s1">tfds.testing.mock_data(num_examples=batch_size):</span>
      <span class="s3">try</span><span class="s1">:</span>
        <span class="s1">ds = tfds.load(</span><span class="s4">&quot;mnist&quot;</span><span class="s3">, </span><span class="s1">split=split)</span>
      <span class="s3">except </span><span class="s1">Exception </span><span class="s3">as </span><span class="s1">e:</span>
        <span class="s1">m = re.search(</span><span class="s4">r'metadata files were not found in (.+/)mnist/'</span><span class="s3">, </span><span class="s1">str(e))</span>
        <span class="s3">if </span><span class="s1">m:</span>
          <span class="s1">msg = (</span><span class="s4">&quot;TFDS mock_data is missing the mnist metadata files. Run the &quot;</span>
                 <span class="s4">&quot;`saved_model_main.py` binary and see where TFDS downloads &quot;</span>
                 <span class="s4">&quot;the mnist data set (typically ~/tensorflow_datasets/mnist). &quot;</span>
                 <span class="s4">f&quot;Copy the `mnist` directory to </span><span class="s3">{</span><span class="s1">m.group(</span><span class="s5">1</span><span class="s1">)</span><span class="s3">} </span><span class="s4">and re-run the test&quot;</span><span class="s1">)</span>
          <span class="s3">raise </span><span class="s1">ValueError(msg) </span><span class="s3">from </span><span class="s1">e</span>
        <span class="s3">else</span><span class="s1">:</span>
          <span class="s3">raise </span><span class="s1">e</span>
  <span class="s3">else</span><span class="s1">:</span>
    <span class="s1">ds = tfds.load(</span><span class="s4">&quot;mnist&quot;</span><span class="s3">, </span><span class="s1">split=split)</span>

  <span class="s3">def </span><span class="s1">_prepare_example(x):</span>
    <span class="s1">image = tf.cast(x[</span><span class="s4">&quot;image&quot;</span><span class="s1">]</span><span class="s3">, </span><span class="s1">tf.float32) / </span><span class="s5">255.0</span>
    <span class="s1">label = tf.one_hot(x[</span><span class="s4">&quot;label&quot;</span><span class="s1">]</span><span class="s3">, </span><span class="s5">10</span><span class="s1">)</span>
    <span class="s3">return </span><span class="s1">(image</span><span class="s3">, </span><span class="s1">label)</span>

  <span class="s1">ds = ds.map(_prepare_example)</span>
  <span class="s0"># drop_remainder=True is important for use with Keras</span>
  <span class="s1">ds = ds.cache().shuffle(</span><span class="s5">1000</span><span class="s1">).batch(batch_size</span><span class="s3">, </span><span class="s1">drop_remainder=</span><span class="s3">True</span><span class="s1">)</span>
  <span class="s3">return </span><span class="s1">ds</span>


<span class="s3">class </span><span class="s1">PureJaxMNIST:</span>
  <span class="s2">&quot;&quot;&quot;An MNIST model written using pure JAX. 
 
  There is an option for the model to skip the classifier layer, for 
  demonstrating reuse of the classifier-less model into a larger model. 
  See README.md. 
  &quot;&quot;&quot;</span>

  <span class="s1">name = </span><span class="s4">&quot;mnist_pure_jax&quot;</span>

  <span class="s1">@staticmethod</span>
  <span class="s3">def </span><span class="s1">predict(params: Sequence[Tuple[Any</span><span class="s3">, </span><span class="s1">Any]]</span><span class="s3">, </span><span class="s1">inputs</span><span class="s3">, </span><span class="s1">with_classifier=</span><span class="s3">True</span><span class="s1">):</span>
    <span class="s2">&quot;&quot;&quot;The prediction function. 
 
    Args: 
      params: a list with pairs of weights and biases for each layer. 
      inputs: the batch of images (B, 28, 28, 1) 
      with_classifier: whether to include the classifier layer. 
 
    Returns: 
      either the predictions (B, 10) if with_classifier=True, or the 
      final set of logits of shape (B, 512). 
    &quot;&quot;&quot;</span>
    <span class="s1">x = inputs.reshape((inputs.shape[</span><span class="s5">0</span><span class="s1">]</span><span class="s3">, </span><span class="s1">-</span><span class="s5">1</span><span class="s1">))  </span><span class="s0"># flatten to f32[B, 784]</span>
    <span class="s3">for </span><span class="s1">w</span><span class="s3">, </span><span class="s1">b </span><span class="s3">in </span><span class="s1">params[:-</span><span class="s5">1</span><span class="s1">]:</span>
      <span class="s1">x = jnp.dot(x</span><span class="s3">, </span><span class="s1">w) + b</span>
      <span class="s1">x = jnp.tanh(x)</span>

    <span class="s3">if not </span><span class="s1">with_classifier:</span>
      <span class="s3">return </span><span class="s1">x</span>
    <span class="s1">final_w</span><span class="s3">, </span><span class="s1">final_b = params[-</span><span class="s5">1</span><span class="s1">]</span>
    <span class="s1">logits = jnp.dot(x</span><span class="s3">, </span><span class="s1">final_w) + final_b</span>
    <span class="s3">return </span><span class="s1">logits - jax.scipy.special.logsumexp(</span>
      <span class="s1">logits</span><span class="s3">, </span><span class="s1">axis=</span><span class="s5">1</span><span class="s3">, </span><span class="s1">keepdims=</span><span class="s3">True</span><span class="s1">)  </span><span class="s0"># type: ignore[attr-defined]</span>

  <span class="s1">@staticmethod</span>
  <span class="s3">def </span><span class="s1">loss(params</span><span class="s3">, </span><span class="s1">inputs</span><span class="s3">, </span><span class="s1">labels):</span>
    <span class="s1">predictions = PureJaxMNIST.predict(params</span><span class="s3">, </span><span class="s1">inputs</span><span class="s3">, </span><span class="s1">with_classifier=</span><span class="s3">True</span><span class="s1">)</span>
    <span class="s3">return </span><span class="s1">-jnp.mean(jnp.sum(predictions * labels</span><span class="s3">, </span><span class="s1">axis=</span><span class="s5">1</span><span class="s1">))</span>

  <span class="s1">@staticmethod</span>
  <span class="s3">def </span><span class="s1">accuracy(predict: Callable</span><span class="s3">, </span><span class="s1">params</span><span class="s3">, </span><span class="s1">dataset):</span>

    <span class="s1">@jax.jit</span>
    <span class="s3">def </span><span class="s1">_per_batch(inputs</span><span class="s3">, </span><span class="s1">labels):</span>
      <span class="s1">target_class = jnp.argmax(labels</span><span class="s3">, </span><span class="s1">axis=</span><span class="s5">1</span><span class="s1">)</span>
      <span class="s1">predicted_class = jnp.argmax(predict(params</span><span class="s3">, </span><span class="s1">inputs)</span><span class="s3">, </span><span class="s1">axis=</span><span class="s5">1</span><span class="s1">)</span>
      <span class="s3">return </span><span class="s1">jnp.mean(predicted_class == target_class)</span>

    <span class="s1">batched = [</span>
      <span class="s1">_per_batch(inputs</span><span class="s3">, </span><span class="s1">labels) </span><span class="s3">for </span><span class="s1">inputs</span><span class="s3">, </span><span class="s1">labels </span><span class="s3">in </span><span class="s1">tfds.as_numpy(dataset)</span>
    <span class="s1">]</span>
    <span class="s3">return </span><span class="s1">jnp.mean(jnp.stack(batched))</span>

  <span class="s1">@staticmethod</span>
  <span class="s3">def </span><span class="s1">update(params</span><span class="s3">, </span><span class="s1">inputs</span><span class="s3">, </span><span class="s1">labels):</span>
    <span class="s1">grads = jax.grad(PureJaxMNIST.loss)(params</span><span class="s3">, </span><span class="s1">inputs</span><span class="s3">, </span><span class="s1">labels)</span>
    <span class="s3">return </span><span class="s1">[(w - step_size * dw</span><span class="s3">, </span><span class="s1">b - step_size * db)</span>
            <span class="s3">for </span><span class="s1">(w</span><span class="s3">, </span><span class="s1">b)</span><span class="s3">, </span><span class="s1">(dw</span><span class="s3">, </span><span class="s1">db) </span><span class="s3">in </span><span class="s1">zip(params</span><span class="s3">, </span><span class="s1">grads)]</span>

  <span class="s1">@staticmethod</span>
  <span class="s3">def </span><span class="s1">train(train_ds</span><span class="s3">, </span><span class="s1">test_ds</span><span class="s3">, </span><span class="s1">num_epochs</span><span class="s3">, </span><span class="s1">with_classifier=</span><span class="s3">True</span><span class="s1">):</span>
    <span class="s2">&quot;&quot;&quot;Trains a pure JAX MNIST predictor. 
 
    Returns: 
      a tuple with two elements: 
        - a predictor function with signature &quot;(Params, ImagesBatch) -&gt; 
        Predictions&quot;. 
          If `with_classifier=False` then the output of the predictor function 
          is the last layer of logits. 
        - the parameters &quot;Params&quot; for the predictor function 
    &quot;&quot;&quot;</span>
    <span class="s1">rng = jax.random.PRNGKey(</span><span class="s5">0</span><span class="s1">)</span>
    <span class="s1">params = [(param_scale * jax.random.normal(rng</span><span class="s3">, </span><span class="s1">(m</span><span class="s3">, </span><span class="s1">n))</span><span class="s3">,</span>
               <span class="s1">param_scale * jax.random.normal(rng</span><span class="s3">, </span><span class="s1">(n</span><span class="s3">,</span><span class="s1">)))</span>
              <span class="s3">for </span><span class="s1">m</span><span class="s3">, </span><span class="s1">n</span><span class="s3">, in </span><span class="s1">zip(layer_sizes[:-</span><span class="s5">1</span><span class="s1">]</span><span class="s3">, </span><span class="s1">layer_sizes[</span><span class="s5">1</span><span class="s1">:])]</span>

    <span class="s3">for </span><span class="s1">epoch </span><span class="s3">in </span><span class="s1">range(num_epochs):</span>
      <span class="s1">start_time = time.time()</span>
      <span class="s3">for </span><span class="s1">inputs</span><span class="s3">, </span><span class="s1">labels </span><span class="s3">in </span><span class="s1">tfds.as_numpy(train_ds):</span>
        <span class="s1">params = jax.jit(PureJaxMNIST.update)(params</span><span class="s3">, </span><span class="s1">inputs</span><span class="s3">, </span><span class="s1">labels)</span>
      <span class="s1">epoch_time = time.time() - start_time</span>
      <span class="s1">train_acc = PureJaxMNIST.accuracy(PureJaxMNIST.predict</span><span class="s3">, </span><span class="s1">params</span><span class="s3">, </span><span class="s1">train_ds)</span>
      <span class="s1">test_acc = PureJaxMNIST.accuracy(PureJaxMNIST.predict</span><span class="s3">, </span><span class="s1">params</span><span class="s3">, </span><span class="s1">test_ds)</span>
      <span class="s1">logging.info(</span><span class="s4">&quot;%s: Epoch %d in %0.2f sec&quot;</span><span class="s3">, </span><span class="s1">PureJaxMNIST.name</span><span class="s3">, </span><span class="s1">epoch</span><span class="s3">,</span>
                   <span class="s1">epoch_time)</span>
      <span class="s1">logging.info(</span><span class="s4">&quot;%s: Training set accuracy %0.2f%%&quot;</span><span class="s3">, </span><span class="s1">PureJaxMNIST.name</span><span class="s3">,</span>
                   <span class="s5">100. </span><span class="s1">* train_acc)</span>
      <span class="s1">logging.info(</span><span class="s4">&quot;%s: Test set accuracy %0.2f%%&quot;</span><span class="s3">, </span><span class="s1">PureJaxMNIST.name</span><span class="s3">,</span>
                   <span class="s5">100. </span><span class="s1">* test_acc)</span>

    <span class="s3">return </span><span class="s1">(functools.partial(</span>
      <span class="s1">PureJaxMNIST.predict</span><span class="s3">, </span><span class="s1">with_classifier=with_classifier)</span><span class="s3">, </span><span class="s1">params)</span>


<span class="s3">class </span><span class="s1">FlaxMNIST:</span>
  <span class="s2">&quot;&quot;&quot;An MNIST model using Flax.&quot;&quot;&quot;</span>

  <span class="s1">name = </span><span class="s4">&quot;mnist_flax&quot;</span>

  <span class="s3">class </span><span class="s1">Module(nn.Module):</span>
    <span class="s2">&quot;&quot;&quot;A simple CNN model for MNIST. 
 
    There is an option for the model to skip the classifier layer, for 
    demonstrating reuse of the classifier-less model into a larger model. 
    See README.md. 
    &quot;&quot;&quot;</span>

    <span class="s1">@nn.compact</span>
    <span class="s3">def </span><span class="s1">__call__(self</span><span class="s3">, </span><span class="s1">x</span><span class="s3">, </span><span class="s1">with_classifier=</span><span class="s3">True</span><span class="s1">):</span>
      <span class="s1">x = nn.Conv(features=</span><span class="s5">32</span><span class="s3">, </span><span class="s1">kernel_size=(</span><span class="s5">3</span><span class="s3">, </span><span class="s5">3</span><span class="s1">))(x)</span>
      <span class="s1">x = nn.relu(x)</span>
      <span class="s1">x = nn.avg_pool(x</span><span class="s3">, </span><span class="s1">window_shape=(</span><span class="s5">2</span><span class="s3">, </span><span class="s5">2</span><span class="s1">)</span><span class="s3">, </span><span class="s1">strides=(</span><span class="s5">2</span><span class="s3">, </span><span class="s5">2</span><span class="s1">))</span>
      <span class="s1">x = nn.Conv(features=</span><span class="s5">64</span><span class="s3">, </span><span class="s1">kernel_size=(</span><span class="s5">3</span><span class="s3">, </span><span class="s5">3</span><span class="s1">))(x)</span>
      <span class="s1">x = nn.relu(x)</span>
      <span class="s1">x = nn.avg_pool(x</span><span class="s3">, </span><span class="s1">window_shape=(</span><span class="s5">2</span><span class="s3">, </span><span class="s5">2</span><span class="s1">)</span><span class="s3">, </span><span class="s1">strides=(</span><span class="s5">2</span><span class="s3">, </span><span class="s5">2</span><span class="s1">))</span>
      <span class="s1">x = x.reshape((x.shape[</span><span class="s5">0</span><span class="s1">]</span><span class="s3">, </span><span class="s1">-</span><span class="s5">1</span><span class="s1">))  </span><span class="s0"># flatten</span>
      <span class="s1">x = nn.Dense(features=</span><span class="s5">256</span><span class="s1">)(x)</span>
      <span class="s1">x = nn.relu(x)</span>
      <span class="s3">if not </span><span class="s1">with_classifier:</span>
        <span class="s3">return </span><span class="s1">x</span>
      <span class="s1">x = nn.Dense(features=</span><span class="s5">10</span><span class="s1">)(x)</span>
      <span class="s1">x = nn.log_softmax(x)</span>
      <span class="s3">return </span><span class="s1">x</span>

  <span class="s0"># Create the model and save it</span>
  <span class="s1">model = Module()</span>

  <span class="s1">@staticmethod</span>
  <span class="s3">def </span><span class="s1">predict(params</span><span class="s3">, </span><span class="s1">inputs</span><span class="s3">, </span><span class="s1">with_classifier=</span><span class="s3">True</span><span class="s1">):</span>
    <span class="s3">return </span><span class="s1">FlaxMNIST.model.apply({</span><span class="s4">&quot;params&quot;</span><span class="s1">: params}</span><span class="s3">,</span>
                                 <span class="s1">inputs</span><span class="s3">,</span>
                                 <span class="s1">with_classifier=with_classifier)</span>

  <span class="s1">@staticmethod</span>
  <span class="s3">def </span><span class="s1">loss(params</span><span class="s3">, </span><span class="s1">inputs</span><span class="s3">, </span><span class="s1">labels):  </span><span class="s0"># Same as the pure JAX example</span>
    <span class="s0"># Must use the classifier layer because the labels are classes</span>
    <span class="s1">predictions = FlaxMNIST.predict(params</span><span class="s3">, </span><span class="s1">inputs</span><span class="s3">, </span><span class="s1">with_classifier=</span><span class="s3">True</span><span class="s1">)</span>
    <span class="s3">return </span><span class="s1">-jnp.mean(jnp.sum(predictions * labels</span><span class="s3">, </span><span class="s1">axis=</span><span class="s5">1</span><span class="s1">))</span>

  <span class="s1">@staticmethod</span>
  <span class="s3">def </span><span class="s1">update(tx</span><span class="s3">, </span><span class="s1">params</span><span class="s3">, </span><span class="s1">opt_state</span><span class="s3">, </span><span class="s1">inputs</span><span class="s3">, </span><span class="s1">labels):</span>
    <span class="s1">grad = jax.grad(FlaxMNIST.loss)(params</span><span class="s3">, </span><span class="s1">inputs</span><span class="s3">, </span><span class="s1">labels)</span>
    <span class="s1">updates</span><span class="s3">, </span><span class="s1">opt_state = tx.update(grad</span><span class="s3">, </span><span class="s1">opt_state)</span>
    <span class="s1">params = optax.apply_updates(params</span><span class="s3">, </span><span class="s1">updates)</span>
    <span class="s3">return </span><span class="s1">params</span><span class="s3">, </span><span class="s1">opt_state</span>

  <span class="s1">@staticmethod</span>
  <span class="s3">def </span><span class="s1">train(train_ds</span><span class="s3">, </span><span class="s1">test_ds</span><span class="s3">, </span><span class="s1">num_epochs</span><span class="s3">, </span><span class="s1">with_classifier=</span><span class="s3">True</span><span class="s1">):</span>
    <span class="s2">&quot;&quot;&quot;Trains a pure JAX MNIST predictor. 
 
    Returns: 
      a tuple with two elements: 
        - a predictor function with signature &quot;(Params, ImagesBatch) -&gt; 
          Predictions&quot;. 
          If `with_classifier=False` then the output of the predictor function 
          is the last layer of logits. 
        - the parameters &quot;Params&quot; for the predictor function 
    &quot;&quot;&quot;</span>
    <span class="s1">rng = jax.random.PRNGKey(</span><span class="s5">0</span><span class="s1">)</span>
    <span class="s1">momentum_mass = </span><span class="s5">0.9</span>

    <span class="s1">init_shape = jnp.ones((</span><span class="s5">1</span><span class="s3">,</span><span class="s1">) + input_shape</span><span class="s3">, </span><span class="s1">jnp.float32)</span>
    <span class="s1">params = FlaxMNIST.model.init(rng</span><span class="s3">, </span><span class="s1">init_shape)[</span><span class="s4">&quot;params&quot;</span><span class="s1">]</span>
    <span class="s1">tx = optax.sgd(learning_rate=step_size</span><span class="s3">, </span><span class="s1">momentum=momentum_mass)</span>
    <span class="s1">opt_state = tx.init(params)</span>

    <span class="s3">for </span><span class="s1">epoch </span><span class="s3">in </span><span class="s1">range(num_epochs):</span>
      <span class="s1">start_time = time.time()</span>
      <span class="s3">for </span><span class="s1">inputs</span><span class="s3">, </span><span class="s1">labels </span><span class="s3">in </span><span class="s1">tfds.as_numpy(train_ds):</span>
        <span class="s1">params</span><span class="s3">, </span><span class="s1">opt_state = jax.jit(FlaxMNIST.update</span><span class="s3">,</span>
                                    <span class="s1">static_argnums=</span><span class="s5">0</span><span class="s1">)(tx</span><span class="s3">, </span><span class="s1">params</span><span class="s3">, </span><span class="s1">opt_state</span><span class="s3">,</span>
                                                      <span class="s1">inputs</span><span class="s3">, </span><span class="s1">labels)</span>
      <span class="s1">epoch_time = time.time() - start_time</span>
      <span class="s0"># Same accuracy function as for the pure JAX example</span>
      <span class="s1">train_acc = PureJaxMNIST.accuracy(FlaxMNIST.predict</span><span class="s3">, </span><span class="s1">params</span><span class="s3">,</span>
                                        <span class="s1">train_ds)</span>
      <span class="s1">test_acc = PureJaxMNIST.accuracy(FlaxMNIST.predict</span><span class="s3">, </span><span class="s1">params</span><span class="s3">,</span>
                                       <span class="s1">test_ds)</span>
      <span class="s1">logging.info(</span><span class="s4">&quot;%s: Epoch %d in %0.2f sec&quot;</span><span class="s3">, </span><span class="s1">FlaxMNIST.name</span><span class="s3">, </span><span class="s1">epoch</span><span class="s3">,</span>
                   <span class="s1">epoch_time)</span>
      <span class="s1">logging.info(</span><span class="s4">&quot;%s: Training set accuracy %0.2f%%&quot;</span><span class="s3">, </span><span class="s1">FlaxMNIST.name</span><span class="s3">,</span>
                   <span class="s5">100. </span><span class="s1">* train_acc)</span>
      <span class="s1">logging.info(</span><span class="s4">&quot;%s: Test set accuracy %0.2f%%&quot;</span><span class="s3">, </span><span class="s1">FlaxMNIST.name</span><span class="s3">,</span>
                   <span class="s5">100. </span><span class="s1">* test_acc)</span>

    <span class="s0"># See discussion in README.md for packaging Flax models for conversion</span>
    <span class="s1">predict_fn = functools.partial(FlaxMNIST.predict</span><span class="s3">,</span>
                                   <span class="s1">with_classifier=with_classifier)</span>
    <span class="s3">return </span><span class="s1">(predict_fn</span><span class="s3">, </span><span class="s1">params)</span>


<span class="s3">def </span><span class="s1">plot_images(ds</span><span class="s3">,</span>
                <span class="s1">nr_rows: int</span><span class="s3">,</span>
                <span class="s1">nr_cols: int</span><span class="s3">,</span>
                <span class="s1">title: str</span><span class="s3">,</span>
                <span class="s1">inference_fn: Optional[Callable] = </span><span class="s3">None</span><span class="s1">):</span>
  <span class="s2">&quot;&quot;&quot;Plots a grid of images with their predictions. 
 
  Params: 
    ds: a tensorflow dataset from where to pick the images and labels. 
    nr_rows, nr_cols: the size of the grid to plot 
    title: the title of the plot 
    inference_fn: if None then print the existing label, else use this function 
      on the batch of images to produce a batch of inference results, which 
      get printed. 
    inference_batch_size: the size of the batch of images passed to 
    `inference_fn`. 
  &quot;&quot;&quot;</span>
  <span class="s1">count = nr_rows * nr_cols</span>
  <span class="s1">fig = plt.figure(figsize=(</span><span class="s5">8.</span><span class="s3">, </span><span class="s5">4.</span><span class="s1">)</span><span class="s3">, </span><span class="s1">num=title)</span>
  <span class="s0"># Get the first batch</span>
  <span class="s1">(images</span><span class="s3">, </span><span class="s1">labels)</span><span class="s3">, </span><span class="s1">= list(tfds.as_numpy(ds.take(</span><span class="s5">1</span><span class="s1">)))</span>
  <span class="s3">if </span><span class="s1">inference_fn:</span>
    <span class="s1">inferred_labels = inference_fn(images)</span>
  <span class="s3">for </span><span class="s1">i</span><span class="s3">, </span><span class="s1">image </span><span class="s3">in </span><span class="s1">enumerate(images[:count]):</span>
    <span class="s1">digit = fig.add_subplot(nr_rows</span><span class="s3">, </span><span class="s1">nr_cols</span><span class="s3">, </span><span class="s1">i + </span><span class="s5">1</span><span class="s1">)</span>
    <span class="s3">if </span><span class="s1">inference_fn:</span>
      <span class="s1">digit_title = </span><span class="s4">f&quot;infer: </span><span class="s3">{</span><span class="s1">np.argmax(inferred_labels[i])</span><span class="s3">}\n</span><span class="s4">&quot;</span>
    <span class="s3">else</span><span class="s1">:</span>
      <span class="s1">digit_title = </span><span class="s4">&quot;&quot;</span>
    <span class="s1">digit_title += </span><span class="s4">f&quot;label: </span><span class="s3">{</span><span class="s1">np.argmax(labels[i])</span><span class="s3">}</span><span class="s4">&quot;</span>
    <span class="s1">digit.set_title(digit_title)</span>
    <span class="s1">plt.imshow(</span>
      <span class="s1">(np.reshape(image</span><span class="s3">, </span><span class="s1">(</span><span class="s5">28</span><span class="s3">, </span><span class="s5">28</span><span class="s1">)) * </span><span class="s5">255</span><span class="s1">).astype(np.uint8)</span><span class="s3">,</span>
      <span class="s1">interpolation=</span><span class="s4">&quot;nearest&quot;</span><span class="s1">)</span>
  <span class="s1">plt.show()</span>
</pre>
</body>
</html>