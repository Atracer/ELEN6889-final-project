<html>
<head>
<title>lax_reference.py</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<style type="text/css">
.s0 { color: #808080;}
.s1 { color: #a9b7c6;}
.s2 { color: #cc7832;}
.s3 { color: #6897bb;}
.s4 { color: #6a8759;}
.s5 { color: #629755; font-style: italic;}
</style>
</head>
<body bgcolor="#2b2b2b">
<table CELLSPACING=0 CELLPADDING=5 COLS=1 WIDTH="100%" BGCOLOR="#606060" >
<tr><td><center>
<font face="Arial, Helvetica" color="#000000">
lax_reference.py</font>
</center></td></tr></table>
<pre><span class="s0"># Copyright 2018 The JAX Authors.</span>
<span class="s0">#</span>
<span class="s0"># Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);</span>
<span class="s0"># you may not use this file except in compliance with the License.</span>
<span class="s0"># You may obtain a copy of the License at</span>
<span class="s0">#</span>
<span class="s0">#     https://www.apache.org/licenses/LICENSE-2.0</span>
<span class="s0">#</span>
<span class="s0"># Unless required by applicable law or agreed to in writing, software</span>
<span class="s0"># distributed under the License is distributed on an &quot;AS IS&quot; BASIS,</span>
<span class="s0"># WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span>
<span class="s0"># See the License for the specific language governing permissions and</span>
<span class="s0"># limitations under the License.</span>


<span class="s2">import </span><span class="s1">builtins</span>
<span class="s2">import </span><span class="s1">collections</span>
<span class="s2">import </span><span class="s1">itertools</span>

<span class="s2">import </span><span class="s1">numpy </span><span class="s2">as </span><span class="s1">np</span>
<span class="s2">import </span><span class="s1">opt_einsum</span>
<span class="s2">import </span><span class="s1">scipy.special</span>

<span class="s2">from </span><span class="s1">jax._src </span><span class="s2">import </span><span class="s1">dtypes</span>
<span class="s2">from </span><span class="s1">jax._src </span><span class="s2">import </span><span class="s1">util</span>

<span class="s1">_slice = builtins.slice</span>
<span class="s1">_max = builtins.max</span>
<span class="s1">_min = builtins.min</span>
<span class="s1">_map = builtins.map</span>

<span class="s1">neg = np.negative</span>
<span class="s1">sign = np.sign</span>
<span class="s1">floor = np.floor</span>
<span class="s1">ceil = np.ceil</span>

<span class="s2">def </span><span class="s1">round(x):</span>
  <span class="s2">return </span><span class="s1">np.trunc(</span>
    <span class="s1">x + np.copysign(np.nextafter(np.array(</span><span class="s3">.5</span><span class="s2">, </span><span class="s1">dtype=x.dtype)</span><span class="s2">,</span>
                                 <span class="s1">np.array(</span><span class="s3">0.</span><span class="s2">, </span><span class="s1">dtype=x.dtype)</span><span class="s2">,</span>
                                 <span class="s1">dtype=x.dtype)</span><span class="s2">, </span><span class="s1">x)).astype(x.dtype)</span>

<span class="s1">nextafter = np.nextafter</span>

<span class="s1">is_finite = np.isfinite</span>

<span class="s1">exp = np.exp</span>
<span class="s1">expm1 = np.expm1</span>
<span class="s1">log = np.log</span>
<span class="s1">log1p = np.log1p</span>
<span class="s1">tanh = np.tanh</span>
<span class="s1">sin = np.sin</span>
<span class="s1">cos = np.cos</span>
<span class="s1">atan2 = np.arctan2</span>

<span class="s1">sqrt = np.sqrt</span>
<span class="s1">rsqrt = </span><span class="s2">lambda </span><span class="s1">x: np.ones_like(x) / np.sqrt(x)</span>
<span class="s1">cbrt = np.cbrt</span>

<span class="s1">square = np.square</span>
<span class="s1">reciprocal = np.reciprocal</span>
<span class="s1">tan = np.tan</span>
<span class="s1">asin = np.arcsin</span>
<span class="s1">acos = np.arccos</span>
<span class="s1">atan = np.arctan</span>
<span class="s1">sinh = np.sinh</span>
<span class="s1">cosh = np.cosh</span>
<span class="s1">asinh = np.arcsinh</span>
<span class="s1">acosh = np.arccosh</span>
<span class="s1">atanh = np.arctanh</span>

<span class="s2">def </span><span class="s1">logistic(x): </span><span class="s2">return </span><span class="s3">1 </span><span class="s1">/ (</span><span class="s3">1 </span><span class="s1">+ np.exp(-x))</span>
<span class="s2">def </span><span class="s1">betainc(a</span><span class="s2">, </span><span class="s1">b</span><span class="s2">, </span><span class="s1">x): </span><span class="s2">return </span><span class="s1">scipy.special.betainc(a</span><span class="s2">, </span><span class="s1">b</span><span class="s2">, </span><span class="s1">x).astype(x.dtype)</span>
<span class="s2">def </span><span class="s1">lgamma(x): </span><span class="s2">return </span><span class="s1">scipy.special.gammaln(x).astype(x.dtype)</span>
<span class="s2">def </span><span class="s1">digamma(x): </span><span class="s2">return </span><span class="s1">scipy.special.digamma(x).astype(x.dtype)</span>
<span class="s1">igamma = scipy.special.gammainc</span>
<span class="s1">igammac = scipy.special.gammaincc</span>
<span class="s2">def </span><span class="s1">erf(x): </span><span class="s2">return </span><span class="s1">scipy.special.erf(x).astype(x.dtype)</span>
<span class="s2">def </span><span class="s1">erfc(x): </span><span class="s2">return </span><span class="s1">scipy.special.erfc(x).astype(x.dtype)</span>
<span class="s2">def </span><span class="s1">erf_inv(x): </span><span class="s2">return </span><span class="s1">scipy.special.erfinv(x).astype(x.dtype)</span>

<span class="s2">def </span><span class="s1">bessel_i0e(x): </span><span class="s2">return </span><span class="s1">scipy.special.i0e(x).astype(x.dtype)</span>
<span class="s2">def </span><span class="s1">bessel_i1e(x): </span><span class="s2">return </span><span class="s1">scipy.special.i1e(x).astype(x.dtype)</span>

<span class="s1">real = np.real</span>
<span class="s1">imag = np.imag</span>

<span class="s2">def </span><span class="s1">conj(x):</span>
  <span class="s2">return </span><span class="s1">np.conj(x) + np.complex64(</span><span class="s3">0</span><span class="s1">)</span>

<span class="s2">def </span><span class="s1">complex(x</span><span class="s2">, </span><span class="s1">y):</span>
  <span class="s2">return </span><span class="s1">x + np.complex64(</span><span class="s3">1j</span><span class="s1">) * y</span>

<span class="s1">abs = np.absolute</span>
<span class="s1">pow = np.power</span>

<span class="s1">bitwise_not = np.bitwise_not</span>
<span class="s1">bitwise_and = np.bitwise_and</span>
<span class="s1">bitwise_or = np.bitwise_or</span>
<span class="s1">bitwise_xor = np.bitwise_xor</span>

<span class="s1">add = np.add</span>
<span class="s1">sub = np.subtract</span>
<span class="s1">mul = np.multiply</span>

<span class="s2">def </span><span class="s1">div(lhs</span><span class="s2">, </span><span class="s1">rhs):</span>
  <span class="s2">if </span><span class="s1">dtypes.issubdtype(dtypes.result_type(lhs)</span><span class="s2">, </span><span class="s1">np.integer):</span>
    <span class="s1">quotient = np.floor_divide(lhs</span><span class="s2">, </span><span class="s1">rhs)</span>
    <span class="s1">select = np.logical_and(np.sign(lhs) != np.sign(rhs)</span><span class="s2">,</span>
                             <span class="s1">np.remainder(lhs</span><span class="s2">, </span><span class="s1">rhs) != </span><span class="s3">0</span><span class="s1">)</span>
    <span class="s2">return </span><span class="s1">np.where(select</span><span class="s2">, </span><span class="s1">quotient + </span><span class="s3">1</span><span class="s2">, </span><span class="s1">quotient)</span>
  <span class="s2">else</span><span class="s1">:</span>
    <span class="s2">return </span><span class="s1">np.divide(lhs</span><span class="s2">, </span><span class="s1">rhs)</span>

<span class="s2">def </span><span class="s1">rem(lhs</span><span class="s2">, </span><span class="s1">rhs):</span>
  <span class="s2">return </span><span class="s1">np.sign(lhs) * np.remainder(np.abs(lhs)</span><span class="s2">, </span><span class="s1">np.abs(rhs))</span>

<span class="s1">max = np.maximum</span>
<span class="s1">min = np.minimum</span>

<span class="s1">shift_left = np.left_shift</span>
<span class="s1">shift_right_arithmetic = np.right_shift</span>
<span class="s0"># TODO shift_right_logical</span>

<span class="s2">def </span><span class="s1">population_count(x):</span>
  <span class="s2">assert </span><span class="s1">np.issubdtype(x.dtype</span><span class="s2">, </span><span class="s1">np.integer)</span>
  <span class="s1">dtype = x.dtype</span>
  <span class="s1">iinfo = np.iinfo(x.dtype)</span>
  <span class="s2">if </span><span class="s1">np.iinfo(x.dtype).bits &lt; </span><span class="s3">32</span><span class="s1">:</span>
    <span class="s2">assert </span><span class="s1">iinfo.kind </span><span class="s2">in </span><span class="s1">(</span><span class="s4">'i'</span><span class="s2">, </span><span class="s4">'u'</span><span class="s1">)</span>
    <span class="s1">x = x.astype(np.uint32 </span><span class="s2">if </span><span class="s1">iinfo.kind == </span><span class="s4">'u' </span><span class="s2">else </span><span class="s1">np.int32)</span>
  <span class="s2">if </span><span class="s1">iinfo.kind == </span><span class="s4">'i'</span><span class="s1">:</span>
    <span class="s1">x = x.view(</span><span class="s4">f&quot;uint</span><span class="s2">{</span><span class="s1">np.iinfo(x.dtype).bits</span><span class="s2">}</span><span class="s4">&quot;</span><span class="s1">)</span>
  <span class="s2">assert </span><span class="s1">x.dtype </span><span class="s2">in </span><span class="s1">(np.uint32</span><span class="s2">, </span><span class="s1">np.uint64)</span>
  <span class="s1">m = [</span>
      <span class="s1">np.uint64(</span><span class="s3">0x5555555555555555</span><span class="s1">)</span><span class="s2">,  </span><span class="s0"># binary: 0101...</span>
      <span class="s1">np.uint64(</span><span class="s3">0x3333333333333333</span><span class="s1">)</span><span class="s2">,  </span><span class="s0"># binary: 00110011..</span>
      <span class="s1">np.uint64(</span><span class="s3">0x0f0f0f0f0f0f0f0f</span><span class="s1">)</span><span class="s2">,  </span><span class="s0"># binary:  4 zeros,  4 ones ...</span>
      <span class="s1">np.uint64(</span><span class="s3">0x00ff00ff00ff00ff</span><span class="s1">)</span><span class="s2">,  </span><span class="s0"># binary:  8 zeros,  8 ones ...</span>
      <span class="s1">np.uint64(</span><span class="s3">0x0000ffff0000ffff</span><span class="s1">)</span><span class="s2">,  </span><span class="s0"># binary: 16 zeros, 16 ones ...</span>
      <span class="s1">np.uint64(</span><span class="s3">0x00000000ffffffff</span><span class="s1">)</span><span class="s2">,  </span><span class="s0"># binary: 32 zeros, 32 ones</span>
  <span class="s1">]</span>

  <span class="s2">if </span><span class="s1">x.dtype == np.uint32:</span>
    <span class="s1">m = list(map(np.uint32</span><span class="s2">, </span><span class="s1">m[:-</span><span class="s3">1</span><span class="s1">]))</span>

  <span class="s1">x = (x &amp; m[</span><span class="s3">0</span><span class="s1">]) + ((x &gt;&gt;  </span><span class="s3">1</span><span class="s1">) &amp; m[</span><span class="s3">0</span><span class="s1">])  </span><span class="s0"># put count of each  2 bits into those  2 bits</span>
  <span class="s1">x = (x &amp; m[</span><span class="s3">1</span><span class="s1">]) + ((x &gt;&gt;  </span><span class="s3">2</span><span class="s1">) &amp; m[</span><span class="s3">1</span><span class="s1">])  </span><span class="s0"># put count of each  4 bits into those  4 bits</span>
  <span class="s1">x = (x &amp; m[</span><span class="s3">2</span><span class="s1">]) + ((x &gt;&gt;  </span><span class="s3">4</span><span class="s1">) &amp; m[</span><span class="s3">2</span><span class="s1">])  </span><span class="s0"># put count of each  8 bits into those  8 bits</span>
  <span class="s1">x = (x &amp; m[</span><span class="s3">3</span><span class="s1">]) + ((x &gt;&gt;  </span><span class="s3">8</span><span class="s1">) &amp; m[</span><span class="s3">3</span><span class="s1">])  </span><span class="s0"># put count of each 16 bits into those 16 bits</span>
  <span class="s1">x = (x &amp; m[</span><span class="s3">4</span><span class="s1">]) + ((x &gt;&gt; </span><span class="s3">16</span><span class="s1">) &amp; m[</span><span class="s3">4</span><span class="s1">])  </span><span class="s0"># put count of each 32 bits into those 32 bits</span>
  <span class="s2">if </span><span class="s1">x.dtype == np.uint64:</span>
    <span class="s1">x = (x &amp; m[</span><span class="s3">5</span><span class="s1">]) + ((x &gt;&gt; </span><span class="s3">32</span><span class="s1">) &amp; m[</span><span class="s3">5</span><span class="s1">])  </span><span class="s0"># put count of each 64 bits into those 64 bits</span>
  <span class="s2">return </span><span class="s1">x.astype(dtype)</span>

<span class="s2">def </span><span class="s1">clz(x):</span>
  <span class="s2">assert </span><span class="s1">np.issubdtype(x.dtype</span><span class="s2">, </span><span class="s1">np.integer)</span>
  <span class="s1">nbits = np.iinfo(x.dtype).bits</span>
  <span class="s1">mask = (</span><span class="s3">2 </span><span class="s1">** np.arange(nbits</span><span class="s2">, </span><span class="s1">dtype=x.dtype))[::-</span><span class="s3">1</span><span class="s1">]</span>
  <span class="s1">bits = (x[...</span><span class="s2">, None</span><span class="s1">] &amp; mask).astype(np.bool_)</span>
  <span class="s1">out = np.argmax(bits</span><span class="s2">, </span><span class="s1">axis=-</span><span class="s3">1</span><span class="s1">).astype(x.dtype)</span>
  <span class="s1">out[x == </span><span class="s3">0</span><span class="s1">] = nbits</span>
  <span class="s2">return </span><span class="s1">out</span>

<span class="s1">eq = np.equal</span>
<span class="s1">ne = np.not_equal</span>
<span class="s1">ge = np.greater_equal</span>
<span class="s1">gt = np.greater</span>
<span class="s1">le = np.less_equal</span>
<span class="s1">lt = np.less</span>

<span class="s2">def </span><span class="s1">convert_element_type(operand</span><span class="s2">, </span><span class="s1">dtype):</span>
  <span class="s2">return </span><span class="s1">np.asarray(operand</span><span class="s2">, </span><span class="s1">dtype=dtype)</span>

<span class="s2">def </span><span class="s1">bitcast_convert_type(operand</span><span class="s2">, </span><span class="s1">dtype):</span>
  <span class="s2">return </span><span class="s1">np.asarray(operand).view(dtype)</span>

<span class="s2">def </span><span class="s1">clamp(min</span><span class="s2">, </span><span class="s1">operand</span><span class="s2">, </span><span class="s1">max):</span>
  <span class="s2">return </span><span class="s1">np.clip(operand</span><span class="s2">, </span><span class="s1">np.clip(min</span><span class="s2">, None, </span><span class="s1">max)</span><span class="s2">, </span><span class="s1">max).astype(operand.dtype)</span>

<span class="s2">def </span><span class="s1">concatenate(operands</span><span class="s2">, </span><span class="s1">dimension):</span>
  <span class="s2">return </span><span class="s1">np.concatenate(operands</span><span class="s2">, </span><span class="s1">axis=dimension)</span>

<span class="s2">def </span><span class="s1">conv(lhs</span><span class="s2">, </span><span class="s1">rhs</span><span class="s2">, </span><span class="s1">window_strides</span><span class="s2">, </span><span class="s1">padding):</span>
  <span class="s1">pads = padtype_to_pads(lhs.shape[</span><span class="s3">2</span><span class="s1">:]</span><span class="s2">, </span><span class="s1">rhs.shape[</span><span class="s3">2</span><span class="s1">:]</span><span class="s2">, </span><span class="s1">window_strides</span><span class="s2">, </span><span class="s1">padding)</span>
  <span class="s2">return </span><span class="s1">_conv(lhs</span><span class="s2">, </span><span class="s1">rhs</span><span class="s2">, </span><span class="s1">window_strides</span><span class="s2">, </span><span class="s1">pads)</span>

<span class="s2">def </span><span class="s1">conv_with_general_padding(</span>
    <span class="s1">lhs</span><span class="s2">, </span><span class="s1">rhs</span><span class="s2">, </span><span class="s1">window_strides</span><span class="s2">, </span><span class="s1">padding</span><span class="s2">, </span><span class="s1">lhs_dilation</span><span class="s2">, </span><span class="s1">rhs_dilation):</span>
  <span class="s2">return </span><span class="s1">_conv(_dilate(lhs</span><span class="s2">, </span><span class="s1">lhs_dilation)</span><span class="s2">, </span><span class="s1">_dilate(rhs</span><span class="s2">, </span><span class="s1">rhs_dilation)</span><span class="s2">,</span>
               <span class="s1">window_strides</span><span class="s2">, </span><span class="s1">padding)</span>

<span class="s2">def </span><span class="s1">conv_general_dilated(lhs</span><span class="s2">, </span><span class="s1">rhs</span><span class="s2">, </span><span class="s1">window_strides</span><span class="s2">, </span><span class="s1">padding</span><span class="s2">, </span><span class="s1">lhs_dilation</span><span class="s2">,</span>
                         <span class="s1">rhs_dilation</span><span class="s2">, </span><span class="s1">dimension_numbers):</span>
  <span class="s1">lhs_perm</span><span class="s2">, </span><span class="s1">rhs_perm</span><span class="s2">, </span><span class="s1">out_perm = _conv_general_permutations(dimension_numbers)</span>
  <span class="s2">if </span><span class="s1">isinstance(padding</span><span class="s2">, </span><span class="s1">str):</span>
    <span class="s1">padding = padtype_to_pads(np.take(lhs.shape</span><span class="s2">, </span><span class="s1">lhs_perm)[</span><span class="s3">2</span><span class="s1">:]</span><span class="s2">,</span>
                              <span class="s1">np.take(rhs.shape</span><span class="s2">, </span><span class="s1">rhs_perm)[</span><span class="s3">2</span><span class="s1">:]</span><span class="s2">,</span>
                              <span class="s1">window_strides</span><span class="s2">, </span><span class="s1">padding)</span>
  <span class="s1">trans_lhs = transpose(lhs</span><span class="s2">, </span><span class="s1">lhs_perm)</span>
  <span class="s1">trans_rhs = transpose(rhs</span><span class="s2">, </span><span class="s1">rhs_perm)</span>
  <span class="s1">out = conv_with_general_padding(trans_lhs</span><span class="s2">, </span><span class="s1">trans_rhs</span><span class="s2">, </span><span class="s1">window_strides</span><span class="s2">, </span><span class="s1">padding</span><span class="s2">,</span>
                                  <span class="s1">lhs_dilation</span><span class="s2">, </span><span class="s1">rhs_dilation)</span>
  <span class="s2">return </span><span class="s1">transpose(out</span><span class="s2">, </span><span class="s1">np.argsort(out_perm))</span>

<span class="s1">dot = np.dot</span>

<span class="s2">def </span><span class="s1">dot_general(lhs</span><span class="s2">, </span><span class="s1">rhs</span><span class="s2">, </span><span class="s1">dimension_numbers):</span>
  <span class="s1">(lhs_contracting</span><span class="s2">, </span><span class="s1">rhs_contracting)</span><span class="s2">, </span><span class="s1">(lhs_batch</span><span class="s2">, </span><span class="s1">rhs_batch) = dimension_numbers</span>
  <span class="s1">new_id = itertools.count()</span>
  <span class="s1">lhs_axis_ids = [next(new_id) </span><span class="s2">for </span><span class="s1">_ </span><span class="s2">in </span><span class="s1">lhs.shape]</span>
  <span class="s1">rhs_axis_ids = [next(new_id) </span><span class="s2">for </span><span class="s1">_ </span><span class="s2">in </span><span class="s1">rhs.shape]</span>
  <span class="s1">lhs_out_axis_ids = lhs_axis_ids[:]</span>
  <span class="s1">rhs_out_axis_ids = rhs_axis_ids[:]</span>

  <span class="s2">for </span><span class="s1">lhs_axis</span><span class="s2">, </span><span class="s1">rhs_axis </span><span class="s2">in </span><span class="s1">zip(lhs_contracting</span><span class="s2">, </span><span class="s1">rhs_contracting):</span>
    <span class="s1">shared_id = next(new_id)</span>
    <span class="s1">lhs_axis_ids[lhs_axis] = shared_id</span>
    <span class="s1">rhs_axis_ids[rhs_axis] = shared_id</span>
    <span class="s1">lhs_out_axis_ids[lhs_axis] = </span><span class="s2">None</span>
    <span class="s1">rhs_out_axis_ids[rhs_axis] = </span><span class="s2">None</span>

  <span class="s1">batch_ids = []</span>
  <span class="s2">for </span><span class="s1">lhs_axis</span><span class="s2">, </span><span class="s1">rhs_axis </span><span class="s2">in </span><span class="s1">zip(lhs_batch</span><span class="s2">, </span><span class="s1">rhs_batch):</span>
    <span class="s1">shared_id = next(new_id)</span>
    <span class="s1">lhs_axis_ids[lhs_axis] = shared_id</span>
    <span class="s1">rhs_axis_ids[rhs_axis] = shared_id</span>
    <span class="s1">lhs_out_axis_ids[lhs_axis] = </span><span class="s2">None</span>
    <span class="s1">rhs_out_axis_ids[rhs_axis] = </span><span class="s2">None</span>
    <span class="s1">batch_ids.append(shared_id)</span>

  <span class="s1">not_none = </span><span class="s2">lambda </span><span class="s1">x: x </span><span class="s2">is not None</span>
  <span class="s1">out_axis_ids = filter(not_none</span><span class="s2">,</span>
                        <span class="s1">batch_ids + lhs_out_axis_ids + rhs_out_axis_ids)</span>
  <span class="s2">assert </span><span class="s1">lhs.dtype == rhs.dtype</span>
  <span class="s1">dtype = np.float32 </span><span class="s2">if </span><span class="s1">lhs.dtype == dtypes.bfloat16 </span><span class="s2">else None</span>
  <span class="s1">out = np.einsum(lhs</span><span class="s2">, </span><span class="s1">lhs_axis_ids</span><span class="s2">, </span><span class="s1">rhs</span><span class="s2">, </span><span class="s1">rhs_axis_ids</span><span class="s2">, </span><span class="s1">out_axis_ids</span><span class="s2">,</span>
                   <span class="s1">dtype=dtype)</span>
  <span class="s2">return </span><span class="s1">out.astype(dtypes.bfloat16) </span><span class="s2">if </span><span class="s1">lhs.dtype == dtypes.bfloat16 </span><span class="s2">else </span><span class="s1">out</span>

<span class="s2">def </span><span class="s1">broadcast(operand</span><span class="s2">, </span><span class="s1">sizes):</span>
  <span class="s2">return </span><span class="s1">np.broadcast_to(operand</span><span class="s2">, </span><span class="s1">sizes + np.shape(operand))</span>

<span class="s2">def </span><span class="s1">broadcast_in_dim(operand</span><span class="s2">, </span><span class="s1">shape</span><span class="s2">, </span><span class="s1">broadcast_dimensions):</span>
  <span class="s1">in_reshape = np.ones(len(shape)</span><span class="s2">, </span><span class="s1">dtype=np.int32)</span>
  <span class="s2">for </span><span class="s1">i</span><span class="s2">, </span><span class="s1">bd </span><span class="s2">in </span><span class="s1">enumerate(broadcast_dimensions):</span>
    <span class="s1">in_reshape[bd] = operand.shape[i]</span>
  <span class="s2">return </span><span class="s1">np.broadcast_to(np.reshape(operand</span><span class="s2">, </span><span class="s1">in_reshape)</span><span class="s2">, </span><span class="s1">shape)</span>

<span class="s1">sum = np.sum</span>

<span class="s1">squeeze = np.squeeze</span>

<span class="s2">def </span><span class="s1">reshape(operand</span><span class="s2">, </span><span class="s1">new_sizes</span><span class="s2">, </span><span class="s1">dimensions=</span><span class="s2">None</span><span class="s1">):</span>
  <span class="s2">if </span><span class="s1">dimensions </span><span class="s2">is None</span><span class="s1">:</span>
    <span class="s1">dimensions = range(len(np.shape(operand)))</span>
  <span class="s2">return </span><span class="s1">np.reshape(np.transpose(operand</span><span class="s2">, </span><span class="s1">dimensions)</span><span class="s2">, </span><span class="s1">new_sizes)</span>

<span class="s2">def </span><span class="s1">pad(operand</span><span class="s2">, </span><span class="s1">padding_value</span><span class="s2">, </span><span class="s1">padding_config):</span>
  <span class="s0"># https://www.tensorflow.org/xla/operation_semantics#pad</span>
  <span class="s1">lo</span><span class="s2">, </span><span class="s1">hi</span><span class="s2">, </span><span class="s1">interior = util.unzip3(padding_config)</span>
  <span class="s0"># Handle first the positive edge padding and interior</span>
  <span class="s1">lo_pos</span><span class="s2">, </span><span class="s1">hi_pos = np.clip(lo</span><span class="s2">, </span><span class="s3">0</span><span class="s2">, None</span><span class="s1">)</span><span class="s2">, </span><span class="s1">np.clip(hi</span><span class="s2">, </span><span class="s3">0</span><span class="s2">, None</span><span class="s1">)</span>
  <span class="s1">outshape = np.add(np.add(np.add(lo_pos</span><span class="s2">, </span><span class="s1">hi_pos)</span><span class="s2">, </span><span class="s1">operand.shape)</span><span class="s2">,</span>
                     <span class="s1">np.multiply(interior</span><span class="s2">, </span><span class="s1">np.subtract(operand.shape</span><span class="s2">, </span><span class="s3">1</span><span class="s1">)))</span>
  <span class="s1">out = np.full(outshape</span><span class="s2">, </span><span class="s1">padding_value</span><span class="s2">, </span><span class="s1">operand.dtype)</span>
  <span class="s1">lhs_slices = tuple(_slice(l </span><span class="s2">if </span><span class="s1">l &gt; </span><span class="s3">0 </span><span class="s2">else </span><span class="s3">0</span><span class="s2">, </span><span class="s1">-h </span><span class="s2">if </span><span class="s1">h &gt; </span><span class="s3">0 </span><span class="s2">else None, </span><span class="s1">step)</span>
                     <span class="s2">for </span><span class="s1">l</span><span class="s2">, </span><span class="s1">h</span><span class="s2">, </span><span class="s1">step </span><span class="s2">in </span><span class="s1">zip(lo_pos</span><span class="s2">, </span><span class="s1">hi_pos</span><span class="s2">, </span><span class="s1">np.add(</span><span class="s3">1</span><span class="s2">, </span><span class="s1">interior)))</span>
  <span class="s1">out[lhs_slices] = operand</span>
  <span class="s1">trim_slices = tuple(_slice(-l </span><span class="s2">if </span><span class="s1">l &lt; </span><span class="s3">0 </span><span class="s2">else </span><span class="s3">0</span><span class="s2">, </span><span class="s1">h </span><span class="s2">if </span><span class="s1">h &lt; </span><span class="s3">0 </span><span class="s2">else None</span><span class="s1">)</span>
                     <span class="s2">for </span><span class="s1">l</span><span class="s2">, </span><span class="s1">h </span><span class="s2">in </span><span class="s1">zip(lo</span><span class="s2">, </span><span class="s1">hi))</span>
  <span class="s2">return </span><span class="s1">out[trim_slices]</span>

<span class="s2">def </span><span class="s1">rev(operand</span><span class="s2">, </span><span class="s1">dimensions):</span>
  <span class="s1">dimensions = frozenset(dimensions)</span>
  <span class="s1">indexer = (_slice(</span><span class="s2">None, None, </span><span class="s1">-</span><span class="s3">1</span><span class="s1">) </span><span class="s2">if </span><span class="s1">d </span><span class="s2">in </span><span class="s1">dimensions </span><span class="s2">else </span><span class="s1">_slice(</span><span class="s2">None</span><span class="s1">)</span>
             <span class="s2">for </span><span class="s1">d </span><span class="s2">in </span><span class="s1">range(np.ndim(operand)))</span>
  <span class="s2">return </span><span class="s1">operand[tuple(indexer)]</span>

<span class="s1">select = np.where</span>

<span class="s2">def </span><span class="s1">slice(operand</span><span class="s2">, </span><span class="s1">start_indices</span><span class="s2">, </span><span class="s1">limit_indices</span><span class="s2">, </span><span class="s1">strides=</span><span class="s2">None</span><span class="s1">):  </span><span class="s0"># pylint: disable=redefined-builtin</span>
  <span class="s2">if </span><span class="s1">strides </span><span class="s2">is None</span><span class="s1">:</span>
    <span class="s1">strides = np.ones(len(start_indices)).astype(int)</span>
  <span class="s1">slices = tuple(_map(_slice</span><span class="s2">, </span><span class="s1">start_indices</span><span class="s2">, </span><span class="s1">limit_indices</span><span class="s2">, </span><span class="s1">strides))</span>
  <span class="s2">return </span><span class="s1">operand[slices]</span>

<span class="s2">def </span><span class="s1">dynamic_slice(operand</span><span class="s2">, </span><span class="s1">start_indices</span><span class="s2">, </span><span class="s1">slice_sizes):</span>
  <span class="s1">out = np.zeros(slice_sizes</span><span class="s2">, </span><span class="s1">dtype=operand.dtype)</span>
  <span class="s1">idx = tuple(_slice(start</span><span class="s2">, </span><span class="s1">start+size)</span>
              <span class="s2">for </span><span class="s1">start</span><span class="s2">, </span><span class="s1">size </span><span class="s2">in </span><span class="s1">zip(start_indices</span><span class="s2">, </span><span class="s1">slice_sizes))</span>
  <span class="s1">section = operand[idx]</span>
  <span class="s1">out[tuple(_slice(</span><span class="s2">None, </span><span class="s1">stop) </span><span class="s2">for </span><span class="s1">stop </span><span class="s2">in </span><span class="s1">section.shape)] = section</span>
  <span class="s2">return </span><span class="s1">out</span>

<span class="s2">def </span><span class="s1">dynamic_update_slice(operand</span><span class="s2">, </span><span class="s1">update</span><span class="s2">, </span><span class="s1">start_indices):</span>
  <span class="s1">slices = tuple(_map(_slice</span><span class="s2">, </span><span class="s1">start_indices</span><span class="s2">, </span><span class="s1">np.add(start_indices</span><span class="s2">, </span><span class="s1">update.shape)))</span>
  <span class="s1">updated_operand = np.copy(operand)</span>
  <span class="s1">updated_operand[slices] = update</span>
  <span class="s2">return </span><span class="s1">updated_operand</span>

<span class="s1">transpose = np.transpose</span>

<span class="s2">def </span><span class="s1">reduce(operand</span><span class="s2">, </span><span class="s1">init_value</span><span class="s2">, </span><span class="s1">computation</span><span class="s2">, </span><span class="s1">dimensions):  </span><span class="s0"># pylint: disable=redefined-builtin</span>
  <span class="s1">reducer = _make_reducer(computation</span><span class="s2">, </span><span class="s1">init_value)</span>
  <span class="s2">return </span><span class="s1">reducer(operand</span><span class="s2">, </span><span class="s1">tuple(dimensions)).astype(np.asarray(operand).dtype)</span>

<span class="s2">def </span><span class="s1">reduce_window(operand</span><span class="s2">, </span><span class="s1">init_value</span><span class="s2">, </span><span class="s1">computation</span><span class="s2">, </span><span class="s1">window_dimensions</span><span class="s2">,</span>
                  <span class="s1">window_strides</span><span class="s2">, </span><span class="s1">padding</span><span class="s2">, </span><span class="s1">base_dilation):</span>
  <span class="s1">op</span><span class="s2">, </span><span class="s1">dims</span><span class="s2">, </span><span class="s1">strides = operand</span><span class="s2">, </span><span class="s1">window_dimensions</span><span class="s2">, </span><span class="s1">window_strides</span>
  <span class="s2">if </span><span class="s1">isinstance(padding</span><span class="s2">, </span><span class="s1">str):</span>
    <span class="s1">pads = padtype_to_pads(op.shape</span><span class="s2">, </span><span class="s1">dims</span><span class="s2">, </span><span class="s1">strides</span><span class="s2">, </span><span class="s1">padding)</span>
  <span class="s2">else</span><span class="s1">:</span>
    <span class="s1">pads = padding</span>
  <span class="s1">op = op.reshape((</span><span class="s3">1</span><span class="s2">, </span><span class="s3">1</span><span class="s1">) + op.shape)</span>
  <span class="s2">if </span><span class="s1">base_dilation:</span>
    <span class="s1">op = _dilate(op</span><span class="s2">, </span><span class="s1">base_dilation</span><span class="s2">, </span><span class="s1">init_value)</span>
  <span class="s1">view = _conv_view(op</span><span class="s2">, </span><span class="s1">(</span><span class="s3">1</span><span class="s2">, </span><span class="s3">1</span><span class="s1">) + dims</span><span class="s2">, </span><span class="s1">strides</span><span class="s2">, </span><span class="s1">pads</span><span class="s2">,</span>
                    <span class="s1">pad_value=init_value)[</span><span class="s3">0</span><span class="s1">]</span>
  <span class="s1">view = view.reshape(view.shape[</span><span class="s3">1</span><span class="s1">:</span><span class="s3">1</span><span class="s1">+len(dims)] + (-</span><span class="s3">1</span><span class="s2">,</span><span class="s1">))</span>
  <span class="s1">reducer = _make_reducer(computation</span><span class="s2">, </span><span class="s1">init_value)</span>
  <span class="s2">return </span><span class="s1">reducer(view</span><span class="s2">, </span><span class="s1">axis=-</span><span class="s3">1</span><span class="s1">)</span>

<span class="s0"># TODO(mattjj): select_and_scatter</span>

<span class="s1">sort = np.sort</span>

<span class="s2">def </span><span class="s1">sort_key_val(keys</span><span class="s2">, </span><span class="s1">values</span><span class="s2">, </span><span class="s1">dimension=-</span><span class="s3">1</span><span class="s1">):</span>
  <span class="s1">idxs = list(np.ix_(*[np.arange(d) </span><span class="s2">for </span><span class="s1">d </span><span class="s2">in </span><span class="s1">keys.shape]))</span>
  <span class="s1">idxs[dimension] = np.argsort(keys</span><span class="s2">, </span><span class="s1">axis=dimension)</span>
  <span class="s2">return </span><span class="s1">keys[tuple(idxs)]</span><span class="s2">, </span><span class="s1">values[tuple(idxs)]</span>

<span class="s0">### conv util</span>

<span class="s2">def </span><span class="s1">_conv(lhs</span><span class="s2">, </span><span class="s1">rhs</span><span class="s2">, </span><span class="s1">window_strides</span><span class="s2">, </span><span class="s1">pads):</span>
  <span class="s1">view</span><span class="s2">, </span><span class="s1">view_axes</span><span class="s2">, </span><span class="s1">rhs_axes</span><span class="s2">, </span><span class="s1">out_axes = _conv_view(</span>
      <span class="s1">lhs</span><span class="s2">, </span><span class="s1">rhs.shape</span><span class="s2">, </span><span class="s1">window_strides</span><span class="s2">, </span><span class="s1">pads</span><span class="s2">, </span><span class="s3">0.</span><span class="s1">)</span>
  <span class="s2">return </span><span class="s1">opt_einsum.contract(</span>
      <span class="s1">view</span><span class="s2">, </span><span class="s1">view_axes</span><span class="s2">, </span><span class="s1">rhs</span><span class="s2">, </span><span class="s1">rhs_axes</span><span class="s2">, </span><span class="s1">out_axes</span><span class="s2">, </span><span class="s1">use_blas=</span><span class="s2">True</span><span class="s1">)</span>

<span class="s2">def </span><span class="s1">padtype_to_pads(in_shape</span><span class="s2">, </span><span class="s1">filter_shape</span><span class="s2">, </span><span class="s1">window_strides</span><span class="s2">, </span><span class="s1">padding):</span>
  <span class="s2">if </span><span class="s1">padding.upper() == </span><span class="s4">'SAME' </span><span class="s2">or </span><span class="s1">padding.upper() == </span><span class="s4">'SAME_LOWER'</span><span class="s1">:</span>
    <span class="s1">out_shape = np.ceil(np.true_divide(in_shape</span><span class="s2">, </span><span class="s1">window_strides)).astype(int)</span>
    <span class="s1">pad_sizes = [_max((out_size - </span><span class="s3">1</span><span class="s1">) * stride + filter_size - in_size</span><span class="s2">, </span><span class="s3">0</span><span class="s1">)</span>
                 <span class="s2">for </span><span class="s1">out_size</span><span class="s2">, </span><span class="s1">stride</span><span class="s2">, </span><span class="s1">filter_size</span><span class="s2">, </span><span class="s1">in_size</span>
                 <span class="s2">in </span><span class="s1">zip(out_shape</span><span class="s2">, </span><span class="s1">window_strides</span><span class="s2">, </span><span class="s1">filter_shape</span><span class="s2">, </span><span class="s1">in_shape)]</span>
    <span class="s2">if </span><span class="s1">padding.upper() == </span><span class="s4">'SAME'</span><span class="s1">:</span>
      <span class="s2">return </span><span class="s1">[</span>
          <span class="s1">(pad_size // </span><span class="s3">2</span><span class="s2">, </span><span class="s1">pad_size - pad_size // </span><span class="s3">2</span><span class="s1">) </span><span class="s2">for </span><span class="s1">pad_size </span><span class="s2">in </span><span class="s1">pad_sizes</span>
      <span class="s1">]</span>
    <span class="s2">else</span><span class="s1">:</span>
      <span class="s2">return </span><span class="s1">[</span>
          <span class="s1">(pad_size - pad_size // </span><span class="s3">2</span><span class="s2">, </span><span class="s1">pad_size // </span><span class="s3">2</span><span class="s1">) </span><span class="s2">for </span><span class="s1">pad_size </span><span class="s2">in </span><span class="s1">pad_sizes</span>
      <span class="s1">]</span>
  <span class="s2">else</span><span class="s1">:</span>
    <span class="s2">return </span><span class="s1">[(</span><span class="s3">0</span><span class="s2">, </span><span class="s3">0</span><span class="s1">)] * len(in_shape)</span>

<span class="s2">def </span><span class="s1">_conv_view(lhs</span><span class="s2">, </span><span class="s1">rhs_shape</span><span class="s2">, </span><span class="s1">window_strides</span><span class="s2">, </span><span class="s1">pads</span><span class="s2">, </span><span class="s1">pad_value):</span>
  <span class="s5">&quot;&quot;&quot;Compute the view (and its axes) of a convolution or window reduction.&quot;&quot;&quot;</span>
  <span class="s2">if </span><span class="s1">(_min(lhs.ndim</span><span class="s2">, </span><span class="s1">len(rhs_shape)) &lt; </span><span class="s3">2 </span><span class="s2">or </span><span class="s1">lhs.ndim != len(rhs_shape)</span>
      <span class="s2">or </span><span class="s1">lhs.shape[</span><span class="s3">1</span><span class="s1">] != rhs_shape[</span><span class="s3">1</span><span class="s1">]):</span>
    <span class="s2">raise </span><span class="s1">ValueError(</span><span class="s4">'Dimension mismatch'</span><span class="s1">)</span>
  <span class="s2">if </span><span class="s1">len(window_strides) != len(rhs_shape) - </span><span class="s3">2</span><span class="s1">:</span>
    <span class="s2">raise </span><span class="s1">ValueError(</span><span class="s4">'Wrong number of strides for spatial dimensions'</span><span class="s1">)</span>
  <span class="s2">if </span><span class="s1">len(pads) != len(rhs_shape) - </span><span class="s3">2</span><span class="s1">:</span>
    <span class="s2">raise </span><span class="s1">ValueError(</span><span class="s4">'Wrong number of pads for spatial dimensions'</span><span class="s1">)</span>

  <span class="s1">lhs = _pad(lhs</span><span class="s2">, </span><span class="s1">[(</span><span class="s3">0</span><span class="s2">, </span><span class="s3">0</span><span class="s1">)] * </span><span class="s3">2 </span><span class="s1">+ list(pads)</span><span class="s2">, </span><span class="s1">pad_value)</span>
  <span class="s1">in_shape = lhs.shape[</span><span class="s3">2</span><span class="s1">:]</span>
  <span class="s1">filter_shape = rhs_shape[</span><span class="s3">2</span><span class="s1">:]</span>
  <span class="s1">dim = len(filter_shape)  </span><span class="s0"># number of 'spatial' dimensions in convolution</span>

  <span class="s1">out_strides = np.multiply(window_strides</span><span class="s2">, </span><span class="s1">lhs.strides[</span><span class="s3">2</span><span class="s1">:])</span>
  <span class="s1">view_strides = lhs.strides[:</span><span class="s3">1</span><span class="s1">] + tuple(out_strides) + lhs.strides[</span><span class="s3">1</span><span class="s1">:]</span>

  <span class="s1">out_shape = np.floor_divide(</span>
      <span class="s1">np.subtract(in_shape</span><span class="s2">, </span><span class="s1">filter_shape)</span><span class="s2">, </span><span class="s1">window_strides) + </span><span class="s3">1</span>
  <span class="s1">view_shape = lhs.shape[:</span><span class="s3">1</span><span class="s1">] + tuple(out_shape) + rhs_shape[</span><span class="s3">1</span><span class="s1">:]</span>

  <span class="s1">view = np.lib.stride_tricks.as_strided(lhs</span><span class="s2">, </span><span class="s1">view_shape</span><span class="s2">, </span><span class="s1">view_strides)</span>

  <span class="s1">view_axes = list(range(view.ndim))</span>
  <span class="s1">sum_axes = view_axes[-dim-</span><span class="s3">1</span><span class="s1">:]</span>
  <span class="s1">rhs_axes = [view.ndim] + sum_axes</span>
  <span class="s1">out_axes = [</span><span class="s3">0</span><span class="s2">, </span><span class="s1">view.ndim] + list(range(</span><span class="s3">1</span><span class="s2">, </span><span class="s1">dim+</span><span class="s3">1</span><span class="s1">))</span>

  <span class="s2">return </span><span class="s1">view</span><span class="s2">, </span><span class="s1">view_axes</span><span class="s2">, </span><span class="s1">rhs_axes</span><span class="s2">, </span><span class="s1">out_axes</span>

<span class="s2">def </span><span class="s1">_pad(arr</span><span class="s2">, </span><span class="s1">pads</span><span class="s2">, </span><span class="s1">pad_value):</span>
  <span class="s1">out = np.pad(arr</span><span class="s2">, </span><span class="s1">np.maximum(</span><span class="s3">0</span><span class="s2">, </span><span class="s1">pads)</span><span class="s2">, </span><span class="s1">mode=</span><span class="s4">'constant'</span><span class="s2">,</span>
                <span class="s1">constant_values=pad_value).astype(arr.dtype)</span>
  <span class="s1">slices = tuple(_slice(abs(lo) </span><span class="s2">if </span><span class="s1">lo &lt; </span><span class="s3">0 </span><span class="s2">else </span><span class="s3">0</span><span class="s2">, </span><span class="s1">hi % dim </span><span class="s2">if </span><span class="s1">hi &lt; </span><span class="s3">0 </span><span class="s2">else None</span><span class="s1">)</span>
                 <span class="s2">for </span><span class="s1">(lo</span><span class="s2">, </span><span class="s1">hi)</span><span class="s2">, </span><span class="s1">dim </span><span class="s2">in </span><span class="s1">zip(pads</span><span class="s2">, </span><span class="s1">np.shape(arr)))</span>
  <span class="s2">return </span><span class="s1">out[slices]</span>

<span class="s2">def </span><span class="s1">_dilate(operand</span><span class="s2">, </span><span class="s1">factors</span><span class="s2">, </span><span class="s1">fill_value=</span><span class="s3">0</span><span class="s1">):</span>
  <span class="s0"># this logic is like lax.pad, but with two leading dimensions, no edge</span>
  <span class="s0"># padding, and factors are at least 1 (interior padding is at least 0)</span>
  <span class="s1">outspace = np.add(operand.shape[</span><span class="s3">2</span><span class="s1">:]</span><span class="s2">,</span>
                     <span class="s1">np.multiply(np.subtract(factors</span><span class="s2">, </span><span class="s3">1</span><span class="s1">)</span><span class="s2">,</span>
                                  <span class="s1">np.subtract(operand.shape[</span><span class="s3">2</span><span class="s1">:]</span><span class="s2">, </span><span class="s3">1</span><span class="s1">)))</span>
  <span class="s1">out = np.full(operand.shape[:</span><span class="s3">2</span><span class="s1">] + tuple(outspace)</span><span class="s2">, </span><span class="s1">fill_value</span><span class="s2">, </span><span class="s1">operand.dtype)</span>
  <span class="s1">lhs_slices = tuple(_slice(</span><span class="s2">None, None, </span><span class="s1">step) </span><span class="s2">for </span><span class="s1">step </span><span class="s2">in </span><span class="s1">factors)</span>
  <span class="s1">out[(_slice(</span><span class="s2">None</span><span class="s1">)</span><span class="s2">,</span><span class="s1">) * </span><span class="s3">2 </span><span class="s1">+ lhs_slices] = operand</span>
  <span class="s2">return </span><span class="s1">out</span>

<span class="s2">def </span><span class="s1">_conv_general_permutations(dimension_numbers):</span>
  <span class="s1">lhs_spec</span><span class="s2">, </span><span class="s1">rhs_spec</span><span class="s2">, </span><span class="s1">out_spec = dimension_numbers</span>
  <span class="s1">rhs_perm = ((rhs_spec.index(</span><span class="s4">'O'</span><span class="s1">)</span><span class="s2">, </span><span class="s1">rhs_spec.index(</span><span class="s4">'I'</span><span class="s1">))</span>
              <span class="s1">+ tuple(i </span><span class="s2">for </span><span class="s1">i</span><span class="s2">, </span><span class="s1">c </span><span class="s2">in </span><span class="s1">enumerate(rhs_spec) </span><span class="s2">if </span><span class="s1">c </span><span class="s2">not in </span><span class="s1">{</span><span class="s4">'O'</span><span class="s2">, </span><span class="s4">'I'</span><span class="s1">}))</span>
  <span class="s1">lhs_perm = ((lhs_spec.index(</span><span class="s4">'N'</span><span class="s1">)</span><span class="s2">, </span><span class="s1">lhs_spec.index(</span><span class="s4">'C'</span><span class="s1">))</span>
              <span class="s1">+ tuple(sorted((i </span><span class="s2">for </span><span class="s1">i</span><span class="s2">, </span><span class="s1">c </span><span class="s2">in </span><span class="s1">enumerate(lhs_spec)</span>
                              <span class="s2">if </span><span class="s1">c </span><span class="s2">not in </span><span class="s1">{</span><span class="s4">'N'</span><span class="s2">, </span><span class="s4">'C'</span><span class="s1">})</span><span class="s2">,</span>
                             <span class="s1">key=</span><span class="s2">lambda </span><span class="s1">i: rhs_spec.index(lhs_spec[i]))))</span>
  <span class="s1">out_perm = ((out_spec.index(</span><span class="s4">'N'</span><span class="s1">)</span><span class="s2">, </span><span class="s1">out_spec.index(</span><span class="s4">'C'</span><span class="s1">))</span>
              <span class="s1">+ tuple(sorted((i </span><span class="s2">for </span><span class="s1">i</span><span class="s2">, </span><span class="s1">c </span><span class="s2">in </span><span class="s1">enumerate(out_spec)</span>
                              <span class="s2">if </span><span class="s1">c </span><span class="s2">not in </span><span class="s1">{</span><span class="s4">'N'</span><span class="s2">, </span><span class="s4">'C'</span><span class="s1">})</span><span class="s2">,</span>
                             <span class="s1">key=</span><span class="s2">lambda </span><span class="s1">i: rhs_spec.index(out_spec[i]))))</span>
  <span class="s2">return </span><span class="s1">lhs_perm</span><span class="s2">, </span><span class="s1">rhs_perm</span><span class="s2">, </span><span class="s1">out_perm</span>

<span class="s0">### reduce util</span>

<span class="s2">def </span><span class="s1">_make_reducer(py_binop</span><span class="s2">, </span><span class="s1">init_val):</span>
  <span class="s5">&quot;&quot;&quot;Make a reducer function given a Python binop and an initial value.&quot;&quot;&quot;</span>
  <span class="s0"># It's tempting to use np.ufunc.reduce (even with a ufunc generated by</span>
  <span class="s0"># np.frompyfunc(py_binop)), but this may not agree with custom init_val.</span>
  <span class="s0"># We make an attempt to uncover an underlying numpy ufunc (which might be</span>
  <span class="s0"># wrapped by autograd or lax) and check its identity against init_val.</span>
  <span class="s1">monoid_record = _monoids.get(getattr(py_binop</span><span class="s2">, </span><span class="s4">'__name__'</span><span class="s1">))</span>
  <span class="s2">if </span><span class="s1">monoid_record:</span>
    <span class="s1">reducer</span><span class="s2">, </span><span class="s1">monoid_identity = monoid_record</span>
    <span class="s2">if </span><span class="s1">init_val == monoid_identity(dtypes.result_type(init_val)):</span>
      <span class="s2">return </span><span class="s1">reducer</span>
  <span class="s2">return </span><span class="s1">_reducer_from_pyfunc(py_binop</span><span class="s2">, </span><span class="s1">init_val)</span>

<span class="s2">def </span><span class="s1">_get_max_identity(dt):</span>
  <span class="s2">return </span><span class="s1">-np.inf </span><span class="s2">if </span><span class="s1">dtypes.issubdtype(dt</span><span class="s2">, </span><span class="s1">np.floating) </span><span class="s2">else </span><span class="s1">np.iinfo(dt).min</span>

<span class="s2">def </span><span class="s1">_get_min_identity(dt):</span>
  <span class="s2">return </span><span class="s1">np.inf </span><span class="s2">if </span><span class="s1">dtypes.issubdtype(dt</span><span class="s2">, </span><span class="s1">np.floating) </span><span class="s2">else </span><span class="s1">np.iinfo(dt).max</span>

<span class="s2">def </span><span class="s1">_identity_getter(op):</span>
  <span class="s2">return lambda </span><span class="s1">dtype: np.asarray(op.identity</span><span class="s2">, </span><span class="s1">dtype=dtype)</span>

<span class="s1">MonoidRecord = collections.namedtuple(</span><span class="s4">'MonoidRecord'</span><span class="s2">, </span><span class="s1">[</span><span class="s4">'reducer'</span><span class="s2">, </span><span class="s4">'identity'</span><span class="s1">])</span>
<span class="s1">_monoids = {</span>
    <span class="s4">'max'</span><span class="s1">: MonoidRecord(np.maximum.reduce</span><span class="s2">, </span><span class="s1">_get_max_identity)</span><span class="s2">,</span>
    <span class="s4">'min'</span><span class="s1">: MonoidRecord(np.minimum.reduce</span><span class="s2">, </span><span class="s1">_get_min_identity)</span><span class="s2">,</span>
    <span class="s4">'add'</span><span class="s1">: MonoidRecord(np.add.reduce</span><span class="s2">, </span><span class="s1">_identity_getter(np.add))</span><span class="s2">,</span>
    <span class="s4">'mul'</span><span class="s1">: MonoidRecord(np.multiply.reduce</span><span class="s2">, </span><span class="s1">_identity_getter(np.multiply))</span><span class="s2">,</span>
    <span class="s4">'multiply'</span><span class="s1">: MonoidRecord(np.multiply.reduce</span><span class="s2">,</span>
                             <span class="s1">_identity_getter(np.multiply))</span><span class="s2">,</span>
    <span class="s4">'logical_and'</span><span class="s1">: MonoidRecord(np.logical_and.reduce</span><span class="s2">,</span>
                                <span class="s1">_identity_getter(np.logical_and))</span><span class="s2">,</span>
    <span class="s4">'logical_or'</span><span class="s1">: MonoidRecord(np.logical_or.reduce</span><span class="s2">,</span>
                               <span class="s1">_identity_getter(np.logical_or))</span><span class="s2">,</span>
<span class="s1">}</span>

<span class="s2">def </span><span class="s1">_reducer_from_pyfunc(py_binop</span><span class="s2">, </span><span class="s1">init_val):</span>
  <span class="s2">def </span><span class="s1">reducer(operand</span><span class="s2">, </span><span class="s1">axis=</span><span class="s3">0</span><span class="s1">):</span>
    <span class="s1">axis = range(np.ndim(operand)) </span><span class="s2">if </span><span class="s1">axis </span><span class="s2">is None else </span><span class="s1">axis</span>
    <span class="s1">result = np.full(np.delete(np.shape(operand)</span><span class="s2">, </span><span class="s1">axis)</span><span class="s2">, </span><span class="s1">init_val</span><span class="s2">,</span>
                      <span class="s1">dtype=np.asarray(operand).dtype)</span>
    <span class="s2">for </span><span class="s1">idx</span><span class="s2">, </span><span class="s1">_ </span><span class="s2">in </span><span class="s1">np.ndenumerate(operand):</span>
      <span class="s1">out_idx = tuple(np.delete(idx</span><span class="s2">, </span><span class="s1">axis))</span>
      <span class="s1">result[out_idx] = py_binop(result[out_idx]</span><span class="s2">, </span><span class="s1">operand[idx])</span>
    <span class="s2">return </span><span class="s1">result</span>
  <span class="s2">return </span><span class="s1">reducer</span>
</pre>
</body>
</html>