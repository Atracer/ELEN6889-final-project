<html>
<head>
<title>jax2tf.py</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<style type="text/css">
.s0 { color: #808080;}
.s1 { color: #a9b7c6;}
.s2 { color: #629755; font-style: italic;}
.s3 { color: #cc7832;}
.s4 { color: #6a8759;}
.s5 { color: #6897bb;}
</style>
</head>
<body bgcolor="#2b2b2b">
<table CELLSPACING=0 CELLPADDING=5 COLS=1 WIDTH="100%" BGCOLOR="#606060" >
<tr><td><center>
<font face="Arial, Helvetica" color="#000000">
jax2tf.py</font>
</center></td></tr></table>
<pre><span class="s0"># Copyright 2020 The JAX Authors.</span>
<span class="s0">#</span>
<span class="s0"># Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);</span>
<span class="s0"># you may not use this file except in compliance with the License.</span>
<span class="s0"># You may obtain a copy of the License at</span>
<span class="s0">#</span>
<span class="s0">#     https://www.apache.org/licenses/LICENSE-2.0</span>
<span class="s0">#</span>
<span class="s0"># Unless required by applicable law or agreed to in writing, software</span>
<span class="s0"># distributed under the License is distributed on an &quot;AS IS&quot; BASIS,</span>
<span class="s0"># WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span>
<span class="s0"># See the License for the specific language governing permissions and</span>
<span class="s0"># limitations under the License.</span>
<span class="s2">&quot;&quot;&quot;Provides JAX and TensorFlow interoperation APIs.&quot;&quot;&quot;</span>
<span class="s3">import </span><span class="s1">dataclasses</span>
<span class="s3">from </span><span class="s1">functools </span><span class="s3">import </span><span class="s1">partial</span>
<span class="s3">import </span><span class="s1">contextlib</span>
<span class="s3">import </span><span class="s1">operator</span>
<span class="s3">import </span><span class="s1">os</span>
<span class="s3">import </span><span class="s1">re</span>
<span class="s3">import </span><span class="s1">threading</span>
<span class="s3">from </span><span class="s1">typing </span><span class="s3">import </span><span class="s1">(</span>
    <span class="s1">Any</span><span class="s3">, </span><span class="s1">Callable</span><span class="s3">, </span><span class="s1">Dict</span><span class="s3">, </span><span class="s1">Iterable</span><span class="s3">, </span><span class="s1">List</span><span class="s3">, </span><span class="s1">Optional</span><span class="s3">, </span><span class="s1">Sequence</span><span class="s3">, </span><span class="s1">Tuple</span><span class="s3">, </span><span class="s1">Union</span><span class="s3">,</span>
    <span class="s1">cast)</span>
<span class="s3">import </span><span class="s1">warnings</span>

<span class="s3">from </span><span class="s1">absl </span><span class="s3">import </span><span class="s1">logging</span>
<span class="s3">import </span><span class="s1">numpy </span><span class="s3">as </span><span class="s1">np</span>

<span class="s3">import </span><span class="s1">jax</span>
<span class="s3">from </span><span class="s1">jax </span><span class="s3">import </span><span class="s1">lax</span>
<span class="s3">from </span><span class="s1">jax </span><span class="s3">import </span><span class="s1">config</span>
<span class="s3">from </span><span class="s1">jax </span><span class="s3">import </span><span class="s1">custom_derivatives</span>
<span class="s3">from </span><span class="s1">jax </span><span class="s3">import </span><span class="s1">random</span>
<span class="s3">from </span><span class="s1">jax </span><span class="s3">import </span><span class="s1">numpy </span><span class="s3">as </span><span class="s1">jnp</span>
<span class="s3">from </span><span class="s1">jax </span><span class="s3">import </span><span class="s1">tree_util</span>
<span class="s3">from </span><span class="s1">jax </span><span class="s3">import </span><span class="s1">sharding</span>
<span class="s3">from </span><span class="s1">jax.experimental </span><span class="s3">import </span><span class="s1">maps</span>
<span class="s3">from </span><span class="s1">jax.experimental.jax2tf </span><span class="s3">import </span><span class="s1">shape_poly</span>
<span class="s3">from </span><span class="s1">jax.experimental.jax2tf </span><span class="s3">import </span><span class="s1">impl_no_xla</span>
<span class="s3">from </span><span class="s1">jax.experimental.jax2tf </span><span class="s3">import </span><span class="s1">jax_export</span>
<span class="s3">from </span><span class="s1">jax.interpreters </span><span class="s3">import </span><span class="s1">xla</span>

<span class="s3">from </span><span class="s1">jax._src </span><span class="s3">import </span><span class="s1">ad_checkpoint</span>
<span class="s3">from </span><span class="s1">jax._src </span><span class="s3">import </span><span class="s1">ad_util</span>
<span class="s3">from </span><span class="s1">jax._src </span><span class="s3">import </span><span class="s1">api</span>
<span class="s3">from </span><span class="s1">jax._src </span><span class="s3">import </span><span class="s1">api_util</span>
<span class="s3">from </span><span class="s1">jax._src </span><span class="s3">import </span><span class="s1">core</span>
<span class="s3">from </span><span class="s1">jax._src </span><span class="s3">import </span><span class="s1">dispatch</span>
<span class="s3">from </span><span class="s1">jax._src </span><span class="s3">import </span><span class="s1">dtypes</span>
<span class="s3">from </span><span class="s1">jax._src </span><span class="s3">import </span><span class="s1">linear_util </span><span class="s3">as </span><span class="s1">lu</span>
<span class="s3">from </span><span class="s1">jax._src </span><span class="s3">import </span><span class="s1">pjit</span>
<span class="s3">from </span><span class="s1">jax._src </span><span class="s3">import </span><span class="s1">prng</span>
<span class="s3">from </span><span class="s1">jax._src </span><span class="s3">import </span><span class="s1">random </span><span class="s3">as </span><span class="s1">random_internal</span>
<span class="s3">from </span><span class="s1">jax._src </span><span class="s3">import </span><span class="s1">source_info_util</span>
<span class="s3">from </span><span class="s1">jax._src </span><span class="s3">import </span><span class="s1">util</span>
<span class="s3">from </span><span class="s1">jax._src.interpreters </span><span class="s3">import </span><span class="s1">ad</span>
<span class="s3">from </span><span class="s1">jax._src.interpreters </span><span class="s3">import </span><span class="s1">mlir</span>
<span class="s3">from </span><span class="s1">jax._src.interpreters </span><span class="s3">import </span><span class="s1">pxla</span>
<span class="s3">from </span><span class="s1">jax._src.lax </span><span class="s3">import </span><span class="s1">control_flow </span><span class="s3">as </span><span class="s1">lax_control_flow</span>
<span class="s3">from </span><span class="s1">jax._src.lax </span><span class="s3">import </span><span class="s1">lax </span><span class="s3">as </span><span class="s1">lax_internal</span>
<span class="s3">from </span><span class="s1">jax._src.lax </span><span class="s3">import </span><span class="s1">linalg </span><span class="s3">as </span><span class="s1">lax_linalg</span>
<span class="s3">from </span><span class="s1">jax._src.lax </span><span class="s3">import </span><span class="s1">slicing </span><span class="s3">as </span><span class="s1">lax_slicing</span>
<span class="s3">from </span><span class="s1">jax._src.lax </span><span class="s3">import </span><span class="s1">windowed_reductions </span><span class="s3">as </span><span class="s1">lax_windowed_reductions</span>
<span class="s3">from </span><span class="s1">jax._src.lib </span><span class="s3">import </span><span class="s1">xla_client</span>
<span class="s3">from </span><span class="s1">jax._src.numpy.ufuncs </span><span class="s3">import </span><span class="s1">logaddexp</span>

<span class="s3">import </span><span class="s1">tensorflow </span><span class="s3">as </span><span class="s1">tf  </span><span class="s0"># type: ignore[import]</span>

<span class="s0"># These don't have public equivalents.</span>
<span class="s0"># pylint: disable=g-direct-tensorflow-import</span>
<span class="s3">from </span><span class="s1">tensorflow.compiler.tf2xla.python </span><span class="s3">import </span><span class="s1">xla </span><span class="s3">as </span><span class="s1">tfxla  </span><span class="s0"># type: ignore[import]</span>
<span class="s3">from </span><span class="s1">tensorflow.compiler.xla </span><span class="s3">import </span><span class="s1">xla_data_pb2  </span><span class="s0"># type: ignore[import]</span>
<span class="s3">from </span><span class="s1">tensorflow.core.framework </span><span class="s3">import </span><span class="s1">attr_value_pb2  </span><span class="s0"># type: ignore[import]</span>
<span class="s3">try</span><span class="s1">:</span>
  <span class="s3">from </span><span class="s1">tensorflow.python.compiler.xla.experimental </span><span class="s3">import </span><span class="s1">xla_sharding  </span><span class="s0"># type: ignore[import]</span>
<span class="s3">except </span><span class="s1">ModuleNotFoundError:</span>
  <span class="s0"># This can be removed when TF 2.10 support is no longer needed.</span>
  <span class="s3">from </span><span class="s1">tensorflow.compiler.xla.experimental.xla_sharding </span><span class="s3">import </span><span class="s1">xla_sharding  </span><span class="s0"># type: ignore[import]</span>
<span class="s3">from </span><span class="s1">tensorflow.python.framework </span><span class="s3">import </span><span class="s1">ops </span><span class="s3">as </span><span class="s1">tf_ops  </span><span class="s0"># type: ignore[import]</span>
<span class="s3">from </span><span class="s1">tensorflow.python.eager </span><span class="s3">import </span><span class="s1">context </span><span class="s3">as </span><span class="s1">tf_context  </span><span class="s0"># type: ignore[import]</span>
<span class="s0"># pylint: enable=g-direct-tensorflow-import</span>

<span class="s1">NameStack = source_info_util.NameStack</span>
<span class="s1">PolyShape = shape_poly.PolyShape</span>

<span class="s0"># A temporary internal flag, to enable the wrapping of jax.jit functions</span>
<span class="s0"># with tf.function(jit_compile=True). See #7389. This change has triggered a</span>
<span class="s0"># number of failures in TF. We keep this until we are confident that it does</span>
<span class="s0"># not create problems.</span>
<span class="s0"># TODO(b/207464757): figure out why this change breaks test</span>
<span class="s1">_WRAP_JAX_JIT_WITH_TF_FUNCTION = </span><span class="s3">False</span>

<span class="s0"># The scope name need to be a valid TensorFlow name. See</span>
<span class="s0"># https://github.com/tensorflow/tensorflow/blob/r2.3/tensorflow/core/framework/node_def_util.cc#L731</span>
<span class="s1">_VALID_SCOPE_REGEX = re.compile(</span><span class="s4">&quot;^[A-Za-z0-9.][A-Za-z0-9_.</span><span class="s3">\\</span><span class="s4">/&gt;-]*$&quot;</span><span class="s1">)</span>
<span class="s1">_INVALID_SCOPE_CHAR = re.compile(</span><span class="s4">&quot;[^A-Za-z0-9_.</span><span class="s3">\\</span><span class="s4">/-]&quot;</span><span class="s1">)</span>

<span class="s1">map = util.safe_map</span>
<span class="s1">zip = util.safe_zip</span>


<span class="s3">def </span><span class="s1">_sanitize_scope_name(name):</span>
  <span class="s1">scope_name = _INVALID_SCOPE_CHAR.sub(</span><span class="s4">&quot;_&quot;</span><span class="s3">, </span><span class="s1">name)</span>
  <span class="s3">if not </span><span class="s1">_VALID_SCOPE_REGEX.match(scope_name):</span>
    <span class="s1">scope_name = </span><span class="s4">f&quot;.</span><span class="s3">{</span><span class="s1">scope_name</span><span class="s3">}</span><span class="s4">&quot;</span>
  <span class="s3">return </span><span class="s1">scope_name</span>


<span class="s0"># A value suitable in a TF tracing context: tf.Tensor, tf.Variable,</span>
<span class="s0"># or Python scalar or numpy.ndarray. (A tf.EagerTensor is a tf.Tensor.)</span>
<span class="s1">TfVal = Any</span>
<span class="s1">DType = Any</span>
<span class="s1">PrecisionType = int  </span><span class="s0"># Enum xla_data.PrecisionConfig.Precision</span>

<span class="s3">def </span><span class="s1">_is_tfval(v: TfVal) -&gt; bool:</span>
  <span class="s3">if </span><span class="s1">isinstance(v</span><span class="s3">, </span><span class="s1">(tf.Tensor</span><span class="s3">, </span><span class="s1">tf.Variable)):</span>
    <span class="s3">return True</span>
  <span class="s3">try</span><span class="s1">:</span>
    <span class="s0"># Include all convertible types, even if not supported on accelerators.</span>
    <span class="s3">with </span><span class="s1">tf.device(</span><span class="s4">&quot;CPU&quot;</span><span class="s1">):</span>
      <span class="s1">tf.constant(v)</span>
    <span class="s3">return True</span>
  <span class="s3">except</span><span class="s1">:</span>
    <span class="s3">return False</span>


<span class="s0"># The implementation rules for primitives. The rule will be called with the</span>
<span class="s0"># arguments (TfVal) and must return TfVal (or a sequence thereof,</span>
<span class="s0"># if primitive.multiple_results). The exception are primarily the</span>
<span class="s0"># control-flow primitives.</span>
<span class="s1">tf_impl: Dict[core.Primitive</span><span class="s3">, </span><span class="s1">Callable[...</span><span class="s3">, </span><span class="s1">Any]] = {}</span>

<span class="s0"># Some primitive implementation rules need the abstract values of arguments</span>
<span class="s0"># and the results. This is the case for the primitives implemented using</span>
<span class="s0"># _convert_jax_impl and those that need to adjust the shape of the outputs</span>
<span class="s0"># due to missing TF shape inference rules for TFXLA ops. The rules for these</span>
<span class="s0"># primitives should be added to `tf_impl_with_avals`.</span>
<span class="s0"># The abstract value are passed to the implementation as two special kwargs</span>
<span class="s0"># `_in_avals` (a tuple of core.ShapedArray) and `_out_aval` (a</span>
<span class="s0"># core.ShapedArray, or a tuple thereof when primitive.multiple_results).</span>
<span class="s1">tf_impl_with_avals: Dict[core.Primitive</span><span class="s3">, </span><span class="s1">Callable[...</span><span class="s3">, </span><span class="s1">Any]] = {}</span>

<span class="s0"># XLA is not linked in all environments when converting a primitive. If this is</span>
<span class="s0"># the case, we first search for implementation rules for primitives in the</span>
<span class="s0"># following map. These implementations are workarounds, making use of TF ops</span>
<span class="s0"># that do work when XLA is not linked in.</span>
<span class="s1">tf_impl_no_xla = impl_no_xla.tf_impl_no_xla</span>

<span class="s0"># In order to ensure that JAX picks up the proper user-frame for source</span>
<span class="s0"># locations we will register the TensorFlow source path as an internal</span>
<span class="s0"># path with source_info_util. The typical stack when a JAX primitive</span>
<span class="s0"># conversion happens is:</span>
<span class="s0">#    jax2tf.process_primitive  (top of stack)</span>
<span class="s0">#    jax tracing machinery ...</span>
<span class="s0">#    tf.custom_gradient machinery ...</span>
<span class="s0">#    jax2tf.converted_fun</span>
<span class="s0">#    tf function machinery ...</span>
<span class="s0">#    user code invokes the converted function on TF tensors</span>
<span class="s0">#</span>
<span class="s0"># We need to skip over not only JAX internal frames, but TF internal frames</span>
<span class="s0"># also.</span>
<span class="s0"># We register the TensorFlow source path lazily</span>
<span class="s1">_has_registered_tf_source_path = </span><span class="s3">False</span>

<span class="s3">class </span><span class="s1">_ThreadLocalState(threading.local):</span>
  <span class="s3">def </span><span class="s1">__init__(self):</span>
    <span class="s0"># XLA is not linked in all environments; when converting a primitive, if this</span>
    <span class="s0"># variable is disabled, we try harder to use only standard TF ops if they are</span>
    <span class="s0"># applicable to the concrete use case; if the resulting conversion path ends up</span>
    <span class="s0"># requiring a TFXLA operation, an exception is thrown instead.</span>
    <span class="s1">self.enable_xla = </span><span class="s3">True</span>

    <span class="s0"># Keep track if we are inside a call_tf. In that context we disable the</span>
    <span class="s0"># safety check that we are not inside JAX transformations.</span>
    <span class="s1">self.inside_call_tf = </span><span class="s3">False</span>

    <span class="s0"># Maps dimension variables to TF expressions, for non-native lowering</span>
    <span class="s1">self.shape_env: Sequence[Tuple[str</span><span class="s3">, </span><span class="s1">TfVal]] = ()</span>

    <span class="s0"># Whether to actually include XLA op metadata in the generated TF ops</span>
    <span class="s0"># TODO(b/189306134): implement support for XLA metadata</span>
    <span class="s1">self.include_xla_op_metadata = </span><span class="s3">False</span>

    <span class="s0"># A cache for the tf.convert_to_tensor for constants. We try to preserve</span>
    <span class="s0"># sharing for constants, to enable tf.Graph to take advantage of it.</span>
    <span class="s0"># See https://github.com/google/jax/issues/7992.</span>
    <span class="s1">self.constant_cache = </span><span class="s3">None  </span><span class="s0"># None means that we don't use a cache. We</span>
    <span class="s0"># may be outside a conversion scope.</span>

    <span class="s0"># A cache for the outside tf name_scope when the converted</span>
    <span class="s0"># function is running. We will add this as the prefix to the generated tf op</span>
    <span class="s0"># name. For example, the tf op name will be like</span>
    <span class="s0"># &quot;{tf_outer_name_scope}/JAX_NAME_STACKS&quot;</span>
    <span class="s1">self.tf_outer_name_scope = </span><span class="s4">&quot;&quot;</span>

<span class="s1">_thread_local_state = _ThreadLocalState()</span>

<span class="s3">def </span><span class="s1">_get_current_name_stack() -&gt; Union[NameStack</span><span class="s3">, </span><span class="s1">str]:</span>
  <span class="s3">return </span><span class="s1">source_info_util.current_name_stack()</span>

<span class="s1">@contextlib.contextmanager</span>
<span class="s3">def </span><span class="s1">inside_call_tf():</span>
  <span class="s0"># Set the inside_call_tf flag for a context.</span>
  <span class="s1">prev = _thread_local_state.inside_call_tf</span>
  <span class="s1">_thread_local_state.inside_call_tf = </span><span class="s3">True</span>
  <span class="s3">try</span><span class="s1">:</span>
    <span class="s3">yield</span>
  <span class="s3">finally</span><span class="s1">:</span>
    <span class="s1">_thread_local_state.inside_call_tf = prev</span>


<span class="s1">@partial(api_util.api_hook</span><span class="s3">, </span><span class="s1">tag=</span><span class="s4">&quot;jax2tf_convert&quot;</span><span class="s1">)</span>
<span class="s3">def </span><span class="s1">convert(fun_jax: Callable</span><span class="s3">,</span>
            <span class="s1">*</span><span class="s3">,</span>
            <span class="s1">polymorphic_shapes=</span><span class="s3">None,</span>
            <span class="s1">with_gradient=</span><span class="s3">True,</span>
            <span class="s1">enable_xla=</span><span class="s3">True,</span>
            <span class="s0"># TODO(necula): remove the experimental flag</span>
            <span class="s1">experimental_native_lowering=</span><span class="s4">&quot;default&quot;</span><span class="s3">,</span>
            <span class="s1">native_serialization=</span><span class="s4">&quot;default&quot;</span><span class="s3">,</span>
            <span class="s1">native_serialization_platforms=()</span><span class="s3">,</span>
            <span class="s1">native_serialization_strict_checks=</span><span class="s3">True</span><span class="s1">) -&gt; Callable:</span>
  <span class="s2">&quot;&quot;&quot;Allows calling a JAX function from a TensorFlow program. 
 
  See 
  [README](https://github.com/google/jax/blob/main/jax/experimental/jax2tf/README.md) 
  for more details about usage and common problems. 
 
  Args: 
    fun_jax: target JAX function to be called. Its arguments and return value 
      should be JAX arrays, or nested standard Python containers 
      (tuple/list/dict) thereof (pytrees). 
    polymorphic_shapes: Specifies input shapes to be treated polymorphically 
      during lowering. 
 
      .. warning:: The shape-polymorphic lowering is an experimental feature. 
        It is meant to be sound, but it is known to reject some JAX programs 
        that are shape polymorphic. The details of this feature can change. 
 
      It should be `None` (all arguments are monomorphic), a single PolyShape 
      or string (applies to all arguments), or a tuple/list of the same length 
      as the function arguments. For each argument the shape specification 
      should be `None` (monomorphic argument), or a Python object with the 
      same pytree structure as the argument. 
      See [how optional parameters are matched to 
      arguments](https://jax.readthedocs.io/en/latest/pytrees.html#applying-optional-parameters-to-pytrees). 
 
      A shape specification for an array argument should be an object 
      `PolyShape(dim0, dim1, ..., dimn)` 
      where each `dim` is a dimension specification: a positive integer denoting 
      a monomorphic dimension of the given size, or a string denoting a 
      dimension variable assumed to range over non-zero dimension sizes, or 
      the special placeholder string &quot;_&quot; denoting a monomorphic dimension 
      whose size is given by the actual argument. As a shortcut, an Ellipsis 
      suffix in the list of dimension specifications stands for a list of &quot;_&quot; 
      placeholders. 
 
      For convenience, a shape specification can also be given as a string 
      representation, e.g.: &quot;batch, ...&quot;, &quot;batch, height, width, _&quot;, possibly 
      with surrounding parentheses: &quot;(batch, ...)&quot;. 
 
      The lowering fails if it cannot ensure that the it would produce the same 
      sequence of TF ops for any non-zero values of the dimension variables. 
 
      polymorphic_shapes are only supported for positional arguments; shape 
      polymorphism is not supported for keyword arguments. 
 
      See [the README](https://github.com/google/jax/blob/main/jax/experimental/jax2tf/README.md#shape-polymorphic-conversion) 
      for more details. 
 
    with_gradient: if set (default), add a tf.custom_gradient to the lowered 
      function, by converting the ``jax.vjp(fun)``. This means that reverse-mode 
      TensorFlow AD is supported for the output TensorFlow function, and the 
      value of the gradient will be JAX-accurate. 
    enable_xla: if set (default), use the simplest conversion 
      and use XLA TF ops when necessary. These ops are known to create issues 
      for the TFLite and TFjs converters. For those cases, unset this parameter 
      so the lowering tries harder to use non-XLA TF ops to lower the 
      function and aborts if this is not possible. Cannot be set to `False` 
      when using `native_serialization`. 
    native_serialization: serialize the JAX function natively to 
      StableHLO with compatibility guarantees. This makes it easier to have 
      confidence that the code executed when calling this function from 
      TensorFlow is exactly the same as JAX would run natively. 
      The &quot;default&quot; value defers to `False` if `enable_xla` 
      is set to `False` or to the configuration flag 
      `--jax2tf_default_native_serialization` otherwise. 
      Native serialization cannot be used with `enable_xla=False`. 
    native_serialization_platforms: In conjunction with 
      `native_serialization`, specify the platform(s) 
      for which to lower the code. Must be a tuple of 
      strings, including a subset of: 'cpu', 'cuda', 'rocm', 'tpu'. 
      The default (empty tuple), specifies the JAX default 
      backend on the machine where the lowering is done. 
    native_serialization_strict_checks: In conjunction with 
      `native_serialization`, enable the following 
      checks: (A) the lowered computation is executed on a platform for which it 
      was lowered; (B) the serialized computation contains only custom calls 
      with targets that are guaranteed to be stable, (more to come). 
 
  Returns: 
    A version of `fun_jax` that expects TfVals as arguments (or 
    tuple/lists/dicts thereof), and returns TfVals as outputs, and uses 
    only TensorFlow ops and thus can be called from a TensorFlow program. 
  &quot;&quot;&quot;</span>
  <span class="s3">if </span><span class="s1">native_serialization == </span><span class="s4">&quot;default&quot;</span><span class="s1">:</span>
    <span class="s3">if not </span><span class="s1">enable_xla:</span>
      <span class="s1">native_serialization = </span><span class="s3">False</span>
    <span class="s3">else</span><span class="s1">:</span>
      <span class="s1">native_serialization = config.jax2tf_default_native_serialization</span>

  <span class="s3">if </span><span class="s1">native_serialization </span><span class="s3">and not </span><span class="s1">enable_xla:</span>
    <span class="s3">raise </span><span class="s1">ValueError(</span>
        <span class="s4">&quot;native_serialization is not supported with enable_xla=False&quot;</span><span class="s1">)</span>

  <span class="s3">if </span><span class="s1">native_serialization_platforms:</span>
    <span class="s3">if not </span><span class="s1">native_serialization:</span>
      <span class="s1">warnings.warn(</span>
          <span class="s4">&quot;using native_serialization_platforms without native_serialization. &quot;</span>
          <span class="s4">&quot;The parameter will have no effect, since the same code is serialized &quot;</span>
          <span class="s4">&quot;for all platforms without native_serialization.&quot;</span><span class="s1">)</span>

    <span class="s3">if </span><span class="s1">(</span><span class="s3">not </span><span class="s1">isinstance(native_serialization_platforms</span><span class="s3">, </span><span class="s1">(list</span><span class="s3">, </span><span class="s1">tuple)) </span><span class="s3">or</span>
        <span class="s3">not </span><span class="s1">all(p </span><span class="s3">in </span><span class="s1">[</span><span class="s4">&quot;tpu&quot;</span><span class="s3">, </span><span class="s4">&quot;cpu&quot;</span><span class="s3">, </span><span class="s4">&quot;gpu&quot;</span><span class="s1">] </span><span class="s3">for </span><span class="s1">p </span><span class="s3">in </span><span class="s1">native_serialization_platforms)):</span>
      <span class="s3">raise </span><span class="s1">ValueError(</span>
          <span class="s4">&quot;native_serialization_platforms must be a sequence &quot;</span>
          <span class="s4">&quot;containing a subset of {'cpu', 'gpu', 'tpu'}. &quot;</span>
          <span class="s4">f&quot;Got: </span><span class="s3">{</span><span class="s1">native_serialization_platforms</span><span class="s3">}</span><span class="s4">&quot;</span><span class="s1">)</span>
    <span class="s1">native_serialization_platforms = tuple(native_serialization_platforms)</span>
    <span class="s3">if </span><span class="s1">len(native_serialization_platforms) &gt; </span><span class="s5">1</span><span class="s1">:</span>
      <span class="s3">raise </span><span class="s1">NotImplementedError(</span>
          <span class="s4">&quot;native_serialization_platforms is not yet implemented for multiple platforms&quot;</span><span class="s1">)</span>

  <span class="s1">api.check_callable(fun_jax)</span>
  <span class="s1">fun_name = getattr(fun_jax</span><span class="s3">, </span><span class="s4">&quot;__name__&quot;</span><span class="s3">, </span><span class="s4">&quot;unknown&quot;</span><span class="s1">)</span>
  <span class="s1">name_stack = util.wrap_name(fun_name</span><span class="s3">, </span><span class="s4">&quot;jax2tf&quot;</span><span class="s1">)</span>
  <span class="s3">def </span><span class="s1">converted_fun_tf(*args_tf: TfVal</span><span class="s3">, </span><span class="s1">**kwargs_tf: TfVal) -&gt; TfVal:</span>

    <span class="s3">try</span><span class="s1">:</span>
      <span class="s1">prev_enable_xla = _thread_local_state.enable_xla</span>
      <span class="s1">prev_include_xla_op_metadata = _thread_local_state.include_xla_op_metadata</span>
      <span class="s1">prev_tf_outer_name_scope = _thread_local_state.tf_outer_name_scope</span>

      <span class="s1">_thread_local_state.tf_outer_name_scope = tf.get_current_name_scope()</span>

      <span class="s0"># TODO: is there a better way to check if we are inside a transformation?</span>
      <span class="s3">if not </span><span class="s1">core.trace_state_clean() </span><span class="s3">and not </span><span class="s1">_thread_local_state.inside_call_tf:</span>
        <span class="s0"># It is Ok to nest convert when we are inside a call_tf</span>
        <span class="s3">raise </span><span class="s1">ValueError(</span>
            <span class="s4">&quot;convert must be used outside all JAX transformations.&quot; </span><span class="s1">+</span>
            <span class="s4">f&quot;Trace state: </span><span class="s3">{</span><span class="s1">core.thread_local_state.trace_state.trace_stack</span><span class="s3">}</span><span class="s4">&quot;</span><span class="s1">)</span>

      <span class="s1">fun_flat_jax</span><span class="s3">, </span><span class="s1">args_flat_tf</span><span class="s3">, </span><span class="s1">in_tree</span><span class="s3">, </span><span class="s1">out_tree_thunk = flatten_fun_jax(</span>
          <span class="s1">fun_jax</span><span class="s3">, </span><span class="s1">args_tf</span><span class="s3">, </span><span class="s1">kwargs_tf)</span>
      <span class="s0"># out_tree_thunk will be ready after we call fun_flat_jax below.</span>

      <span class="s0"># Expand the polymorphic_shapes to match the args_flat_tf. The polymorphic_shapes</span>
      <span class="s0"># argument refers to positional arguments only.</span>
      <span class="s3">if </span><span class="s1">polymorphic_shapes </span><span class="s3">is None or </span><span class="s1">isinstance(polymorphic_shapes</span><span class="s3">,</span>
                                                  <span class="s1">(PolyShape</span><span class="s3">, </span><span class="s1">str)):</span>
        <span class="s1">polymorphic_shapes_ = (polymorphic_shapes</span><span class="s3">,</span><span class="s1">) * len(args_tf)</span>
      <span class="s3">else</span><span class="s1">:</span>
        <span class="s3">if not </span><span class="s1">(isinstance(polymorphic_shapes</span><span class="s3">, </span><span class="s1">Sequence) </span><span class="s3">and</span>
                <span class="s1">len(polymorphic_shapes) == len(args_tf)):</span>
          <span class="s1">msg = (</span>
              <span class="s4">&quot;polymorphic_shapes must be a sequence with the same length as &quot;</span>
              <span class="s4">&quot;the positional argument list &quot;</span>
              <span class="s4">f&quot;(</span><span class="s3">{</span><span class="s1">len(args_tf)</span><span class="s3">}</span><span class="s4">). Got polymorphic_shapes=</span><span class="s3">{</span><span class="s1">repr(polymorphic_shapes)</span><span class="s3">}</span><span class="s4">.&quot;</span>
          <span class="s1">)</span>
          <span class="s3">raise </span><span class="s1">TypeError(msg)</span>
        <span class="s1">polymorphic_shapes_ = tuple(polymorphic_shapes)</span>

      <span class="s1">polymorphic_shapes_flat = tuple(</span>
          <span class="s1">api_util.flatten_axes(</span>
              <span class="s4">&quot;jax2tf.convert polymorphic_shapes&quot;</span><span class="s3">, </span><span class="s1">in_tree</span><span class="s3">,</span>
              <span class="s1">(polymorphic_shapes_</span><span class="s3">, </span><span class="s1">{k: </span><span class="s3">None for </span><span class="s1">k </span><span class="s3">in </span><span class="s1">kwargs_tf.keys()})))</span>

      <span class="s1">args_and_avals = tuple(</span>
          <span class="s1">map(preprocess_arg_tf</span><span class="s3">, </span><span class="s1">range(len(args_flat_tf))</span><span class="s3">, </span><span class="s1">args_flat_tf</span><span class="s3">,</span>
              <span class="s1">polymorphic_shapes_flat))</span>
      <span class="s1">args_flat_tf</span><span class="s3">, </span><span class="s1">args_avals_flat = util.unzip2(args_and_avals)</span>

      <span class="s3">if </span><span class="s1">native_serialization:</span>
        <span class="s1">shape_env = ()</span>
        <span class="s3">if </span><span class="s1">native_serialization_platforms:</span>
          <span class="s1">lowering_platform = native_serialization_platforms[</span><span class="s5">0</span><span class="s1">]</span>
        <span class="s3">else</span><span class="s1">:</span>
          <span class="s1">lowering_platform = </span><span class="s3">None</span>
        <span class="s1">exported: Optional[jax_export.Exported] = jax_export.serialize_native(</span>
            <span class="s1">fun_flat_jax</span><span class="s3">, </span><span class="s1">args_avals_flat</span><span class="s3">,</span>
            <span class="s1">lowering_platform=lowering_platform</span><span class="s3">,</span>
            <span class="s1">strict_checks=native_serialization_strict_checks)</span>
        <span class="s3">def </span><span class="s1">run_fun_flat_as_tf(</span>
            <span class="s1">args_flat_tf: Sequence[TfVal]</span>
        <span class="s1">) -&gt; Tuple[Tuple[TfVal</span><span class="s3">, </span><span class="s1">...]</span><span class="s3">, </span><span class="s1">Tuple[core.ShapedArray</span><span class="s3">, </span><span class="s1">...]]:</span>
          <span class="s1">outs_tf</span><span class="s3">, </span><span class="s1">out_avals = run_exported_as_tf(</span>
              <span class="s1">args_avals_flat</span><span class="s3">, </span><span class="s1">args_flat_tf</span><span class="s3">, </span><span class="s1">exported</span><span class="s3">,  </span><span class="s0"># type: ignore</span>
              <span class="s1">native_serialization_strict_checks)</span>
          <span class="s3">return </span><span class="s1">outs_tf</span><span class="s3">, </span><span class="s1">out_avals</span>
      <span class="s3">else</span><span class="s1">:</span>
        <span class="s1">dim_vars</span><span class="s3">, </span><span class="s1">get_dim_values_jax = shape_poly.prepare_dim_var_env(</span>
            <span class="s1">args_avals_flat)</span>
        <span class="s1">dim_values</span><span class="s3">, </span><span class="s1">_ = _interpret_fun_jax(get_dim_values_jax</span><span class="s3">, </span><span class="s1">args_flat_tf</span><span class="s3">,</span>
                                           <span class="s1">args_avals_flat</span><span class="s3">, </span><span class="s1">name_stack)</span>
        <span class="s1">shape_env = zip(dim_vars</span><span class="s3">, </span><span class="s1">dim_values)  </span><span class="s0"># type: ignore</span>
        <span class="s1">exported = </span><span class="s3">None</span>
        <span class="s3">def </span><span class="s1">run_fun_flat_as_tf(</span>
            <span class="s1">args_flat_tf: Sequence[TfVal]</span>
        <span class="s1">) -&gt; Tuple[Tuple[TfVal</span><span class="s3">, </span><span class="s1">...]</span><span class="s3">, </span><span class="s1">Tuple[core.ShapedArray</span><span class="s3">, </span><span class="s1">...]]:</span>
          <span class="s1">outs_tf</span><span class="s3">, </span><span class="s1">out_avals = _interpret_fun_jax(</span>
              <span class="s1">fun_flat_jax</span><span class="s3">,</span>
              <span class="s1">args_flat_tf</span><span class="s3">, </span><span class="s1">args_avals_flat</span><span class="s3">,</span>
              <span class="s1">name_stack</span><span class="s3">,</span>
              <span class="s1">fresh_constant_cache=</span><span class="s3">True</span><span class="s1">)</span>
          <span class="s3">return </span><span class="s1">outs_tf</span><span class="s3">, </span><span class="s1">out_avals</span>

      <span class="s3">assert not </span><span class="s1">_thread_local_state.shape_env</span><span class="s3">, </span><span class="s4">f&quot;Unexpected shape environment </span><span class="s3">{</span><span class="s1">_thread_local_state.shape_env</span><span class="s3">}</span><span class="s4">&quot;</span>

      <span class="s1">_thread_local_state.enable_xla = enable_xla</span>

      <span class="s0"># TODO(b/189306134): implement support for XLA metadata</span>
      <span class="s1">_thread_local_state.include_xla_op_metadata = </span><span class="s3">False</span>

      <span class="s1">_thread_local_state.shape_env = shape_env</span>
      <span class="s3">global </span><span class="s1">_has_registered_tf_source_path</span>
      <span class="s3">if not </span><span class="s1">_has_registered_tf_source_path:</span>
        <span class="s1">source_info_util.register_exclusion(os.path.dirname(tf.__file__))</span>
        <span class="s1">_has_registered_tf_source_path = </span><span class="s3">True</span>

      <span class="s3">if </span><span class="s1">with_gradient:</span>

        <span class="s1">@tf.custom_gradient</span>
        <span class="s3">def </span><span class="s1">converted_fun_flat_with_custom_gradient_tf(*args_flat_tf: TfVal) -&gt; TfVal:</span>
          <span class="s1">outs_tf</span><span class="s3">, </span><span class="s1">out_avals = run_fun_flat_as_tf(args_flat_tf)</span>
          <span class="s3">return </span><span class="s1">(tuple(outs_tf)</span><span class="s3">,</span>
                  <span class="s1">_make_custom_gradient_fn_tf(</span>
                      <span class="s1">fun_flat_jax=fun_flat_jax</span><span class="s3">,</span>
                      <span class="s1">args_flat_tf=args_flat_tf</span><span class="s3">,</span>
                      <span class="s1">args_avals_flat=args_avals_flat</span><span class="s3">,</span>
                      <span class="s1">polymorphic_shapes_flat=polymorphic_shapes_flat</span><span class="s3">,</span>
                      <span class="s1">out_avals=out_avals</span><span class="s3">,</span>
                      <span class="s1">native_serialization=native_serialization</span><span class="s3">,</span>
                      <span class="s1">native_serialization_platforms=native_serialization_platforms</span><span class="s3">,</span>
                      <span class="s1">native_serialization_strict_checks=native_serialization_strict_checks</span><span class="s3">,</span>
                      <span class="s1">exported_primal=exported))</span>

        <span class="s1">out_flat_tf = converted_fun_flat_with_custom_gradient_tf(*args_flat_tf)</span>
      <span class="s3">else</span><span class="s1">:</span>
        <span class="s1">outs_tf</span><span class="s3">, </span><span class="s1">out_avals = run_fun_flat_as_tf(args_flat_tf)</span>
        <span class="s1">message = (</span><span class="s4">&quot;The jax2tf-converted function does not support gradients. &quot;</span>
                   <span class="s4">&quot;Use `with_gradient` parameter to enable gradients&quot;</span><span class="s1">)</span>
        <span class="s0"># We use PreventGradient, which is propagated through a SavedModel.</span>
        <span class="s1">out_flat_tf = [</span>
            <span class="s1">tf.raw_ops.PreventGradient(input=o</span><span class="s3">, </span><span class="s1">message=message)</span>
            <span class="s3">for </span><span class="s1">o </span><span class="s3">in </span><span class="s1">outs_tf</span>
        <span class="s1">]</span>
    <span class="s3">finally</span><span class="s1">:</span>
      <span class="s1">_thread_local_state.shape_env = ()</span>
      <span class="s1">_thread_local_state.enable_xla = prev_enable_xla</span>
      <span class="s1">_thread_local_state.include_xla_op_metadata = prev_include_xla_op_metadata</span>
      <span class="s1">_thread_local_state.tf_outer_name_scope = prev_tf_outer_name_scope</span>

    <span class="s1">out_flat_tf = [tf.identity(x</span><span class="s3">, </span><span class="s4">&quot;jax2tf_out&quot;</span><span class="s1">) </span><span class="s3">for </span><span class="s1">x </span><span class="s3">in </span><span class="s1">out_flat_tf]</span>
    <span class="s1">out_tf = tree_util.tree_unflatten(out_tree_thunk()</span><span class="s3">, </span><span class="s1">out_flat_tf)</span>
    <span class="s3">return </span><span class="s1">out_tf</span>

  <span class="s3">return </span><span class="s1">converted_fun_tf</span>


<span class="s3">def </span><span class="s1">dtype_of_val(val: TfVal) -&gt; DType:</span>
  <span class="s2">&quot;&quot;&quot;Computes the TensorFlow dtype using JAX's typing rules. 
 
  If the value is a tf.Tensor, it starts with its dtype. If the value is a 
  constant it uses JAX to infer its dtype. The resulting dtype follows the 
  JAX type inference rules, and depends on the value of the 
  JAX_ENABLE_X64 flag. 
 
  See README.md for how 64-bit values are treated. 
  &quot;&quot;&quot;</span>
  <span class="s1">tval</span><span class="s3">, </span><span class="s1">_ = _tfval_to_tensor_jax_dtype(val)</span>
  <span class="s3">return </span><span class="s1">tval.dtype</span>

<span class="s0"># Internals</span>

<span class="s3">def </span><span class="s1">flatten_fun_jax(fun_jax: Callable</span><span class="s3">, </span><span class="s1">args_tf: Sequence[TfVal]</span><span class="s3">,</span>
                    <span class="s1">kwargs_tf: Dict[str</span><span class="s3">, </span><span class="s1">TfVal]</span>
                    <span class="s1">) -&gt; Tuple[Callable</span><span class="s3">, </span><span class="s1">Sequence[TfVal]</span><span class="s3">, </span><span class="s1">Any</span><span class="s3">, </span><span class="s1">Callable]:</span>
  <span class="s2">&quot;&quot;&quot;Wraps the function to take a (flat) list of positional args. 
 
  jax2tf works better and is simpler when the JAX function takes and returns 
  just a tuple of values (no pytrees, no kwargs). This is in part because 
  jax.vjp does not support kwargs and we can only set 
  tf.custom_gradient on functions with flat arguments and results 
 
  Returns: 
     * the wrapped JAX function taking and returning a flat list of arguments 
     * the flat list of TF arguments 
     * the in_tree corresponding to the tuple (args_tf, kwargs_tf) 
     * a thunk that can be called after the wrapped function has been called 
       to return the output pytree. 
  &quot;&quot;&quot;</span>
  <span class="s0"># TODO(necula): technically we should use TF's flattening and unflattening</span>
  <span class="s0"># because we are working with TF values.</span>
  <span class="s1">args_flat_tf</span><span class="s3">, </span><span class="s1">in_tree = tree_util.tree_flatten((args_tf</span><span class="s3">, </span><span class="s1">kwargs_tf))</span>

  <span class="s1">out_tree_ref = </span><span class="s3">None</span>
  <span class="s3">def </span><span class="s1">fun_flat_jax(*args_flat_jax):</span>
    <span class="s1">tree_args</span><span class="s3">, </span><span class="s1">tree_kwargs = tree_util.tree_unflatten(in_tree</span><span class="s3">, </span><span class="s1">args_flat_jax)</span>
    <span class="s1">tree_res = fun_jax(*tree_args</span><span class="s3">, </span><span class="s1">**tree_kwargs)</span>
    <span class="s1">res_flat_jax</span><span class="s3">, </span><span class="s1">out_tree = tree_util.tree_flatten(tree_res)</span>
    <span class="s3">nonlocal </span><span class="s1">out_tree_ref</span>
    <span class="s3">assert </span><span class="s1">out_tree_ref </span><span class="s3">is None or </span><span class="s1">out_tree_ref == out_tree</span>
    <span class="s1">out_tree_ref = out_tree</span>
    <span class="s3">return </span><span class="s1">res_flat_jax</span>

  <span class="s3">if </span><span class="s1">hasattr(fun_jax</span><span class="s3">, </span><span class="s4">&quot;lower&quot;</span><span class="s1">):</span>
    <span class="s0"># If the fun_jax is already a jit(f) or pjit(f), we must</span>
    <span class="s0"># preserve the lowering function. This will be used in the _lower_native_and_run.</span>
    <span class="s0"># We rely on the fact that the lowering is the same for the function</span>
    <span class="s0"># taking pytrees, and the one taking flat args.</span>
    <span class="s3">def </span><span class="s1">fun_flat_jax_lower(*args_flat_jax</span><span class="s3">, </span><span class="s1">_experimental_lowering_platform):</span>
      <span class="s1">tree_args</span><span class="s3">, </span><span class="s1">tree_kwargs = tree_util.tree_unflatten(in_tree</span><span class="s3">, </span><span class="s1">args_flat_jax)</span>
      <span class="s1">lowered = fun_jax.lower(</span>
          <span class="s1">*tree_args</span><span class="s3">,</span>
          <span class="s1">_experimental_lowering_platform=_experimental_lowering_platform</span><span class="s3">,</span>
          <span class="s1">**tree_kwargs)</span>
      <span class="s1">out_tree = lowered.out_tree</span>
      <span class="s3">nonlocal </span><span class="s1">out_tree_ref</span>
      <span class="s3">assert </span><span class="s1">out_tree_ref </span><span class="s3">is None or </span><span class="s1">out_tree_ref == out_tree</span>
      <span class="s1">out_tree_ref = out_tree</span>
      <span class="s3">return </span><span class="s1">lowered</span>
    <span class="s1">setattr(fun_flat_jax</span><span class="s3">, </span><span class="s4">&quot;lower&quot;</span><span class="s3">, </span><span class="s1">fun_flat_jax_lower)</span>

  <span class="s3">return </span><span class="s1">fun_flat_jax</span><span class="s3">, </span><span class="s1">args_flat_tf</span><span class="s3">, </span><span class="s1">in_tree</span><span class="s3">, lambda</span><span class="s1">: out_tree_ref</span>

<span class="s3">def </span><span class="s1">preprocess_arg_tf(arg_idx: int</span><span class="s3">,</span>
                      <span class="s1">arg_tf: TfVal</span><span class="s3">,</span>
                      <span class="s1">polymorphic_shape: Optional[str]</span>
                      <span class="s1">) -&gt; Tuple[TfVal</span><span class="s3">, </span><span class="s1">core.ShapedArray]:</span>
  <span class="s3">if not </span><span class="s1">_is_tfval(arg_tf):</span>
    <span class="s1">msg = (</span><span class="s4">f&quot;Argument </span><span class="s3">{</span><span class="s1">arg_tf</span><span class="s3">} </span><span class="s4">of type </span><span class="s3">{</span><span class="s1">type(arg_tf)</span><span class="s3">} </span><span class="s4">of jax2tf.convert(f) should &quot;</span>
           <span class="s4">&quot;be NumPy array, scalar, tf.Variable, or tf.Tensor&quot;</span><span class="s1">)</span>
    <span class="s3">raise </span><span class="s1">TypeError(msg)</span>

  <span class="s0"># May cast the args_flat to JAX types, using JAX's interpretation</span>
  <span class="s0"># of types of constants.</span>
  <span class="s1">arg_tf</span><span class="s3">, </span><span class="s1">arg_jax_dtype = _tfval_to_tensor_jax_dtype(arg_tf)</span>
  <span class="s0"># Name input tensors; do this after we have cast the arguments</span>
  <span class="s1">arg_tf = tf.identity(arg_tf</span><span class="s3">, </span><span class="s4">f&quot;jax2tf_arg_</span><span class="s3">{</span><span class="s1">arg_idx</span><span class="s3">}</span><span class="s4">&quot;</span><span class="s1">)</span>

  <span class="s0"># Fix the shape for TF1</span>
  <span class="s1">tf_arg_shape = np.shape(arg_tf)</span>
  <span class="s1">arg_shape = tuple(d.value </span><span class="s3">if </span><span class="s1">isinstance(d</span><span class="s3">, </span><span class="s1">tf.compat.v1.Dimension) </span><span class="s3">else </span><span class="s1">d </span><span class="s3">for </span><span class="s1">d </span><span class="s3">in </span><span class="s1">tf_arg_shape)</span>

  <span class="s1">arg_aval = shape_poly.arg_aval(arg_shape</span><span class="s3">, </span><span class="s1">arg_jax_dtype</span><span class="s3">, </span><span class="s1">polymorphic_shape)</span>
  <span class="s3">return </span><span class="s1">arg_tf</span><span class="s3">, </span><span class="s1">arg_aval</span>


<span class="s3">def </span><span class="s1">_make_custom_gradient_fn_tf(*</span><span class="s3">,</span>
                                <span class="s1">fun_flat_jax: Callable</span><span class="s3">,</span>
                                <span class="s1">args_flat_tf: Sequence[TfVal]</span><span class="s3">,</span>
                                <span class="s1">polymorphic_shapes_flat: Sequence[str]</span><span class="s3">,</span>
                                <span class="s1">args_avals_flat: Sequence[core.ShapedArray]</span><span class="s3">,</span>
                                <span class="s1">out_avals: Sequence[core.ShapedArray]</span><span class="s3">,</span>
                                <span class="s1">native_serialization: Union[str</span><span class="s3">, </span><span class="s1">bool]</span><span class="s3">,</span>
                                <span class="s1">native_serialization_platforms: Sequence[str]</span><span class="s3">,</span>
                                <span class="s1">native_serialization_strict_checks: bool</span><span class="s3">,</span>
                                <span class="s1">exported_primal: Optional[jax_export.Exported]):</span>
  <span class="s2">&quot;&quot;&quot;Prepares the TF function to be used with tf.custom_gradient. 
 
  &quot;&quot;&quot;</span>

  <span class="s3">def </span><span class="s1">grad_fn_tf(*out_cts_flat_tf: TfVal</span><span class="s3">,</span>
                 <span class="s1">variables=</span><span class="s3">None</span><span class="s1">):</span>
    <span class="s3">if </span><span class="s1">variables:</span>
      <span class="s3">raise </span><span class="s1">ValueError(</span>
          <span class="s4">&quot;Unexpected variables used in forward pass. &quot;</span>
          <span class="s4">&quot;This should not happen for first-order differentiation. &quot;</span>
          <span class="s4">f&quot;</span><span class="s3">{</span><span class="s1">variables=</span><span class="s3">}</span><span class="s4">&quot;</span><span class="s1">)</span>

    <span class="s1">out_cts_flat_polymorphic_shapes = tuple(str(out_aval.shape)  </span><span class="s0"># Note: may be _DimExpr, not just DimVar</span>
                                            <span class="s3">for </span><span class="s1">out_aval </span><span class="s3">in </span><span class="s1">out_avals)  </span><span class="s0"># type: ignore</span>
    <span class="s1">vjp_polymorphic_shapes = [</span>
        <span class="s1">polymorphic_shapes_flat</span><span class="s3">, </span><span class="s1">out_cts_flat_polymorphic_shapes</span>
    <span class="s1">]</span>

    <span class="s3">def </span><span class="s1">fun_vjp_jax(args_flat_jax</span><span class="s3">, </span><span class="s1">out_cts_flat_jax):</span>
      <span class="s0"># One may think that we can get the pullback while we are converting</span>
      <span class="s0"># the main function in the first place. That is problematic, because the</span>
      <span class="s0"># pullback may contain captured tracers from the conversion of the</span>
      <span class="s0"># main function. Those tracers will confuse the conversion of the</span>
      <span class="s0"># pullback. So, we construct the vjp anew and we convert it separately.</span>
      <span class="s1">_</span><span class="s3">, </span><span class="s1">pullback_jax = jax.vjp(fun_flat_jax</span><span class="s3">, </span><span class="s1">*args_flat_jax)</span>

      <span class="s3">def </span><span class="s1">fix_out_ct(out_ct_jax</span><span class="s3">, </span><span class="s1">out_ct_aval: core.ShapedArray):</span>
        <span class="s0"># If the primal function has outputs of integer or bool types, and if we are</span>
        <span class="s0"># under a tf.function context, then TF will pass None in _out_cts_flat</span>
        <span class="s0"># in place of these values. We should change these to float0 or</span>
        <span class="s0"># else JAX gets unhappy. See issue #6975.</span>
        <span class="s3">if </span><span class="s1">out_ct_jax </span><span class="s3">is not None</span><span class="s1">:</span>
          <span class="s3">return </span><span class="s1">out_ct_jax</span>
        <span class="s3">assert </span><span class="s1">core.primal_dtype_to_tangent_dtype(out_ct_aval.dtype) == dtypes.float0</span><span class="s3">, </span><span class="s4">f&quot;</span><span class="s3">{</span><span class="s1">out_ct_jax=</span><span class="s3">}</span><span class="s4">&quot;</span>
        <span class="s0"># Note that out_ct_aval.shape contains dimension variable from the</span>
        <span class="s0"># primal function scope. It is Ok to use them here because we</span>
        <span class="s0"># use the same shape variables for the VJP function.</span>
        <span class="s3">return </span><span class="s1">jnp.zeros(out_ct_aval.shape</span><span class="s3">, </span><span class="s1">dtype=_tf_np_dtype_for_float0)</span>

      <span class="s1">out_cts_fixed_flat = list(map(fix_out_ct</span><span class="s3">, </span><span class="s1">out_cts_flat_jax</span><span class="s3">, </span><span class="s1">out_avals))</span>
      <span class="s1">in_cts_flat_jax = pullback_jax(out_cts_fixed_flat)</span>

      <span class="s3">def </span><span class="s1">fix_in_ct(in_ct_jax</span><span class="s3">, </span><span class="s1">arg_aval: core.ShapedArray):</span>
        <span class="s3">if </span><span class="s1">jnp.issubdtype(arg_aval.dtype</span><span class="s3">, </span><span class="s1">jnp.inexact):</span>
          <span class="s3">return </span><span class="s1">in_ct_jax</span>
        <span class="s3">else</span><span class="s1">:</span>
          <span class="s3">assert </span><span class="s1">in_ct_jax.dtype == dtypes.float0</span>
          <span class="s3">return </span><span class="s1">jnp.zeros(arg_aval.shape</span><span class="s3">, </span><span class="s1">_tf_np_dtype_for_float0)</span>

      <span class="s1">in_cts_fixed_flat_jax = tuple(map(fix_in_ct</span><span class="s3">, </span><span class="s1">in_cts_flat_jax</span><span class="s3">, </span><span class="s1">args_avals_flat))</span>
      <span class="s3">return </span><span class="s1">in_cts_fixed_flat_jax</span>

    <span class="s3">if </span><span class="s1">exported_primal </span><span class="s3">is not None</span><span class="s1">:</span>
      <span class="s0"># Native lowering</span>
      <span class="s1">all_in_shardings = [pxla._UNSPECIFIED] * len(exported_primal.in_avals)</span>
      <span class="s3">for </span><span class="s1">idx</span><span class="s3">, </span><span class="s1">in_s </span><span class="s3">in </span><span class="s1">zip(sorted(exported_primal.module_kept_var_idx)</span><span class="s3">,</span>
                           <span class="s1">exported_primal.in_shardings):</span>
        <span class="s1">all_in_shardings[idx] = in_s  </span><span class="s0"># type: ignore</span>
      <span class="s1">all_shardings = all_in_shardings + list(exported_primal.out_shardings)</span>
      <span class="s0"># We cannot mix unspecified and specified shardings. Make the unspecified</span>
      <span class="s0"># ones replicated</span>
      <span class="s1">specified_shardings = [</span>
          <span class="s1">s </span><span class="s3">for </span><span class="s1">s </span><span class="s3">in </span><span class="s1">all_shardings </span><span class="s3">if not </span><span class="s1">pxla._is_unspecified(s)]</span>
      <span class="s3">if </span><span class="s5">0 </span><span class="s1">&lt; len(specified_shardings) &lt; len(all_shardings):</span>
        <span class="s0"># There are some specified, but not all</span>
        <span class="s1">in_s = specified_shardings[</span><span class="s5">0</span><span class="s1">]  </span><span class="s0"># pjit will enforce that all have same devices</span>
        <span class="s3">assert </span><span class="s1">isinstance(in_s</span><span class="s3">, </span><span class="s1">sharding.XLACompatibleSharding)</span>
        <span class="s1">replicated_s = sharding.GSPMDSharding.get_replicated(in_s._device_assignment)</span>
        <span class="s1">all_shardings = [</span>
            <span class="s1">s </span><span class="s3">if not </span><span class="s1">pxla._is_unspecified(s) </span><span class="s3">else </span><span class="s1">replicated_s</span>
            <span class="s3">for </span><span class="s1">s </span><span class="s3">in </span><span class="s1">all_shardings]</span>
      <span class="s0"># Since fun_vjp_jax takes two tuples of arguments we must split the in_shardings</span>
      <span class="s1">vjp_in_args_shardings: Any</span>
      <span class="s1">vjp_in_out_ct_shardings: Any</span>
      <span class="s1">vjp_in_shardings: Any</span>
      <span class="s1">vjp_in_args_shardings</span><span class="s3">, </span><span class="s1">vjp_in_out_ct_shardings = util.split_list(all_shardings</span><span class="s3">,</span>
                                                                       <span class="s1">[len(exported_primal.in_avals)])</span>
      <span class="s0"># pjit front-end does not like all-unspecified</span>
      <span class="s3">if </span><span class="s1">all(pxla._is_unspecified(s) </span><span class="s3">for </span><span class="s1">s </span><span class="s3">in </span><span class="s1">vjp_in_args_shardings):</span>
        <span class="s1">vjp_in_args_shardings = pxla._UNSPECIFIED</span>
      <span class="s3">else</span><span class="s1">:</span>
        <span class="s1">vjp_in_args_shardings = tuple(vjp_in_args_shardings)</span>
      <span class="s3">if </span><span class="s1">all(pxla._is_unspecified(s) </span><span class="s3">for </span><span class="s1">s </span><span class="s3">in </span><span class="s1">vjp_in_out_ct_shardings):</span>
        <span class="s1">vjp_in_out_ct_shardings = pxla._UNSPECIFIED</span>
      <span class="s3">else</span><span class="s1">:</span>
        <span class="s1">vjp_in_out_ct_shardings = tuple(vjp_in_out_ct_shardings)</span>

      <span class="s3">if </span><span class="s1">pxla._is_unspecified(vjp_in_args_shardings) </span><span class="s3">and </span><span class="s1">pxla._is_unspecified(vjp_in_args_shardings):</span>
        <span class="s1">vjp_in_shardings = pxla._UNSPECIFIED</span>
      <span class="s3">else</span><span class="s1">:</span>
        <span class="s1">vjp_in_shardings = (vjp_in_args_shardings</span><span class="s3">, </span><span class="s1">vjp_in_out_ct_shardings)</span>
      <span class="s1">fun_vjp_jax = pjit.pjit(fun_vjp_jax</span><span class="s3">,</span>
                              <span class="s1">in_shardings=vjp_in_shardings</span><span class="s3">,</span>
                              <span class="s1">out_shardings=vjp_in_args_shardings)</span>
    <span class="s0"># TODO: enable higher-order gradients</span>
    <span class="s3">with </span><span class="s1">tf.name_scope(</span><span class="s4">&quot;jax2tf_vjp&quot;</span><span class="s1">):</span>
      <span class="s1">in_cts_flat = convert(</span>
          <span class="s1">fun_vjp_jax</span><span class="s3">,</span>
          <span class="s1">with_gradient=</span><span class="s3">False,</span>
          <span class="s1">polymorphic_shapes=vjp_polymorphic_shapes</span><span class="s3">,</span>
          <span class="s1">native_serialization=native_serialization</span><span class="s3">,</span>
          <span class="s1">native_serialization_platforms=native_serialization_platforms</span><span class="s3">,</span>
          <span class="s1">native_serialization_strict_checks=native_serialization_strict_checks</span>
      <span class="s1">)(args_flat_tf</span><span class="s3">, </span><span class="s1">out_cts_flat_tf)</span>
    <span class="s3">return </span><span class="s1">in_cts_flat</span>

  <span class="s3">return </span><span class="s1">grad_fn_tf</span>

<span class="s1">@contextlib.contextmanager</span>
<span class="s3">def </span><span class="s1">_extended_name_stack(extra_name_stack: Optional[str]):</span>
  <span class="s1">name_ctx = (source_info_util.extend_name_stack(extra_name_stack)</span>
      <span class="s3">if </span><span class="s1">extra_name_stack</span>
      <span class="s3">else </span><span class="s1">contextlib.nullcontext())</span>
  <span class="s3">with </span><span class="s1">name_ctx:</span>
    <span class="s3">yield</span>
  <span class="s3">return</span>


<span class="s3">def </span><span class="s1">_interpret_fun_jax(</span>
    <span class="s1">fun_jax: Callable</span><span class="s3">,</span>
    <span class="s1">args_tf: Sequence[TfVal]</span><span class="s3">,</span>
    <span class="s1">args_avals: Sequence[core.ShapedArray]</span><span class="s3">,</span>
    <span class="s1">extra_name_stack: Optional[str]</span><span class="s3">,</span>
    <span class="s1">fresh_constant_cache: bool = </span><span class="s3">False,</span>
<span class="s1">) -&gt; Tuple[Tuple[TfVal</span><span class="s3">, </span><span class="s1">...]</span><span class="s3">, </span><span class="s1">Tuple[core.ShapedArray</span><span class="s3">, </span><span class="s1">...]]:</span>
  <span class="s3">with </span><span class="s1">core.new_base_main(TensorFlowTrace) </span><span class="s3">as </span><span class="s1">main:  </span><span class="s0"># type: ignore</span>
    <span class="s1">subtrace_fun = _interpret_subtrace(lu.wrap_init(fun_jax)</span><span class="s3">, </span><span class="s1">main</span><span class="s3">, </span><span class="s1">args_avals)</span>
    <span class="s3">with </span><span class="s1">_extended_name_stack(extra_name_stack):</span>
      <span class="s3">with </span><span class="s1">core.new_sublevel():</span>
        <span class="s1">out_vals: Sequence[Tuple[TfVal</span><span class="s3">, </span><span class="s1">core.ShapedArray]] = \</span>
            <span class="s1">_call_wrapped_with_new_constant_cache(subtrace_fun</span><span class="s3">, </span><span class="s1">args_tf</span><span class="s3">,</span>
                                                  <span class="s1">fresh_constant_cache=fresh_constant_cache)</span>

      <span class="s3">del </span><span class="s1">main</span>

  <span class="s3">return </span><span class="s1">util.unzip2(out_vals)</span>


<span class="s3">def </span><span class="s1">run_exported_as_tf(args_avals: Sequence[core.ShapedArray]</span><span class="s3">,</span>
                       <span class="s1">args_tf: Sequence[TfVal]</span><span class="s3">,</span>
                       <span class="s1">exported: jax_export.Exported</span><span class="s3">,</span>
                       <span class="s1">native_serialization_strict_checks: bool</span><span class="s3">,</span>
                       <span class="s1">) -&gt; Tuple[Tuple[TfVal</span><span class="s3">, </span><span class="s1">...]</span><span class="s3">, </span><span class="s1">Tuple[core.ShapedArray</span><span class="s3">, </span><span class="s1">...]]:</span>
  <span class="s2">&quot;&quot;&quot;Runs the `exported` as an XlaCallModule TF op.&quot;&quot;&quot;</span>
  <span class="s1">out_shapes = tuple(</span>
      <span class="s1">tuple(d </span><span class="s3">if </span><span class="s1">type(d) </span><span class="s3">is </span><span class="s1">int </span><span class="s3">else None</span>
            <span class="s3">for </span><span class="s1">d </span><span class="s3">in </span><span class="s1">out_aval.shape)</span>
      <span class="s3">for </span><span class="s1">out_aval </span><span class="s3">in </span><span class="s1">exported.out_avals)</span>

  <span class="s3">def </span><span class="s1">_out_type(jax_type):</span>
    <span class="s3">if </span><span class="s1">jax_type == dtypes.float0:</span>
      <span class="s3">return </span><span class="s1">dtypes.bool_</span>
    <span class="s3">return </span><span class="s1">jax_type</span>
  <span class="s1">out_types = tuple(_out_type(out_aval.dtype) </span><span class="s3">for </span><span class="s1">out_aval </span><span class="s3">in </span><span class="s1">exported.out_avals)</span>

  <span class="s1">args_avals = [aval </span><span class="s3">for </span><span class="s1">i</span><span class="s3">, </span><span class="s1">aval </span><span class="s3">in </span><span class="s1">enumerate(args_avals) </span><span class="s3">if </span><span class="s1">i </span><span class="s3">in </span><span class="s1">exported.module_kept_var_idx]</span>
  <span class="s1">args_tf = [atf </span><span class="s3">for </span><span class="s1">i</span><span class="s3">, </span><span class="s1">atf </span><span class="s3">in </span><span class="s1">enumerate(args_tf) </span><span class="s3">if </span><span class="s1">i </span><span class="s3">in </span><span class="s1">exported.module_kept_var_idx]</span>

  <span class="s1">call_module_attrs = dict(</span>
      <span class="s1">version=exported.xla_call_module_version</span><span class="s3">,</span>
      <span class="s1">Tout=out_types</span><span class="s3">,</span>
      <span class="s1">Sout=out_shapes)</span>

  <span class="s3">if </span><span class="s1">exported.xla_call_module_version &gt;= </span><span class="s5">3</span><span class="s1">:</span>
    <span class="s3">if </span><span class="s1">native_serialization_strict_checks:</span>
      <span class="s1">call_module_attrs[</span><span class="s4">&quot;platforms&quot;</span><span class="s1">] = (exported.lowering_platform.upper()</span><span class="s3">,</span><span class="s1">)</span>
    <span class="s3">else</span><span class="s1">:</span>
      <span class="s1">call_module_attrs[</span><span class="s4">&quot;platforms&quot;</span><span class="s1">] = ()  </span><span class="s0"># No platform checking</span>

  <span class="s3">if </span><span class="s1">logging.vlog_is_on(</span><span class="s5">3</span><span class="s1">):</span>
    <span class="s0"># We already logged the MLIR module when we exported it.</span>
    <span class="s1">logging.vlog(</span><span class="s5">3</span><span class="s3">, </span><span class="s4">&quot;XlaCallModule %s&quot;</span><span class="s3">, </span><span class="s1">str(call_module_attrs))</span>

  <span class="s1">call_module_attrs[</span><span class="s4">&quot;module&quot;</span><span class="s1">] = exported.mlir_module_serialized</span>

  <span class="s0"># Apply the shardings on arguments and results for pjit. This is redundant</span>
  <span class="s0"># because the mlir_module_text will already contain the shardings, but it</span>
  <span class="s0"># makes it easier for tools like the TPU inference converter to see the</span>
  <span class="s0"># sharding without digging into the `module` attribute of the `XlaCallModule`</span>
  <span class="s0"># op, in the same way as it is done for the legacy jax2tf conversion.</span>
  <span class="s0"># Do not apply XlaSharding for REPLICATED, on inputs and outputs.</span>
  <span class="s0"># This is an agreed convention, and also improves usability under TF eager.</span>
  <span class="s0"># See b/255511660.</span>
  <span class="s3">if </span><span class="s1">exported.in_shardings </span><span class="s3">is not None</span><span class="s1">:</span>
    <span class="s1">args_tf = tuple(</span>
      <span class="s1">map(partial(_shard_value</span><span class="s3">, </span><span class="s1">skip_replicated_sharding=tf.executing_eagerly())</span><span class="s3">,</span>
          <span class="s1">args_tf</span><span class="s3">, </span><span class="s1">args_avals</span><span class="s3">, </span><span class="s1">exported.in_shardings))</span>
  <span class="s1">res = tfxla.call_module(args_tf</span><span class="s3">, </span><span class="s1">**call_module_attrs)</span>
  <span class="s3">if </span><span class="s1">exported.out_shardings </span><span class="s3">is not None</span><span class="s1">:</span>
    <span class="s1">res = list(map(partial(_shard_value</span><span class="s3">, </span><span class="s1">skip_replicated_sharding=tf.executing_eagerly())</span><span class="s3">,</span>
                   <span class="s1">res</span><span class="s3">, </span><span class="s1">exported.out_avals</span><span class="s3">, </span><span class="s1">exported.out_shardings))</span>

  <span class="s0"># Convert the results to the needed TF types</span>
  <span class="s3">def </span><span class="s1">_convert_res(res_val</span><span class="s3">, </span><span class="s1">res_jax_type):</span>
    <span class="s1">conversion_dtype = _to_tf_dtype(res_jax_type)</span>
    <span class="s3">if </span><span class="s1">conversion_dtype != res_jax_type:</span>
      <span class="s3">return </span><span class="s1">tf.cast(res_val</span><span class="s3">, </span><span class="s1">conversion_dtype)</span>
    <span class="s3">else</span><span class="s1">:</span>
      <span class="s3">return </span><span class="s1">res_val</span>

  <span class="s1">res = tuple(</span>
      <span class="s1">_convert_res(res_val</span><span class="s3">, </span><span class="s1">out_aval.dtype)</span>
      <span class="s3">for </span><span class="s1">res_val</span><span class="s3">, </span><span class="s1">out_aval </span><span class="s3">in </span><span class="s1">zip(res</span><span class="s3">, </span><span class="s1">exported.out_avals))</span>
  <span class="s3">return </span><span class="s1">res</span><span class="s3">, </span><span class="s1">tuple(exported.out_avals)</span>


<span class="s3">def </span><span class="s1">_call_wrapped_with_new_constant_cache(fun: lu.WrappedFun</span><span class="s3">,</span>
                                          <span class="s1">in_vals: Sequence[TfVal]</span><span class="s3">,</span>
                                          <span class="s1">fresh_constant_cache: bool = </span><span class="s3">False</span>
                                          <span class="s1">) -&gt; Sequence[Tuple[TfVal</span><span class="s3">, </span><span class="s1">core.ShapedArray]]:</span>
  <span class="s3">try</span><span class="s1">:</span>
    <span class="s1">prev_constant_cache = _thread_local_state.constant_cache</span>
    <span class="s0"># Start a new cache, so that we don't share constants across tf.function</span>
    <span class="s0"># boundaries.</span>
    <span class="s3">if </span><span class="s1">fresh_constant_cache:</span>
      <span class="s1">_thread_local_state.constant_cache = {}</span>
    <span class="s3">else</span><span class="s1">:</span>
      <span class="s1">prev_constant_cache_keys = set(prev_constant_cache.keys()) </span><span class="s3">if </span><span class="s1">prev_constant_cache </span><span class="s3">is not None else </span><span class="s1">set()</span>
    <span class="s1">out_vals: Sequence[Tuple[TfVal</span><span class="s3">, </span><span class="s1">core.ShapedArray]] = \</span>
        <span class="s1">fun.call_wrapped(*in_vals)</span>
  <span class="s3">finally</span><span class="s1">:</span>
    <span class="s3">if </span><span class="s1">(</span><span class="s3">not </span><span class="s1">fresh_constant_cache </span><span class="s3">and</span>
        <span class="s1">prev_constant_cache </span><span class="s3">is not None and</span>
        <span class="s1">_WRAP_JAX_JIT_WITH_TF_FUNCTION):</span>
      <span class="s1">newly_added_keys = set(prev_constant_cache.keys()) - prev_constant_cache_keys</span>
      <span class="s0"># Delete the newly added keys</span>
      <span class="s3">for </span><span class="s1">k </span><span class="s3">in </span><span class="s1">newly_added_keys:</span>
        <span class="s3">del </span><span class="s1">prev_constant_cache[k]</span>
    <span class="s1">_thread_local_state.constant_cache = prev_constant_cache</span>
  <span class="s3">return </span><span class="s1">out_vals</span>

<span class="s3">def </span><span class="s1">_convert_jax_impl(impl_jax: Callable</span><span class="s3">, </span><span class="s1">*</span><span class="s3">,</span>
                      <span class="s1">multiple_results=</span><span class="s3">True,</span>
                      <span class="s1">with_physical_avals=</span><span class="s3">False,</span>
                      <span class="s1">extra_name_stack: Optional[str] = </span><span class="s3">None</span><span class="s1">) -&gt; Callable:</span>
  <span class="s2">&quot;&quot;&quot;Convert the JAX implementation of a primitive. 
 
  Args: 
    impl_jax: typically the impl-rule for a primitive, with signature 
      `(*args_jax: JaxVal, **kwargs) -&gt; Sequence[JaxVal]`. This function implements 
        a primitive in terms of other primitives. 
    multiple_results: whether `impl_jax` returns a sequence of results. 
    extra_name_stack: additional element to add to the name stack for the 
      converted ops. 
 
  Returns: 
     a function with signature `(*args_tf: TfVal, _in_avals, _out_aval, **kwargs) 
     -&gt; Sequence[TfVal]`. 
  &quot;&quot;&quot;</span>

  <span class="s3">def </span><span class="s1">wrapped_tf(*args_tf: TfVal</span><span class="s3">, </span><span class="s1">_in_avals: Sequence[core.ShapedArray]</span><span class="s3">,</span>
                 <span class="s1">_out_aval: core.ShapedArray</span><span class="s3">,</span>
                 <span class="s1">**kwargs) -&gt; Sequence[TfVal]:</span>

    <span class="s3">if </span><span class="s1">with_physical_avals:</span>
      <span class="s1">_in_avals = map(_jax_physical_aval</span><span class="s3">, </span><span class="s1">_in_avals)</span>
      <span class="s1">_out_aval = _jax_physical_aval(_out_aval)</span>

    <span class="s0"># We wrap the impl_jax to always return a tuple of results.</span>
    <span class="s3">def </span><span class="s1">impl_multiple_results_jax(*args_jax):</span>
      <span class="s1">results_jax = impl_jax(*args_jax</span><span class="s3">, </span><span class="s1">**kwargs)</span>
      <span class="s3">return </span><span class="s1">results_jax </span><span class="s3">if </span><span class="s1">multiple_results </span><span class="s3">else </span><span class="s1">[results_jax]</span>

    <span class="s1">results_tf</span><span class="s3">, </span><span class="s1">_ = _interpret_fun_jax(</span>
        <span class="s1">impl_multiple_results_jax</span><span class="s3">, </span><span class="s1">args_tf</span><span class="s3">, </span><span class="s1">_in_avals</span><span class="s3">,</span>
        <span class="s1">extra_name_stack)</span>
    <span class="s3">return </span><span class="s1">results_tf </span><span class="s3">if </span><span class="s1">multiple_results </span><span class="s3">else </span><span class="s1">results_tf[</span><span class="s5">0</span><span class="s1">]</span>

  <span class="s3">return </span><span class="s1">wrapped_tf</span>


<span class="s1">@lu.transformation</span>
<span class="s3">def </span><span class="s1">_interpret_subtrace(main: core.MainTrace</span><span class="s3">,</span>
                        <span class="s1">in_avals: Sequence[core.ShapedArray]</span><span class="s3">,</span>
                        <span class="s1">*in_vals: TfVal):</span>
  <span class="s1">trace = TensorFlowTrace(main</span><span class="s3">, </span><span class="s1">core.cur_sublevel())</span>
  <span class="s1">in_tracers = tuple(</span>
      <span class="s1">TensorFlowTracer(trace</span><span class="s3">, </span><span class="s1">val</span><span class="s3">, </span><span class="s1">aval)</span>
      <span class="s3">for </span><span class="s1">val</span><span class="s3">, </span><span class="s1">aval </span><span class="s3">in </span><span class="s1">zip(in_vals</span><span class="s3">, </span><span class="s1">in_avals))</span>
  <span class="s1">outs = </span><span class="s3">yield </span><span class="s1">in_tracers</span><span class="s3">, </span><span class="s1">{}  </span><span class="s0"># type: Sequence[TfVal]</span>
  <span class="s1">out_tracers: Iterable[TensorFlowTracer] = (</span>
      <span class="s1">map(trace.full_raise</span><span class="s3">, </span><span class="s1">outs))  </span><span class="s0"># type: ignore</span>
  <span class="s1">out_vals_with_avals: Sequence[Tuple[TfVal</span><span class="s3">, </span><span class="s1">core.ShapedArray]] = (</span>
      <span class="s1">tuple((t.val</span><span class="s3">, </span><span class="s1">t.aval) </span><span class="s3">for </span><span class="s1">t </span><span class="s3">in </span><span class="s1">out_tracers))</span>
  <span class="s3">yield </span><span class="s1">out_vals_with_avals</span>


<span class="s3">def </span><span class="s1">_interpret_jaxpr(jaxpr: core.ClosedJaxpr</span><span class="s3">, </span><span class="s1">*args_tf: TfVal</span><span class="s3">,</span>
                     <span class="s1">extra_name_stack: Optional[str]</span><span class="s3">,</span>
                     <span class="s1">fresh_constant_cache: bool = </span><span class="s3">True</span><span class="s1">) -&gt; Sequence[TfVal]:</span>
  <span class="s2">&quot;&quot;&quot;Evaluates a Jaxpr with tf.Tensor arguments. 
 
  This is most often used as the body of a tf.function, or tf.switch_case, 
  in which case it should use a fresh constant cache. 
  The output is a sequence of TfVal, suitable for use with TF. 
  &quot;&quot;&quot;</span>
  <span class="s1">outs_tf</span><span class="s3">, </span><span class="s1">_ = _interpret_fun_jax(core.jaxpr_as_fun(jaxpr)</span><span class="s3">,</span>
                                  <span class="s1">args_tf</span><span class="s3">, </span><span class="s1">jaxpr.in_avals</span><span class="s3">, </span><span class="s1">extra_name_stack</span><span class="s3">,</span>
                                  <span class="s1">fresh_constant_cache=fresh_constant_cache)</span>
  <span class="s3">return </span><span class="s1">outs_tf</span>


<span class="s3">def </span><span class="s1">_jax_physical_aval(aval: core.ShapedArray) -&gt; core.ShapedArray:</span>
  <span class="s2">&quot;&quot;&quot;Converts JAX avals from logical to physical, if relevant. 
 
  JAX might have avals whose logical vs physical shape/dtype may 
  differ, and only the physical view is expected to possibly 
  relate to TF. TF impl rules should operate on the physical form. 
 
  A JAX logical aval might even correspond, in principle, to several 
  physical avals, but we don't support those here. Instead we assert 
  there is only one and return it. 
  &quot;&quot;&quot;</span>
  <span class="s3">if </span><span class="s1">core.is_opaque_dtype(aval.dtype):</span>
    <span class="s1">physical_aval</span><span class="s3">, </span><span class="s1">= aval.dtype._rules.physical_avals(aval)</span>
    <span class="s3">assert </span><span class="s1">(len(physical_aval.shape) &gt;= len(aval.shape) </span><span class="s3">and</span>
            <span class="s1">physical_aval.shape[:len(aval.shape)] == aval.shape)</span><span class="s3">, </span><span class="s1">(physical_aval</span><span class="s3">, </span><span class="s1">aval)</span>
    <span class="s3">return </span><span class="s1">physical_aval</span>
  <span class="s3">return </span><span class="s1">aval</span>

<span class="s3">def </span><span class="s1">_jax_physical_dtype(dtype):</span>
  <span class="s0"># assuming () is a fine stand-in shape</span>
  <span class="s3">return </span><span class="s1">_jax_physical_aval(core.ShapedArray(()</span><span class="s3">, </span><span class="s1">dtype)).dtype</span>


<span class="s3">def </span><span class="s1">_aval_to_tf_shape(aval: core.ShapedArray) -&gt; Tuple[Optional[int]</span><span class="s3">, </span><span class="s1">...]:</span>

  <span class="s2">&quot;&quot;&quot;Generate a TF shape, possibly containing None for polymorphic dimensions.&quot;&quot;&quot;</span>
  <span class="s1">aval = _jax_physical_aval(aval)</span>
  <span class="s3">return </span><span class="s1">tuple(map(</span><span class="s3">lambda </span><span class="s1">d: </span><span class="s3">None if </span><span class="s1">shape_poly.is_poly_dim(d) </span><span class="s3">else </span><span class="s1">d</span><span class="s3">,</span>
                   <span class="s1">aval.shape))  </span><span class="s0"># type: ignore[attr-defined]</span>

<span class="s0"># In the TF world, we represent float0 as zeros of this type.</span>
<span class="s1">_tf_np_dtype_for_float0 = np.int32</span>

<span class="s3">def </span><span class="s1">_to_tf_dtype(jax_dtype):</span>
  <span class="s0"># Note that converting _to_tf_dtype and _to_jax_dtype are not inverses,</span>
  <span class="s0"># due to float0 and 64-bit behavior.</span>
  <span class="s3">try</span><span class="s1">:</span>
    <span class="s1">jax_dtype = _jax_physical_dtype(jax_dtype)</span>
  <span class="s3">except </span><span class="s1">TypeError:</span>
    <span class="s0"># `jax_dtype` isn't actually a valid jax dtype (e.g. it is</span>
    <span class="s0"># tf.float32), so there is no physical dtype anyway</span>
    <span class="s3">pass</span>
  <span class="s3">if </span><span class="s1">jax_dtype == dtypes.float0:</span>
    <span class="s1">jax_dtype = _tf_np_dtype_for_float0</span>
  <span class="s3">return </span><span class="s1">tf.dtypes.as_dtype(jax_dtype)</span>


<span class="s3">def </span><span class="s1">_to_jax_dtype(tf_dtype):</span>
  <span class="s0"># Note that converting _to_tf_dtype and _to_jax_dtype are not inverses,</span>
  <span class="s0"># due to float0 and 64-bit behavior.</span>
  <span class="s1">dt = dtypes.canonicalize_dtype(tf_dtype.as_numpy_dtype)</span>
  <span class="s3">if </span><span class="s1">dt </span><span class="s3">not in </span><span class="s1">dtypes._jax_dtype_set:</span>
    <span class="s3">raise </span><span class="s1">TypeError(</span><span class="s4">f&quot;dtype </span><span class="s3">{</span><span class="s1">dt</span><span class="s3">} </span><span class="s4">is not a valid JAX array &quot;</span>
                    <span class="s4">&quot;type. Only arrays of numeric types are supported by JAX.&quot;</span><span class="s1">)</span>
  <span class="s3">return </span><span class="s1">dt</span>


<span class="s3">def </span><span class="s1">_tfval_to_tensor_jax_dtype(val: TfVal</span><span class="s3">,</span>
                               <span class="s1">jax_dtype: Optional[DType] = </span><span class="s3">None,</span>
                               <span class="s1">memoize_constants=</span><span class="s3">False</span><span class="s1">) -&gt; Tuple[TfVal</span><span class="s3">, </span><span class="s1">DType]:</span>
  <span class="s2">&quot;&quot;&quot;Converts a scalar, ndarray, or tf.Tensor to a tf.Tensor with proper type. 
 
  If `jax_dtype` is missing, uses JAX typing rules. 
  See README.md for details regarding 64-bit values. 
 
  Args: 
    val: a scalar, ndarray, tf.Tensor, or tf.Variable 
    jax_dtype: an optional dtype to use. If missing, uses JAX type inference 
      rules for constants. 
    memoize_constants: whether to memoize TF constants. We can't do this 
      everywhere, we may be outside of a conversion scope. 
 
  Returns: 
    a tuple with a tf.Tensor with the type as needed by JAX, and the JAX type. 
  &quot;&quot;&quot;</span>
  <span class="s3">if </span><span class="s1">isinstance(val</span><span class="s3">, </span><span class="s1">(tf.Tensor</span><span class="s3">, </span><span class="s1">tf.Variable)):</span>
    <span class="s1">jax_dtype = jax_dtype </span><span class="s3">or </span><span class="s1">_to_jax_dtype(val.dtype)  </span><span class="s0"># Give JAX a chance to pick the type</span>
    <span class="s1">conversion_dtype = _to_tf_dtype(jax_dtype)</span>
    <span class="s3">if </span><span class="s1">conversion_dtype != val.dtype:  </span><span class="s0"># May need to cast for 64-bit values</span>
      <span class="s3">return </span><span class="s1">tf.cast(val</span><span class="s3">, </span><span class="s1">conversion_dtype)</span><span class="s3">, </span><span class="s1">jax_dtype</span>
    <span class="s3">else</span><span class="s1">:</span>
      <span class="s3">return </span><span class="s1">val</span><span class="s3">, </span><span class="s1">jax_dtype</span>
  <span class="s3">else</span><span class="s1">:  </span><span class="s0"># A constant</span>
    <span class="s1">jax_dtype = jax_dtype </span><span class="s3">or </span><span class="s1">xla.abstractify(val).dtype</span>
    <span class="s0"># TODO(document): We assume that the value of a constant does not</span>
    <span class="s0"># change through the scope of the function. But it may be an ndarray, ...</span>
    <span class="s0"># JAX has the same problem when generating HLO.</span>
    <span class="s1">const_key = (id(val)</span><span class="s3">, </span><span class="s1">jax_dtype)</span>
    <span class="s0"># Since we use id(val) as a cache key, we have to make sure that we keep</span>
    <span class="s0"># the previous `val` alive. Otherwise, for a ndarray, it can get garbage</span>
    <span class="s0"># collected and reused for a different value, which would create correctness</span>
    <span class="s0"># issues. We keep the `val` alive by storing in the cache the pair</span>
    <span class="s0"># `(val, tf_val)`.</span>
    <span class="s0"># Only memoize non-scalars. JAX will lift all non-scalar constants as</span>
    <span class="s0"># Jaxpr consts, to the top level of the Jaxpr. This ensures that we see them</span>
    <span class="s0"># early, when entering the Jaxpr, so we create the tf.const early and its</span>
    <span class="s0"># scope is the entire Jaxpr.</span>
    <span class="s1">do_memoize = (memoize_constants </span><span class="s3">and </span><span class="s1">np.size(val) &gt; </span><span class="s5">1 </span><span class="s3">and </span><span class="s1">_thread_local_state.constant_cache </span><span class="s3">is not None</span><span class="s1">)</span>
    <span class="s3">if </span><span class="s1">do_memoize:</span>
      <span class="s1">_</span><span class="s3">, </span><span class="s1">tf_val = _thread_local_state.constant_cache.get(const_key</span><span class="s3">, </span><span class="s1">(</span><span class="s3">None, None</span><span class="s1">))</span>
    <span class="s3">else</span><span class="s1">:</span>
      <span class="s1">tf_val = </span><span class="s3">None</span>
    <span class="s3">if </span><span class="s1">tf_val </span><span class="s3">is None</span><span class="s1">:</span>
      <span class="s1">conversion_dtype = _to_tf_dtype(jax_dtype)</span>
      <span class="s0"># The float0 type is not known to TF.</span>
      <span class="s3">if </span><span class="s1">jax_dtype == dtypes.float0:</span>
        <span class="s1">val = np.zeros(np.shape(val)</span><span class="s3">, </span><span class="s1">conversion_dtype.as_numpy_dtype)</span>
      <span class="s1">tf_val = tf.convert_to_tensor(val</span><span class="s3">, </span><span class="s1">dtype=conversion_dtype)</span>
      <span class="s3">if </span><span class="s1">do_memoize:</span>
        <span class="s1">_thread_local_state.constant_cache[const_key] = (val</span><span class="s3">, </span><span class="s1">tf_val)</span>
    <span class="s3">return </span><span class="s1">tf_val</span><span class="s3">, </span><span class="s1">jax_dtype</span>


<span class="s3">def </span><span class="s1">_eval_shape(shape: Sequence[shape_poly.DimSize]</span><span class="s3">, </span><span class="s1">dtype=</span><span class="s3">None</span><span class="s1">) -&gt; Sequence[TfVal]:</span>
  <span class="s0"># Returns a tuple of shape_poly.dim_as_value_dtype</span>
  <span class="s3">assert </span><span class="s1">all(map(</span><span class="s3">lambda </span><span class="s1">x: x </span><span class="s3">is not None, </span><span class="s1">shape))</span><span class="s3">, </span><span class="s1">(</span>
      <span class="s4">f&quot;Argument shape should be a valid JAX shape but got </span><span class="s3">{</span><span class="s1">shape</span><span class="s3">}</span><span class="s4">&quot;</span><span class="s1">)</span>
  <span class="s3">if </span><span class="s1">dtype </span><span class="s3">is not None</span><span class="s1">:</span>
    <span class="s1">shape = _jax_physical_aval(core.ShapedArray(shape</span><span class="s3">, </span><span class="s1">dtype)).shape</span>
  <span class="s3">if </span><span class="s1">core.is_constant_shape(shape):</span>
    <span class="s3">return </span><span class="s1">tuple(int(d) </span><span class="s3">for </span><span class="s1">d </span><span class="s3">in </span><span class="s1">shape)</span>

  <span class="s1">dim_vars</span><span class="s3">, </span><span class="s1">dim_values = util.unzip2(_thread_local_state.shape_env)</span>
  <span class="s1">eval_shape_jax = shape_poly.get_shape_evaluator(dim_vars</span><span class="s3">, </span><span class="s1">shape)</span>
  <span class="s1">dim_aval = shape_poly.dim_as_value_abstract(</span><span class="s5">1</span><span class="s1">)</span>
  <span class="s1">shape_values_tf</span><span class="s3">, </span><span class="s1">_ = _interpret_fun_jax(eval_shape_jax</span><span class="s3">,</span>
                                          <span class="s1">dim_values</span><span class="s3">, </span><span class="s1">[dim_aval] * len(dim_values)</span><span class="s3">, </span><span class="s4">&quot;&quot;</span><span class="s1">)  </span><span class="s0"># type: ignore</span>
  <span class="s0"># Keep only the non-constant dimensions</span>
  <span class="s3">return </span><span class="s1">tuple(operator.index(d) </span><span class="s3">if </span><span class="s1">core.is_constant_dim(d) </span><span class="s3">else </span><span class="s1">d_tf</span>
               <span class="s3">for </span><span class="s1">d</span><span class="s3">, </span><span class="s1">d_tf </span><span class="s3">in </span><span class="s1">zip(shape</span><span class="s3">, </span><span class="s1">shape_values_tf))</span>


<span class="s3">def </span><span class="s1">_ensure_tf_shape_if_dynamic(x: TfVal</span><span class="s3">, </span><span class="s1">shape):</span>
  <span class="s0"># Update TF tensor `x` with shape `shape` if the shape of `x`` is dynamic.</span>
  <span class="s3">if </span><span class="s1">x.shape.is_fully_defined():</span>
    <span class="s3">return </span><span class="s1">x</span>
  <span class="s3">return </span><span class="s1">tf.ensure_shape(x</span><span class="s3">, </span><span class="s1">shape)</span>


<span class="s3">def </span><span class="s1">_assert_matching_abstract_shape(x: TfVal</span><span class="s3">, </span><span class="s1">shape: Sequence[shape_poly.DimSize]):</span>
  <span class="s2">&quot;&quot;&quot;Asserts that shape matches x.shape in the known dimensions and has 
  dimension polynomials elsewhere.&quot;&quot;&quot;</span>
  <span class="s0"># Ensures that the shape does not contain None; it should contain symbolic expressions.</span>
  <span class="s3">def </span><span class="s1">check_one(xd: Optional[int]</span><span class="s3">, </span><span class="s1">sd: Any):</span>
    <span class="s3">if </span><span class="s1">core.is_constant_dim(sd):</span>
      <span class="s3">return </span><span class="s1">xd == sd</span>
    <span class="s3">else</span><span class="s1">:</span>
      <span class="s3">assert </span><span class="s1">isinstance(sd</span><span class="s3">, </span><span class="s1">shape_poly._DimExpr)</span>
      <span class="s3">return True</span>
  <span class="s3">assert </span><span class="s1">(len(x.shape) == len(shape) </span><span class="s3">and</span>
          <span class="s1">all(check_one(xd</span><span class="s3">, </span><span class="s1">sd)</span>
              <span class="s3">for </span><span class="s1">xd</span><span class="s3">, </span><span class="s1">sd </span><span class="s3">in </span><span class="s1">zip(x.shape</span><span class="s3">, </span><span class="s1">shape)))</span><span class="s3">, </span><span class="s1">\</span>
    <span class="s4">f&quot;Shape </span><span class="s3">{</span><span class="s1">shape</span><span class="s3">} </span><span class="s4">does not match x.shape </span><span class="s3">{</span><span class="s1">x.shape</span><span class="s3">}</span><span class="s4">&quot;</span>

<span class="s0"># TODO(b/26854495): pylint doesn't understand slots and inheritance.</span>
<span class="s0"># pylint: disable=assigning-non-slot</span>


<span class="s3">class </span><span class="s1">TensorFlowTracer(core.Tracer):</span>
  <span class="s2">&quot;&quot;&quot;Tracer class that boxes a TF value and a JAX abstract value. 
 
  In addition to the TF value we carry the JAX abstract value because 
  there are some cases when it cannot be recovered from the value: 
  when we are converting with polymorphic shapes or when the JAX aval 
  has a custom element type. In these cases the shape of the value may 
  have dimensions set to `None`, or it may only correspond to the JAX 
  &quot;physical&quot; (TF/lowering-compatible) shape, so the JAX abstract value 
  may contain more precise information. 
 
  When the value has a partially-known shape, the dimensions marked as `None` 
  must correspond to non-constant dimensions in the abstract value. 
 
  See README.md for details. 
  &quot;&quot;&quot;</span>
  <span class="s0"># val: TfVal</span>
  <span class="s0"># _aval: core.ShapedArray</span>
  <span class="s1">__slots__ = [</span><span class="s4">&quot;val&quot;</span><span class="s3">, </span><span class="s4">&quot;_aval&quot;</span><span class="s1">]</span>

  <span class="s3">def </span><span class="s1">__init__(self</span><span class="s3">, </span><span class="s1">trace: </span><span class="s4">&quot;TensorFlowTrace&quot;</span><span class="s3">, </span><span class="s1">val: TfVal</span><span class="s3">,</span>
               <span class="s1">aval: core.AbstractValue):</span>
    <span class="s1">self._trace = trace</span>
    <span class="s1">self._aval = aval</span>
    <span class="s1">phys_aval = _jax_physical_aval(self._aval)  </span><span class="s0"># type: ignore[arg-type]</span>

    <span class="s3">if </span><span class="s1">isinstance(val</span><span class="s3">, </span><span class="s1">(tf.Tensor</span><span class="s3">, </span><span class="s1">tf.Variable)):</span>
      <span class="s1">val_shape = val.shape</span>

      <span class="s3">if </span><span class="s1">config.jax_enable_checks:</span>
        <span class="s3">assert </span><span class="s1">len(phys_aval.shape) == len(val_shape)</span><span class="s3">, </span><span class="s4">f&quot;_aval.shape=</span><span class="s3">{</span><span class="s1">phys_aval.shape</span><span class="s3">} </span><span class="s4">different rank than </span><span class="s3">{</span><span class="s1">val_shape=</span><span class="s3">}</span><span class="s4">&quot;</span>
        <span class="s0"># To compare types, we must handle float0 in JAX and x64 in TF</span>
        <span class="s3">if </span><span class="s1">phys_aval.dtype == dtypes.float0:</span>
          <span class="s3">assert </span><span class="s1">_to_tf_dtype(phys_aval.dtype) == val.dtype</span><span class="s3">, </span><span class="s4">f&quot;expected </span><span class="s3">{</span><span class="s1">phys_aval.dtype</span><span class="s3">} </span><span class="s4">== </span><span class="s3">{</span><span class="s1">val.dtype</span><span class="s3">}</span><span class="s4">&quot;</span>
        <span class="s3">else</span><span class="s1">:</span>
          <span class="s3">assert </span><span class="s1">phys_aval.dtype == _to_jax_dtype(val.dtype)</span><span class="s3">, </span><span class="s4">f&quot;expected </span><span class="s3">{</span><span class="s1">phys_aval.dtype</span><span class="s3">} </span><span class="s4">== </span><span class="s3">{</span><span class="s1">val.dtype</span><span class="s3">}</span><span class="s4">&quot;</span>

        <span class="s3">for </span><span class="s1">aval_dim</span><span class="s3">, </span><span class="s1">val_dim </span><span class="s3">in </span><span class="s1">zip(phys_aval.shape</span><span class="s3">, </span><span class="s1">val_shape):  </span><span class="s0"># type: ignore[attr-defined]</span>
          <span class="s3">if </span><span class="s1">val_dim </span><span class="s3">is None</span><span class="s1">:</span>
            <span class="s3">assert </span><span class="s1">shape_poly.is_poly_dim(aval_dim)</span><span class="s3">, </span><span class="s4">f&quot;expected </span><span class="s3">{</span><span class="s1">phys_aval.shape</span><span class="s3">} </span><span class="s4">== </span><span class="s3">{</span><span class="s1">val_shape</span><span class="s3">}</span><span class="s4">&quot;  </span><span class="s0"># type: ignore[attr-defined]</span>
          <span class="s3">elif not </span><span class="s1">shape_poly.is_poly_dim(aval_dim):</span>
            <span class="s3">assert </span><span class="s1">aval_dim == val_dim</span><span class="s3">, </span><span class="s4">f&quot;expected </span><span class="s3">{</span><span class="s1">phys_aval.shape</span><span class="s3">} </span><span class="s4">== </span><span class="s3">{</span><span class="s1">val_shape</span><span class="s3">}</span><span class="s4">&quot;  </span><span class="s0"># type: ignore[attr-defined]</span>
          <span class="s3">else</span><span class="s1">:</span>
            <span class="s0"># We have a TF value with known shape, and the abstract shape is a shape variable.</span>
            <span class="s3">try</span><span class="s1">:</span>
              <span class="s1">aval_int = int(_eval_shape([aval_dim]))  </span><span class="s0"># type: ignore</span>
            <span class="s3">except </span><span class="s1">(TypeError</span><span class="s3">, </span><span class="s1">KeyError):</span>
              <span class="s3">continue</span>
            <span class="s3">assert </span><span class="s1">aval_int == val_dim</span><span class="s3">, </span><span class="s4">f&quot;expected </span><span class="s3">{</span><span class="s1">phys_aval.shape</span><span class="s3">} </span><span class="s4">== </span><span class="s3">{</span><span class="s1">val_shape</span><span class="s3">}</span><span class="s4">. Found </span><span class="s3">{</span><span class="s1">aval_int</span><span class="s3">} </span><span class="s4">!= </span><span class="s3">{</span><span class="s1">val_dim</span><span class="s3">}</span><span class="s4">.&quot;  </span><span class="s0"># type: ignore</span>

    <span class="s1">self.val = _tfval_to_tensor_jax_dtype(val</span><span class="s3">,</span>
                                          <span class="s1">phys_aval.dtype</span><span class="s3">,</span>
                                          <span class="s1">memoize_constants=</span><span class="s3">True</span><span class="s1">)[</span><span class="s5">0</span><span class="s1">]  </span><span class="s0"># type: ignore[attr-defined]</span>

  <span class="s1">@property</span>
  <span class="s3">def </span><span class="s1">aval(self):</span>
    <span class="s3">return </span><span class="s1">self._aval</span>

  <span class="s3">def </span><span class="s1">full_lower(self):</span>
    <span class="s3">return </span><span class="s1">self</span>

<span class="s3">def </span><span class="s1">_make_op_metadata(primitive: core.Primitive</span><span class="s3">,</span>
                      <span class="s1">params: Dict</span><span class="s3">, </span><span class="s1">*</span><span class="s3">,</span>
                      <span class="s1">source_info: source_info_util.SourceInfo</span><span class="s3">,</span>
                      <span class="s1">) -&gt; xla_client.OpMetadata:</span>
  <span class="s1">eqn_str = (str(source_info.name_stack) + </span><span class="s4">'/'</span>
             <span class="s1">+ core.str_eqn_compact(primitive.name</span><span class="s3">, </span><span class="s1">params))</span>
  <span class="s1">frame = source_info_util.user_frame(source_info)</span>
  <span class="s3">return </span><span class="s1">xla_client.OpMetadata(</span>
        <span class="s1">op_type=primitive.name</span><span class="s3">,</span>
        <span class="s1">op_name=eqn_str</span><span class="s3">,</span>
        <span class="s1">source_file=xla.get_canonical_source_file(frame) </span><span class="s3">if </span><span class="s1">frame </span><span class="s3">else None,</span>
        <span class="s1">source_line=frame.start_line </span><span class="s3">if </span><span class="s1">frame </span><span class="s3">else None</span><span class="s1">)</span>


<span class="s3">class </span><span class="s1">TensorFlowTrace(core.Trace):</span>
  <span class="s2">&quot;&quot;&quot;Trace class that underlies the jax2tf transformation. 
 
  We are going to ensure that jax2tf.convert is never nested inside other 
  transformations. This is sufficient for intended use cases (converting 
  fully-transformed JAX code). It also simplifies our job because we do not have 
  to handle situations where we apply primitives on a mix of TF values and 
  JAX tracers from an outer transformation. E.g., for addition both the TF 
  values 
  and the JAX tracers have an override and they get confused if they see values 
  from the other world. 
 
  Hence a TFT trace does not interact with non-TFT traces at lower-level. For 
  higher-order control-flow primitives we invoke recursively 
  _interpret_fun on the body of the conditional, which will create a nested TFT. 
 
  We do want to allow transformations nested inside a TensorFlowTrace (TFT), but 
  those will introduce their own MainTrace, and any operations involving those 
  will be done on those traces, i.e., not a concern for TFT. 
  &quot;&quot;&quot;</span>
  <span class="s3">def </span><span class="s1">pure(self</span><span class="s3">, </span><span class="s1">val: TfVal) -&gt; TensorFlowTracer:</span>
    <span class="s2">&quot;&quot;&quot;Lifts a non-Tracer into the TensorFlowTracer. 
 
    This function may be called by way of trace.full_raise. 
    &quot;&quot;&quot;</span>
    <span class="s3">if </span><span class="s1">hasattr(val</span><span class="s3">, </span><span class="s4">&quot;__jax_array__&quot;</span><span class="s1">):</span>
      <span class="s1">val = val.__jax_array__()</span>
      <span class="s3">if </span><span class="s1">isinstance(val</span><span class="s3">, </span><span class="s1">TensorFlowTracer):</span>
        <span class="s3">return </span><span class="s1">val</span>
    <span class="s1">tf_val</span><span class="s3">, </span><span class="s1">jax_dtype = _tfval_to_tensor_jax_dtype(val</span><span class="s3">, </span><span class="s1">memoize_constants=</span><span class="s3">True</span><span class="s1">)</span>
    <span class="s3">return </span><span class="s1">TensorFlowTracer(</span>
        <span class="s1">self</span><span class="s3">, </span><span class="s1">val</span><span class="s3">, </span><span class="s1">core.ShapedArray(tf_val.shape</span><span class="s3">, </span><span class="s1">jax_dtype</span><span class="s3">,</span>
                                    <span class="s1">weak_type=dtypes.is_weakly_typed(val)))</span>

  <span class="s3">def </span><span class="s1">lift(self</span><span class="s3">, </span><span class="s1">val: core.Tracer) -&gt; TensorFlowTracer:</span>
    <span class="s0"># This would be called when we need to raise a tracer from a lower-level</span>
    <span class="s0"># main into the TensorFlowTrace. Since the TensorFlowTrace is never nested</span>
    <span class="s0"># inside another transform, there are no lower-level main traces.</span>
    <span class="s3">assert False</span>

  <span class="s3">def </span><span class="s1">sublift(self</span><span class="s3">, </span><span class="s1">val: TensorFlowTracer) -&gt; TensorFlowTracer:</span>
    <span class="s0"># This is called when we need to raise a tracer from the same main,</span>
    <span class="s0"># but a lower sublevel. This could come from a nested jit.</span>
    <span class="s3">return </span><span class="s1">TensorFlowTracer(self</span><span class="s3">, </span><span class="s1">val.val</span><span class="s3">, </span><span class="s1">val._aval)</span>

  <span class="s3">def </span><span class="s1">process_primitive(self</span><span class="s3">, </span><span class="s1">primitive: core.Primitive</span><span class="s3">,</span>
                        <span class="s1">tracers: Sequence[TensorFlowTracer]</span><span class="s3">,</span>
                        <span class="s1">params) -&gt; TensorFlowTracer:</span>
    <span class="s1">impl</span><span class="s3">, </span><span class="s1">impl_needs_avals = self.get_primitive_impl(primitive)</span>
    <span class="s1">args_avals: Sequence[core.ShapedArray] = tuple(t.aval </span><span class="s3">for </span><span class="s1">t </span><span class="s3">in </span><span class="s1">tracers)</span>
    <span class="s0"># This is a bit conservative, doing abstract_eval even in op-by-op execution</span>
    <span class="s0"># but we needed it for, e.g., shape_polymorphism where only JAX's</span>
    <span class="s0"># abstract evaluation rules can properly track polymorphic shapes.</span>
    <span class="s0"># Unfortunately under op-by-op execution this is a rare occasion where we</span>
    <span class="s0"># need abstract evaluation.</span>
    <span class="s1">out_aval</span><span class="s3">, </span><span class="s1">_ = primitive.abstract_eval(*args_avals</span><span class="s3">, </span><span class="s1">**params)</span>
    <span class="s1">args_tf: Sequence[TfVal] = [t.val </span><span class="s3">for </span><span class="s1">t </span><span class="s3">in </span><span class="s1">tracers]</span>
    <span class="s3">def </span><span class="s1">invoke_impl() -&gt; TfVal:</span>
      <span class="s3">if </span><span class="s1">impl_needs_avals:</span>
        <span class="s3">return </span><span class="s1">impl(</span>
            <span class="s1">*args_tf</span><span class="s3">,</span>
            <span class="s1">_in_avals=args_avals</span><span class="s3">,  </span><span class="s0"># type: ignore</span>
            <span class="s1">_out_aval=out_aval</span><span class="s3">,</span>
            <span class="s1">**params)</span>
      <span class="s3">else</span><span class="s1">:</span>
        <span class="s3">return </span><span class="s1">impl(*args_tf</span><span class="s3">, </span><span class="s1">**params)</span>

    <span class="s1">current_name_stack = _get_current_name_stack()</span>
    <span class="s0"># We don't use `str(name_stack)` because it uses parentheses for</span>
    <span class="s0"># transformations, which aren't allowed in `name_scope`.</span>
    <span class="s1">scope = </span><span class="s4">'/'</span><span class="s1">.join([s.name </span><span class="s3">for </span><span class="s1">s </span><span class="s3">in </span><span class="s1">current_name_stack.stack])  </span><span class="s0"># type: ignore[union-attr]</span>

    <span class="s0"># Here we reset the name scope to the memorized TF name scope</span>
    <span class="s0"># + JAX name stack by using absolute scope.</span>
    <span class="s0"># We need to add a '/' to the name stack string to force `tf.name_scope`</span>
    <span class="s0"># to interpret it as an absolute scope, not a relative scope.</span>
    <span class="s3">if </span><span class="s1">_thread_local_state.tf_outer_name_scope:</span>
      <span class="s1">scope = </span><span class="s4">f&quot;</span><span class="s3">{</span><span class="s1">_thread_local_state.tf_outer_name_scope</span><span class="s3">}</span><span class="s4">/</span><span class="s3">{</span><span class="s1">scope</span><span class="s3">}</span><span class="s4">&quot;</span>

    <span class="s3">if not </span><span class="s1">scope.endswith(</span><span class="s4">&quot;/&quot;</span><span class="s1">):</span>
      <span class="s1">scope = scope + </span><span class="s4">&quot;/&quot;</span>

    <span class="s3">with </span><span class="s1">tf.name_scope(_sanitize_scope_name(scope)):</span>
      <span class="s3">if </span><span class="s1">_thread_local_state.include_xla_op_metadata:</span>
        <span class="s1">op_metadata = _make_op_metadata(primitive</span><span class="s3">, </span><span class="s1">params</span><span class="s3">,</span>
                                        <span class="s1">source_info=source_info_util.current())</span>
        <span class="s1">op_metadata_proto = xla_data_pb2.OpMetadata(</span>
            <span class="s1">op_type=op_metadata.op_type</span><span class="s3">,</span>
            <span class="s1">op_name=op_metadata.op_name</span><span class="s3">,</span>
            <span class="s1">source_file=op_metadata.source_file</span><span class="s3">,</span>
            <span class="s1">source_line=op_metadata.source_line</span>
        <span class="s1">)</span>
        <span class="s3">with </span><span class="s1">tf_ops.get_default_graph()._attr_scope(</span>
            <span class="s1">{</span><span class="s4">&quot;_XlaOpMetadata&quot;</span><span class="s1">: attr_value_pb2.AttrValue(</span>
                <span class="s1">s=op_metadata_proto.SerializeToString())}):</span>
          <span class="s1">val_out = invoke_impl()</span>
      <span class="s3">else</span><span class="s1">:</span>
        <span class="s1">val_out = invoke_impl()</span>

    <span class="s3">if </span><span class="s1">primitive.multiple_results:</span>
      <span class="s1">out = [</span>
          <span class="s1">TensorFlowTracer(self</span><span class="s3">, </span><span class="s1">v</span><span class="s3">, </span><span class="s1">a)</span>
          <span class="s3">for </span><span class="s1">v</span><span class="s3">, </span><span class="s1">a </span><span class="s3">in </span><span class="s1">zip(val_out</span><span class="s3">, </span><span class="s1">out_aval)</span>
      <span class="s1">]  </span><span class="s0"># type: ignore</span>
    <span class="s3">else</span><span class="s1">:</span>
      <span class="s1">out = TensorFlowTracer(self</span><span class="s3">, </span><span class="s1">val_out</span><span class="s3">, </span><span class="s1">out_aval)  </span><span class="s0"># type: ignore</span>

    <span class="s0"># Check that the impl rule returned a value of expected shape and dtype</span>
    <span class="s0"># TODO: adapt this to match polymorphic shapes</span>
    <span class="s3">if </span><span class="s1">config.jax_enable_checks:</span>
      <span class="s3">if </span><span class="s1">primitive.multiple_results:</span>
        <span class="s3">for </span><span class="s1">o</span><span class="s3">, </span><span class="s1">expected_aval </span><span class="s3">in </span><span class="s1">zip(out</span><span class="s3">, </span><span class="s1">out_aval):  </span><span class="s0"># type: ignore</span>
          <span class="s3">assert </span><span class="s1">o.aval.strip_weak_type() == expected_aval.strip_weak_type()</span><span class="s3">, </span><span class="s1">(</span>
              <span class="s4">f&quot;</span><span class="s3">{</span><span class="s1">primitive</span><span class="s3">}</span><span class="s4">: out.aval = </span><span class="s3">{</span><span class="s1">o.aval</span><span class="s3">}</span><span class="s4">; expected </span><span class="s3">{</span><span class="s1">expected_aval</span><span class="s3">}</span><span class="s4">&quot;</span><span class="s1">)</span>
      <span class="s3">else</span><span class="s1">:</span>
        <span class="s3">assert </span><span class="s1">out.aval == out_aval</span><span class="s3">, </span><span class="s1">(  </span><span class="s0"># type: ignore</span>
            <span class="s4">f&quot;</span><span class="s3">{</span><span class="s1">primitive</span><span class="s3">}</span><span class="s4">: out.aval = </span><span class="s3">{</span><span class="s1">out.aval</span><span class="s3">}</span><span class="s4">; expected </span><span class="s3">{</span><span class="s1">out_aval</span><span class="s3">}</span><span class="s4">&quot;</span>
        <span class="s1">)  </span><span class="s0"># type: ignore</span>
    <span class="s3">return </span><span class="s1">out  </span><span class="s0"># type: ignore</span>

  <span class="s3">def </span><span class="s1">process_call(self</span><span class="s3">, </span><span class="s1">call_primitive: core.Primitive</span><span class="s3">, </span><span class="s1">fun: lu.WrappedFun</span><span class="s3">,</span>
                   <span class="s1">tracers: Sequence[TensorFlowTracer]</span><span class="s3">, </span><span class="s1">params):</span>
    <span class="s3">assert </span><span class="s1">call_primitive.multiple_results</span>
    <span class="s1">vals: Sequence[TfVal] = [t.val </span><span class="s3">for </span><span class="s1">t </span><span class="s3">in </span><span class="s1">tracers]</span>
    <span class="s1">avals: Sequence[core.ShapedArray] = tuple(t.aval </span><span class="s3">for </span><span class="s1">t </span><span class="s3">in </span><span class="s1">tracers)</span>
    <span class="s1">interpreted_fun = _interpret_subtrace(fun</span><span class="s3">, </span><span class="s1">self.main</span><span class="s3">, </span><span class="s1">avals)</span>
    <span class="s1">extra_name_stack = </span><span class="s3">None</span>
    <span class="s3">with </span><span class="s1">_extended_name_stack(extra_name_stack):</span>
      <span class="s3">with </span><span class="s1">core.new_sublevel():</span>
        <span class="s1">vals_out = interpreted_fun.call_wrapped(*vals)</span>
    <span class="s3">return </span><span class="s1">[TensorFlowTracer(self</span><span class="s3">, </span><span class="s1">v</span><span class="s3">, </span><span class="s1">a) </span><span class="s3">for </span><span class="s1">v</span><span class="s3">, </span><span class="s1">a </span><span class="s3">in </span><span class="s1">vals_out]</span>

  <span class="s3">def </span><span class="s1">post_process_call(self</span><span class="s3">, </span><span class="s1">call_primitive: core.Primitive</span><span class="s3">,</span>
                        <span class="s1">out_tracers: Sequence[TensorFlowTracer]</span><span class="s3">, </span><span class="s1">params):</span>
    <span class="s0"># We encountered a call primitive whose result (out_tracers) include</span>
    <span class="s0"># TensorFlowTracer that were not passed through its arguments (captured from</span>
    <span class="s0"># the environment).</span>
    <span class="s1">vals = tuple(t.val </span><span class="s3">for </span><span class="s1">t </span><span class="s3">in </span><span class="s1">out_tracers)</span>
    <span class="s1">main = self.main</span>

    <span class="s3">def </span><span class="s1">todo(vals: Sequence[TfVal]):</span>
      <span class="s0"># TODO: is name_stack correct?</span>
      <span class="s1">trace = TensorFlowTrace(main</span><span class="s3">, </span><span class="s1">core.cur_sublevel())</span>
      <span class="s3">return </span><span class="s1">[</span>
          <span class="s1">TensorFlowTracer(trace</span><span class="s3">, </span><span class="s1">v</span><span class="s3">, </span><span class="s1">out_tracer.aval)</span>
          <span class="s3">for </span><span class="s1">v</span><span class="s3">, </span><span class="s1">out_tracer </span><span class="s3">in </span><span class="s1">zip(vals</span><span class="s3">, </span><span class="s1">out_tracers)</span>
      <span class="s1">]</span>

    <span class="s3">return </span><span class="s1">vals</span><span class="s3">, </span><span class="s1">todo</span>

  <span class="s3">def </span><span class="s1">process_map(self</span><span class="s3">, </span><span class="s1">map_primitive</span><span class="s3">, </span><span class="s1">f</span><span class="s3">, </span><span class="s1">tracers</span><span class="s3">, </span><span class="s1">params):</span>
    <span class="s3">raise </span><span class="s1">NotImplementedError(</span><span class="s4">&quot;process_map&quot;</span><span class="s1">)</span>

  <span class="s3">def </span><span class="s1">post_process_map(self</span><span class="s3">, </span><span class="s1">map_primitive</span><span class="s3">, </span><span class="s1">out_tracers</span><span class="s3">, </span><span class="s1">params):</span>
    <span class="s3">raise </span><span class="s1">NotImplementedError(</span><span class="s4">&quot;post_process_map&quot;</span><span class="s1">)</span>

  <span class="s3">def </span><span class="s1">process_custom_jvp_call(self</span><span class="s3">, </span><span class="s1">prim</span><span class="s3">, </span><span class="s1">fun</span><span class="s3">, </span><span class="s1">jvp</span><span class="s3">, </span><span class="s1">tracers</span><span class="s3">, </span><span class="s1">*</span><span class="s3">, </span><span class="s1">symbolic_zeros):</span>
    <span class="s0"># Drop the custom differentiation rule and act like a call primitive. This</span>
    <span class="s0"># behavior is desirable because jax2tf stages code out of the JAX system, so</span>
    <span class="s0"># there are no more JAX differentiation transformations to be applied.</span>
    <span class="s3">del </span><span class="s1">jvp</span><span class="s3">, </span><span class="s1">symbolic_zeros  </span><span class="s0"># Unused.</span>
    <span class="s3">return </span><span class="s1">self.process_call(core.call_p</span><span class="s3">, </span><span class="s1">fun</span><span class="s3">, </span><span class="s1">tracers</span><span class="s3">, </span><span class="s1">{})</span>

  <span class="s3">def </span><span class="s1">post_process_custom_jvp_call(self</span><span class="s3">, </span><span class="s1">out_tracers</span><span class="s3">, </span><span class="s1">_):</span>
    <span class="s3">assert False  </span><span class="s0"># unreachable assuming jax2tf runs with clean trace state</span>

  <span class="s3">def </span><span class="s1">process_custom_vjp_call(self</span><span class="s3">, </span><span class="s1">prim</span><span class="s3">, </span><span class="s1">fun</span><span class="s3">, </span><span class="s1">fwd</span><span class="s3">, </span><span class="s1">bwd</span><span class="s3">, </span><span class="s1">tracers</span><span class="s3">, </span><span class="s1">out_trees):</span>
    <span class="s0"># Drop the custom differentiation rule and act like a call primitive. This</span>
    <span class="s0"># behavior is desirable because jax2tf stages code out of the JAX system, so</span>
    <span class="s0"># there are no more JAX differentiation transformations to be applied.</span>
    <span class="s3">del </span><span class="s1">fwd</span><span class="s3">, </span><span class="s1">bwd</span><span class="s3">, </span><span class="s1">out_trees  </span><span class="s0"># Unused.</span>
    <span class="s3">return </span><span class="s1">self.process_call(core.call_p</span><span class="s3">, </span><span class="s1">fun</span><span class="s3">, </span><span class="s1">tracers</span><span class="s3">, </span><span class="s1">{})</span>

  <span class="s3">def </span><span class="s1">post_process_custom_vjp_call(self</span><span class="s3">, </span><span class="s1">out_tracers</span><span class="s3">, </span><span class="s1">_):</span>
    <span class="s3">assert False  </span><span class="s0"># unreachable assuming jax2tf runs with clean trace state</span>

  <span class="s3">def </span><span class="s1">post_process_custom_vjp_call_fwd(self</span><span class="s3">, </span><span class="s1">*_</span><span class="s3">, </span><span class="s1">**__):</span>
    <span class="s3">assert False  </span><span class="s0"># unreachable assuming jax2tf runs with clean trace state</span>

  <span class="s3">def </span><span class="s1">get_primitive_impl(self</span><span class="s3">, </span><span class="s1">p: core.Primitive) -&gt; Tuple[Callable</span><span class="s3">, </span><span class="s1">bool]:</span>
    <span class="s0"># Returns the primitive implementation and whether the implementation</span>
    <span class="s0"># takes abstract values (see definition of tf_impl_with_avals)</span>
    <span class="s3">if not </span><span class="s1">_thread_local_state.enable_xla:</span>
      <span class="s3">try</span><span class="s1">:</span>
        <span class="s3">return </span><span class="s1">tf_impl_no_xla[p]</span><span class="s3">, True  </span><span class="s0"># Always require avals.</span>
      <span class="s3">except </span><span class="s1">KeyError:</span>
        <span class="s3">pass</span>
    <span class="s3">try</span><span class="s1">:</span>
      <span class="s3">return </span><span class="s1">tf_impl[p]</span><span class="s3">, False</span>
    <span class="s3">except </span><span class="s1">KeyError:</span>
      <span class="s3">try</span><span class="s1">:</span>
        <span class="s3">return </span><span class="s1">tf_impl_with_avals[p]</span><span class="s3">, True</span>
      <span class="s3">except </span><span class="s1">KeyError </span><span class="s3">as </span><span class="s1">err:</span>
        <span class="s1">msg = </span><span class="s4">&quot;TensorFlow interpretation rule for '{}' not implemented&quot;</span>
        <span class="s3">raise </span><span class="s1">NotImplementedError(msg.format(p)) </span><span class="s3">from </span><span class="s1">err</span>

<span class="s3">def </span><span class="s1">_unexpected_primitive(p: core.Primitive</span><span class="s3">, </span><span class="s1">*args</span><span class="s3">, </span><span class="s1">**kwargs):</span>
  <span class="s3">assert False, </span><span class="s4">f&quot;Encountered unexpected primitive </span><span class="s3">{</span><span class="s1">p</span><span class="s3">}</span><span class="s4">&quot;</span>


<span class="s0"># Call primitives are inlined</span>
<span class="s3">for </span><span class="s1">unexpected </span><span class="s3">in </span><span class="s1">[core.call_p</span><span class="s3">, </span><span class="s1">maps.xmap_p]:</span>
  <span class="s1">tf_impl[unexpected] = partial(_unexpected_primitive</span><span class="s3">, </span><span class="s1">unexpected)</span>

<span class="s0"># Primitives that are not yet implemented must be explicitly declared here.</span>
<span class="s1">tf_not_yet_impl = [</span>
    <span class="s4">&quot;clz&quot;</span><span class="s3">,</span>
    <span class="s4">&quot;igamma_grad_a&quot;</span><span class="s3">,</span>
    <span class="s4">&quot;random_gamma_grad&quot;</span><span class="s3">,</span>
    <span class="s4">&quot;reduce_xor&quot;</span><span class="s3">,</span>
    <span class="s4">&quot;schur&quot;</span><span class="s3">,</span>
    <span class="s4">&quot;closed_call&quot;</span><span class="s3">,</span>
    <span class="s4">&quot;unreachable&quot;</span><span class="s3">,</span>
    <span class="s4">&quot;bint&quot;</span><span class="s3">,</span>
    <span class="s4">&quot;getslice&quot;</span><span class="s3">,</span>
    <span class="s4">&quot;full_to_shard&quot;</span><span class="s3">,</span>
    <span class="s4">&quot;shard_to_full&quot;</span><span class="s3">,</span>
    <span class="s4">&quot;pure_callback&quot;</span><span class="s3">,</span>
    <span class="s4">&quot;for&quot;</span><span class="s3">,</span>
    <span class="s4">&quot;inspect_sharding&quot;</span><span class="s3">,</span>
    <span class="s4">&quot;io_callback&quot;</span><span class="s3">,</span>
    <span class="s4">&quot;shard_map&quot;</span><span class="s3">,</span>

    <span class="s0"># Not high priority?</span>
    <span class="s4">&quot;after_all&quot;</span><span class="s3">,</span>
    <span class="s4">&quot;all_to_all&quot;</span><span class="s3">,</span>
    <span class="s4">&quot;check&quot;</span><span class="s3">,</span>
    <span class="s4">&quot;create_token&quot;</span><span class="s3">,</span>
    <span class="s4">&quot;custom_transpose_call&quot;</span><span class="s3">,</span>
    <span class="s4">&quot;custom_vmap_call&quot;</span><span class="s3">,</span>
    <span class="s4">&quot;infeed&quot;</span><span class="s3">,</span>
    <span class="s4">&quot;linear_call&quot;</span><span class="s3">,</span>
    <span class="s4">&quot;outfeed&quot;</span><span class="s3">,</span>
    <span class="s4">&quot;pmax_p&quot;</span><span class="s3">,</span>
    <span class="s4">&quot;pmin&quot;</span><span class="s3">,</span>
    <span class="s4">&quot;ppermute&quot;</span><span class="s3">,</span>
    <span class="s4">&quot;psum&quot;</span><span class="s3">,</span>
    <span class="s4">&quot;pmax&quot;</span><span class="s3">,</span>
    <span class="s4">&quot;pgather&quot;</span><span class="s3">,</span>
    <span class="s4">&quot;reduce_scatter&quot;</span><span class="s3">,</span>
    <span class="s4">&quot;axis_index&quot;</span><span class="s3">,</span>
    <span class="s4">&quot;pdot&quot;</span><span class="s3">,</span>
    <span class="s4">&quot;all_gather&quot;</span><span class="s3">,</span>
    <span class="s4">&quot;lu_pivots_to_permutation&quot;</span><span class="s3">,</span>
    <span class="s4">&quot;xla_pmap&quot;</span><span class="s3">,</span>
    <span class="s4">&quot;geqrf&quot;</span><span class="s3">,</span>
    <span class="s4">&quot;householder_product&quot;</span><span class="s3">,</span>
    <span class="s4">&quot;hessenberg&quot;</span><span class="s3">,</span>
    <span class="s4">&quot;tridiagonal&quot;</span><span class="s3">,</span>
    <span class="s4">&quot;eigh_jacobi&quot;</span><span class="s3">,</span>
<span class="s1">]</span>

<span class="s1">tf_impl[ad_util.stop_gradient_p] = tf.stop_gradient</span>
<span class="s1">tf_impl[ad_util.zeros_like_p] = tf.zeros_like</span>


<span class="s3">def </span><span class="s1">_add(x: TfVal</span><span class="s3">, </span><span class="s1">y: TfVal) -&gt; TfVal:</span>
  <span class="s3">return </span><span class="s1">tf.raw_ops.AddV2(x=x</span><span class="s3">, </span><span class="s1">y=y)</span>


<span class="s1">tf_impl[ad_util.add_jaxvals_p] = _add</span>
<span class="s1">tf_impl[dispatch.device_put_p] = </span><span class="s3">lambda </span><span class="s1">x</span><span class="s3">, </span><span class="s1">device=</span><span class="s3">None, </span><span class="s1">src=</span><span class="s3">None</span><span class="s1">: x</span>
<span class="s1">tf_impl[lax_internal.copy_p] = </span><span class="s3">lambda </span><span class="s1">x: x</span>

<span class="s3">def </span><span class="s1">_neg(x: TfVal) -&gt; TfVal:</span>
  <span class="s3">if </span><span class="s1">x.dtype.is_unsigned:</span>
    <span class="s1">signed_dtype = _UNSIGNED_TO_SIGNED_TABLE[x.dtype]</span>
    <span class="s1">x_signed = tf.cast(x</span><span class="s3">, </span><span class="s1">signed_dtype)</span>
    <span class="s1">res_signed = tf.math.negative(x_signed)</span>
    <span class="s3">return </span><span class="s1">tf.cast(res_signed</span><span class="s3">, </span><span class="s1">x.dtype)</span>
  <span class="s3">else</span><span class="s1">:</span>
    <span class="s3">return </span><span class="s1">tf.math.negative(x)</span>

<span class="s1">tf_impl[lax.neg_p] = _neg</span>


<span class="s3">def </span><span class="s1">_sign(x: TfVal) -&gt; TfVal:</span>
  <span class="s3">if </span><span class="s1">x.dtype.is_unsigned:</span>
    <span class="s0"># TF and XLA do not support tf.math.sign for unsigned types.</span>
    <span class="s3">return </span><span class="s1">tf.where(</span>
        <span class="s1">tf.math.equal(x</span><span class="s3">, </span><span class="s5">0</span><span class="s1">)</span><span class="s3">, </span><span class="s1">tf.constant(</span><span class="s5">0</span><span class="s3">, </span><span class="s1">dtype=x.dtype)</span><span class="s3">,</span>
        <span class="s1">tf.constant(</span><span class="s5">1</span><span class="s3">, </span><span class="s1">dtype=x.dtype))</span>
  <span class="s3">else</span><span class="s1">:</span>
    <span class="s3">return </span><span class="s1">tf.math.sign(x)</span>


<span class="s1">tf_impl[lax.sign_p] = _sign</span>
<span class="s1">tf_impl[lax.floor_p] = tf.math.floor</span>
<span class="s1">tf_impl[lax.ceil_p] = tf.math.ceil</span>


<span class="s3">def </span><span class="s1">_round(operand</span><span class="s3">, </span><span class="s1">*</span><span class="s3">, </span><span class="s1">rounding_method</span><span class="s3">,</span>
           <span class="s1">_in_avals: Sequence[core.ShapedArray]</span><span class="s3">,</span>
           <span class="s1">_out_aval: core.ShapedArray):</span>
  <span class="s3">if </span><span class="s1">rounding_method </span><span class="s3">is </span><span class="s1">lax.RoundingMethod.AWAY_FROM_ZERO:</span>
    <span class="s0"># JAX uses a single HLO op Round here</span>
    <span class="s1">sign = _sign(operand)</span>
    <span class="s1">operand *= sign</span>
    <span class="s1">floor = tf.math.floor(operand)</span>
    <span class="s1">operand -= floor</span>
    <span class="s1">cond = tf.math.equal(operand</span><span class="s3">, </span><span class="s1">tf.constant(np.array(</span><span class="s5">0.5</span><span class="s1">)</span><span class="s3">, </span><span class="s1">operand.dtype))</span>
    <span class="s3">return </span><span class="s1">sign * (</span>
        <span class="s1">tf.where(cond</span><span class="s3">, </span><span class="s1">tf.constant(np.array(</span><span class="s5">1</span><span class="s1">)</span><span class="s3">, </span><span class="s1">operand.dtype)</span><span class="s3">,</span>
                 <span class="s1">tf.math.round(operand)) + floor)</span>
  <span class="s3">else</span><span class="s1">:  </span><span class="s0"># rounding_method is RoundingMethod.TO_NEAREST_EVEN</span>
    <span class="s3">return </span><span class="s1">tf.math.round(operand)</span>

<span class="s1">tf_impl_with_avals[lax.round_p] = _round</span>
<span class="s1">tf_impl[lax.nextafter_p] = tf.math.nextafter</span>


<span class="s3">def </span><span class="s1">_population_count(x):</span>
  <span class="s1">orig_dtype = x.dtype</span>
  <span class="s3">return </span><span class="s1">tf.cast(tf.raw_ops.PopulationCount(x=x)</span><span class="s3">, </span><span class="s1">orig_dtype)</span>


<span class="s1">tf_impl[lax.population_count_p] = _population_count</span>
<span class="s1">tf_impl[lax.is_finite_p] = tf.math.is_finite</span>


<span class="s3">def </span><span class="s1">_abs(x: TfVal) -&gt; TfVal:</span>
  <span class="s0"># TF and XLA do not support tf.math.abs for unsigned types.</span>
  <span class="s3">return </span><span class="s1">tf.math.abs(x) </span><span class="s3">if not </span><span class="s1">x.dtype.is_unsigned </span><span class="s3">else </span><span class="s1">x</span>


<span class="s1">tf_impl[lax.abs_p] = _abs</span>
<span class="s1">tf_impl[lax.pow_p] = tf.math.pow</span>


<span class="s3">def </span><span class="s1">_integer_pow(x</span><span class="s3">, </span><span class="s1">*</span><span class="s3">, </span><span class="s1">y: int</span><span class="s3">, </span><span class="s1">_in_avals: Sequence[core.ShapedArray]</span><span class="s3">,</span>
                 <span class="s1">_out_aval: core.ShapedArray):</span>
  <span class="s0"># Follows the implementation in lax._integer_pow_translation_rule</span>
  <span class="s3">if </span><span class="s1">y == </span><span class="s5">0</span><span class="s1">:</span>
    <span class="s3">return </span><span class="s1">tf.broadcast_to(</span>
        <span class="s1">tf.constant(</span><span class="s5">1</span><span class="s3">, </span><span class="s1">dtype=x.dtype</span><span class="s3">, </span><span class="s1">shape=())</span><span class="s3">, </span><span class="s1">_eval_shape(_out_aval.shape))</span>
  <span class="s1">is_reciprocal = y &lt; </span><span class="s5">0</span>
  <span class="s3">if </span><span class="s1">is_reciprocal:</span>
    <span class="s1">y = -y</span>
  <span class="s1">acc = </span><span class="s3">None</span>
  <span class="s3">while </span><span class="s1">y &gt; </span><span class="s5">0</span><span class="s1">:</span>
    <span class="s3">if </span><span class="s1">y &amp; </span><span class="s5">1</span><span class="s1">:</span>
      <span class="s1">acc = x </span><span class="s3">if </span><span class="s1">acc </span><span class="s3">is None else </span><span class="s1">tf.math.multiply(acc</span><span class="s3">, </span><span class="s1">x)</span>
    <span class="s1">y &gt;&gt;= </span><span class="s5">1</span>
    <span class="s3">if </span><span class="s1">y &gt; </span><span class="s5">0</span><span class="s1">:</span>
      <span class="s1">x = tf.math.multiply(x</span><span class="s3">, </span><span class="s1">x)</span>
  <span class="s3">return </span><span class="s1">tf.math.reciprocal(acc) </span><span class="s3">if </span><span class="s1">is_reciprocal </span><span class="s3">else </span><span class="s1">acc</span>


<span class="s1">tf_impl_with_avals[lax.integer_pow_p] = _integer_pow</span>
<span class="s1">tf_impl[lax.exp_p] = tf.math.exp</span>
<span class="s1">tf_impl[lax.expm1_p] = tf.math.expm1</span>
<span class="s1">tf_impl[lax.log_p] = tf.math.log</span>
<span class="s1">tf_impl[lax.log1p_p] = tf.math.log1p</span>
<span class="s1">tf_impl[lax.tan_p] = tf.math.tan</span>
<span class="s1">tf_impl[lax.tanh_p] = tf.math.tanh</span>
<span class="s1">tf_impl[lax.sin_p] = tf.math.sin</span>
<span class="s1">tf_impl[lax.sinh_p] = tf.math.sinh</span>
<span class="s1">tf_impl[lax.cos_p] = tf.math.cos</span>
<span class="s1">tf_impl[lax.cosh_p] = tf.math.cosh</span>
<span class="s1">tf_impl_with_avals[lax.acos_p] = _convert_jax_impl(</span>
    <span class="s1">lax_internal.acos_impl</span><span class="s3">, </span><span class="s1">multiple_results=</span><span class="s3">False</span><span class="s1">)</span>
<span class="s1">tf_impl_with_avals[lax.asin_p] = _convert_jax_impl(</span>
    <span class="s1">lax_internal.asin_impl</span><span class="s3">, </span><span class="s1">multiple_results=</span><span class="s3">False</span><span class="s1">)</span>
<span class="s1">tf_impl_with_avals[lax.atan_p] = _convert_jax_impl(</span>
    <span class="s1">lax_internal.atan_impl</span><span class="s3">, </span><span class="s1">multiple_results=</span><span class="s3">False</span><span class="s1">)</span>

<span class="s0"># TODO(phawkins): use tf.math.sigmoid here instead.</span>
<span class="s1">tf_impl_with_avals[lax.logistic_p] = _convert_jax_impl(</span>
    <span class="s1">lax_internal.logistic_impl</span><span class="s3">, </span><span class="s1">multiple_results=</span><span class="s3">False</span><span class="s1">)</span>

<span class="s3">def </span><span class="s1">_atan2(y</span><span class="s3">, </span><span class="s1">x</span><span class="s3">, </span><span class="s1">**kwargs):</span>
  <span class="s3">if </span><span class="s1">x.dtype.is_complex </span><span class="s3">or </span><span class="s1">y.dtype.is_complex:</span>
    <span class="s1">complex_component_dtype = {</span>
      <span class="s1">tf.complex64: tf.float32</span><span class="s3">,</span>
      <span class="s1">tf.complex128: tf.float64</span>
    <span class="s1">}.get(y.dtype)</span>
    <span class="s1">zero = tf.constant(</span><span class="s5">0</span><span class="s3">, </span><span class="s1">complex_component_dtype)</span>
    <span class="s1">one = tf.constant(</span><span class="s5">1</span><span class="s3">, </span><span class="s1">complex_component_dtype)</span>
    <span class="s1">i = tf.complex(zero</span><span class="s3">, </span><span class="s1">one)</span>
    <span class="s3">return </span><span class="s1">-i * tf.math.log((x + i * y)/tf.math.sqrt(x * x + y * y))</span>
  <span class="s3">else</span><span class="s1">:</span>
    <span class="s3">return </span><span class="s1">tf.math.atan2(y</span><span class="s3">, </span><span class="s1">x)</span>


<span class="s1">tf_impl[lax.atan2_p] = _atan2</span>
<span class="s1">tf_impl[lax.acosh_p] = tf.math.acosh</span>
<span class="s1">tf_impl[lax.atanh_p] = tf.math.atanh</span>
<span class="s1">tf_impl[lax.asinh_p] = tf.math.asinh</span>

<span class="s1">tf_impl[lax.sqrt_p] = tf.math.sqrt</span>
<span class="s1">tf_impl[lax.rsqrt_p] = tf.math.rsqrt</span>

<span class="s3">def </span><span class="s1">_cbrt(x):</span>
  <span class="s3">return </span><span class="s1">tf.math.sign(x) * tf.math.pow(tf.math.abs(x)</span><span class="s3">, </span><span class="s5">1</span><span class="s1">/</span><span class="s5">3</span><span class="s1">)</span>

<span class="s1">tf_impl[lax.cbrt_p] = _cbrt</span>

<span class="s1">tf_impl[lax.lgamma_p] = tf.math.lgamma</span>
<span class="s1">tf_impl[lax.digamma_p] = tf.math.digamma</span>
<span class="s1">tf_impl[lax.igamma_p] = tf.math.igamma</span>
<span class="s1">tf_impl[lax.igammac_p] = tf.math.igammac</span>
<span class="s1">tf_impl[lax.regularized_incomplete_beta_p] = tf.math.betainc</span>
<span class="s1">tf_impl[lax.erf_p] = tf.math.erf</span>
<span class="s1">tf_impl[lax.erfc_p] = tf.math.erfc</span>
<span class="s1">tf_impl[lax.erf_inv_p] = tf.math.erfinv</span>
<span class="s1">tf_impl[lax.bessel_i0e_p] = tf.math.bessel_i0e</span>
<span class="s1">tf_impl[lax.bessel_i1e_p] = tf.math.bessel_i1e</span>

<span class="s1">tf_impl[lax.complex_p] = tf.complex</span>


<span class="s3">def </span><span class="s1">_conj(x</span><span class="s3">, </span><span class="s1">**kwargs):</span>
  <span class="s0"># The only dtypes that are allowed are: float32, float64, complex64, and</span>
  <span class="s0"># complex128.</span>
  <span class="s3">if </span><span class="s1">x.dtype == tf.float32:</span>
    <span class="s3">return </span><span class="s1">tf.cast(x</span><span class="s3">, </span><span class="s1">tf.complex64)</span>
  <span class="s3">elif </span><span class="s1">x.dtype == tf.float64:</span>
    <span class="s3">return </span><span class="s1">tf.cast(x</span><span class="s3">, </span><span class="s1">tf.complex128)</span>
  <span class="s3">else</span><span class="s1">:</span>
    <span class="s3">return </span><span class="s1">tf.math.conj(x)</span>


<span class="s1">tf_impl[lax.conj_p] = _conj</span>
<span class="s1">tf_impl[lax.real_p] = tf.math.real</span>
<span class="s1">tf_impl[lax.imag_p] = tf.math.imag</span>

<span class="s1">tf_impl[lax.add_p] = _add</span>
<span class="s1">tf_impl[lax.sub_p] = tf.math.subtract</span>
<span class="s1">tf_impl[lax.mul_p] = tf.math.multiply</span>


<span class="s3">def </span><span class="s1">_iota(*</span><span class="s3">, </span><span class="s1">dtype</span><span class="s3">, </span><span class="s1">shape</span><span class="s3">, </span><span class="s1">dimension):</span>
  <span class="s1">dtype = _to_tf_dtype(dtype)</span>
  <span class="s0"># Some dtypes are unsupported, like uint32, so we just fall back to int32.</span>
  <span class="s0"># TODO(mattjj, necula): improve tf.range dtype handling</span>
  <span class="s1">shape_tf = _eval_shape(shape)</span>
  <span class="s1">vec = tf.range(tf.cast(shape_tf[dimension]</span><span class="s3">, </span><span class="s1">tf.int32)</span><span class="s3">, </span><span class="s1">dtype=tf.int32)</span>
  <span class="s1">vec_shape = [-</span><span class="s5">1 </span><span class="s3">if </span><span class="s1">i == dimension </span><span class="s3">else </span><span class="s5">1 </span><span class="s3">for </span><span class="s1">i </span><span class="s3">in </span><span class="s1">range(len(shape))]</span>
  <span class="s3">return </span><span class="s1">tf.cast(tf.broadcast_to(tf.reshape(vec</span><span class="s3">, </span><span class="s1">vec_shape)</span><span class="s3">, </span><span class="s1">shape_tf)</span><span class="s3">, </span><span class="s1">dtype)</span>


<span class="s1">tf_impl[lax.iota_p] = _iota</span>


<span class="s3">def </span><span class="s1">_div(lhs</span><span class="s3">, </span><span class="s1">rhs):</span>
  <span class="s3">if </span><span class="s1">lhs.dtype.is_integer:</span>
    <span class="s1">quotient = tf.math.floordiv(lhs</span><span class="s3">, </span><span class="s1">rhs)</span>
    <span class="s1">select = tf.math.logical_and(</span>
        <span class="s1">tf.not_equal(_sign(lhs)</span><span class="s3">, </span><span class="s1">_sign(rhs))</span><span class="s3">,</span>
        <span class="s1">tf.not_equal(tf.math.floormod(lhs</span><span class="s3">, </span><span class="s1">rhs)</span><span class="s3">, </span><span class="s5">0</span><span class="s1">))</span>
    <span class="s3">return </span><span class="s1">tf.where(select</span><span class="s3">, </span><span class="s1">quotient + </span><span class="s5">1</span><span class="s3">, </span><span class="s1">quotient)</span>
  <span class="s3">else</span><span class="s1">:</span>
    <span class="s3">return </span><span class="s1">tf.math.truediv(lhs</span><span class="s3">, </span><span class="s1">rhs)</span>


<span class="s3">def </span><span class="s1">_rem(lhs</span><span class="s3">, </span><span class="s1">rhs):</span>
  <span class="s3">return </span><span class="s1">_sign(lhs) * tf.math.floormod(_abs(lhs)</span><span class="s3">, </span><span class="s1">_abs(rhs))</span>


<span class="s1">tf_impl[lax.div_p] = _div</span>
<span class="s1">tf_impl[lax.rem_p] = _rem</span>


<span class="s3">def </span><span class="s1">_minmax(x: TfVal</span><span class="s3">, </span><span class="s1">y: TfVal</span><span class="s3">, </span><span class="s1">*</span><span class="s3">, </span><span class="s1">is_min: bool</span><span class="s3">,</span>
            <span class="s1">_in_avals: Sequence[core.ShapedArray]</span><span class="s3">,</span>
            <span class="s1">_out_aval: core.ShapedArray</span><span class="s3">,</span><span class="s1">) -&gt; TfVal:</span>
  <span class="s0"># For complex numbers use lexicographic ordering, like JAX</span>
  <span class="s3">if </span><span class="s1">dtypes.issubdtype(x.dtype.as_numpy_dtype</span><span class="s3">, </span><span class="s1">np.complexfloating):</span>
    <span class="s3">return </span><span class="s1">_convert_jax_impl(</span>
        <span class="s1">partial(lax_internal._minmax_complex_lowering</span><span class="s3">,</span>
                <span class="s1">lax_cmp_pick_x=lax.lt </span><span class="s3">if </span><span class="s1">is_min </span><span class="s3">else </span><span class="s1">lax.gt)</span><span class="s3">,</span>
        <span class="s1">multiple_results=</span><span class="s3">False</span><span class="s1">)(x</span><span class="s3">, </span><span class="s1">y</span><span class="s3">, </span><span class="s1">_in_avals=_in_avals</span><span class="s3">, </span><span class="s1">_out_aval=_out_aval)</span>
  <span class="s3">elif </span><span class="s1">x.dtype.as_numpy_dtype == np.bool_:</span>
    <span class="s3">return </span><span class="s1">(tf.math.logical_and </span><span class="s3">if </span><span class="s1">is_min </span><span class="s3">else </span><span class="s1">tf.math.logical_or)(x</span><span class="s3">, </span><span class="s1">y)</span>
  <span class="s3">else</span><span class="s1">:</span>
    <span class="s3">return </span><span class="s1">(tf.math.minimum </span><span class="s3">if </span><span class="s1">is_min </span><span class="s3">else </span><span class="s1">tf.math.maximum)(x</span><span class="s3">, </span><span class="s1">y)</span>

<span class="s3">def </span><span class="s1">_minmax_scalar(x: TfVal</span><span class="s3">, </span><span class="s1">y: TfVal</span><span class="s3">, </span><span class="s1">*</span><span class="s3">, </span><span class="s1">is_min: bool) -&gt; TfVal:</span>
  <span class="s0"># For reducers we will need min/max for scalars only. In that case we</span>
  <span class="s0"># can construct the AbstractValues outselves, even in the presence of</span>
  <span class="s0"># shape polymorphism.</span>
  <span class="s3">assert </span><span class="s1">len(x.shape) == </span><span class="s5">0 </span><span class="s3">and </span><span class="s1">len(y.shape) == </span><span class="s5">0</span><span class="s3">, </span><span class="s4">f&quot;x: </span><span class="s3">{</span><span class="s1">x.shape</span><span class="s3">}</span><span class="s4">, y: </span><span class="s3">{</span><span class="s1">y.shape</span><span class="s3">}</span><span class="s4">&quot;</span>
  <span class="s1">aval = core.ShapedArray(()</span><span class="s3">, </span><span class="s1">_to_jax_dtype(x.dtype))</span>
  <span class="s3">return </span><span class="s1">_minmax(x</span><span class="s3">, </span><span class="s1">y</span><span class="s3">, </span><span class="s1">is_min=is_min</span><span class="s3">,</span>
                 <span class="s1">_in_avals=[aval</span><span class="s3">, </span><span class="s1">aval]</span><span class="s3">, </span><span class="s1">_out_aval=aval)</span>

<span class="s1">tf_impl_with_avals[lax.max_p] = partial(_minmax</span><span class="s3">, </span><span class="s1">is_min=</span><span class="s3">False</span><span class="s1">)</span>
<span class="s1">tf_impl_with_avals[lax.min_p] = partial(_minmax</span><span class="s3">, </span><span class="s1">is_min=</span><span class="s3">True</span><span class="s1">)</span>

<span class="s0"># Map from TF signed types to TF unsigned types.</span>
<span class="s1">_SIGNED_TO_UNSIGNED_TABLE = {</span>
    <span class="s1">tf.int8: tf.uint8</span><span class="s3">,</span>
    <span class="s1">tf.int16: tf.uint16</span><span class="s3">,</span>
    <span class="s1">tf.int32: tf.uint32</span><span class="s3">,</span>
    <span class="s1">tf.int64: tf.uint64</span><span class="s3">,</span>
<span class="s1">}</span>

<span class="s0"># Map from TF unsigned types to TF signed types.</span>
<span class="s1">_UNSIGNED_TO_SIGNED_TABLE = {u: s </span><span class="s3">for </span><span class="s1">s</span><span class="s3">, </span><span class="s1">u </span><span class="s3">in </span><span class="s1">_SIGNED_TO_UNSIGNED_TABLE.items()}</span>


<span class="s0"># Note: Bitwise operations only yield identical results on unsigned integers!</span>
<span class="s0"># pylint: disable=protected-access</span>
<span class="s3">def </span><span class="s1">_shift_right_arithmetic_raw(x</span><span class="s3">, </span><span class="s1">y):</span>
  <span class="s3">if </span><span class="s1">x.dtype.is_unsigned:</span>
    <span class="s3">assert </span><span class="s1">x.dtype == y.dtype</span>
    <span class="s1">orig_dtype = x.dtype</span>
    <span class="s1">signed_dtype = _UNSIGNED_TO_SIGNED_TABLE[orig_dtype]</span>
    <span class="s1">x = tf.cast(x</span><span class="s3">, </span><span class="s1">signed_dtype)</span>
    <span class="s1">y = tf.cast(y</span><span class="s3">, </span><span class="s1">signed_dtype)</span>
    <span class="s1">res = tf.bitwise.right_shift(x</span><span class="s3">, </span><span class="s1">y)</span>
    <span class="s3">return </span><span class="s1">tf.cast(res</span><span class="s3">, </span><span class="s1">orig_dtype)</span>
  <span class="s3">else</span><span class="s1">:</span>
    <span class="s3">return </span><span class="s1">tf.bitwise.right_shift(x</span><span class="s3">, </span><span class="s1">y)</span>


<span class="s3">def </span><span class="s1">_shift_right_arithmetic(x</span><span class="s3">, </span><span class="s1">y):</span>
  <span class="s0"># TF shift is &quot;implementation defined&quot; if the shift amount is negative</span>
  <span class="s0"># or larger or equal to the size of the value. We implement the XLA</span>
  <span class="s0"># semantics to return the shift by the max value (x_bits - 1).</span>
  <span class="s0"># TODO: it is likely better to add XlaOps for shifts</span>
  <span class="s1">x_bits = </span><span class="s5">8 </span><span class="s1">* x.dtype.size</span>
  <span class="s1">clamp_y = tf.where(_shift_in_bounds(x</span><span class="s3">, </span><span class="s1">y)</span><span class="s3">, </span><span class="s1">y</span><span class="s3">, </span><span class="s1">x_bits - </span><span class="s5">1</span><span class="s1">)</span>
  <span class="s3">return </span><span class="s1">_shift_right_arithmetic_raw(x</span><span class="s3">, </span><span class="s1">clamp_y)</span>


<span class="s1">tf_impl[lax.shift_right_arithmetic_p] = _shift_right_arithmetic</span>


<span class="s3">def </span><span class="s1">_shift_right_logical_raw(x</span><span class="s3">, </span><span class="s1">y):</span>
  <span class="s3">if </span><span class="s1">x.dtype.is_unsigned:</span>
    <span class="s3">return </span><span class="s1">tf.bitwise.right_shift(x</span><span class="s3">, </span><span class="s1">y)</span>
  <span class="s3">else</span><span class="s1">:</span>
    <span class="s3">assert </span><span class="s1">x.dtype == y.dtype</span>
    <span class="s1">orig_dtype = x.dtype</span>
    <span class="s1">unsigned_dtype = _SIGNED_TO_UNSIGNED_TABLE[orig_dtype]</span>
    <span class="s1">x = tf.cast(x</span><span class="s3">, </span><span class="s1">unsigned_dtype)</span>
    <span class="s1">y = tf.cast(y</span><span class="s3">, </span><span class="s1">unsigned_dtype)</span>
    <span class="s1">res = tf.bitwise.right_shift(x</span><span class="s3">, </span><span class="s1">y)</span>
    <span class="s3">return </span><span class="s1">tf.cast(res</span><span class="s3">, </span><span class="s1">orig_dtype)</span>


<span class="s3">def </span><span class="s1">_shift_right_logical(x</span><span class="s3">, </span><span class="s1">y):</span>
  <span class="s0"># TF shift is &quot;implementation defined&quot; if the shift amount is negative</span>
  <span class="s0"># or larger or equal to the size of the value. We implement the XLA semantics</span>
  <span class="s0"># to return 0.</span>
  <span class="s0"># TODO: it is likely better to add XlaOps for shifts</span>
  <span class="s3">return </span><span class="s1">tf.where(</span>
      <span class="s1">_shift_in_bounds(x</span><span class="s3">, </span><span class="s1">y)</span><span class="s3">, </span><span class="s1">_shift_right_logical_raw(x</span><span class="s3">, </span><span class="s1">y)</span><span class="s3">, </span><span class="s1">tf.zeros_like(x))</span>


<span class="s1">tf_impl[lax.shift_right_logical_p] = _shift_right_logical</span>


<span class="s3">def </span><span class="s1">_shift_left(x</span><span class="s3">, </span><span class="s1">y):</span>
  <span class="s0"># TF shift is &quot;implementation defined&quot; if the shift amount is negative</span>
  <span class="s0"># or larger or equal to the size of the value. We implement the XLA semantics</span>
  <span class="s0"># to return 0.</span>
  <span class="s0"># TODO: it is likely better to add XlaOps for shifts</span>
  <span class="s3">return </span><span class="s1">tf.where(</span>
      <span class="s1">_shift_in_bounds(x</span><span class="s3">, </span><span class="s1">y)</span><span class="s3">, </span><span class="s1">tf.bitwise.left_shift(x</span><span class="s3">, </span><span class="s1">y)</span><span class="s3">, </span><span class="s1">tf.zeros_like(x))</span>


<span class="s1">tf_impl[lax.shift_left_p] = _shift_left</span>


<span class="s3">def </span><span class="s1">_shift_in_bounds(x: TfVal</span><span class="s3">, </span><span class="s1">y: TfVal) -&gt; TfVal:</span>
  <span class="s0"># Return the TF expression for when y is within bounds (0 &lt;= y &lt; |x|)</span>
  <span class="s1">x_bits = </span><span class="s5">8 </span><span class="s1">* x.dtype.size</span>
  <span class="s0"># TF does not have comparisons for uint16 and uint32 (despite what the</span>
  <span class="s0"># documentation says)</span>
  <span class="s1">y_comp = tf.cast(</span>
      <span class="s1">y</span><span class="s3">, </span><span class="s1">_UNSIGNED_TO_SIGNED_TABLE[y.dtype]) </span><span class="s3">if </span><span class="s1">y.dtype.is_unsigned </span><span class="s3">else </span><span class="s1">y</span>
  <span class="s1">y_lt_x_bits = tf.math.less(y_comp</span><span class="s3">, </span><span class="s1">x_bits)</span>
  <span class="s1">y_ge_0 = tf.math.greater_equal(y_comp</span><span class="s3">, </span><span class="s5">0</span><span class="s1">)</span>
  <span class="s3">return </span><span class="s1">tf.logical_and(y_lt_x_bits</span><span class="s3">, </span><span class="s1">y_ge_0)</span>


<span class="s3">def </span><span class="s1">_not(x):</span>
  <span class="s2">&quot;&quot;&quot;Computes bitwise not with support for booleans. 
 
  Numpy and JAX support bitwise not for booleans by applying a logical not! 
  This means that applying bitwise_not yields an unexpected result: 
    jnp.bitwise_not(jnp.array([True, False])) 
    &gt;&gt; DeviceArray([False,  True], dtype=bool) 
 
  if you assume that booleans are simply casted to integers. 
    jnp.bitwise_not(jnp.array([True, False]).astype(np.int32)).astype(bool) 
    &gt;&gt; DeviceArray([True,  True], dtype=bool) 
  &quot;&quot;&quot;</span>
  <span class="s3">if </span><span class="s1">x.dtype == tf.bool:</span>
    <span class="s3">return </span><span class="s1">tf.logical_not(x)</span>
  <span class="s3">else</span><span class="s1">:</span>
    <span class="s3">return </span><span class="s1">tf.bitwise.invert(x)</span>


<span class="s1">tf_impl[lax.not_p] = _not</span>


<span class="s3">def </span><span class="s1">handle_boolean_args(f</span><span class="s3">, </span><span class="s1">argnums: Sequence[int]</span><span class="s3">, </span><span class="s1">boolean_f=</span><span class="s3">None</span><span class="s1">):</span>
  <span class="s2">&quot;&quot;&quot;Computes functions with some bool args and bool results using int8. 
 
  This is needed because some TF ops do not work for bool args, e.g., 
  inequalities, min/max. 
 
  Args: 
    f: a TF callable to wrap. It will be called with non-boolean arguments. 
    argnums: the positional arguments that may be booleans. 
    boolean_f: [Optional] a TF callable compatible with boolean 
      arguments. 
 
  Returns: a TF callable that can take a mix of boolean positional arguments 
    (in the positions specified by `argnums`) and some non-boolean positional 
    arguments. If there are no boolean arguments, just calls `f`. Otherwise, 
    it calls `boolean_f` if defined. Otherwise, casts the boolean 
    arguments to `int8`, calls `f`, then casts the result to `bool`. 
  &quot;&quot;&quot;</span>
  <span class="s1">argnums = tf.nest.flatten(argnums)</span>

  <span class="s3">def </span><span class="s1">wrapper(*args: TfVal</span><span class="s3">, </span><span class="s1">**kwargs):</span>
    <span class="s1">argnum_types = {args[i].dtype </span><span class="s3">for </span><span class="s1">i </span><span class="s3">in </span><span class="s1">argnums}</span>
    <span class="s3">if </span><span class="s1">tf.bool </span><span class="s3">not in </span><span class="s1">argnum_types:</span>
      <span class="s3">return </span><span class="s1">f(*args</span><span class="s3">, </span><span class="s1">**kwargs)</span>
    <span class="s3">else</span><span class="s1">:</span>
      <span class="s0"># All argnums should be boolean</span>
      <span class="s3">assert </span><span class="s1">len(argnum_types) == </span><span class="s5">1</span><span class="s3">, </span><span class="s1">argnum_types</span>
      <span class="s3">if </span><span class="s1">boolean_f != </span><span class="s3">None</span><span class="s1">:</span>
        <span class="s3">return </span><span class="s1">boolean_f(*args</span><span class="s3">, </span><span class="s1">**kwargs)</span>
      <span class="s3">else</span><span class="s1">:</span>
        <span class="s1">args_cast = [(tf.cast(a</span><span class="s3">, </span><span class="s1">tf.int8) </span><span class="s3">if </span><span class="s1">i </span><span class="s3">in </span><span class="s1">argnums </span><span class="s3">else </span><span class="s1">a)</span>
                    <span class="s3">for </span><span class="s1">i</span><span class="s3">, </span><span class="s1">a </span><span class="s3">in </span><span class="s1">enumerate(args)]</span>
        <span class="s3">if </span><span class="s4">&quot;_in_avals&quot; </span><span class="s3">in </span><span class="s1">kwargs:</span>

          <span class="s3">def </span><span class="s1">cast_aval(aval):</span>
            <span class="s3">assert </span><span class="s1">aval.dtype == np.bool_</span>
            <span class="s3">return </span><span class="s1">core.ShapedArray(aval.shape</span><span class="s3">, </span><span class="s1">np.int8)</span>

          <span class="s1">_in_avals_cast = [</span>
              <span class="s1">cast_aval(aval) </span><span class="s3">if </span><span class="s1">i </span><span class="s3">in </span><span class="s1">argnums </span><span class="s3">else </span><span class="s1">aval</span>
              <span class="s3">for </span><span class="s1">i</span><span class="s3">, </span><span class="s1">aval </span><span class="s3">in </span><span class="s1">enumerate(kwargs[</span><span class="s4">&quot;_in_avals&quot;</span><span class="s1">])</span>
          <span class="s1">]</span>
          <span class="s1">_out_aval_cast = tf.nest.map_structure(cast_aval</span><span class="s3">, </span><span class="s1">kwargs[</span><span class="s4">&quot;_out_aval&quot;</span><span class="s1">])</span>
          <span class="s1">kwargs = dict(</span>
              <span class="s1">kwargs</span><span class="s3">, </span><span class="s1">_in_avals=_in_avals_cast</span><span class="s3">, </span><span class="s1">_out_aval=_out_aval_cast)</span>
        <span class="s1">out = f(*args_cast</span><span class="s3">, </span><span class="s1">**kwargs)</span>
        <span class="s3">return </span><span class="s1">tf.nest.map_structure(</span><span class="s3">lambda </span><span class="s1">o: tf.cast(o</span><span class="s3">, </span><span class="s1">tf.bool)</span><span class="s3">, </span><span class="s1">out)</span>

  <span class="s3">return </span><span class="s1">wrapper</span>


<span class="s1">tf_impl[lax.or_p] = handle_boolean_args(tf.bitwise.bitwise_or</span><span class="s3">, </span><span class="s1">argnums=(</span><span class="s5">0</span><span class="s3">, </span><span class="s5">1</span><span class="s1">)</span><span class="s3">, </span><span class="s1">boolean_f=tf.logical_or)</span>
<span class="s1">tf_impl[lax.and_p] = handle_boolean_args(tf.bitwise.bitwise_and</span><span class="s3">, </span><span class="s1">argnums=(</span><span class="s5">0</span><span class="s3">, </span><span class="s5">1</span><span class="s1">)</span><span class="s3">, </span><span class="s1">boolean_f=tf.logical_and)</span>
<span class="s1">tf_impl[lax.xor_p] = handle_boolean_args(tf.bitwise.bitwise_xor</span><span class="s3">, </span><span class="s1">argnums=(</span><span class="s5">0</span><span class="s3">, </span><span class="s5">1</span><span class="s1">)</span><span class="s3">, </span><span class="s1">boolean_f=tf.math.logical_xor)</span>

<span class="s1">tf_impl[lax.eq_p] = tf.math.equal</span>
<span class="s1">tf_impl[lax.ne_p] = tf.math.not_equal</span>

<span class="s1">boolean_greater = </span><span class="s3">lambda </span><span class="s1">x</span><span class="s3">,</span><span class="s1">y: tf.logical_and(x</span><span class="s3">, </span><span class="s1">tf.logical_not(y)) </span><span class="s0"># Only one combo: T,F -&gt; T</span>
<span class="s1">boolean_less = </span><span class="s3">lambda </span><span class="s1">x</span><span class="s3">,</span><span class="s1">y: tf.logical_and(tf.logical_not(x)</span><span class="s3">, </span><span class="s1">y) </span><span class="s0"># Only one combo: F,T -&gt; T</span>
<span class="s1">boolean_greater_or_equal = </span><span class="s3">lambda </span><span class="s1">x</span><span class="s3">, </span><span class="s1">y: tf.logical_not(boolean_less(x</span><span class="s3">,</span><span class="s1">y)) </span><span class="s0"># All cases except F,T</span>
<span class="s1">boolean_less_or_equal = </span><span class="s3">lambda </span><span class="s1">x</span><span class="s3">, </span><span class="s1">y: tf.logical_not(boolean_greater(x</span><span class="s3">,</span><span class="s1">y)) </span><span class="s0"># All cases except T,F</span>

<span class="s1">tf_impl[lax.gt_p] = handle_boolean_args(tf.math.greater</span><span class="s3">, </span><span class="s1">argnums=(</span><span class="s5">0</span><span class="s3">, </span><span class="s5">1</span><span class="s1">)</span><span class="s3">, </span><span class="s1">boolean_f=boolean_greater)</span>
<span class="s1">tf_impl[lax.lt_p] = handle_boolean_args(tf.math.less</span><span class="s3">, </span><span class="s1">argnums=(</span><span class="s5">0</span><span class="s3">, </span><span class="s5">1</span><span class="s1">)</span><span class="s3">, </span><span class="s1">boolean_f=boolean_less)</span>
<span class="s1">tf_impl[lax.ge_p] = handle_boolean_args(tf.math.greater_equal</span><span class="s3">, </span><span class="s1">argnums=(</span><span class="s5">0</span><span class="s3">, </span><span class="s5">1</span><span class="s1">)</span><span class="s3">, </span><span class="s1">boolean_f=boolean_greater_or_equal)</span>
<span class="s1">tf_impl[lax.le_p] = handle_boolean_args(tf.math.less_equal</span><span class="s3">, </span><span class="s1">argnums=(</span><span class="s5">0</span><span class="s3">, </span><span class="s5">1</span><span class="s1">)</span><span class="s3">, </span><span class="s1">boolean_f=boolean_less_or_equal)</span>

<span class="s1">tf_impl[lax.linalg.cholesky_p] = tf.linalg.cholesky</span>


<span class="s3">def </span><span class="s1">_convert_element_type(operand</span><span class="s3">, </span><span class="s1">*</span><span class="s3">, </span><span class="s1">new_dtype</span><span class="s3">, </span><span class="s1">weak_type=</span><span class="s3">False</span><span class="s1">):</span>
  <span class="s1">old_dtype = operand.dtype.as_numpy_dtype</span>
  <span class="s3">if </span><span class="s1">(dtypes.issubdtype(old_dtype</span><span class="s3">, </span><span class="s1">np.complexfloating) </span><span class="s3">and</span>
      <span class="s3">not </span><span class="s1">dtypes.issubdtype(new_dtype</span><span class="s3">, </span><span class="s1">np.complexfloating)):</span>
    <span class="s1">operand = tf.math.real(operand)</span>
  <span class="s3">if </span><span class="s1">(dtypes.issubdtype(old_dtype</span><span class="s3">, </span><span class="s1">np.floating) </span><span class="s3">and</span>
      <span class="s3">not </span><span class="s1">(dtypes.issubdtype(new_dtype</span><span class="s3">, </span><span class="s1">np.floating) </span><span class="s3">or </span><span class="s1">dtypes.issubdtype(</span>
          <span class="s1">new_dtype</span><span class="s3">, </span><span class="s1">np.complexfloating) </span><span class="s3">or </span><span class="s1">new_dtype == np.bool_)):</span>
    <span class="s1">sign = _sign(operand)</span>
    <span class="s1">operand = sign * tf.math.floor(sign * operand)</span>
  <span class="s3">return </span><span class="s1">tf.dtypes.cast(operand</span><span class="s3">, </span><span class="s1">_to_tf_dtype(new_dtype))</span>


<span class="s1">tf_impl[lax.convert_element_type_p] = _convert_element_type</span>


<span class="s3">def </span><span class="s1">_bitcast_convert_type(operand</span><span class="s3">, </span><span class="s1">new_dtype):</span>
  <span class="s3">if </span><span class="s1">operand.dtype == new_dtype:</span>
    <span class="s3">return </span><span class="s1">operand</span>
  <span class="s3">return </span><span class="s1">tf.bitcast(operand</span><span class="s3">, </span><span class="s1">_to_tf_dtype(new_dtype))</span>


<span class="s1">tf_impl[lax.bitcast_convert_type_p] = _bitcast_convert_type</span>


<span class="s3">def </span><span class="s1">_clamp(minval</span><span class="s3">, </span><span class="s1">operand</span><span class="s3">, </span><span class="s1">maxval</span><span class="s3">, </span><span class="s1">*</span><span class="s3">, </span><span class="s1">_in_avals</span><span class="s3">, </span><span class="s1">_out_aval):</span>
  <span class="s0"># The below permits mirroring the behavior of JAX when maxval &lt; minval</span>
  <span class="s1">op_shape_tf_val = _eval_shape(_in_avals[</span><span class="s5">1</span><span class="s1">].shape</span><span class="s3">, </span><span class="s1">_in_avals[</span><span class="s5">1</span><span class="s1">].dtype)</span>
  <span class="s1">maxval = tf.broadcast_to(maxval</span><span class="s3">, </span><span class="s1">op_shape_tf_val)</span>
  <span class="s1">minval = tf.math.minimum(tf.broadcast_to(minval</span><span class="s3">, </span><span class="s1">op_shape_tf_val)</span><span class="s3">, </span><span class="s1">maxval)</span>
  <span class="s3">return </span><span class="s1">tf.clip_by_value(operand</span><span class="s3">, </span><span class="s1">minval</span><span class="s3">, </span><span class="s1">maxval)</span>


<span class="s1">tf_impl_with_avals[lax.clamp_p] = _clamp</span>


<span class="s3">def </span><span class="s1">_concatenate(*operands</span><span class="s3">, </span><span class="s1">dimension):</span>
  <span class="s3">return </span><span class="s1">tf.concat(operands</span><span class="s3">, </span><span class="s1">axis=dimension)</span>


<span class="s1">tf_impl[lax.concatenate_p] = _concatenate</span>


<span class="s3">def </span><span class="s1">_conv_general_dimension_numbers_proto(dimension_numbers):</span>
  <span class="s2">&quot;&quot;&quot;Converts a ConvDimensionNumbers to an XLA ConvolutionDimensionNumbers.&quot;&quot;&quot;</span>
  <span class="s3">assert </span><span class="s1">isinstance(dimension_numbers</span><span class="s3">, </span><span class="s1">lax.ConvDimensionNumbers)</span>
  <span class="s1">lhs_spec</span><span class="s3">, </span><span class="s1">rhs_spec</span><span class="s3">, </span><span class="s1">out_spec = dimension_numbers</span>
  <span class="s1">proto = xla_data_pb2.ConvolutionDimensionNumbers()</span>
  <span class="s1">proto.input_batch_dimension = lhs_spec[</span><span class="s5">0</span><span class="s1">]</span>
  <span class="s1">proto.input_feature_dimension = lhs_spec[</span><span class="s5">1</span><span class="s1">]</span>
  <span class="s1">proto.output_batch_dimension = out_spec[</span><span class="s5">0</span><span class="s1">]</span>
  <span class="s1">proto.output_feature_dimension = out_spec[</span><span class="s5">1</span><span class="s1">]</span>
  <span class="s1">proto.kernel_output_feature_dimension = rhs_spec[</span><span class="s5">0</span><span class="s1">]</span>
  <span class="s1">proto.kernel_input_feature_dimension = rhs_spec[</span><span class="s5">1</span><span class="s1">]</span>
  <span class="s1">proto.input_spatial_dimensions.extend(lhs_spec[</span><span class="s5">2</span><span class="s1">:])</span>
  <span class="s1">proto.kernel_spatial_dimensions.extend(rhs_spec[</span><span class="s5">2</span><span class="s1">:])</span>
  <span class="s1">proto.output_spatial_dimensions.extend(out_spec[</span><span class="s5">2</span><span class="s1">:])</span>
  <span class="s3">return </span><span class="s1">proto</span>


<span class="s3">def </span><span class="s1">_precision_config_proto(precision: Optional[Tuple[PrecisionType</span><span class="s3">,</span>
                                                      <span class="s1">PrecisionType]]):</span>
  <span class="s2">&quot;&quot;&quot;Convert an integer to an XLA.PrecisionConfig.&quot;&quot;&quot;</span>
  <span class="s3">if </span><span class="s1">precision </span><span class="s3">is None</span><span class="s1">:</span>
    <span class="s3">return None</span>

  <span class="s1">proto = xla_data_pb2.PrecisionConfig()</span>
  <span class="s1">proto.operand_precision.append(int(precision[</span><span class="s5">0</span><span class="s1">]))</span>
  <span class="s1">proto.operand_precision.append(int(precision[</span><span class="s5">1</span><span class="s1">]))</span>
  <span class="s3">return </span><span class="s1">proto</span>


<span class="s3">def </span><span class="s1">_conv_general_dilated(lhs</span><span class="s3">, </span><span class="s1">rhs</span><span class="s3">, </span><span class="s1">*</span><span class="s3">,</span>
                          <span class="s1">window_strides</span><span class="s3">, </span><span class="s1">padding</span><span class="s3">, </span><span class="s1">lhs_dilation</span><span class="s3">,</span>
                          <span class="s1">rhs_dilation</span><span class="s3">,</span>
                          <span class="s1">dimension_numbers: lax.ConvDimensionNumbers</span><span class="s3">,</span>
                          <span class="s1">feature_group_count: int</span><span class="s3">,</span>
                          <span class="s1">batch_group_count: int</span><span class="s3">,</span>
                          <span class="s1">precision: Optional[Tuple[PrecisionType</span><span class="s3">, </span><span class="s1">PrecisionType]]</span><span class="s3">,</span>
                          <span class="s1">preferred_element_type: Optional[DType]</span><span class="s3">,</span>
                          <span class="s1">_in_avals: Sequence[core.ShapedArray]</span><span class="s3">,</span>
                          <span class="s1">_out_aval: core.ShapedArray):</span>
  <span class="s2">&quot;&quot;&quot;Implementation of lax.conv_general_dilated_p using XlaConv.&quot;&quot;&quot;</span>
  <span class="s1">out_tf_shape = _aval_to_tf_shape(_out_aval)</span>
  <span class="s1">dnums_proto = _conv_general_dimension_numbers_proto(dimension_numbers)</span>
  <span class="s1">precision_config_proto = _precision_config_proto(precision)</span>

  <span class="s3">def </span><span class="s1">gen_conv(lhs</span><span class="s3">, </span><span class="s1">rhs</span><span class="s3">, </span><span class="s1">preferred_element_type: Optional[DType]):</span>
    <span class="s1">tf_version = tuple(int(v) </span><span class="s3">for </span><span class="s1">v </span><span class="s3">in </span><span class="s1">tf.__version__.split(</span><span class="s4">&quot;.&quot;</span><span class="s1">)[:</span><span class="s5">2</span><span class="s1">])</span>
    <span class="s3">if </span><span class="s1">tf_version &gt;= (</span><span class="s5">2</span><span class="s3">, </span><span class="s5">8</span><span class="s1">):</span>
      <span class="s0"># TODO(necula): remove when 2.8.0 is the stable TF version (and supports</span>
      <span class="s0"># batch_group_count.</span>
      <span class="s1">padding_tf = [_eval_shape(p) </span><span class="s3">for </span><span class="s1">p </span><span class="s3">in </span><span class="s1">padding]</span>
      <span class="s1">out = tfxla.conv(</span>
          <span class="s1">lhs</span><span class="s3">, </span><span class="s1">rhs</span><span class="s3">, </span><span class="s1">window_strides</span><span class="s3">, </span><span class="s1">padding_tf</span><span class="s3">, </span><span class="s1">lhs_dilation</span><span class="s3">, </span><span class="s1">rhs_dilation</span><span class="s3">,</span>
          <span class="s1">dnums_proto</span><span class="s3">,</span>
          <span class="s1">feature_group_count=feature_group_count</span><span class="s3">,</span>
          <span class="s1">batch_group_count=batch_group_count</span><span class="s3">,</span>
          <span class="s1">precision_config=precision_config_proto</span><span class="s3">,</span>
          <span class="s1">preferred_element_type=preferred_element_type</span><span class="s3">,</span>
          <span class="s1">use_v2=</span><span class="s3">True</span><span class="s1">)</span>
    <span class="s3">else</span><span class="s1">:</span>
      <span class="s3">if </span><span class="s1">batch_group_count != </span><span class="s5">1</span><span class="s1">:</span>
        <span class="s3">raise </span><span class="s1">ValueError(</span>
            <span class="s4">&quot;The batch_group_count parameter for conv requires TF version &quot;</span>
            <span class="s4">&quot;at least 2.8.0. You may want to use tf-nightly.&quot;</span><span class="s1">)</span>
      <span class="s1">padding_tf = [_eval_shape(p) </span><span class="s3">for </span><span class="s1">p </span><span class="s3">in </span><span class="s1">padding]</span>
      <span class="s1">out = tfxla.conv(</span>
          <span class="s1">lhs</span><span class="s3">, </span><span class="s1">rhs</span><span class="s3">, </span><span class="s1">window_strides</span><span class="s3">, </span><span class="s1">padding_tf</span><span class="s3">, </span><span class="s1">lhs_dilation</span><span class="s3">, </span><span class="s1">rhs_dilation</span><span class="s3">,</span>
          <span class="s1">dnums_proto</span><span class="s3">,</span>
          <span class="s1">feature_group_count=feature_group_count</span><span class="s3">,</span>
          <span class="s1">precision_config=precision_config_proto</span><span class="s3">,</span>
          <span class="s1">preferred_element_type=preferred_element_type</span><span class="s3">,</span>
          <span class="s1">use_v2=</span><span class="s3">True</span><span class="s1">)</span>
    <span class="s0"># TODO: implement shape inference for XlaConv</span>
    <span class="s1">out = _ensure_tf_shape_if_dynamic(out</span><span class="s3">, </span><span class="s1">out_tf_shape)</span>
    <span class="s3">if </span><span class="s1">_WRAP_JAX_JIT_WITH_TF_FUNCTION:</span>
      <span class="s1">out = tf.stop_gradient(out)  </span><span class="s0"># See #7839</span>
    <span class="s3">return </span><span class="s1">out</span>

  <span class="s0"># Follow the lowering for complex convolutions from</span>
  <span class="s0"># lax._conv_general_dilated_translation. We can use the same conversion on all</span>
  <span class="s0"># platforms because on XLA:TPU the compiler does the same as a rewrite.</span>
  <span class="s1">preferred_float_et: Optional[Any]</span>
  <span class="s3">if </span><span class="s1">np.issubdtype(_in_avals[</span><span class="s5">0</span><span class="s1">].dtype</span><span class="s3">, </span><span class="s1">np.complexfloating):</span>
    <span class="s3">if </span><span class="s1">preferred_element_type </span><span class="s3">is not None</span><span class="s1">:</span>
      <span class="s0"># Convert complex dtype to types used for real and imaginary parts</span>
      <span class="s3">assert </span><span class="s1">np.issubdtype(preferred_element_type</span><span class="s3">, </span><span class="s1">np.complexfloating)</span>
      <span class="s1">preferred_float_et = (</span>
          <span class="s1">np.float64 </span><span class="s3">if </span><span class="s1">preferred_element_type == np.complex128 </span><span class="s3">else </span><span class="s1">np.float32)</span>
    <span class="s3">else</span><span class="s1">:</span>
      <span class="s1">preferred_float_et = </span><span class="s3">None</span>
    <span class="s1">lhs_real</span><span class="s3">, </span><span class="s1">lhs_imag = tf.math.real(lhs)</span><span class="s3">, </span><span class="s1">tf.math.imag(lhs)</span>
    <span class="s1">rhs_real</span><span class="s3">, </span><span class="s1">rhs_imag = tf.math.real(rhs)</span><span class="s3">, </span><span class="s1">tf.math.imag(rhs)</span>
    <span class="s1">k1 = gen_conv(_add(lhs_real</span><span class="s3">, </span><span class="s1">lhs_imag)</span><span class="s3">, </span><span class="s1">rhs_real</span><span class="s3">, </span><span class="s1">preferred_float_et)</span>
    <span class="s1">k2 = gen_conv(lhs_real</span><span class="s3">, </span><span class="s1">tf.math.subtract(rhs_imag</span><span class="s3">, </span><span class="s1">rhs_real)</span><span class="s3">,</span>
                  <span class="s1">preferred_float_et)</span>
    <span class="s1">k3 = gen_conv(lhs_imag</span><span class="s3">, </span><span class="s1">_add(rhs_real</span><span class="s3">, </span><span class="s1">rhs_imag)</span><span class="s3">, </span><span class="s1">preferred_float_et)</span>
    <span class="s3">return </span><span class="s1">tf.complex(tf.math.subtract(k1</span><span class="s3">, </span><span class="s1">k3)</span><span class="s3">, </span><span class="s1">_add(k1</span><span class="s3">, </span><span class="s1">k2))</span>
  <span class="s3">else</span><span class="s1">:</span>
    <span class="s3">return </span><span class="s1">gen_conv(lhs</span><span class="s3">, </span><span class="s1">rhs</span><span class="s3">, </span><span class="s1">preferred_element_type)</span>


<span class="s1">tf_impl_with_avals[lax.conv_general_dilated_p] = _conv_general_dilated</span>


<span class="s3">def </span><span class="s1">_dot_general(lhs</span><span class="s3">, </span><span class="s1">rhs</span><span class="s3">, </span><span class="s1">*</span><span class="s3">, </span><span class="s1">dimension_numbers</span><span class="s3">,</span>
                 <span class="s1">precision: Optional[Tuple[PrecisionType</span><span class="s3">, </span><span class="s1">PrecisionType]]</span><span class="s3">,</span>
                 <span class="s1">preferred_element_type: Optional[DType]</span><span class="s3">,</span>
                 <span class="s1">_in_avals: Sequence[core.ShapedArray]</span><span class="s3">,</span>
                 <span class="s1">_out_aval: core.ShapedArray):</span>
  <span class="s2">&quot;&quot;&quot;Implementation of lax.dot_general_p in terms of tf.linalg.einsum.&quot;&quot;&quot;</span>
  <span class="s1">(lhs_contracting</span><span class="s3">, </span><span class="s1">rhs_contracting)</span><span class="s3">, </span><span class="s1">(lhs_batch</span><span class="s3">, </span><span class="s1">rhs_batch) = dimension_numbers</span>
  <span class="s1">dnums_proto = xla_data_pb2.DotDimensionNumbers()</span>
  <span class="s1">dnums_proto.lhs_contracting_dimensions.extend(lhs_contracting)</span>
  <span class="s1">dnums_proto.rhs_contracting_dimensions.extend(rhs_contracting)</span>
  <span class="s1">dnums_proto.lhs_batch_dimensions.extend(lhs_batch)</span>
  <span class="s1">dnums_proto.rhs_batch_dimensions.extend(rhs_batch)</span>
  <span class="s1">precision_config_proto = _precision_config_proto(precision)</span>
  <span class="s1">res = tfxla.dot_general(</span>
      <span class="s1">lhs</span><span class="s3">,</span>
      <span class="s1">rhs</span><span class="s3">,</span>
      <span class="s1">dnums_proto</span><span class="s3">,</span>
      <span class="s1">precision_config_proto</span><span class="s3">,</span>
      <span class="s1">preferred_element_type=preferred_element_type</span><span class="s3">,</span>
      <span class="s1">use_v2=</span><span class="s3">True</span><span class="s1">)</span>
  <span class="s3">if </span><span class="s1">_WRAP_JAX_JIT_WITH_TF_FUNCTION:</span>
    <span class="s1">res = tf.stop_gradient(res)  </span><span class="s0"># See #7839</span>
  <span class="s3">return </span><span class="s1">res</span>


<span class="s1">tf_impl_with_avals[lax.dot_general_p] = _dot_general</span>


<span class="s3">def </span><span class="s1">_broadcast_in_dim(operand</span><span class="s3">, </span><span class="s1">*</span><span class="s3">, </span><span class="s1">shape</span><span class="s3">, </span><span class="s1">broadcast_dimensions</span><span class="s3">,</span>
                      <span class="s1">_in_avals: Sequence[core.ShapedArray]</span><span class="s3">,</span>
                      <span class="s1">_out_aval: core.ShapedArray):</span>
  <span class="s0"># for i in range(len(operand.shape)):</span>
  <span class="s0">#   result.shape[bcast_dims[i]] &lt;- operand.shape[i]</span>
  <span class="s0"># bcast_dims must be strictly increasing.</span>
  <span class="s0"># len(bcast_dims) == len(operand.shape)</span>
  <span class="s1">op_shape = _in_avals[</span><span class="s5">0</span><span class="s1">].shape</span>
  <span class="s1">dtype = _in_avals[</span><span class="s5">0</span><span class="s1">].dtype</span>
  <span class="s1">add_1s_shape = [</span><span class="s5">1</span><span class="s1">] * len(shape)</span>
  <span class="s3">for </span><span class="s1">i</span><span class="s3">, </span><span class="s1">broadcast_dim_i </span><span class="s3">in </span><span class="s1">enumerate(broadcast_dimensions):</span>
    <span class="s1">add_1s_shape[broadcast_dim_i] = op_shape[i]</span>
  <span class="s1">with_1s = tf.reshape(operand</span><span class="s3">, </span><span class="s1">_eval_shape(add_1s_shape</span><span class="s3">, </span><span class="s1">dtype=dtype))</span>
  <span class="s3">return </span><span class="s1">tf.broadcast_to(with_1s</span><span class="s3">, </span><span class="s1">_eval_shape(shape</span><span class="s3">, </span><span class="s1">dtype=dtype))</span>


<span class="s1">tf_impl_with_avals[lax.broadcast_in_dim_p] = _broadcast_in_dim</span>


<span class="s3">def </span><span class="s1">_empty(*</span><span class="s3">, </span><span class="s1">dtype):</span>
  <span class="s3">if </span><span class="s1">core.is_opaque_dtype(dtype):</span>
    <span class="s3">raise </span><span class="s1">NotImplementedError  </span><span class="s0"># TODO(frostig,mattjj): jax2tf handlers</span>
  <span class="s3">return </span><span class="s1">tf.constant(np.array(</span><span class="s5">0</span><span class="s3">, </span><span class="s1">dtype=dtype))</span>


<span class="s1">tf_impl[lax_internal.empty_p] = _empty</span>


<span class="s3">def </span><span class="s1">_reshape(operand</span><span class="s3">, </span><span class="s1">*</span><span class="s3">, </span><span class="s1">new_sizes</span><span class="s3">, </span><span class="s1">dimensions</span><span class="s3">, </span><span class="s1">_in_avals</span><span class="s3">, </span><span class="s1">_out_aval):</span>
  <span class="s3">if </span><span class="s1">dimensions </span><span class="s3">is None</span><span class="s1">:</span>
    <span class="s1">dimensions = tf.range(tf.rank(operand))</span>
  <span class="s1">new_sizes_tf = _eval_shape(new_sizes</span><span class="s3">, </span><span class="s1">_in_avals[</span><span class="s5">0</span><span class="s1">].dtype)</span>
  <span class="s3">return </span><span class="s1">tf.reshape(tf.transpose(operand</span><span class="s3">, </span><span class="s1">dimensions)</span><span class="s3">, </span><span class="s1">new_sizes_tf)</span>


<span class="s1">tf_impl_with_avals[lax.reshape_p] = _reshape</span>


<span class="s3">def </span><span class="s1">_squeeze(operand</span><span class="s3">, </span><span class="s1">*</span><span class="s3">, </span><span class="s1">dimensions</span><span class="s3">, </span><span class="s1">_in_avals</span><span class="s3">, </span><span class="s1">_out_aval):</span>
  <span class="s1">op_aval = _jax_physical_aval(_in_avals[</span><span class="s5">0</span><span class="s1">])</span>
  <span class="s1">op_shape = op_aval.shape</span>
  <span class="s1">new_shape = tuple(d </span><span class="s3">for </span><span class="s1">i</span><span class="s3">, </span><span class="s1">d </span><span class="s3">in </span><span class="s1">enumerate(op_shape) </span><span class="s3">if </span><span class="s1">i </span><span class="s3">not in </span><span class="s1">dimensions)</span>
  <span class="s1">new_shape_tf = _eval_shape(new_shape</span><span class="s3">, </span><span class="s1">op_aval.dtype)</span>
  <span class="s3">return </span><span class="s1">tf.reshape(operand</span><span class="s3">, </span><span class="s1">new_shape_tf)</span>


<span class="s1">tf_impl_with_avals[lax.squeeze_p] = _squeeze</span>


<span class="s3">def </span><span class="s1">_pad(operand</span><span class="s3">, </span><span class="s1">padding_value</span><span class="s3">, </span><span class="s1">*</span><span class="s3">, </span><span class="s1">padding_config</span><span class="s3">,</span>
         <span class="s1">_in_avals: Sequence[core.ShapedArray]</span><span class="s3">,</span>
         <span class="s1">_out_aval: core.ShapedArray):</span>
  <span class="s1">low</span><span class="s3">, </span><span class="s1">high</span><span class="s3">, </span><span class="s1">interior = util.unzip3(map(_eval_shape</span><span class="s3">, </span><span class="s1">padding_config))  </span><span class="s0"># type: ignore</span>
  <span class="s1">out = tfxla.pad(operand</span><span class="s3">, </span><span class="s1">padding_value</span><span class="s3">, </span><span class="s1">low</span><span class="s3">, </span><span class="s1">high</span><span class="s3">, </span><span class="s1">interior)</span>
  <span class="s0"># TODO: implement shape inference for XlaPad (when some padding_config is constant)</span>
  <span class="s1">out = _ensure_tf_shape_if_dynamic(out</span><span class="s3">, </span><span class="s1">_aval_to_tf_shape(_out_aval))</span>
  <span class="s3">if </span><span class="s1">_WRAP_JAX_JIT_WITH_TF_FUNCTION:</span>
    <span class="s1">out = tf.stop_gradient(out)  </span><span class="s0"># See #7839</span>
  <span class="s3">return </span><span class="s1">out</span>


<span class="s1">tf_impl_with_avals[lax.pad_p] = _pad</span>


<span class="s3">def </span><span class="s1">_rev(operand</span><span class="s3">, </span><span class="s1">*</span><span class="s3">, </span><span class="s1">dimensions):</span>
  <span class="s3">return </span><span class="s1">tf.reverse(operand</span><span class="s3">, </span><span class="s1">dimensions)</span>


<span class="s1">tf_impl[lax.rev_p] = _rev</span>


<span class="s3">def </span><span class="s1">_where(which</span><span class="s3">, </span><span class="s1">*cases):</span>
  <span class="s3">if </span><span class="s1">which.dtype == tf.bool:</span>
    <span class="s3">assert </span><span class="s1">len(cases) &lt;= </span><span class="s5">2</span>
    <span class="s3">return </span><span class="s1">cases </span><span class="s3">if </span><span class="s1">len(cases) == </span><span class="s5">1 </span><span class="s3">else </span><span class="s1">tf.where(which</span><span class="s3">, </span><span class="s1">cases[</span><span class="s5">1</span><span class="s1">]</span><span class="s3">, </span><span class="s1">cases[</span><span class="s5">0</span><span class="s1">])</span>

  <span class="s3">def </span><span class="s1">_select(offset</span><span class="s3">, </span><span class="s1">cases):</span>
    <span class="s3">assert </span><span class="s1">len(cases) &gt; </span><span class="s5">0</span>
    <span class="s3">if </span><span class="s1">len(cases) == </span><span class="s5">1</span><span class="s1">:</span>
      <span class="s3">return </span><span class="s1">cases[</span><span class="s5">0</span><span class="s1">]</span>
    <span class="s1">mid = len(cases) // </span><span class="s5">2</span>
    <span class="s3">return </span><span class="s1">tf.where(tf.less(which</span><span class="s3">, </span><span class="s1">offset + mid)</span><span class="s3">,</span>
                    <span class="s1">_select(offset</span><span class="s3">, </span><span class="s1">cases[:mid])</span><span class="s3">,</span>
                    <span class="s1">_select(mid</span><span class="s3">, </span><span class="s1">cases[mid:]))</span>

  <span class="s3">return </span><span class="s1">_select(</span><span class="s5">0</span><span class="s3">, </span><span class="s1">cases)</span>


<span class="s1">tf_impl[lax.select_n_p] = _where</span>


<span class="s3">def </span><span class="s1">_transpose(operand</span><span class="s3">, </span><span class="s1">*</span><span class="s3">, </span><span class="s1">permutation):</span>
  <span class="s3">return </span><span class="s1">tf.transpose(operand</span><span class="s3">, </span><span class="s1">perm=permutation)</span>


<span class="s1">tf_impl[lax.transpose_p] = _transpose</span>

<span class="s1">axes_to_axis = </span><span class="s3">lambda </span><span class="s1">func: </span><span class="s3">lambda </span><span class="s1">operand</span><span class="s3">, </span><span class="s1">axes: func(operand</span><span class="s3">, </span><span class="s1">axis=axes)</span>

<span class="s0"># reduce_sum and reduce_prod are not supported for bool</span>
<span class="s1">tf_impl[lax.reduce_sum_p] = axes_to_axis(tf.reduce_sum)</span>
<span class="s1">tf_impl[lax.reduce_prod_p] = axes_to_axis(tf.reduce_prod)</span>
<span class="s1">tf_impl[lax.reduce_max_p] = handle_boolean_args(</span>
  <span class="s1">axes_to_axis(tf.reduce_max)</span><span class="s3">, </span><span class="s1">argnums=[</span><span class="s5">0</span><span class="s1">]</span><span class="s3">,</span>
  <span class="s1">boolean_f=axes_to_axis(tf.reduce_any)) </span><span class="s0"># Max is T if any one is T</span>
<span class="s1">tf_impl[lax.reduce_min_p] = handle_boolean_args(</span>
  <span class="s1">axes_to_axis(tf.reduce_min)</span><span class="s3">, </span><span class="s1">argnums=[</span><span class="s5">0</span><span class="s1">]</span><span class="s3">,</span>
  <span class="s1">boolean_f=axes_to_axis(tf.reduce_all)) </span><span class="s0"># Min is F if not all are T</span>
<span class="s1">tf_impl[lax.reduce_or_p] = axes_to_axis(tf.reduce_any)</span>
<span class="s1">tf_impl[lax.reduce_and_p] = axes_to_axis(tf.reduce_all)</span>


<span class="s3">def </span><span class="s1">_argminmax(is_min: bool</span><span class="s3">, </span><span class="s1">operand: TfVal</span><span class="s3">, </span><span class="s1">axes: Sequence[int]</span><span class="s3">,</span>
               <span class="s1">index_dtype: DType</span><span class="s3">,</span>
               <span class="s1">_in_avals: Sequence[core.ShapedArray]</span><span class="s3">,</span>
               <span class="s1">_out_aval: core.ShapedArray):</span>
  <span class="s0"># Follow the JAX implementation, using a XlaReduce with a custom comparator</span>
  <span class="s3">if </span><span class="s1">is_min:</span>
    <span class="s1">extra_name_stack = </span><span class="s4">&quot;argmin&quot;</span>
    <span class="s1">value_comparator = lax.lt</span>
    <span class="s1">get_identity = lax_internal._get_min_identity</span>
  <span class="s3">else</span><span class="s1">:</span>
    <span class="s1">extra_name_stack = </span><span class="s4">&quot;argmax&quot;</span>
    <span class="s1">value_comparator = lax.gt</span>
    <span class="s1">get_identity = lax_internal._get_max_identity</span>

  <span class="s1">res = _convert_jax_impl(</span>
      <span class="s1">partial(lax_internal._compute_argminmax</span><span class="s3">, </span><span class="s1">value_comparator</span><span class="s3">, </span><span class="s1">get_identity)</span><span class="s3">,</span>
      <span class="s1">multiple_results=</span><span class="s3">False,</span>
      <span class="s1">extra_name_stack=extra_name_stack)(</span>
          <span class="s1">operand</span><span class="s3">,</span>
          <span class="s1">index_dtype=index_dtype</span><span class="s3">,</span>
          <span class="s1">axes=axes</span><span class="s3">,</span>
          <span class="s1">_in_avals=_in_avals</span><span class="s3">,</span>
          <span class="s1">_out_aval=_out_aval)</span>
  <span class="s3">return </span><span class="s1">res</span>


<span class="s1">tf_impl_with_avals[lax.argmin_p] = partial(_argminmax</span><span class="s3">, True</span><span class="s1">)</span>
<span class="s1">tf_impl_with_avals[lax.argmax_p] = partial(_argminmax</span><span class="s3">, False</span><span class="s1">)</span>


<span class="s1">_add_fn = tf.function(_add</span><span class="s3">, </span><span class="s1">autograph=</span><span class="s3">False</span><span class="s1">)</span>
<span class="s1">_ge_fn = tf.function(tf.math.greater_equal</span><span class="s3">, </span><span class="s1">autograph=</span><span class="s3">False</span><span class="s1">)</span>


<span class="s3">def </span><span class="s1">_select_and_gather_add(</span>
    <span class="s1">tangents: TfVal</span><span class="s3">, </span><span class="s1">operand: TfVal</span><span class="s3">, </span><span class="s1">select_prim: core.Primitive</span><span class="s3">,</span>
    <span class="s1">window_dimensions: Sequence[int]</span><span class="s3">, </span><span class="s1">window_strides: Sequence[int]</span><span class="s3">,</span>
    <span class="s1">base_dilation: Sequence[int]</span><span class="s3">, </span><span class="s1">window_dilation: Sequence[int]</span><span class="s3">,</span>
    <span class="s1">padding: Sequence[Tuple[int</span><span class="s3">, </span><span class="s1">int]]</span><span class="s3">, </span><span class="s1">_in_avals: Sequence[core.ShapedArray]</span><span class="s3">,</span>
    <span class="s1">_out_aval: core.ShapedArray):</span>
  <span class="s0"># Note: this function follows the pattern in</span>
  <span class="s0"># jax.lax._select_and_gather_add_translation.</span>
  <span class="s1">dtype = operand.dtype</span>
  <span class="s1">nbits = dtypes.finfo(dtype.as_numpy_dtype).bits</span>

  <span class="s0"># Specializing the function for 64 bits. Only up to 32 bits are supported on TPU,</span>
  <span class="s0"># we thus intend to let the code throw a different exception on this platform.</span>
  <span class="s1">max_bits = </span><span class="s5">64</span>

  <span class="s3">assert </span><span class="s1">nbits &lt;= max_bits</span>
  <span class="s1">double_word_reduction = nbits * </span><span class="s5">2 </span><span class="s1">&lt;= max_bits</span>

  <span class="s1">const = </span><span class="s3">lambda </span><span class="s1">dtype</span><span class="s3">, </span><span class="s1">x: tf.constant(np.array(x)</span><span class="s3">, </span><span class="s1">dtype)</span>

  <span class="s3">if </span><span class="s1">double_word_reduction:</span>
    <span class="s1">word_dtype = lax_internal._UINT_DTYPES[nbits]</span>
    <span class="s1">double_word_dtype = lax_internal._UINT_DTYPES[nbits * </span><span class="s5">2</span><span class="s1">]</span>

    <span class="s0"># Packs two values into a tuple.</span>
    <span class="s3">def </span><span class="s1">pack(a</span><span class="s3">, </span><span class="s1">b):</span>
      <span class="s1">a = _bitcast_convert_type(a</span><span class="s3">, </span><span class="s1">word_dtype)</span>
      <span class="s1">b = _bitcast_convert_type(b</span><span class="s3">, </span><span class="s1">word_dtype)</span>
      <span class="s1">a = _convert_element_type(a</span><span class="s3">, </span><span class="s1">new_dtype=double_word_dtype)</span>
      <span class="s1">b = _convert_element_type(b</span><span class="s3">, </span><span class="s1">new_dtype=double_word_dtype)</span>
      <span class="s1">a = tf.bitwise.left_shift(a</span><span class="s3">, </span><span class="s1">const(double_word_dtype</span><span class="s3">, </span><span class="s1">nbits))</span>
      <span class="s3">return </span><span class="s1">tf.bitwise.bitwise_or(a</span><span class="s3">, </span><span class="s1">b)</span>

    <span class="s0"># Unpacks the first element of a tuple.</span>
    <span class="s3">def </span><span class="s1">fst(t):</span>
      <span class="s3">assert </span><span class="s1">t.dtype == double_word_dtype</span>
      <span class="s1">st = _shift_right_logical(t</span><span class="s3">, </span><span class="s1">const(double_word_dtype</span><span class="s3">, </span><span class="s1">nbits))</span>
      <span class="s3">return </span><span class="s1">_bitcast_convert_type(</span>
          <span class="s1">_convert_element_type(st</span><span class="s3">, </span><span class="s1">new_dtype=word_dtype)</span><span class="s3">, </span><span class="s1">dtype)</span>

    <span class="s0"># Unpacks the second element of a tuple.</span>
    <span class="s3">def </span><span class="s1">snd(t):</span>
      <span class="s3">return </span><span class="s1">_bitcast_convert_type(</span>
          <span class="s1">_convert_element_type(t</span><span class="s3">, </span><span class="s1">new_dtype=word_dtype)</span><span class="s3">, </span><span class="s1">dtype)</span>

  <span class="s3">else</span><span class="s1">:</span>
    <span class="s3">raise </span><span class="s1">NotImplementedError(</span>
        <span class="s4">f&quot;TODO: need to pack </span><span class="s3">{</span><span class="s1">nbits * </span><span class="s5">2</span><span class="s3">} </span><span class="s4">bits but this platform can only go up to </span><span class="s3">{</span><span class="s1">max_bits</span><span class="s3">} </span><span class="s4">bits.&quot;</span>
    <span class="s1">)</span>

  <span class="s3">assert </span><span class="s1">select_prim </span><span class="s3">is </span><span class="s1">lax.ge_p </span><span class="s3">or </span><span class="s1">select_prim </span><span class="s3">is </span><span class="s1">lax.le_p</span><span class="s3">, </span><span class="s1">select_prim</span>

  <span class="s3">def </span><span class="s1">reducer(x</span><span class="s3">, </span><span class="s1">y):</span>
    <span class="s1">which = tf_impl[select_prim]</span>
    <span class="s3">return </span><span class="s1">tf_impl[lax.select_n_p](which(fst(x)</span><span class="s3">, </span><span class="s1">fst(y))</span><span class="s3">, </span><span class="s1">y</span><span class="s3">, </span><span class="s1">x)</span>

  <span class="s1">init = -np.inf </span><span class="s3">if </span><span class="s1">select_prim </span><span class="s3">is </span><span class="s1">lax.ge_p </span><span class="s3">else </span><span class="s1">np.inf</span>
  <span class="s1">init_identity = </span><span class="s3">lambda </span><span class="s1">x: pack(const(dtype</span><span class="s3">, </span><span class="s1">init)</span><span class="s3">, </span><span class="s1">const(dtype</span><span class="s3">, </span><span class="s5">0</span><span class="s1">))</span>

  <span class="s1">out = _specialized_reduce_window(</span>
      <span class="s1">reducer</span><span class="s3">,</span>
      <span class="s1">init_identity</span><span class="s3">,</span>
      <span class="s1">pack(operand</span><span class="s3">, </span><span class="s1">tangents)</span><span class="s3">,</span>
      <span class="s1">window_dimensions=window_dimensions</span><span class="s3">,</span>
      <span class="s1">window_strides=window_strides</span><span class="s3">,</span>
      <span class="s1">padding=padding</span><span class="s3">,</span>
      <span class="s1">base_dilation=base_dilation</span><span class="s3">,</span>
      <span class="s1">window_dilation=window_dilation</span><span class="s3">,</span>
      <span class="s1">_in_avals=_in_avals</span><span class="s3">,</span>
      <span class="s1">_out_aval=_out_aval)</span>

  <span class="s3">return </span><span class="s1">snd(out)</span>


<span class="s1">tf_impl_with_avals[lax.select_and_gather_add_p] = _select_and_gather_add</span>


<span class="s3">def </span><span class="s1">_common_reduce_window(operand</span><span class="s3">, </span><span class="s1">init_val</span><span class="s3">, </span><span class="s1">reducer</span><span class="s3">, </span><span class="s1">window_dimensions</span><span class="s3">,</span>
                          <span class="s1">window_strides</span><span class="s3">, </span><span class="s1">padding</span><span class="s3">, </span><span class="s1">base_dilation</span><span class="s3">,</span>
                          <span class="s1">window_dilation</span><span class="s3">, </span><span class="s1">_in_avals</span><span class="s3">, </span><span class="s1">_out_aval):</span>
  <span class="s1">o_spec = tf.TensorSpec(()</span><span class="s3">, </span><span class="s1">dtype=operand.dtype)</span>
  <span class="s1">reducer_fn = tf.function(</span>
      <span class="s1">reducer</span><span class="s3">, </span><span class="s1">autograph=</span><span class="s3">False</span><span class="s1">).get_concrete_function(o_spec</span><span class="s3">, </span><span class="s1">o_spec)</span>

  <span class="s3">if not </span><span class="s1">isinstance(init_val</span><span class="s3">, </span><span class="s1">(tf.Tensor</span><span class="s3">, </span><span class="s1">tf.Variable)):</span>
    <span class="s1">init_val = tf.constant(init_val</span><span class="s3">, </span><span class="s1">operand.dtype)</span>
  <span class="s1">window_dimensions_tf = _eval_shape(window_dimensions)</span>
  <span class="s1">window_strides_tf = _eval_shape(window_strides)</span>
  <span class="s1">window_dilation_tf = _eval_shape(window_dilation)</span>
  <span class="s1">base_dilation_tf = _eval_shape(base_dilation)</span>
  <span class="s1">padding_tf = [_eval_shape(p) </span><span class="s3">for </span><span class="s1">p </span><span class="s3">in </span><span class="s1">padding]</span>
  <span class="s1">out = tfxla.reduce_window(</span>
      <span class="s1">operand</span><span class="s3">,</span>
      <span class="s1">init_val</span><span class="s3">,</span>
      <span class="s1">reducer_fn</span><span class="s3">,</span>
      <span class="s1">window_dimensions_tf</span><span class="s3">,</span>
      <span class="s1">window_strides_tf</span><span class="s3">,</span>
      <span class="s1">base_dilations=base_dilation_tf</span><span class="s3">,</span>
      <span class="s1">window_dilations=window_dilation_tf</span><span class="s3">,</span>
      <span class="s1">padding=padding_tf)</span>
  <span class="s0"># TODO: implement shape inference for XlaReduceWindow</span>
  <span class="s1">out = _ensure_tf_shape_if_dynamic(out</span><span class="s3">, </span><span class="s1">_aval_to_tf_shape(_out_aval))</span>
  <span class="s3">if </span><span class="s1">_WRAP_JAX_JIT_WITH_TF_FUNCTION:</span>
    <span class="s1">out = tf.stop_gradient(out)  </span><span class="s0"># See #7839</span>
  <span class="s3">return </span><span class="s1">out</span>


<span class="s3">def </span><span class="s1">_reduce_window(*args</span><span class="s3">, </span><span class="s1">jaxpr</span><span class="s3">, </span><span class="s1">consts</span><span class="s3">, </span><span class="s1">window_dimensions</span><span class="s3">,</span>
                   <span class="s1">window_strides</span><span class="s3">, </span><span class="s1">padding</span><span class="s3">, </span><span class="s1">base_dilation</span><span class="s3">, </span><span class="s1">window_dilation</span><span class="s3">,</span>
                   <span class="s1">_in_avals</span><span class="s3">, </span><span class="s1">_out_aval):</span>
  <span class="s2">&quot;&quot;&quot;TensorFlow implementation of reduce_window. 
 
  Args: 
    operands: N dimensional arrays containing elements of type T 
    init_values: starting values of the reduction 
    jaxpr: the jaxpr corresponding to the reduction function 
    consts: the constants associated with jaxpr. 
    window_dimensions: array of integers for window dimension values 
    window_strides: array of integers for window stride values 
    padding: array of pairs of integers for padding values 
    base_dilation: array of integers for base dilation values 
    window_dilation: array of integers for window dilation values 
 
  Returns: 
    The reduced operand. 
  &quot;&quot;&quot;</span>
  <span class="s3">assert </span><span class="s1">len(consts) == </span><span class="s5">0</span><span class="s3">, </span><span class="s4">&quot;Reduction computation cannot have constants&quot;</span>
  <span class="s1">operands</span><span class="s3">, </span><span class="s1">init_values = util.split_list(args</span><span class="s3">, </span><span class="s1">[len(args) // </span><span class="s5">2</span><span class="s1">])</span>

  <span class="s3">if </span><span class="s1">len(operands) != </span><span class="s5">1</span><span class="s1">:</span>
    <span class="s3">raise </span><span class="s1">NotImplementedError(</span><span class="s4">&quot;jax2tf does not support variadic reduce_window&quot;</span><span class="s1">)</span>

  <span class="s3">def </span><span class="s1">reducer(arg1: TfVal</span><span class="s3">, </span><span class="s1">arg2: TfVal) -&gt; TfVal:</span>
    <span class="s1">closed_jaxpr = core.ClosedJaxpr(jaxpr</span><span class="s3">, </span><span class="s1">consts)</span>
    <span class="s1">res</span><span class="s3">, </span><span class="s1">= _interpret_jaxpr(closed_jaxpr</span><span class="s3">, </span><span class="s1">arg1</span><span class="s3">, </span><span class="s1">arg2</span><span class="s3">, </span><span class="s1">extra_name_stack=</span><span class="s3">None</span><span class="s1">)</span>
    <span class="s3">return </span><span class="s1">res</span>

  <span class="s3">return </span><span class="s1">(_common_reduce_window(operands[</span><span class="s5">0</span><span class="s1">]</span><span class="s3">, </span><span class="s1">init_values[</span><span class="s5">0</span><span class="s1">]</span><span class="s3">, </span><span class="s1">reducer</span><span class="s3">,</span>
                                <span class="s1">window_dimensions</span><span class="s3">, </span><span class="s1">window_strides</span><span class="s3">, </span><span class="s1">padding</span><span class="s3">,</span>
                                <span class="s1">base_dilation</span><span class="s3">, </span><span class="s1">window_dilation</span><span class="s3">, </span><span class="s1">_in_avals</span><span class="s3">,</span>
                                <span class="s1">_out_aval[</span><span class="s5">0</span><span class="s1">])</span><span class="s3">,</span><span class="s1">)</span>


<span class="s3">def </span><span class="s1">_specialized_reduce_window(reducer</span><span class="s3">,</span>
                               <span class="s1">identity</span><span class="s3">,</span>
                               <span class="s1">operand</span><span class="s3">,</span>
                               <span class="s1">*</span><span class="s3">,</span>
                               <span class="s1">window_dimensions</span><span class="s3">,</span>
                               <span class="s1">window_strides</span><span class="s3">,</span>
                               <span class="s1">padding</span><span class="s3">,</span>
                               <span class="s1">base_dilation</span><span class="s3">,</span>
                               <span class="s1">window_dilation</span><span class="s3">,</span>
                               <span class="s1">_in_avals</span><span class="s3">,</span>
                               <span class="s1">_out_aval</span><span class="s3">,</span>
                               <span class="s1">name=</span><span class="s3">None</span><span class="s1">):</span>
  <span class="s2">&quot;&quot;&quot;Wraps the TensorFlow reduce window operation based on a reducer and an 
 
  identity function defining the initial value of the reduction depending on 
  the dtype of the operand. 
 
  Args: 
    reducer: reduction function of type TfVal -&gt; TfVal -&gt; TfVal 
    identity: function that takes a TensorFlow dtype as a parameter and returns 
      the starting value of the reduction. 
    operand: N dimensional array containing elements of type T 
    window_dimensions: array of integers for window dimension values 
    window_strides: array of integers for window stride values 
    padding: array of pairs of integers for padding values 
    base_dilation: array of integers for base dilation values 
    window_dilation: array of integers for window dilation values 
    name: the name of the specialized reduce window primitive for which this 
      conversion function is called. This information may help to choose a 
      different conversion path (optional) 
 
  Returns: 
    The reduced operand. 
  &quot;&quot;&quot;</span>
  <span class="s3">return </span><span class="s1">_common_reduce_window(operand</span><span class="s3">, </span><span class="s1">identity(operand.dtype)</span><span class="s3">, </span><span class="s1">reducer</span><span class="s3">,</span>
                               <span class="s1">window_dimensions</span><span class="s3">, </span><span class="s1">window_strides</span><span class="s3">, </span><span class="s1">padding</span><span class="s3">,</span>
                               <span class="s1">base_dilation</span><span class="s3">, </span><span class="s1">window_dilation</span><span class="s3">, </span><span class="s1">_in_avals</span><span class="s3">,</span>
                               <span class="s1">_out_aval)</span>


<span class="s3">def </span><span class="s1">_get_max_identity(tf_dtype):</span>
  <span class="s1">numpy_tf_dtype = tf_dtype.as_numpy_dtype</span>
  <span class="s3">if </span><span class="s1">tf_dtype == tf.bfloat16 </span><span class="s3">or </span><span class="s1">dtypes.issubdtype(numpy_tf_dtype</span><span class="s3">, </span><span class="s1">np.inexact):</span>
    <span class="s3">return </span><span class="s1">numpy_tf_dtype(-np.inf)</span>
  <span class="s3">elif </span><span class="s1">dtypes.issubdtype(numpy_tf_dtype</span><span class="s3">, </span><span class="s1">np.integer):</span>
    <span class="s3">return </span><span class="s1">dtypes.iinfo(numpy_tf_dtype).min</span>
  <span class="s3">else</span><span class="s1">:</span>
    <span class="s3">assert </span><span class="s1">dtypes.issubdtype(</span>
        <span class="s1">numpy_tf_dtype</span><span class="s3">, </span><span class="s1">np.bool_)</span><span class="s3">, </span><span class="s1">(</span><span class="s4">f&quot;</span><span class="s3">{</span><span class="s1">tf_dtype</span><span class="s3">} </span><span class="s4">has no defined max identity&quot;</span><span class="s1">)</span>
    <span class="s3">return False</span>


<span class="s3">def </span><span class="s1">_get_min_identity(tf_dtype):</span>
  <span class="s1">numpy_tf_dtype = tf_dtype.as_numpy_dtype</span>
  <span class="s3">if </span><span class="s1">tf_dtype == tf.bfloat16 </span><span class="s3">or </span><span class="s1">dtypes.issubdtype(numpy_tf_dtype</span><span class="s3">, </span><span class="s1">np.inexact):</span>
    <span class="s3">return </span><span class="s1">numpy_tf_dtype(np.inf)</span>
  <span class="s3">elif </span><span class="s1">dtypes.issubdtype(numpy_tf_dtype</span><span class="s3">, </span><span class="s1">np.integer):</span>
    <span class="s3">return </span><span class="s1">dtypes.iinfo(numpy_tf_dtype).max</span>
  <span class="s3">else</span><span class="s1">:</span>
    <span class="s3">assert </span><span class="s1">dtypes.issubdtype(</span>
        <span class="s1">numpy_tf_dtype</span><span class="s3">, </span><span class="s1">np.bool_)</span><span class="s3">, </span><span class="s1">(</span><span class="s4">f&quot;</span><span class="s3">{</span><span class="s1">tf_dtype</span><span class="s3">} </span><span class="s4">has no defined min identity&quot;</span><span class="s1">)</span>
    <span class="s3">return True</span>


<span class="s0"># pylint: disable=protected-access</span>
<span class="s1">tf_impl_with_avals[lax.reduce_window_sum_p] = (</span>
    <span class="s1">partial(_specialized_reduce_window</span><span class="s3">, </span><span class="s1">_add</span><span class="s3">, lambda </span><span class="s1">x: </span><span class="s5">0</span><span class="s3">,</span>
            <span class="s1">name=</span><span class="s4">&quot;reduce_window_sum&quot;</span><span class="s1">))</span>
<span class="s1">tf_impl_with_avals[lax.reduce_window_min_p] = (</span>
    <span class="s1">partial(_specialized_reduce_window</span><span class="s3">,</span>
            <span class="s1">partial(_minmax_scalar</span><span class="s3">, </span><span class="s1">is_min=</span><span class="s3">True</span><span class="s1">)</span><span class="s3">,</span>
            <span class="s1">_get_min_identity</span><span class="s3">,</span>
            <span class="s1">name=</span><span class="s4">&quot;reduce_window_min&quot;</span><span class="s1">))</span>
<span class="s1">tf_impl_with_avals[lax.reduce_window_max_p] = (</span>
    <span class="s1">partial(_specialized_reduce_window</span><span class="s3">,</span>
            <span class="s1">partial(_minmax_scalar</span><span class="s3">, </span><span class="s1">is_min=</span><span class="s3">False</span><span class="s1">)</span><span class="s3">,</span>
            <span class="s1">_get_max_identity</span><span class="s3">,</span>
            <span class="s1">name=</span><span class="s4">&quot;reduce_window_max&quot;</span><span class="s1">))</span>
<span class="s1">tf_impl_with_avals[lax.reduce_window_p] = _reduce_window</span>
<span class="s0"># pylint: enable=protected-access</span>

<span class="s3">def </span><span class="s1">_reduce(*operands: TfVal</span><span class="s3">,</span>
            <span class="s1">computation: Callable</span><span class="s3">,</span>
            <span class="s1">jaxpr: core.Jaxpr</span><span class="s3">,</span>
            <span class="s1">consts:  Sequence[Any]</span><span class="s3">,</span>
            <span class="s1">dimensions: Sequence[int]</span><span class="s3">,</span>
            <span class="s1">_in_avals: Sequence[core.ShapedArray]</span><span class="s3">,</span>
            <span class="s1">_out_aval: core.ShapedArray) -&gt; Sequence[TfVal]:</span>
  <span class="s3">del </span><span class="s1">computation</span>
  <span class="s3">assert not </span><span class="s1">consts</span>
  <span class="s3">assert </span><span class="s1">len(operands) % </span><span class="s5">2 </span><span class="s1">== </span><span class="s5">0</span>
  <span class="s0"># operands: op1, op2, ..., init_val1, init_val2, ...</span>
  <span class="s0"># reducer takes op1[i], op2[i], ..., init_val1, init_val2, ...</span>
  <span class="s1">nr_operands = len(operands) // </span><span class="s5">2</span>
  <span class="s1">init_vals = operands[nr_operands:]</span>
  <span class="s1">operands = operands[</span><span class="s5">0</span><span class="s1">:nr_operands]</span>

  <span class="s1">reducer_arg_spec = tuple([tf.TensorSpec(()</span><span class="s3">, </span><span class="s1">op.dtype) </span><span class="s3">for </span><span class="s1">op </span><span class="s3">in </span><span class="s1">init_vals] * </span><span class="s5">2</span><span class="s1">)</span>

  <span class="s3">def </span><span class="s1">reducer_computation(*args: TfVal) -&gt; TfVal:</span>
    <span class="s1">closed_jaxpr = core.ClosedJaxpr(jaxpr</span><span class="s3">, </span><span class="s1">consts)</span>
    <span class="s1">res = _interpret_jaxpr(closed_jaxpr</span><span class="s3">, </span><span class="s1">*args</span><span class="s3">, </span><span class="s1">extra_name_stack=</span><span class="s3">None</span><span class="s1">)</span>
    <span class="s3">return </span><span class="s1">res</span>

  <span class="s1">xla_reducer_computation = (</span>
      <span class="s1">tf.function(reducer_computation</span><span class="s3">,</span>
                  <span class="s1">autograph=</span><span class="s3">False</span><span class="s1">).get_concrete_function(*reducer_arg_spec))</span>

  <span class="s1">outs = tfxla.variadic_reduce(operands</span><span class="s3">, </span><span class="s1">init_vals</span><span class="s3">,</span>
                               <span class="s1">dimensions_to_reduce=dimensions</span><span class="s3">,</span>
                               <span class="s1">reducer=xla_reducer_computation)</span>
  <span class="s3">if </span><span class="s1">_WRAP_JAX_JIT_WITH_TF_FUNCTION:</span>
    <span class="s1">outs = tuple(tf.stop_gradient(out) </span><span class="s3">for </span><span class="s1">out </span><span class="s3">in </span><span class="s1">outs)  </span><span class="s0"># See #7839</span>
  <span class="s3">return </span><span class="s1">outs</span>

<span class="s1">tf_impl_with_avals[lax.reduce_p] = _reduce</span>


<span class="s0"># We use lax.cumred_reduce_window_impl to convert cummax,</span>
<span class="s0"># cummin, cumsum and cumprod. This is efficient on TPU, but the complexity is</span>
<span class="s0"># O(n^2) on other backends. This may be implemented using associative_scan</span>
<span class="s0"># instead to favor different backends.</span>
<span class="s3">def </span><span class="s1">_cumred(lax_reduce_fn: Callable</span><span class="s3">,</span>
            <span class="s1">lax_reduce_window_fn: Callable</span><span class="s3">,</span>
            <span class="s1">extra_name_stack: str):</span>
  <span class="s3">if </span><span class="s1">config.jax2tf_associative_scan_reductions:</span>
    <span class="s3">return </span><span class="s1">_convert_jax_impl(partial(lax_control_flow.associative_scan</span><span class="s3">,</span>
                                     <span class="s1">lax_reduce_fn)</span><span class="s3">,</span>
                             <span class="s1">multiple_results=</span><span class="s3">False,</span>
                             <span class="s1">extra_name_stack=extra_name_stack)</span>
  <span class="s3">else</span><span class="s1">:</span>
    <span class="s3">return </span><span class="s1">_convert_jax_impl(partial(lax_control_flow.cumred_reduce_window_impl</span><span class="s3">,</span>
                                     <span class="s1">lax_reduce_window_fn)</span><span class="s3">,</span>
                             <span class="s1">multiple_results=</span><span class="s3">False,</span>
                             <span class="s1">extra_name_stack=extra_name_stack)</span>


<span class="s1">tf_impl_with_avals[lax.cummax_p] = _cumred(</span>
    <span class="s1">lax_reduce_window_fn=lax_windowed_reductions._reduce_window_max</span><span class="s3">,</span>
    <span class="s1">lax_reduce_fn=lax.max</span><span class="s3">,</span>
    <span class="s1">extra_name_stack=</span><span class="s4">&quot;cummax&quot;</span><span class="s1">)</span>
<span class="s1">tf_impl_with_avals[lax.cummin_p] = _cumred(</span>
    <span class="s1">lax_reduce_window_fn=lax_windowed_reductions._reduce_window_min</span><span class="s3">,</span>
    <span class="s1">lax_reduce_fn=lax.min</span><span class="s3">,</span>
    <span class="s1">extra_name_stack=</span><span class="s4">&quot;cummin&quot;</span><span class="s1">)</span>
<span class="s1">tf_impl_with_avals[lax.cumlogsumexp_p] = _cumred(</span>
    <span class="s1">lax_reduce_window_fn=lax_windowed_reductions._reduce_window_logaddexp</span><span class="s3">,</span>
    <span class="s1">lax_reduce_fn=logaddexp</span><span class="s3">,</span>
    <span class="s1">extra_name_stack=</span><span class="s4">&quot;cumlogsumexp&quot;</span><span class="s1">)</span>
<span class="s1">tf_impl_with_avals[lax.cumsum_p] = _cumred(</span>
    <span class="s1">lax_reduce_window_fn=lax_windowed_reductions._reduce_window_sum</span><span class="s3">,</span>
    <span class="s1">lax_reduce_fn=lax.add</span><span class="s3">,</span>
    <span class="s1">extra_name_stack=</span><span class="s4">&quot;cumsum&quot;</span><span class="s1">)</span>
<span class="s1">tf_impl_with_avals[lax.cumprod_p] = _cumred(</span>
    <span class="s1">lax_reduce_window_fn=lax_windowed_reductions._reduce_window_prod</span><span class="s3">,</span>
    <span class="s1">lax_reduce_fn=lax.mul</span><span class="s3">,</span>
    <span class="s1">extra_name_stack=</span><span class="s4">&quot;cumprod&quot;</span><span class="s1">)</span>


<span class="s3">def </span><span class="s1">_select_and_scatter(operand</span><span class="s3">, </span><span class="s1">source</span><span class="s3">, </span><span class="s1">init_value</span><span class="s3">, </span><span class="s1">select_jaxpr</span><span class="s3">,</span>
                        <span class="s1">select_consts</span><span class="s3">, </span><span class="s1">scatter_jaxpr</span><span class="s3">, </span><span class="s1">scatter_consts</span><span class="s3">,</span>
                        <span class="s1">window_dimensions</span><span class="s3">, </span><span class="s1">window_strides</span><span class="s3">, </span><span class="s1">padding):</span>
  <span class="s3">raise </span><span class="s1">NotImplementedError(</span><span class="s4">&quot;TODO: jax2tf can not convert _select_and_scatter&quot;</span><span class="s1">)</span>


<span class="s1">tf_impl[lax.select_and_scatter_p] = _select_and_scatter</span>


<span class="s1">@partial(handle_boolean_args</span><span class="s3">, </span><span class="s1">argnums=(</span><span class="s5">0</span><span class="s3">, </span><span class="s5">1</span><span class="s1">))</span>
<span class="s3">def </span><span class="s1">_select_and_scatter_add(source</span><span class="s3">, </span><span class="s1">operand</span><span class="s3">, </span><span class="s1">*</span><span class="s3">, </span><span class="s1">select_prim</span><span class="s3">, </span><span class="s1">window_dimensions</span><span class="s3">,</span>
                            <span class="s1">window_strides</span><span class="s3">, </span><span class="s1">padding</span><span class="s3">, </span><span class="s1">_in_avals</span><span class="s3">, </span><span class="s1">_out_aval):</span>
  <span class="s1">init_value = tf.zeros(()</span><span class="s3">, </span><span class="s1">operand.dtype)</span>
  <span class="s1">select_fn = (</span>
      <span class="s1">tf.function(tf_impl[select_prim]</span><span class="s3">, </span><span class="s1">autograph=</span><span class="s3">False</span><span class="s1">).get_concrete_function(</span>
          <span class="s1">init_value</span><span class="s3">, </span><span class="s1">init_value))</span>
  <span class="s1">scatter_fn = _add_fn.get_concrete_function(init_value</span><span class="s3">, </span><span class="s1">init_value)</span>
  <span class="s1">out = tfxla.select_and_scatter(operand</span><span class="s3">, </span><span class="s1">window_dimensions</span><span class="s3">, </span><span class="s1">window_strides</span><span class="s3">,</span>
                                 <span class="s1">padding</span><span class="s3">, </span><span class="s1">source</span><span class="s3">, </span><span class="s1">init_value</span><span class="s3">, </span><span class="s1">select_fn</span><span class="s3">,</span>
                                 <span class="s1">scatter_fn)</span>
  <span class="s1">out = _ensure_tf_shape_if_dynamic(out</span><span class="s3">, </span><span class="s1">_aval_to_tf_shape(_out_aval))</span>
  <span class="s3">if </span><span class="s1">_WRAP_JAX_JIT_WITH_TF_FUNCTION:</span>
    <span class="s1">out = tf.stop_gradient(out)  </span><span class="s0"># See #7839</span>
  <span class="s3">return </span><span class="s1">out</span>


<span class="s1">tf_impl_with_avals[lax.select_and_scatter_add_p] = _select_and_scatter_add</span>


<span class="s3">def </span><span class="s1">_random_seed_impl(seeds: TfVal</span><span class="s3">, </span><span class="s1">*</span><span class="s3">, </span><span class="s1">impl</span><span class="s3">, </span><span class="s1">_in_avals</span><span class="s3">, </span><span class="s1">_out_aval):</span>

  <span class="s3">def </span><span class="s1">impl_wrapper(seeds: TfVal</span><span class="s3">, </span><span class="s1">*</span><span class="s3">, </span><span class="s1">impl):</span>
    <span class="s3">return </span><span class="s1">prng.random_seed_impl_base(seeds</span><span class="s3">, </span><span class="s1">impl=impl)</span>

  <span class="s1">converted_impl = _convert_jax_impl(</span>
      <span class="s1">impl_wrapper</span><span class="s3">, </span><span class="s1">multiple_results=</span><span class="s3">False, </span><span class="s1">with_physical_avals=</span><span class="s3">True,</span>
      <span class="s1">extra_name_stack=</span><span class="s4">&quot;random_seed&quot;</span><span class="s1">)</span>
  <span class="s3">return </span><span class="s1">converted_impl(</span>
      <span class="s1">seeds</span><span class="s3">, </span><span class="s1">impl=impl</span><span class="s3">, </span><span class="s1">_in_avals=_in_avals</span><span class="s3">, </span><span class="s1">_out_aval=_out_aval)</span>

<span class="s1">tf_impl_with_avals[prng.random_seed_p] = _random_seed_impl</span>


<span class="s3">def </span><span class="s1">_random_split_impl(keys: TfVal</span><span class="s3">, </span><span class="s1">*</span><span class="s3">, </span><span class="s1">count</span><span class="s3">, </span><span class="s1">_in_avals</span><span class="s3">, </span><span class="s1">_out_aval):</span>
  <span class="s1">keys_aval</span><span class="s3">, </span><span class="s1">= _in_avals</span>

  <span class="s3">def </span><span class="s1">impl_wrapper(keys: TfVal</span><span class="s3">, </span><span class="s1">*</span><span class="s3">, </span><span class="s1">count):</span>
    <span class="s3">return </span><span class="s1">prng.random_split_impl_base(</span>
        <span class="s1">keys_aval.dtype.impl</span><span class="s3">, </span><span class="s1">keys</span><span class="s3">, </span><span class="s1">keys_aval.ndim</span><span class="s3">, </span><span class="s1">count=count)</span>

  <span class="s1">converted_impl = _convert_jax_impl(</span>
      <span class="s1">impl_wrapper</span><span class="s3">, </span><span class="s1">multiple_results=</span><span class="s3">False, </span><span class="s1">with_physical_avals=</span><span class="s3">True,</span>
      <span class="s1">extra_name_stack=</span><span class="s4">&quot;random_split&quot;</span><span class="s1">)</span>
  <span class="s3">return </span><span class="s1">converted_impl(</span>
      <span class="s1">keys</span><span class="s3">, </span><span class="s1">count=count</span><span class="s3">, </span><span class="s1">_in_avals=_in_avals</span><span class="s3">, </span><span class="s1">_out_aval=_out_aval)</span>

<span class="s1">tf_impl_with_avals[prng.random_split_p] = _random_split_impl</span>


<span class="s3">def </span><span class="s1">_random_fold_in_impl(keys: TfVal</span><span class="s3">, </span><span class="s1">msgs: TfVal</span><span class="s3">, </span><span class="s1">*</span><span class="s3">, </span><span class="s1">_in_avals</span><span class="s3">, </span><span class="s1">_out_aval):</span>
  <span class="s1">keys_aval</span><span class="s3">, </span><span class="s1">_ = _in_avals</span>

  <span class="s3">def </span><span class="s1">impl_wrapper(keys: TfVal</span><span class="s3">, </span><span class="s1">msgs: TfVal):</span>
    <span class="s3">return </span><span class="s1">prng.random_fold_in_impl_base(</span>
        <span class="s1">keys_aval.dtype.impl</span><span class="s3">, </span><span class="s1">keys</span><span class="s3">, </span><span class="s1">msgs</span><span class="s3">, </span><span class="s1">keys_aval.shape)</span>

  <span class="s1">converted_impl = _convert_jax_impl(</span>
      <span class="s1">impl_wrapper</span><span class="s3">, </span><span class="s1">multiple_results=</span><span class="s3">False, </span><span class="s1">with_physical_avals=</span><span class="s3">True,</span>
      <span class="s1">extra_name_stack=</span><span class="s4">&quot;random_fold_in&quot;</span><span class="s1">)</span>
  <span class="s3">return </span><span class="s1">converted_impl(</span>
      <span class="s1">keys</span><span class="s3">, </span><span class="s1">msgs</span><span class="s3">, </span><span class="s1">_in_avals=_in_avals</span><span class="s3">, </span><span class="s1">_out_aval=_out_aval)</span>

<span class="s1">tf_impl_with_avals[prng.random_fold_in_p] = _random_fold_in_impl</span>


<span class="s3">def </span><span class="s1">_random_bits_impl(keys: TfVal</span><span class="s3">, </span><span class="s1">*</span><span class="s3">, </span><span class="s1">bit_width</span><span class="s3">, </span><span class="s1">shape</span><span class="s3">, </span><span class="s1">_in_avals</span><span class="s3">, </span><span class="s1">_out_aval):</span>
  <span class="s1">keys_aval</span><span class="s3">, </span><span class="s1">= _in_avals</span>

  <span class="s3">def </span><span class="s1">impl_wrapper(keys: TfVal</span><span class="s3">, </span><span class="s1">**kwargs):</span>
    <span class="s3">return </span><span class="s1">prng.random_bits_impl_base(</span>
        <span class="s1">keys_aval.dtype.impl</span><span class="s3">, </span><span class="s1">keys</span><span class="s3">, </span><span class="s1">keys_aval.ndim</span><span class="s3">,</span>
        <span class="s1">bit_width=bit_width</span><span class="s3">, </span><span class="s1">shape=shape)</span>

  <span class="s1">converted_impl = _convert_jax_impl(</span>
      <span class="s1">impl_wrapper</span><span class="s3">, </span><span class="s1">multiple_results=</span><span class="s3">False, </span><span class="s1">with_physical_avals=</span><span class="s3">True,</span>
      <span class="s1">extra_name_stack=</span><span class="s4">&quot;random_bits&quot;</span><span class="s1">)</span>
  <span class="s3">return </span><span class="s1">converted_impl(keys</span><span class="s3">, </span><span class="s1">bit_width=bit_width</span><span class="s3">, </span><span class="s1">shape=shape</span><span class="s3">,</span>
                        <span class="s1">_in_avals=_in_avals</span><span class="s3">, </span><span class="s1">_out_aval=_out_aval)</span>

<span class="s1">tf_impl_with_avals[prng.random_bits_p] = _random_bits_impl</span>


<span class="s3">def </span><span class="s1">_random_wrap_impl(base_arr: TfVal</span><span class="s3">, </span><span class="s1">*</span><span class="s3">, </span><span class="s1">impl</span><span class="s3">, </span><span class="s1">_in_avals</span><span class="s3">, </span><span class="s1">_out_aval):</span>
  <span class="s3">return </span><span class="s1">base_arr</span>

<span class="s1">tf_impl_with_avals[prng.random_wrap_p] = _random_wrap_impl</span>


<span class="s3">def </span><span class="s1">_random_unwrap_impl(keys: TfVal</span><span class="s3">, </span><span class="s1">*</span><span class="s3">, </span><span class="s1">_in_avals</span><span class="s3">, </span><span class="s1">_out_aval):</span>
  <span class="s3">return </span><span class="s1">keys</span>

<span class="s1">tf_impl_with_avals[prng.random_unwrap_p] = _random_unwrap_impl</span>


<span class="s3">def </span><span class="s1">_threefry2x32_jax_impl(*args: TfVal</span><span class="s3">, </span><span class="s1">_in_avals</span><span class="s3">, </span><span class="s1">_out_aval):</span>
  <span class="s1">res = _convert_jax_impl(</span>
      <span class="s1">partial(prng._threefry2x32_lowering</span><span class="s3">, </span><span class="s1">use_rolled_loops=</span><span class="s3">False</span><span class="s1">)</span><span class="s3">,</span>
      <span class="s1">multiple_results=</span><span class="s3">True, </span><span class="s1">extra_name_stack=</span><span class="s4">&quot;threefry&quot;</span><span class="s1">)(</span>
          <span class="s1">*args</span><span class="s3">, </span><span class="s1">_in_avals=_in_avals</span><span class="s3">, </span><span class="s1">_out_aval=_out_aval)</span>
  <span class="s3">return </span><span class="s1">res</span>


<span class="s1">tf_impl_with_avals[prng.threefry2x32_p] = _threefry2x32_jax_impl</span>

<span class="s0"># Use the vmap implementation, otherwise on TPU the performance is really bad</span>
<span class="s0"># With use_vmap=True on, we get about the same performance for JAX and jax2tf.</span>
<span class="s1">tf_impl_with_avals[random.random_gamma_p] = _convert_jax_impl(</span>
    <span class="s1">partial(random_internal._gamma_impl</span><span class="s3">, </span><span class="s1">use_vmap=</span><span class="s3">True</span><span class="s1">)</span><span class="s3">,</span>
    <span class="s1">multiple_results=</span><span class="s3">False, </span><span class="s1">extra_name_stack=</span><span class="s4">&quot;random_gamma&quot;</span><span class="s1">)</span>


<span class="s3">def </span><span class="s1">_rng_bit_generator(key: TfVal</span><span class="s3">, </span><span class="s1">*</span><span class="s3">, </span><span class="s1">shape</span><span class="s3">, </span><span class="s1">dtype</span><span class="s3">, </span><span class="s1">algorithm) -&gt; Sequence[TfVal]:</span>
  <span class="s1">is_uint32_key = key.dtype == _to_tf_dtype(jnp.uint32)</span>
  <span class="s3">if </span><span class="s1">is_uint32_key:</span>
    <span class="s1">key = tf.reshape(key</span><span class="s3">, </span><span class="s1">(</span><span class="s5">2</span><span class="s3">, </span><span class="s5">2</span><span class="s1">))</span>
    <span class="s1">key = tfxla.bitcast_convert_type(key</span><span class="s3">, </span><span class="s1">_to_tf_dtype(jnp.uint64))</span>
  <span class="s1">shape_tf = _eval_shape(shape)</span>
  <span class="s0"># JAX uses XLA algorithm enums; tfxla uses tf.random.Algorithm</span>
  <span class="s3">if </span><span class="s1">algorithm == lax.RandomAlgorithm.RNG_THREE_FRY:</span>
    <span class="s1">algorithm_tf = tf.random.Algorithm.THREEFRY</span>
  <span class="s3">elif </span><span class="s1">algorithm == lax.RandomAlgorithm.RNG_PHILOX:</span>
    <span class="s1">algorithm_tf = tf.random.Algorithm.PHILOX</span>
  <span class="s3">elif </span><span class="s1">algorithm == lax.RandomAlgorithm.RNG_DEFAULT:</span>
    <span class="s1">algorithm_tf = tf.random.Algorithm.AUTO_SELECT</span>
  <span class="s3">else</span><span class="s1">:</span>
    <span class="s3">assert False</span>
  <span class="s1">(new_key</span><span class="s3">, </span><span class="s1">res) = tfxla.rng_bit_generator(algorithm_tf.value</span><span class="s3">, </span><span class="s1">key</span><span class="s3">, </span><span class="s1">shape_tf</span><span class="s3">,</span>
                                           <span class="s1">dtype=_to_tf_dtype(dtype))</span>
  <span class="s3">if </span><span class="s1">is_uint32_key:</span>
    <span class="s1">new_key = tfxla.bitcast_convert_type(new_key</span><span class="s3">, </span><span class="s1">_to_tf_dtype(jnp.uint32))</span>
    <span class="s1">new_key = tf.reshape(new_key</span><span class="s3">, </span><span class="s1">(</span><span class="s5">4</span><span class="s3">,</span><span class="s1">))</span>
  <span class="s3">if </span><span class="s1">_WRAP_JAX_JIT_WITH_TF_FUNCTION:</span>
    <span class="s0"># See #7839</span>
    <span class="s1">new_key = tf.stop_gradient(new_key)</span>
    <span class="s1">res = tf.stop_gradient(res)</span>
  <span class="s3">return </span><span class="s1">new_key</span><span class="s3">, </span><span class="s1">res</span>


<span class="s1">tf_impl[lax.rng_bit_generator_p] = _rng_bit_generator</span>


<span class="s3">def </span><span class="s1">_rng_uniform(minval: TfVal</span><span class="s3">, </span><span class="s1">maxval: TfVal</span><span class="s3">, </span><span class="s1">*</span><span class="s3">, </span><span class="s1">shape) -&gt; TfVal:</span>
  <span class="s1">shape_tf = _eval_shape(shape)</span>
  <span class="s3">return </span><span class="s1">tf.random.uniform(shape_tf</span><span class="s3">, </span><span class="s1">minval=minval</span><span class="s3">, </span><span class="s1">maxval=maxval</span><span class="s3">, </span><span class="s1">dtype=minval.dtype)</span>

<span class="s1">tf_impl[lax.rng_uniform_p] = _rng_uniform</span>


<span class="s3">def </span><span class="s1">_iota_2x32_shape(*</span><span class="s3">, </span><span class="s1">shape):</span>
  <span class="s3">def </span><span class="s1">_add(x</span><span class="s3">, </span><span class="s1">y): </span><span class="s3">return </span><span class="s1">x + y</span>
  <span class="s3">def </span><span class="s1">_mul(x</span><span class="s3">, </span><span class="s1">y): </span><span class="s3">return </span><span class="s1">x * y</span>
  <span class="s3">def </span><span class="s1">_cast32(xs): </span><span class="s3">return </span><span class="s1">tf.dtypes.cast(xs</span><span class="s3">, </span><span class="s1">_to_tf_dtype(jnp.uint32))</span>
  <span class="s1">iotas = [_iota(dtype=jnp.uint64</span><span class="s3">, </span><span class="s1">shape=shape</span><span class="s3">, </span><span class="s1">dimension=dimension)</span>
           <span class="s3">for </span><span class="s1">dimension </span><span class="s3">in </span><span class="s1">range(len(shape))]</span>
  <span class="s1">counts = prng.bcast_iotas_to_reshaped_iota(_add</span><span class="s3">, </span><span class="s1">_mul</span><span class="s3">, </span><span class="s1">shape</span><span class="s3">, </span><span class="s1">iotas)</span>
  <span class="s1">counts_lo = _cast32(counts)</span>
  <span class="s1">counts_hi = _cast32(tf.bitwise.right_shift(counts</span><span class="s3">, </span><span class="s5">32</span><span class="s1">))</span>
  <span class="s3">return </span><span class="s1">counts_hi</span><span class="s3">, </span><span class="s1">counts_lo</span>

<span class="s1">tf_impl[prng.iota_2x32_shape_p] = _iota_2x32_shape</span>


<span class="s3">def </span><span class="s1">_gather_dimensions_proto(indices_shape</span><span class="s3">, </span><span class="s1">dimension_numbers):</span>
  <span class="s1">proto = xla_data_pb2.GatherDimensionNumbers()</span>
  <span class="s1">proto.offset_dims.extend(dimension_numbers.offset_dims)</span>
  <span class="s1">proto.collapsed_slice_dims.extend(dimension_numbers.collapsed_slice_dims)</span>
  <span class="s1">proto.start_index_map.extend(dimension_numbers.start_index_map)</span>
  <span class="s3">assert </span><span class="s1">indices_shape</span>
  <span class="s1">proto.index_vector_dim = len(indices_shape) - </span><span class="s5">1</span>
  <span class="s3">return </span><span class="s1">proto</span>


<span class="s3">def </span><span class="s1">_maybe_cast_to_int64(x: TfVal) -&gt; TfVal:</span>
  <span class="s3">if </span><span class="s1">x.dtype != tf.int32 </span><span class="s3">and </span><span class="s1">x.dtype != tf.int64:</span>
    <span class="s3">return </span><span class="s1">tf.cast(x</span><span class="s3">, </span><span class="s1">tf.int64)</span>
  <span class="s3">return </span><span class="s1">x</span>


<span class="s1">@partial(handle_boolean_args</span><span class="s3">, </span><span class="s1">argnums=[</span><span class="s5">0</span><span class="s1">])</span>
<span class="s3">def </span><span class="s1">_gather(operand</span><span class="s3">, </span><span class="s1">start_indices</span><span class="s3">, </span><span class="s1">*</span><span class="s3">, </span><span class="s1">dimension_numbers</span><span class="s3">, </span><span class="s1">slice_sizes: core.Shape</span><span class="s3">,</span>
            <span class="s1">indices_are_sorted</span><span class="s3">, </span><span class="s1">unique_indices</span><span class="s3">, </span><span class="s1">mode</span><span class="s3">, </span><span class="s1">fill_value</span><span class="s3">,</span>
            <span class="s1">_in_avals: Sequence[core.ShapedArray]</span><span class="s3">,</span>
            <span class="s1">_out_aval: core.ShapedArray):</span>
  <span class="s2">&quot;&quot;&quot;Tensorflow implementation of gather.&quot;&quot;&quot;</span>
  <span class="s3">if </span><span class="s1">mode == lax.GatherScatterMode.FILL_OR_DROP:</span>
    <span class="s1">gather_fill_fn = _convert_jax_impl(lax_slicing._gather_fill</span><span class="s3">,</span>
                                       <span class="s1">multiple_results=</span><span class="s3">False</span><span class="s1">)</span>
    <span class="s3">return </span><span class="s1">gather_fill_fn(</span>
        <span class="s1">operand</span><span class="s3">, </span><span class="s1">start_indices</span><span class="s3">, </span><span class="s1">dimension_numbers=dimension_numbers</span><span class="s3">,</span>
        <span class="s1">slice_sizes=slice_sizes</span><span class="s3">, </span><span class="s1">unique_indices=unique_indices</span><span class="s3">,</span>
        <span class="s1">indices_are_sorted=indices_are_sorted</span><span class="s3">, </span><span class="s1">fill_value=fill_value</span><span class="s3">,</span>
        <span class="s1">output_shape=_out_aval.shape</span><span class="s3">, </span><span class="s1">_in_avals=_in_avals</span><span class="s3">, </span><span class="s1">_out_aval=_out_aval)</span>

  <span class="s1">operand_aval = _in_avals[</span><span class="s5">0</span><span class="s1">]</span>
  <span class="s1">start_indices = _maybe_cast_to_int64(start_indices)</span>
  <span class="s3">if </span><span class="s1">core.is_opaque_dtype(operand_aval.dtype):</span>
    <span class="s1">opaque_shape = _jax_physical_aval(operand_aval).shape[len(operand_aval.shape):]</span>
    <span class="s1">trailing_offset_dims = [len(_out_aval.shape) + i </span><span class="s3">for </span><span class="s1">i </span><span class="s3">in </span><span class="s1">range(len(opaque_shape))]</span>
    <span class="s1">dimension_numbers = dimension_numbers._replace(</span>
        <span class="s1">offset_dims=(*dimension_numbers.offset_dims</span><span class="s3">, </span><span class="s1">*trailing_offset_dims))</span>
    <span class="s1">slice_sizes = (*slice_sizes</span><span class="s3">, </span><span class="s1">*opaque_shape)</span>
  <span class="s1">proto = _gather_dimensions_proto(start_indices.shape</span><span class="s3">, </span><span class="s1">dimension_numbers)</span>
  <span class="s1">slice_sizes_tf = _eval_shape(slice_sizes)</span>
  <span class="s1">out = tfxla.gather(operand</span><span class="s3">, </span><span class="s1">start_indices</span><span class="s3">, </span><span class="s1">proto</span><span class="s3">, </span><span class="s1">slice_sizes_tf</span><span class="s3">,</span>
                     <span class="s1">indices_are_sorted)</span>
  <span class="s1">out = _ensure_tf_shape_if_dynamic(out</span><span class="s3">, </span><span class="s1">_aval_to_tf_shape(_out_aval))</span>
  <span class="s3">if </span><span class="s1">_WRAP_JAX_JIT_WITH_TF_FUNCTION:</span>
    <span class="s1">out = tf.stop_gradient(out)  </span><span class="s0"># See #7839</span>
  <span class="s3">return </span><span class="s1">out</span>


<span class="s1">tf_impl_with_avals[lax.gather_p] = _gather</span>


<span class="s3">def </span><span class="s1">_slice(operand</span><span class="s3">, </span><span class="s1">start_indices</span><span class="s3">, </span><span class="s1">limit_indices</span><span class="s3">, </span><span class="s1">strides</span><span class="s3">, </span><span class="s1">_in_avals</span><span class="s3">,</span>
           <span class="s1">_out_aval):</span>
  <span class="s3">if </span><span class="s1">strides </span><span class="s3">is None</span><span class="s1">:</span>
    <span class="s1">strides = [</span><span class="s5">1</span><span class="s1">] * len(start_indices)</span>
  <span class="s1">slices = tuple(</span>
      <span class="s1">map(slice</span><span class="s3">, </span><span class="s1">_eval_shape(start_indices)</span><span class="s3">, </span><span class="s1">_eval_shape(limit_indices)</span><span class="s3">,</span>
          <span class="s1">_eval_shape(strides)))</span>
  <span class="s1">out = operand[slices]</span>
  <span class="s0"># TODO(b/184503314): improve shape inference for __getitem__</span>
  <span class="s0"># E.g., operand.shape=(b, 5, 3), start_indices=(0, 1, 1), limit_indices=(b, 5, 3), strides=(1, 2, 1)</span>
  <span class="s1">out = _ensure_tf_shape_if_dynamic(out</span><span class="s3">, </span><span class="s1">_aval_to_tf_shape(_out_aval))</span>
  <span class="s3">return </span><span class="s1">out</span>


<span class="s1">tf_impl_with_avals[lax.slice_p] = _slice</span>


<span class="s3">def </span><span class="s1">_dynamic_slice(operand</span><span class="s3">, </span><span class="s1">*start_indices</span><span class="s3">, </span><span class="s1">slice_sizes: core.Shape</span><span class="s3">,</span>
                   <span class="s1">_in_avals: Sequence[core.ShapedArray]</span><span class="s3">,</span>
                   <span class="s1">_out_aval: core.ShapedArray):</span>
  <span class="s1">start_indices = _maybe_cast_to_int64(tf.stack(start_indices))</span>
  <span class="s1">operand_aval = _in_avals[</span><span class="s5">0</span><span class="s1">]</span>
  <span class="s3">if </span><span class="s1">core.is_opaque_dtype(operand_aval.dtype):</span>
    <span class="s1">opaque_shape = _jax_physical_aval(operand_aval).shape[len(operand_aval.shape):]</span>
    <span class="s1">slice_sizes = (*slice_sizes</span><span class="s3">, </span><span class="s1">*opaque_shape)</span>
    <span class="s1">start_indices = tf.concat([start_indices</span><span class="s3">, </span><span class="s1">tf.zeros((len(opaque_shape)</span><span class="s3">,</span><span class="s1">)</span><span class="s3">,</span>
                                                       <span class="s1">dtype=start_indices.dtype)]</span><span class="s3">,</span>
                              <span class="s1">axis=</span><span class="s5">0</span><span class="s1">)</span>

  <span class="s1">slice_sizes_tf = _eval_shape(slice_sizes)</span>
  <span class="s1">res = tfxla.dynamic_slice(operand</span><span class="s3">, </span><span class="s1">start_indices</span><span class="s3">, </span><span class="s1">size_indices=slice_sizes_tf)</span>
  <span class="s3">if </span><span class="s1">_WRAP_JAX_JIT_WITH_TF_FUNCTION:</span>
    <span class="s1">res = tf.stop_gradient(res)  </span><span class="s0"># See #7839</span>
  <span class="s3">return </span><span class="s1">res</span>


<span class="s1">tf_impl_with_avals[lax.dynamic_slice_p] = _dynamic_slice</span>


<span class="s3">def </span><span class="s1">_dynamic_update_slice(operand</span><span class="s3">, </span><span class="s1">update</span><span class="s3">, </span><span class="s1">*start_indices</span><span class="s3">,</span>
                          <span class="s1">_in_avals: Sequence[core.ShapedArray]</span><span class="s3">,</span>
                          <span class="s1">_out_aval: core.ShapedArray):</span>
  <span class="s1">start_indices = _maybe_cast_to_int64(tf.stack(start_indices))</span>
  <span class="s1">operand_aval = _in_avals[</span><span class="s5">0</span><span class="s1">]</span>
  <span class="s3">if </span><span class="s1">core.is_opaque_dtype(operand_aval.dtype):</span>
    <span class="s1">opaque_shape = _jax_physical_aval(operand_aval).shape[len(operand_aval.shape):]</span>
    <span class="s1">start_indices = tf.concat([start_indices</span><span class="s3">, </span><span class="s1">tf.zeros((len(opaque_shape)</span><span class="s3">,</span><span class="s1">)</span><span class="s3">,</span>
                                                       <span class="s1">dtype=start_indices.dtype)]</span><span class="s3">,</span>
                              <span class="s1">axis=</span><span class="s5">0</span><span class="s1">)</span>
  <span class="s1">out = tfxla.dynamic_update_slice(operand</span><span class="s3">, </span><span class="s1">update</span><span class="s3">, </span><span class="s1">start_indices)</span>
  <span class="s3">if </span><span class="s1">_WRAP_JAX_JIT_WITH_TF_FUNCTION:</span>
    <span class="s1">out = tf.stop_gradient(out)  </span><span class="s0"># See #7839</span>
  <span class="s3">return </span><span class="s1">out</span>


<span class="s1">tf_impl_with_avals[lax.dynamic_update_slice_p] = _dynamic_update_slice</span>


<span class="s3">def </span><span class="s1">_scatter_dimensions_proto(indices_shape</span><span class="s3">, </span><span class="s1">dimension_numbers):</span>
  <span class="s1">proto = xla_data_pb2.ScatterDimensionNumbers()</span>
  <span class="s1">proto.update_window_dims.extend(dimension_numbers.update_window_dims)</span>
  <span class="s1">proto.inserted_window_dims.extend(dimension_numbers.inserted_window_dims)</span>
  <span class="s1">proto.scatter_dims_to_operand_dims.extend(</span>
      <span class="s1">dimension_numbers.scatter_dims_to_operand_dims)</span>
  <span class="s3">assert </span><span class="s1">indices_shape</span>
  <span class="s1">proto.index_vector_dim = len(indices_shape) - </span><span class="s5">1</span>
  <span class="s3">return </span><span class="s1">proto</span>


<span class="s3">def </span><span class="s1">_scatter(operand</span><span class="s3">, </span><span class="s1">scatter_indices</span><span class="s3">, </span><span class="s1">updates</span><span class="s3">, </span><span class="s1">*</span><span class="s3">, </span><span class="s1">update_jaxpr</span><span class="s3">, </span><span class="s1">update_consts</span><span class="s3">,</span>
             <span class="s1">dimension_numbers</span><span class="s3">, </span><span class="s1">indices_are_sorted</span><span class="s3">, </span><span class="s1">unique_indices</span><span class="s3">, </span><span class="s1">mode</span><span class="s3">,</span>
             <span class="s1">_in_avals: Sequence[core.ShapedArray]</span><span class="s3">,</span>
             <span class="s1">_out_aval: core.ShapedArray):</span>
  <span class="s3">del </span><span class="s1">unique_indices</span>

  <span class="s3">if </span><span class="s1">mode == lax.GatherScatterMode.CLIP:</span>
    <span class="s1">clip_fn = _convert_jax_impl(lax_slicing._clamp_scatter_indices</span><span class="s3">,</span>
                                <span class="s1">multiple_results=</span><span class="s3">False</span><span class="s1">)</span>
    <span class="s1">scatter_indices = clip_fn(</span>
        <span class="s1">operand</span><span class="s3">, </span><span class="s1">scatter_indices</span><span class="s3">, </span><span class="s1">updates</span><span class="s3">, </span><span class="s1">dnums=dimension_numbers</span><span class="s3">,</span>
        <span class="s1">_in_avals=_in_avals</span><span class="s3">, </span><span class="s1">_out_aval=_in_avals[</span><span class="s5">1</span><span class="s1">])</span>

  <span class="s3">assert </span><span class="s1">len(update_consts) == </span><span class="s5">0</span><span class="s3">, </span><span class="s4">&quot;Update computation cannot have constants&quot;</span>

  <span class="s1">proto = _scatter_dimensions_proto(scatter_indices.shape</span><span class="s3">, </span><span class="s1">dimension_numbers)</span>

  <span class="s3">def </span><span class="s1">update_computation(arg1: TfVal</span><span class="s3">, </span><span class="s1">arg2: TfVal) -&gt; TfVal:</span>
    <span class="s1">closed_jaxpr = core.ClosedJaxpr(update_jaxpr</span><span class="s3">, </span><span class="s1">update_consts)</span>
    <span class="s1">res</span><span class="s3">, </span><span class="s1">= _interpret_jaxpr(closed_jaxpr</span><span class="s3">, </span><span class="s1">arg1</span><span class="s3">, </span><span class="s1">arg2</span><span class="s3">, </span><span class="s1">extra_name_stack=</span><span class="s3">None</span><span class="s1">)</span>
    <span class="s3">return </span><span class="s1">res</span>

  <span class="s1">o_spec = tf.TensorSpec(()</span><span class="s3">, </span><span class="s1">dtype=operand.dtype)</span>
  <span class="s1">xla_update_computation = (</span>
      <span class="s1">tf.function(update_computation</span><span class="s3">,</span>
                  <span class="s1">autograph=</span><span class="s3">False</span><span class="s1">).get_concrete_function(o_spec</span><span class="s3">, </span><span class="s1">o_spec))</span>
  <span class="s1">out = tfxla.scatter(</span>
      <span class="s1">operand</span><span class="s3">,</span>
      <span class="s1">scatter_indices</span><span class="s3">,</span>
      <span class="s1">updates</span><span class="s3">,</span>
      <span class="s1">xla_update_computation</span><span class="s3">,</span>
      <span class="s1">proto</span><span class="s3">,</span>
      <span class="s1">indices_are_sorted=indices_are_sorted)</span>
  <span class="s3">if </span><span class="s1">_WRAP_JAX_JIT_WITH_TF_FUNCTION:</span>
    <span class="s1">out = tf.stop_gradient(out)  </span><span class="s0"># See #7839</span>
  <span class="s3">return </span><span class="s1">out</span>


<span class="s1">tf_impl_with_avals[lax.scatter_p] = _scatter</span>
<span class="s1">tf_impl_with_avals[lax.scatter_min_p] = _scatter</span>
<span class="s1">tf_impl_with_avals[lax.scatter_max_p] = _scatter</span>
<span class="s1">tf_impl_with_avals[lax.scatter_mul_p] = _scatter</span>
<span class="s1">tf_impl_with_avals[lax.scatter_add_p] = _scatter</span>


<span class="s3">def </span><span class="s1">_cond(index: TfVal</span><span class="s3">, </span><span class="s1">*operands: TfVal</span><span class="s3">, </span><span class="s1">branches: Sequence[core.ClosedJaxpr]</span><span class="s3">,</span>
          <span class="s1">linear: Sequence[bool]) -&gt; Sequence[TfVal]:</span>
  <span class="s3">del </span><span class="s1">linear</span>
  <span class="s0"># tf.cond needs lambdas with no arguments.</span>
  <span class="s1">branches_tf = [</span>
      <span class="s1">partial(_interpret_jaxpr</span><span class="s3">, </span><span class="s1">jaxpr</span><span class="s3">, </span><span class="s1">*operands</span><span class="s3">,</span>
              <span class="s0"># Same name stack as the XLA translation of cond_p</span>
              <span class="s1">extra_name_stack=</span><span class="s4">f&quot;branch_</span><span class="s3">{</span><span class="s1">i</span><span class="s3">}</span><span class="s4">_fun&quot;</span><span class="s1">)</span>
      <span class="s3">for </span><span class="s1">i</span><span class="s3">, </span><span class="s1">jaxpr </span><span class="s3">in </span><span class="s1">enumerate(branches)</span>
  <span class="s1">]</span>
  <span class="s0"># Same name stack as XLA translation of cond_p</span>
  <span class="s0"># Note: extend_name_stack is a contextmanager, which is callable as a decorator.</span>
  <span class="s1">branches_tf = list(map(source_info_util.extend_name_stack(</span><span class="s4">&quot;cond&quot;</span><span class="s1">)</span><span class="s3">,  </span><span class="s0"># type: ignore[arg-type]</span>
      <span class="s1">branches_tf))</span>
  <span class="s3">return </span><span class="s1">tf.switch_case(index</span><span class="s3">, </span><span class="s1">branches_tf)</span>


<span class="s1">tf_impl[lax.cond_p] = _cond</span>


<span class="s3">def </span><span class="s1">_while(*args: TfVal</span><span class="s3">, </span><span class="s1">cond_nconsts: int</span><span class="s3">, </span><span class="s1">cond_jaxpr: core.ClosedJaxpr</span><span class="s3">,</span>
           <span class="s1">body_nconsts: int</span><span class="s3">, </span><span class="s1">body_jaxpr: core.ClosedJaxpr) -&gt; Sequence[TfVal]:</span>
  <span class="s1">cond_consts</span><span class="s3">, </span><span class="s1">body_consts</span><span class="s3">, </span><span class="s1">init_carry = util.split_list(</span>
      <span class="s1">args</span><span class="s3">, </span><span class="s1">[cond_nconsts</span><span class="s3">, </span><span class="s1">body_nconsts])</span>
  <span class="s3">if </span><span class="s1">cond_jaxpr.out_avals[</span><span class="s5">0</span><span class="s1">].shape:  </span><span class="s0"># type: ignore[attr-defined]</span>
    <span class="s0"># The conditional is not a scalar, this must be a batched while</span>
    <span class="s3">return </span><span class="s1">_batched_cond_while(</span>
        <span class="s1">*args</span><span class="s3">,</span>
        <span class="s1">cond_nconsts=cond_nconsts</span><span class="s3">,</span>
        <span class="s1">cond_jaxpr=cond_jaxpr</span><span class="s3">,</span>
        <span class="s1">body_nconsts=body_nconsts</span><span class="s3">,</span>
        <span class="s1">body_jaxpr=body_jaxpr)</span>

  <span class="s0"># The conditional must return a single value to TF</span>
  <span class="s3">def </span><span class="s1">cond_tf_func(*args: TfVal) -&gt; TfVal:</span>
    <span class="s1">pred</span><span class="s3">, </span><span class="s1">= _interpret_jaxpr(cond_jaxpr</span><span class="s3">, </span><span class="s1">*cond_consts</span><span class="s3">, </span><span class="s1">*args</span><span class="s3">,</span>
                             <span class="s0"># Same name stack as the XLA translation of while_p</span>
                             <span class="s1">extra_name_stack=</span><span class="s4">&quot;while/cond&quot;</span><span class="s1">)</span>
    <span class="s3">return </span><span class="s1">pred</span>

  <span class="s1">body_tf_func = partial(_interpret_jaxpr</span><span class="s3">, </span><span class="s1">body_jaxpr</span><span class="s3">, </span><span class="s1">*body_consts</span><span class="s3">,</span>
                         <span class="s1">extra_name_stack=</span><span class="s4">&quot;while/body&quot;</span><span class="s1">)</span>
  <span class="s0"># Sometimes TF infers more specific shapes for the init_carry, and this has</span>
  <span class="s0"># led to errors: &quot;enters the loop with shape (1,), but has shape (None,) after one iteration&quot;</span>
  <span class="s1">shape_invariants = [tf.TensorShape(_aval_to_tf_shape(_out_aval))</span>
                      <span class="s3">for </span><span class="s1">_out_aval </span><span class="s3">in </span><span class="s1">body_jaxpr.out_avals]</span>
  <span class="s3">return </span><span class="s1">tf.while_loop(cond_tf_func</span><span class="s3">, </span><span class="s1">body_tf_func</span><span class="s3">, </span><span class="s1">init_carry</span><span class="s3">,</span>
                       <span class="s1">shape_invariants=shape_invariants)</span>


<span class="s3">def </span><span class="s1">_batched_cond_while(*args: TfVal</span><span class="s3">, </span><span class="s1">cond_nconsts: int</span><span class="s3">,</span>
                        <span class="s1">cond_jaxpr: core.ClosedJaxpr</span><span class="s3">, </span><span class="s1">body_nconsts: int</span><span class="s3">,</span>
                        <span class="s1">body_jaxpr: core.ClosedJaxpr) -&gt; Sequence[TfVal]:</span>
  <span class="s2">&quot;&quot;&quot;Interprets a while_loop with a batched condition. 
 
  A batched while has a conditional that returns a tensor of booleans, and 
  a body that returns a list of tensors whose leading dimensions match those 
  of the conditional tensor. 
 
  We need to turn it into a while with scalar boolean conditional. We will 
  expand the loop carry to include a prefix with the current tensor boolean 
  condition. We prepend to the loop the first calculation of the tensor boolean 
  condition. The loop condition will use a &quot;reduce_any&quot; to calculate a scalar 
  boolean from the tensor boolean condition. The end of the loop body will 
  compute the new carry using a &quot;tf.where&quot;, and we compute the new tensor 
  boolean condition. 
  &quot;&quot;&quot;</span>
  <span class="s1">cond_consts</span><span class="s3">, </span><span class="s1">body_consts</span><span class="s3">, </span><span class="s1">init_carry = util.split_list(</span>
      <span class="s1">args</span><span class="s3">, </span><span class="s1">[cond_nconsts</span><span class="s3">, </span><span class="s1">body_nconsts])</span>
  <span class="s0"># Initial computation of batched condition</span>
  <span class="s1">init_pred_b</span><span class="s3">, </span><span class="s1">= _interpret_jaxpr(cond_jaxpr</span><span class="s3">, </span><span class="s1">*cond_consts</span><span class="s3">, </span><span class="s1">*init_carry</span><span class="s3">,</span>
                                  <span class="s1">extra_name_stack=</span><span class="s4">&quot;while/body_pred&quot;</span><span class="s1">)</span>

  <span class="s3">def </span><span class="s1">new_cond_tf_func(pred_b: TfVal</span><span class="s3">, </span><span class="s1">*carry: TfVal) -&gt; TfVal:</span>
    <span class="s1">pred = tf.reduce_any(pred_b</span><span class="s3">, </span><span class="s1">axis=list(range(len(pred_b.shape))))</span>
    <span class="s3">return </span><span class="s1">pred</span>

  <span class="s3">def </span><span class="s1">new_body_tf_func(pred_b: TfVal</span><span class="s3">, </span><span class="s1">*carry: TfVal) -&gt; Sequence[TfVal]:</span>
    <span class="s1">new_carry: Sequence[TfVal] = _interpret_jaxpr(body_jaxpr</span><span class="s3">, </span><span class="s1">*body_consts</span><span class="s3">,</span>
                                                  <span class="s1">*carry</span><span class="s3">,</span>
                                                  <span class="s1">extra_name_stack=</span><span class="s4">&quot;while/body&quot;</span><span class="s1">)</span>
    <span class="s0"># We repeat those carries for which the loop termination condition is false</span>
    <span class="s3">def </span><span class="s1">select_one_carry(new_c: TfVal</span><span class="s3">, </span><span class="s1">c: TfVal</span><span class="s3">, </span><span class="s1">c_aval: core.ShapedArray) -&gt; TfVal:</span>
      <span class="s1">pred_b_bcast = _broadcast_in_dim(</span>
          <span class="s1">pred_b</span><span class="s3">,</span>
          <span class="s1">shape=_jax_physical_aval(c_aval).shape</span><span class="s3">,  </span><span class="s0"># a JAX shape</span>
          <span class="s1">broadcast_dimensions=list(range(len(pred_b.shape)))</span><span class="s3">,</span>
          <span class="s1">_in_avals=cond_jaxpr.out_avals</span><span class="s3">,</span>
          <span class="s1">_out_aval=core.ShapedArray(c_aval.shape</span><span class="s3">, </span><span class="s1">np.bool_))</span>
      <span class="s3">return </span><span class="s1">tf.where(pred_b_bcast</span><span class="s3">, </span><span class="s1">new_c</span><span class="s3">, </span><span class="s1">c)</span>

    <span class="s1">selected_carry: Sequence[TfVal] = list(map(select_one_carry</span><span class="s3">, </span><span class="s1">new_carry</span><span class="s3">, </span><span class="s1">carry</span><span class="s3">, </span><span class="s1">body_jaxpr.out_avals))</span>
    <span class="s1">next_pred_b</span><span class="s3">, </span><span class="s1">= _interpret_jaxpr(cond_jaxpr</span><span class="s3">, </span><span class="s1">*cond_consts</span><span class="s3">, </span><span class="s1">*selected_carry</span><span class="s3">,</span>
                                    <span class="s1">extra_name_stack=</span><span class="s4">&quot;body_pred&quot;</span><span class="s1">)</span>
    <span class="s3">return </span><span class="s1">(next_pred_b</span><span class="s3">, </span><span class="s1">*selected_carry)</span>

  <span class="s1">_</span><span class="s3">, </span><span class="s1">*res_carry = tf.while_loop(new_cond_tf_func</span><span class="s3">, </span><span class="s1">new_body_tf_func</span><span class="s3">,</span>
                                <span class="s1">(init_pred_b</span><span class="s3">, </span><span class="s1">*init_carry))</span>
  <span class="s3">return </span><span class="s1">res_carry</span>


<span class="s1">tf_impl[lax.while_p] = _while</span>

<span class="s0"># We use the scan impl rule to rewrite in terms of while.</span>
<span class="s1">tf_impl_with_avals[lax.scan_p] = _convert_jax_impl(</span>
    <span class="s1">lax_control_flow._scan_impl</span><span class="s3">,</span>
    <span class="s1">extra_name_stack=</span><span class="s4">&quot;scan&quot;</span><span class="s1">)</span>

<span class="s1">tf_impl_with_avals[ad_checkpoint.remat_p] = \</span>
  <span class="s1">_convert_jax_impl(partial(ad_checkpoint.remat_lowering</span><span class="s3">,</span>
                            <span class="s0"># TODO: jax2tf cannot discriminate by platform</span>
                            <span class="s1">is_gpu_platform=</span><span class="s3">False</span><span class="s1">)</span><span class="s3">,</span>
                    <span class="s1">multiple_results=</span><span class="s3">True,</span>
                    <span class="s1">extra_name_stack=</span><span class="s4">&quot;checkpoint&quot;</span><span class="s1">)</span>

<span class="s1">tf_impl[ad_checkpoint.name_p] = </span><span class="s3">lambda </span><span class="s1">x</span><span class="s3">, </span><span class="s1">*</span><span class="s3">, </span><span class="s1">name: x</span>

<span class="s0"># TODO: Remove once tensorflow is 2.10.0 everywhere.</span>
<span class="s3">if </span><span class="s1">hasattr(tfxla</span><span class="s3">, </span><span class="s4">'optimization_barrier'</span><span class="s1">):</span>
  <span class="s1">tf_impl[lax_control_flow.optimization_barrier_p] = tfxla.optimization_barrier</span>

<span class="s3">def </span><span class="s1">_top_k(operand: TfVal</span><span class="s3">, </span><span class="s1">k: int) -&gt; Tuple[TfVal</span><span class="s3">, </span><span class="s1">TfVal]:</span>
  <span class="s0"># Some types originally incompatible with tf.math.top_k can be promoted</span>
  <span class="s0"># to a compatible type without loss of precision.</span>
  <span class="s3">def </span><span class="s1">promote_tf_dtype(tf_dtype):</span>
    <span class="s3">if </span><span class="s1">tf_dtype </span><span class="s3">in </span><span class="s1">[tf.bool</span><span class="s3">, </span><span class="s1">tf.uint8</span><span class="s3">, </span><span class="s1">tf.uint16]:</span>
      <span class="s3">return </span><span class="s1">tf.uint32</span>
    <span class="s3">if </span><span class="s1">tf_dtype </span><span class="s3">in </span><span class="s1">[tf.int8</span><span class="s3">, </span><span class="s1">tf.int16]:</span>
      <span class="s3">return </span><span class="s1">tf.int32</span>
    <span class="s3">if </span><span class="s1">tf_dtype </span><span class="s3">is </span><span class="s1">tf.float16:</span>
      <span class="s3">return </span><span class="s1">tf.float32</span>
    <span class="s3">return None</span>

  <span class="s1">conversion_dtype = promote_tf_dtype(operand.dtype)</span>
  <span class="s3">if </span><span class="s1">conversion_dtype:</span>
    <span class="s1">values</span><span class="s3">, </span><span class="s1">indices = tf.math.top_k(</span>
        <span class="s1">tf.dtypes.cast(operand</span><span class="s3">, </span><span class="s1">conversion_dtype)</span><span class="s3">, </span><span class="s1">k=k</span><span class="s3">, </span><span class="s1">sorted=</span><span class="s3">True</span><span class="s1">)</span>
    <span class="s3">return </span><span class="s1">tf.dtypes.cast(values</span><span class="s3">, </span><span class="s1">operand.dtype)</span><span class="s3">, </span><span class="s1">indices</span>
  <span class="s3">else</span><span class="s1">:</span>
    <span class="s3">return </span><span class="s1">tf.math.top_k(operand</span><span class="s3">, </span><span class="s1">k=k</span><span class="s3">, </span><span class="s1">sorted=</span><span class="s3">True</span><span class="s1">)</span>


<span class="s1">tf_impl[lax.top_k_p] = _top_k</span>


<span class="s3">def </span><span class="s1">_approx_top_k(operand: TfVal</span><span class="s3">, </span><span class="s1">k: int</span><span class="s3">, </span><span class="s1">reduction_dimension: int</span><span class="s3">,</span>
                  <span class="s1">recall_target: float</span><span class="s3">, </span><span class="s1">is_max_k: bool</span><span class="s3">,</span>
                  <span class="s1">reduction_input_size_override: int</span><span class="s3">,</span>
                  <span class="s1">aggregate_to_topk: bool) -&gt; Tuple[TfVal</span><span class="s3">, </span><span class="s1">TfVal]:</span>
  <span class="s3">if </span><span class="s1">is_max_k:</span>
    <span class="s3">return </span><span class="s1">tf.math.approx_max_k(operand</span><span class="s3">, </span><span class="s1">k</span><span class="s3">, </span><span class="s1">reduction_dimension</span><span class="s3">, </span><span class="s1">recall_target</span><span class="s3">,</span>
                                <span class="s1">reduction_input_size_override</span><span class="s3">,</span>
                                <span class="s1">aggregate_to_topk)</span>
  <span class="s3">else</span><span class="s1">:</span>
    <span class="s3">return </span><span class="s1">tf.math.approx_min_k(operand</span><span class="s3">, </span><span class="s1">k</span><span class="s3">, </span><span class="s1">reduction_dimension</span><span class="s3">, </span><span class="s1">recall_target</span><span class="s3">,</span>
                                <span class="s1">reduction_input_size_override</span><span class="s3">,</span>
                                <span class="s1">aggregate_to_topk)</span>


<span class="s1">tf_impl[lax.approx_top_k_p] = _approx_top_k</span>


<span class="s3">def </span><span class="s1">_sort(*operands: TfVal</span><span class="s3">, </span><span class="s1">dimension: int</span><span class="s3">, </span><span class="s1">is_stable: bool</span><span class="s3">,</span>
          <span class="s1">num_keys: int) -&gt; Tuple[TfVal</span><span class="s3">, </span><span class="s1">...]:</span>
  <span class="s3">assert </span><span class="s5">1 </span><span class="s1">&lt;= num_keys &lt;= len(operands)</span>
  <span class="s3">assert </span><span class="s5">0 </span><span class="s1">&lt;= dimension &lt; len(</span>
      <span class="s1">operands[</span><span class="s5">0</span><span class="s1">].shape</span>
  <span class="s1">)</span><span class="s3">, </span><span class="s4">f&quot;Invalid </span><span class="s3">{</span><span class="s1">dimension</span><span class="s3">} </span><span class="s4">for ndim </span><span class="s3">{</span><span class="s1">len(operands[</span><span class="s5">0</span><span class="s1">].shape)</span><span class="s3">}</span><span class="s4">&quot;</span>

  <span class="s1">comparator_spec: List[tf.TensorSpec] = []</span>
  <span class="s1">comparator_jax_in_avals: List[core.ShapedArray] = []</span>
  <span class="s3">for </span><span class="s1">op </span><span class="s3">in </span><span class="s1">operands:</span>
    <span class="s1">o_spec = tf.TensorSpec(()</span><span class="s3">, </span><span class="s1">dtype=op.dtype)</span>
    <span class="s1">comparator_spec.extend([o_spec</span><span class="s3">, </span><span class="s1">o_spec])</span>
    <span class="s1">o_aval = core.ShapedArray(()</span><span class="s3">, </span><span class="s1">_to_jax_dtype(op.dtype))</span>
    <span class="s1">comparator_jax_in_avals.extend([o_aval</span><span class="s3">, </span><span class="s1">o_aval])</span>

  <span class="s0"># Use the same comparator that JAX uses when compiling to XLA, to get the</span>
  <span class="s0"># proper NaN/Inf total order, and the lexicographic ordering.</span>
  <span class="s0"># The comparator is a 2N-argument TF function, with arguments [2k] and [2k +1]</span>
  <span class="s0"># corresponding to two scalars from operand[k].</span>
  <span class="s3">def </span><span class="s1">lexicographic_comparator(*tf_args: TfVal) -&gt; TfVal:</span>
    <span class="s3">return </span><span class="s1">_convert_jax_impl(</span>
        <span class="s1">lax_internal._sort_lt_comparator</span><span class="s3">, </span><span class="s1">multiple_results=</span><span class="s3">False</span><span class="s1">)(</span>
            <span class="s1">*tf_args</span><span class="s3">,</span>
            <span class="s1">_in_avals=comparator_jax_in_avals</span><span class="s3">,</span>
            <span class="s1">_out_aval=core.ShapedArray(()</span><span class="s3">, </span><span class="s1">np.bool_)</span><span class="s3">,</span>
            <span class="s1">num_keys=num_keys)</span>

  <span class="s1">xla_comparator_computation = (</span>
      <span class="s1">tf.function(lexicographic_comparator</span><span class="s3">,</span>
                  <span class="s1">autograph=</span><span class="s3">False</span><span class="s1">).get_concrete_function(*comparator_spec))</span>
  <span class="s1">results = tfxla.variadic_sort(</span>
      <span class="s1">operands</span><span class="s3">,</span>
      <span class="s1">dimension=dimension</span><span class="s3">,</span>
      <span class="s1">is_stable=is_stable</span><span class="s3">,</span>
      <span class="s1">comparator=xla_comparator_computation)</span>
  <span class="s3">if </span><span class="s1">_WRAP_JAX_JIT_WITH_TF_FUNCTION:</span>
    <span class="s1">results = tuple(tf.stop_gradient(out) </span><span class="s3">for </span><span class="s1">out </span><span class="s3">in </span><span class="s1">results)  </span><span class="s0"># See #7839</span>
  <span class="s3">return </span><span class="s1">results</span>


<span class="s1">tf_impl[lax.sort_p] = _sort</span>


<span class="s3">def </span><span class="s1">_fft(x</span><span class="s3">, </span><span class="s1">fft_type</span><span class="s3">, </span><span class="s1">fft_lengths):</span>
  <span class="s1">FFT</span><span class="s3">, </span><span class="s1">IFFT</span><span class="s3">, </span><span class="s1">RFFT</span><span class="s3">, </span><span class="s1">IRFFT = list(map(xla_client.FftType</span><span class="s3">, </span><span class="s1">[</span><span class="s5">0</span><span class="s3">, </span><span class="s5">1</span><span class="s3">, </span><span class="s5">2</span><span class="s3">, </span><span class="s5">3</span><span class="s1">]))</span>
  <span class="s3">if </span><span class="s1">fft_type == IRFFT:</span>
    <span class="s1">expected_lengths = x.shape[-len(fft_lengths):-</span><span class="s5">1</span><span class="s1">] + ((x.shape[-</span><span class="s5">1</span><span class="s1">] - </span><span class="s5">1</span><span class="s1">) * </span><span class="s5">2</span><span class="s3">,</span><span class="s1">)</span>
  <span class="s3">else</span><span class="s1">:</span>
    <span class="s1">expected_lengths = x.shape[-len(fft_lengths):]</span>
  <span class="s3">if </span><span class="s1">expected_lengths != fft_lengths:</span>
    <span class="s3">raise </span><span class="s1">NotImplementedError(</span>
        <span class="s4">f&quot;Unsupported </span><span class="s3">{</span><span class="s1">fft_lengths=</span><span class="s3">} </span><span class="s4">for </span><span class="s3">{</span><span class="s1">fft_type=</span><span class="s3">} </span><span class="s4">of &quot;</span>
        <span class="s4">f&quot;array with shape=</span><span class="s3">{</span><span class="s1">x.shape</span><span class="s3">}</span><span class="s4">.&quot;</span><span class="s1">)</span>
  <span class="s1">tf_funcs = {</span>
      <span class="s1">FFT: [tf.signal.fft</span><span class="s3">, </span><span class="s1">tf.signal.fft2d</span><span class="s3">, </span><span class="s1">tf.signal.fft3d]</span><span class="s3">,</span>
      <span class="s1">IFFT: [tf.signal.ifft</span><span class="s3">, </span><span class="s1">tf.signal.ifft2d</span><span class="s3">, </span><span class="s1">tf.signal.ifft3d]</span><span class="s3">,</span>
      <span class="s1">RFFT: [tf.signal.rfft</span><span class="s3">, </span><span class="s1">tf.signal.rfft2d</span><span class="s3">, </span><span class="s1">tf.signal.rfft3d]</span><span class="s3">,</span>
      <span class="s1">IRFFT: [tf.signal.irfft</span><span class="s3">, </span><span class="s1">tf.signal.irfft2d</span><span class="s3">, </span><span class="s1">tf.signal.irfft3d]</span>
  <span class="s1">}</span>
  <span class="s3">return </span><span class="s1">tf_funcs[fft_type][len(fft_lengths) - </span><span class="s5">1</span><span class="s1">](x)</span>


<span class="s1">tf_impl[lax.fft_p] = _fft</span>


<span class="s3">def </span><span class="s1">_qr(operand</span><span class="s3">, </span><span class="s1">full_matrices):</span>
  <span class="s3">return </span><span class="s1">tf.linalg.qr(operand</span><span class="s3">, </span><span class="s1">full_matrices=full_matrices)</span>


<span class="s1">tf_impl[lax.linalg.qr_p] = _qr</span>


<span class="s3">def </span><span class="s1">_svd(operand</span><span class="s3">, </span><span class="s1">full_matrices</span><span class="s3">, </span><span class="s1">compute_uv):</span>
  <span class="s1">result = tf.linalg.svd(operand</span><span class="s3">, </span><span class="s1">full_matrices</span><span class="s3">, </span><span class="s1">compute_uv)</span>
  <span class="s3">if not </span><span class="s1">compute_uv:</span>
    <span class="s3">return </span><span class="s1">result</span><span class="s3">,</span>
  <span class="s1">s</span><span class="s3">, </span><span class="s1">u</span><span class="s3">, </span><span class="s1">v = result</span>
  <span class="s3">return </span><span class="s1">s</span><span class="s3">, </span><span class="s1">u</span><span class="s3">, </span><span class="s1">tf.linalg.adjoint(v)</span>


<span class="s1">tf_impl[lax.linalg.svd_p] = _svd</span>


<span class="s3">def </span><span class="s1">_eig(operand: TfVal</span><span class="s3">, </span><span class="s1">compute_left_eigenvectors: bool</span><span class="s3">,</span>
         <span class="s1">compute_right_eigenvectors: bool):</span>
  <span class="s3">if </span><span class="s1">compute_left_eigenvectors </span><span class="s3">and </span><span class="s1">compute_right_eigenvectors:</span>
    <span class="s0"># TODO(bchetioui): didn't find a 100% reliable, easy and satisfying way to</span>
    <span class="s0"># sort the left eigenvectors in the right order. The jax.numpy.linalg API</span>
    <span class="s0"># suggests to me that left eigenvectors are anyway seldom used, so I</span>
    <span class="s0"># think it is acceptable to leave as unimplemented for now.</span>
    <span class="s1">msg = (</span><span class="s4">&quot;Conversion of eig is not implemented when both &quot;</span>
           <span class="s4">&quot;compute_left_eigenvectors and compute_right_eigenvectors are set &quot;</span>
           <span class="s4">&quot;to True.&quot;</span><span class="s1">)</span>
    <span class="s3">raise </span><span class="s1">NotImplementedError(msg)</span>
  <span class="s3">elif not </span><span class="s1">(compute_left_eigenvectors </span><span class="s3">or </span><span class="s1">compute_right_eigenvectors):</span>
    <span class="s3">return </span><span class="s1">tuple([tf.linalg.eigvals(operand)])</span>
  <span class="s3">elif </span><span class="s1">compute_right_eigenvectors:</span>
    <span class="s3">return </span><span class="s1">tuple(tf.linalg.eig(operand))</span>
  <span class="s3">else</span><span class="s1">:  </span><span class="s0"># compute_left_eigenvectors == True</span>
    <span class="s1">wH</span><span class="s3">, </span><span class="s1">vl = tf.linalg.eig(tf.linalg.adjoint(operand))</span>
    <span class="s1">wHH = tf.math.conj(wH)</span>
    <span class="s3">return </span><span class="s1">tuple([wHH</span><span class="s3">, </span><span class="s1">vl])</span>


<span class="s1">tf_impl[lax.linalg.eig_p] = _eig</span>


<span class="s3">def </span><span class="s1">_eigh(operand: TfVal</span><span class="s3">, </span><span class="s1">lower: bool</span><span class="s3">, </span><span class="s1">sort_eigenvalues: bool</span><span class="s3">, </span><span class="s1">_in_avals</span><span class="s3">,</span>
          <span class="s1">_out_aval):</span>
  <span class="s3">del </span><span class="s1">sort_eigenvalues</span>
  <span class="s3">if </span><span class="s1">operand.shape[-</span><span class="s5">1</span><span class="s1">] == </span><span class="s5">0</span><span class="s1">:</span>
    <span class="s1">v</span><span class="s3">, </span><span class="s1">w = operand</span><span class="s3">, </span><span class="s1">tf.reshape(operand</span><span class="s3">, </span><span class="s1">_eval_shape(_in_avals[</span><span class="s5">0</span><span class="s1">].shape[:-</span><span class="s5">1</span><span class="s1">]))</span>
  <span class="s3">else</span><span class="s1">:</span>
    <span class="s3">if not </span><span class="s1">lower:</span>
      <span class="s1">operand = tf.linalg.adjoint(operand)</span>
    <span class="s1">w</span><span class="s3">, </span><span class="s1">v = tf.linalg.eigh(operand)</span>
  <span class="s1">cast_type = {</span>
      <span class="s1">tf.complex64: tf.float32</span><span class="s3">,</span>
      <span class="s1">tf.complex128: tf.float64</span>
  <span class="s1">}.get(operand.dtype)</span>
  <span class="s3">if </span><span class="s1">cast_type </span><span class="s3">is not None</span><span class="s1">:</span>
    <span class="s1">w = tf.cast(w</span><span class="s3">, </span><span class="s1">cast_type)</span>
  <span class="s3">return </span><span class="s1">v</span><span class="s3">, </span><span class="s1">w</span>


<span class="s1">tf_impl_with_avals[lax.linalg.eigh_p] = _eigh</span>


<span class="s3">def </span><span class="s1">_lu(operand: TfVal</span><span class="s3">, </span><span class="s1">_in_avals</span><span class="s3">, </span><span class="s1">_out_aval):</span>
  <span class="s3">return </span><span class="s1">_convert_jax_impl(lax_linalg._lu_python</span><span class="s3">, </span><span class="s1">extra_name_stack=</span><span class="s4">&quot;lu&quot;</span><span class="s1">)(</span>
      <span class="s1">operand</span><span class="s3">, </span><span class="s1">_in_avals=_in_avals</span><span class="s3">, </span><span class="s1">_out_aval=_out_aval)</span>


<span class="s1">tf_impl_with_avals[lax.linalg.lu_p] = _lu</span>


<span class="s3">def </span><span class="s1">_triangular_solve(a: TfVal</span><span class="s3">, </span><span class="s1">b: TfVal</span><span class="s3">, </span><span class="s1">*</span><span class="s3">, </span><span class="s1">left_side: bool</span><span class="s3">, </span><span class="s1">lower: bool</span><span class="s3">,</span>
                      <span class="s1">transpose_a: bool</span><span class="s3">, </span><span class="s1">conjugate_a: bool</span><span class="s3">, </span><span class="s1">unit_diagonal: bool</span><span class="s3">,</span>
                      <span class="s1">_in_avals: Sequence[core.ShapedArray]</span><span class="s3">,</span>
                      <span class="s1">_out_aval: core.ShapedArray):</span>
  <span class="s3">if </span><span class="s1">unit_diagonal:</span>
    <span class="s1">a_aval</span><span class="s3">, </span><span class="s1">_ = _in_avals</span>
    <span class="s1">a_shape = _eval_shape(a_aval.shape)</span>
    <span class="s1">a = tf.linalg.set_diag(a</span><span class="s3">, </span><span class="s1">tf.ones(a_shape[:-</span><span class="s5">1</span><span class="s1">]</span><span class="s3">, </span><span class="s1">dtype=a.dtype))</span>
  <span class="s3">if not </span><span class="s1">left_side:</span>
    <span class="s1">rank = len(a.shape)</span>
    <span class="s1">transpose_dimensions = list(range(rank - </span><span class="s5">2</span><span class="s1">)) + [rank - </span><span class="s5">1</span><span class="s3">, </span><span class="s1">rank - </span><span class="s5">2</span><span class="s1">]</span>
    <span class="s1">a = tf.transpose(a</span><span class="s3">, </span><span class="s1">transpose_dimensions)</span>
    <span class="s1">b = tf.transpose(b</span><span class="s3">, </span><span class="s1">transpose_dimensions)</span>
    <span class="s1">lower = </span><span class="s3">not </span><span class="s1">lower</span>
  <span class="s0"># adjoint == transpose for real dtypes, so special care need only be taken</span>
  <span class="s0"># for complex types.</span>
  <span class="s3">if </span><span class="s1">a.dtype </span><span class="s3">in </span><span class="s1">[tf.complex64</span><span class="s3">, </span><span class="s1">tf.complex128]:</span>
    <span class="s3">if </span><span class="s1">(transpose_a </span><span class="s3">and not </span><span class="s1">conjugate_a) </span><span class="s3">or </span><span class="s1">(</span><span class="s3">not </span><span class="s1">transpose_a </span><span class="s3">and </span><span class="s1">conjugate_a):</span>
      <span class="s1">a = tf.math.conj(a)</span>
  <span class="s1">result = tf.linalg.triangular_solve(a</span><span class="s3">, </span><span class="s1">b</span><span class="s3">, </span><span class="s1">lower=lower</span><span class="s3">, </span><span class="s1">adjoint=transpose_a)</span>
  <span class="s3">if not </span><span class="s1">left_side:</span>
    <span class="s1">result = tf.transpose(result</span><span class="s3">, </span><span class="s1">transpose_dimensions)</span>
  <span class="s3">return </span><span class="s1">result</span>


<span class="s1">tf_impl_with_avals[lax.linalg.triangular_solve_p] = _triangular_solve</span>


<span class="s3">def </span><span class="s1">_linear_solve(*args: TfVal</span><span class="s3">, </span><span class="s1">const_lengths</span><span class="s3">, </span><span class="s1">jaxprs</span><span class="s3">, </span><span class="s1">_in_avals</span><span class="s3">, </span><span class="s1">_out_aval):</span>
  <span class="s3">return </span><span class="s1">_convert_jax_impl(lax_control_flow._custom_linear_solve_impl</span><span class="s3">,</span>
                           <span class="s1">extra_name_stack=</span><span class="s4">&quot;linear_solve&quot;</span><span class="s1">)(</span>
      <span class="s1">*args</span><span class="s3">,</span>
      <span class="s1">const_lengths=const_lengths</span><span class="s3">,</span>
      <span class="s1">jaxprs=jaxprs</span><span class="s3">,</span>
      <span class="s1">_in_avals=_in_avals</span><span class="s3">,</span>
      <span class="s1">_out_aval=_out_aval)</span>


<span class="s1">tf_impl_with_avals[lax.linear_solve_p] = _linear_solve</span>

<span class="s3">def </span><span class="s1">_tridiagonal_solve(*args: TfVal</span><span class="s3">, </span><span class="s1">_in_avals</span><span class="s3">, </span><span class="s1">_out_aval</span><span class="s3">, </span><span class="s1">**params):</span>
  <span class="s3">return </span><span class="s1">_convert_jax_impl(lax_linalg._tridiagonal_solve_jax</span><span class="s3">,</span>
                           <span class="s1">multiple_results=</span><span class="s3">False,</span>
                           <span class="s1">extra_name_stack=</span><span class="s4">&quot;tridiagonal_solve&quot;</span><span class="s1">)(</span>
      <span class="s1">*args</span><span class="s3">,</span>
      <span class="s1">_in_avals=_in_avals</span><span class="s3">,</span>
      <span class="s1">_out_aval=_out_aval)</span>


<span class="s1">tf_impl_with_avals[lax.linalg.tridiagonal_solve_p] = _tridiagonal_solve</span>

<span class="s3">def </span><span class="s1">_custom_jvp_call(*args: TfVal</span><span class="s3">, </span><span class="s1">call_jaxpr: core.ClosedJaxpr</span><span class="s3">,</span>
                           <span class="s1">jvp_jaxpr_thunk: Callable</span><span class="s3">,</span>
                           <span class="s1">num_consts: int) -&gt; Sequence[TfVal]:</span>
  <span class="s0"># TODO(necula): ensure that there is no AD transformation in scope</span>
  <span class="s3">del </span><span class="s1">jvp_jaxpr_thunk</span><span class="s3">, </span><span class="s1">num_consts</span>
  <span class="s3">return </span><span class="s1">_interpret_jaxpr(call_jaxpr</span><span class="s3">, </span><span class="s1">*args</span><span class="s3">, </span><span class="s1">extra_name_stack=</span><span class="s4">&quot;custom_jvp&quot;</span><span class="s3">,</span>
                          <span class="s1">fresh_constant_cache=</span><span class="s3">False</span><span class="s1">)</span>


<span class="s1">tf_impl[custom_derivatives.custom_jvp_call_p] = _custom_jvp_call</span>


<span class="s3">def </span><span class="s1">_custom_vjp_call_jaxpr(*args: TfVal</span><span class="s3">, </span><span class="s1">fun_jaxpr: core.ClosedJaxpr</span><span class="s3">,</span>
                           <span class="s1">**_) -&gt; Sequence[TfVal]:</span>
  <span class="s0"># TODO(necula): ensure that there is no AD transformation in scope</span>
  <span class="s3">return </span><span class="s1">_interpret_jaxpr(fun_jaxpr</span><span class="s3">, </span><span class="s1">*args</span><span class="s3">, </span><span class="s1">extra_name_stack=</span><span class="s4">&quot;custom_vjp&quot;</span><span class="s3">,</span>
                          <span class="s1">fresh_constant_cache=</span><span class="s3">False</span><span class="s1">)</span>


<span class="s1">tf_impl[custom_derivatives.custom_vjp_call_jaxpr_p] = _custom_vjp_call_jaxpr</span>


<span class="s3">def </span><span class="s1">_custom_lin(*args: TfVal</span><span class="s3">, </span><span class="s1">**_) -&gt; Sequence[TfVal]:</span>
  <span class="s3">raise </span><span class="s1">TypeError(</span><span class="s4">&quot;can't apply forward-mode autodiff (jvp) to a custom_vjp &quot;</span>
                  <span class="s4">&quot;function.&quot;</span><span class="s1">)</span>


<span class="s1">tf_impl[ad.custom_lin_p] = _custom_lin</span>


<span class="s3">def </span><span class="s1">split_to_logical_devices(tensor: TfVal</span><span class="s3">,</span>
                             <span class="s1">partition_dimensions: pxla.PartitionsOrReplicated):</span>
  <span class="s2">&quot;&quot;&quot;Like TPUMPStrategy.experimental_split_to_logical_devices. 
 
  For jax2tf purposes we want to avoid needing to thread the `strategy` object 
  through the generated computation. It seems that the original function needs 
  the strategy object only for error checking, which we assume is done upstream 
  by JAX. 
 
  Args: 
    tensor: Input tensor to annotate. 
    partition_dimensions: A list of integers, with one integer per tensor 
      dimension, specifying in how many parts the dimension should be split. The 
      product of integers must equal the number of devices per replica. 
    use_sharding_op: whether to use a sharding op, or not. 
 
  Returns: 
    an annotated tensor. 
  &quot;&quot;&quot;</span>
  <span class="s0"># TODO: this is only for sharded_jit. Either remove, or implement in terms</span>
  <span class="s0"># of _shard_values.</span>
  <span class="s3">if </span><span class="s1">partition_dimensions </span><span class="s3">is None</span><span class="s1">:</span>
    <span class="s3">return </span><span class="s1">xla_sharding.replicate(tensor</span><span class="s3">, </span><span class="s1">use_sharding_op=</span><span class="s3">True</span><span class="s1">)</span>
  <span class="s1">num_partition_splits = np.prod(partition_dimensions)</span>
  <span class="s1">tile_assignment = np.arange(num_partition_splits).reshape(</span>
      <span class="s1">partition_dimensions)</span>
  <span class="s3">return </span><span class="s1">xla_sharding.tile(tensor</span><span class="s3">, </span><span class="s1">tile_assignment</span><span class="s3">, </span><span class="s1">use_sharding_op=</span><span class="s3">True</span><span class="s1">)</span>


<span class="s3">def </span><span class="s1">_shard_value(val: TfVal</span><span class="s3">,</span>
                 <span class="s1">aval: core.ShapedArray</span><span class="s3">,</span>
                 <span class="s1">sd: sharding.XLACompatibleSharding</span><span class="s3">, </span><span class="s1">*</span><span class="s3">,</span>
                 <span class="s1">skip_replicated_sharding: bool) -&gt; TfVal:</span>
  <span class="s2">&quot;&quot;&quot;Apply sharding to a TfVal.&quot;&quot;&quot;</span>
  <span class="s3">if </span><span class="s1">pxla._is_unspecified(sd):</span>
    <span class="s3">return </span><span class="s1">val</span>

  <span class="s1">sharding_proto: xla_client.OpSharding = cast(</span>
      <span class="s1">xla_client.OpSharding</span><span class="s3">, </span><span class="s1">sd._to_xla_op_sharding(aval.ndim))</span>

  <span class="s3">if </span><span class="s1">skip_replicated_sharding </span><span class="s3">and </span><span class="s1">pxla.is_op_sharding_replicated(sharding_proto):</span>
    <span class="s3">return </span><span class="s1">val</span>

  <span class="s0"># To use xla_sharding.py, we must have a xla_data_pb2.OpSharding.</span>
  <span class="s1">xla_sharding_proto: xla_data_pb2.OpSharding = (</span>
      <span class="s1">xla_data_pb2.OpSharding(</span>
          <span class="s1">type=int(sharding_proto.type)</span><span class="s3">,</span>
          <span class="s1">tile_assignment_dimensions=sharding_proto.tile_assignment_dimensions</span><span class="s3">,</span>
          <span class="s1">tile_assignment_devices=sharding_proto.tile_assignment_devices</span><span class="s3">,</span>
          <span class="s1">replicate_on_last_tile_dim=sharding_proto.replicate_on_last_tile_dim</span><span class="s3">,</span>
          <span class="s1">last_tile_dims=sharding_proto.last_tile_dims))</span>
  <span class="s3">if </span><span class="s1">tf_context.executing_eagerly():</span>
    <span class="s3">raise </span><span class="s1">ValueError(</span>
        <span class="s4">&quot;A jit function with sharded arguments or results must be used under a `tf.function` context. &quot;</span>
        <span class="s4">&quot;See https://github.com/google/jax/blob/main/jax/experimental/jax2tf/README.md#support-for-partitioning for a discussion&quot;</span><span class="s1">)</span>

  <span class="s3">return </span><span class="s1">xla_sharding.Sharding(proto=xla_sharding_proto).apply_to_tensor(</span>
      <span class="s1">val</span><span class="s3">, </span><span class="s1">use_sharding_op=</span><span class="s3">True</span><span class="s1">)</span>


<span class="s3">def </span><span class="s1">_pjit(*args: TfVal</span><span class="s3">,</span>
          <span class="s1">jaxpr: core.ClosedJaxpr</span><span class="s3">,</span>
          <span class="s1">in_shardings: Sequence[sharding.XLACompatibleSharding]</span><span class="s3">,</span>
          <span class="s1">out_shardings: Sequence[sharding.XLACompatibleSharding]</span><span class="s3">,</span>
          <span class="s1">resource_env: maps.ResourceEnv</span><span class="s3">,</span>
          <span class="s1">donated_invars</span><span class="s3">,</span>
          <span class="s1">name: str</span><span class="s3">,</span>
          <span class="s1">keep_unused: bool</span><span class="s3">,</span>
          <span class="s1">inline: bool</span><span class="s3">,</span>
          <span class="s1">_in_avals: Sequence[core.ShapedArray]</span><span class="s3">,</span>
          <span class="s1">_out_aval: Sequence[core.ShapedArray]) -&gt; TfVal:</span>
  <span class="s3">del </span><span class="s1">donated_invars</span>
  <span class="s0"># Apply sharding annotation to the arguments</span>
  <span class="s1">sharded_args: Sequence[TfVal] = tuple(</span>
      <span class="s1">map(partial(_shard_value</span><span class="s3">,</span>
                  <span class="s1">skip_replicated_sharding=</span><span class="s3">not </span><span class="s1">_thread_local_state.enable_xla)</span><span class="s3">,</span>
          <span class="s1">args</span><span class="s3">, </span><span class="s1">_in_avals</span><span class="s3">, </span><span class="s1">in_shardings))</span>
  <span class="s1">results = _interpret_jaxpr(jaxpr</span><span class="s3">, </span><span class="s1">*sharded_args</span><span class="s3">,</span>
                              <span class="s1">extra_name_stack=util.wrap_name(name</span><span class="s3">, </span><span class="s4">&quot;pjit&quot;</span><span class="s1">)</span><span class="s3">,</span>
                              <span class="s1">fresh_constant_cache=</span><span class="s3">False</span><span class="s1">)</span>
  <span class="s1">sharded_results: Sequence[TfVal] = tuple(</span>
      <span class="s1">map(partial(_shard_value</span><span class="s3">,</span>
                  <span class="s1">skip_replicated_sharding=</span><span class="s3">not </span><span class="s1">_thread_local_state.enable_xla)</span><span class="s3">,</span>
          <span class="s1">results</span><span class="s3">, </span><span class="s1">_out_aval</span><span class="s3">, </span><span class="s1">out_shardings))</span>
  <span class="s3">return </span><span class="s1">tuple(sharded_results)</span>


<span class="s1">tf_impl_with_avals[pjit.pjit_p] = _pjit</span>


<span class="s3">def </span><span class="s1">_pjit_sharding_constraint(arg: TfVal</span><span class="s3">, </span><span class="s1">*</span><span class="s3">,</span>
                              <span class="s1">sharding: sharding.NamedSharding</span><span class="s3">,</span>
                              <span class="s1">resource_env: maps.ResourceEnv</span><span class="s3">,</span>
                              <span class="s1">_in_avals: Sequence[core.ShapedArray]</span><span class="s3">,</span>
                              <span class="s1">_out_aval: core.ShapedArray</span><span class="s3">,</span>
                              <span class="s1">**kwargs) -&gt; TfVal:</span>
  <span class="s3">return </span><span class="s1">_shard_value(arg</span><span class="s3">, </span><span class="s1">_in_avals[</span><span class="s5">0</span><span class="s1">]</span><span class="s3">, </span><span class="s1">sharding</span><span class="s3">, </span><span class="s1">skip_replicated_sharding=</span><span class="s3">False</span><span class="s1">)</span>


<span class="s1">tf_impl_with_avals[pjit.sharding_constraint_p] = _pjit_sharding_constraint</span>

<span class="s3">def </span><span class="s1">_dimension_size_jax2tf(op: TfVal</span><span class="s3">, </span><span class="s1">*</span><span class="s3">, </span><span class="s1">dimension</span><span class="s3">, </span><span class="s1">_in_avals</span><span class="s3">, </span><span class="s1">_out_aval):</span>
  <span class="s1">dim_tf = tf.shape(op)[dimension]</span>
  <span class="s3">if </span><span class="s1">dim_tf.dtype != _to_tf_dtype(_out_aval.dtype):</span>
    <span class="s3">return </span><span class="s1">_convert_element_type(dim_tf</span><span class="s3">, </span><span class="s1">new_dtype=_out_aval.dtype</span><span class="s3">,</span>
                                 <span class="s1">weak_type=_out_aval.weak_type)</span>
  <span class="s3">else</span><span class="s1">:</span>
    <span class="s3">return </span><span class="s1">dim_tf</span>

<span class="s1">tf_impl_with_avals[shape_poly.dimension_size_p] = _dimension_size_jax2tf</span>

<span class="s3">def </span><span class="s1">_dim_as_value_jax2tf(dim: shape_poly.DimSize):</span>
  <span class="s1">dim_tf</span><span class="s3">, </span><span class="s1">= _eval_shape((dim</span><span class="s3">,</span><span class="s1">))</span>
  <span class="s3">return </span><span class="s1">dim_tf</span>

<span class="s1">tf_impl[shape_poly.dim_as_value_p] = _dim_as_value_jax2tf</span>

<span class="s3">def </span><span class="s1">_reduce_precision(x</span><span class="s3">, </span><span class="s1">*</span><span class="s3">, </span><span class="s1">exponent_bits</span><span class="s3">, </span><span class="s1">mantissa_bits):</span>
  <span class="s3">return </span><span class="s1">tfxla.reduce_precision(x</span><span class="s3">, </span><span class="s1">exponent_bits=exponent_bits</span><span class="s3">,</span>
                                <span class="s1">mantissa_bits=mantissa_bits)</span>

<span class="s1">tf_impl[lax.reduce_precision_p] = _reduce_precision</span>

<span class="s3">def </span><span class="s1">_register_checkpoint_pytrees():</span>
  <span class="s2">&quot;&quot;&quot;Registers TF custom container types as pytrees.&quot;&quot;&quot;</span>
  <span class="s1">m = tf.Module()</span>
  <span class="s0"># The types here are automagically changed by TensorFlow's checkpointing</span>
  <span class="s0"># infrastructure.</span>
  <span class="s1">m.a = (tf.Module()</span><span class="s3">, </span><span class="s1">tf.Module())</span>
  <span class="s1">m.b = [tf.Module()</span><span class="s3">, </span><span class="s1">tf.Module()]</span>
  <span class="s1">m.c = {</span><span class="s4">&quot;a&quot;</span><span class="s1">: tf.Module()}</span>
  <span class="s1">tuple_wrapper = type(m.a)</span>
  <span class="s1">list_wrapper = type(m.b)</span>
  <span class="s1">dict_wrapper = type(m.c)</span>

  <span class="s0"># TF AutoTrackable swaps container types out for wrappers.</span>
  <span class="s3">assert </span><span class="s1">tuple_wrapper </span><span class="s3">is not </span><span class="s1">tuple</span>
  <span class="s3">assert </span><span class="s1">list_wrapper </span><span class="s3">is not </span><span class="s1">list</span>
  <span class="s3">assert </span><span class="s1">dict_wrapper </span><span class="s3">is not </span><span class="s1">dict</span>

  <span class="s1">jax.tree_util.register_pytree_node(tuple_wrapper</span><span class="s3">, lambda </span><span class="s1">xs:</span>
                                     <span class="s1">(tuple(xs)</span><span class="s3">, None</span><span class="s1">)</span><span class="s3">, lambda </span><span class="s1">_</span><span class="s3">, </span><span class="s1">xs: tuple(xs))</span>

  <span class="s1">jax.tree_util.register_pytree_node(list_wrapper</span><span class="s3">, lambda </span><span class="s1">xs: (tuple(xs)</span><span class="s3">, None</span><span class="s1">)</span><span class="s3">,</span>
                                     <span class="s3">lambda </span><span class="s1">_</span><span class="s3">, </span><span class="s1">xs: list(xs))</span>

  <span class="s1">jax.tree_util.register_pytree_node(</span>
      <span class="s1">dict_wrapper</span><span class="s3">,</span>
      <span class="s3">lambda </span><span class="s1">s: (tuple(s.values())</span><span class="s3">, </span><span class="s1">tuple(s.keys()))</span><span class="s3">,</span>
      <span class="s3">lambda </span><span class="s1">k</span><span class="s3">, </span><span class="s1">xs: dict_wrapper(zip(k</span><span class="s3">, </span><span class="s1">xs)))</span>


<span class="s1">_register_checkpoint_pytrees()</span>
</pre>
</body>
</html>