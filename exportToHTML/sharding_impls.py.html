<html>
<head>
<title>sharding_impls.py</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<style type="text/css">
.s0 { color: #808080;}
.s1 { color: #a9b7c6;}
.s2 { color: #cc7832;}
.s3 { color: #629755; font-style: italic;}
.s4 { color: #6a8759;}
.s5 { color: #6897bb;}
</style>
</head>
<body bgcolor="#2b2b2b">
<table CELLSPACING=0 CELLPADDING=5 COLS=1 WIDTH="100%" BGCOLOR="#606060" >
<tr><td><center>
<font face="Arial, Helvetica" color="#000000">
sharding_impls.py</font>
</center></td></tr></table>
<pre><span class="s0"># Copyright 2021 The JAX Authors.</span>
<span class="s0">#</span>
<span class="s0"># Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);</span>
<span class="s0"># you may not use this file except in compliance with the License.</span>
<span class="s0"># You may obtain a copy of the License at</span>
<span class="s0">#</span>
<span class="s0">#     https://www.apache.org/licenses/LICENSE-2.0</span>
<span class="s0">#</span>
<span class="s0"># Unless required by applicable law or agreed to in writing, software</span>
<span class="s0"># distributed under the License is distributed on an &quot;AS IS&quot; BASIS,</span>
<span class="s0"># WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span>
<span class="s0"># See the License for the specific language governing permissions and</span>
<span class="s0"># limitations under the License.</span>

<span class="s2">from </span><span class="s1">__future__ </span><span class="s2">import </span><span class="s1">annotations</span>

<span class="s2">import </span><span class="s1">functools</span>
<span class="s2">from </span><span class="s1">collections </span><span class="s2">import </span><span class="s1">Counter</span>
<span class="s2">import </span><span class="s1">operator </span><span class="s2">as </span><span class="s1">op</span>
<span class="s2">from </span><span class="s1">typing </span><span class="s2">import </span><span class="s1">(Any</span><span class="s2">, </span><span class="s1">Sequence</span><span class="s2">, </span><span class="s1">List</span><span class="s2">, </span><span class="s1">Tuple</span><span class="s2">, </span><span class="s1">Optional</span><span class="s2">, </span><span class="s1">Mapping</span><span class="s2">, </span><span class="s1">Dict</span><span class="s2">, </span><span class="s1">Set</span><span class="s2">,</span>
                    <span class="s1">FrozenSet</span><span class="s2">, </span><span class="s1">Union</span><span class="s2">, </span><span class="s1">cast)</span>

<span class="s2">import </span><span class="s1">jax</span>
<span class="s2">from </span><span class="s1">jax._src </span><span class="s2">import </span><span class="s1">core</span>
<span class="s2">from </span><span class="s1">jax._src </span><span class="s2">import </span><span class="s1">mesh </span><span class="s2">as </span><span class="s1">mesh_lib</span>
<span class="s2">from </span><span class="s1">jax._src </span><span class="s2">import </span><span class="s1">sharding</span>
<span class="s2">from </span><span class="s1">jax._src.util </span><span class="s2">import </span><span class="s1">safe_map</span><span class="s2">, </span><span class="s1">safe_zip</span><span class="s2">, </span><span class="s1">use_cpp_class</span><span class="s2">, </span><span class="s1">use_cpp_method</span>
<span class="s2">from </span><span class="s1">jax._src.lib </span><span class="s2">import </span><span class="s1">xla_client </span><span class="s2">as </span><span class="s1">xc</span>
<span class="s2">from </span><span class="s1">jax.interpreters </span><span class="s2">import </span><span class="s1">mlir</span>
<span class="s2">from </span><span class="s1">jax._src.interpreters </span><span class="s2">import </span><span class="s1">pxla</span>

<span class="s2">import </span><span class="s1">numpy </span><span class="s2">as </span><span class="s1">np</span>

<span class="s1">Shape = Tuple[int</span><span class="s2">, </span><span class="s1">...]</span>
<span class="s1">Device = xc.Device</span>
<span class="s1">Index = Tuple[slice</span><span class="s2">, </span><span class="s1">...]</span>
<span class="s1">XLADeviceAssignment = Sequence[Device]</span>


<span class="s0"># Shardings that inherit from XLACompatibleSharding should implement the</span>
<span class="s0"># `_device_assignment` property and `_to_xla_op_sharding` method.</span>
<span class="s1">@use_cpp_class(xc.XLACompatibleSharding)</span>
<span class="s2">class </span><span class="s1">XLACompatibleSharding(sharding.Sharding):</span>
  <span class="s3">&quot;&quot;&quot;A `Sharding` that describes shardings expressible to XLA. 
 
  Any ``Sharding`` that is a subclass of ``XLACompatibleSharding`` will work 
  with all JAX APIs and transformations that use XLA. 
  &quot;&quot;&quot;</span>

  <span class="s0"># Abstract methods below that subclasses should implement.</span>

  <span class="s1">@property</span>
  <span class="s2">def </span><span class="s1">_device_assignment(self) -&gt; XLADeviceAssignment:</span>
    <span class="s2">raise </span><span class="s1">NotImplementedError(</span><span class="s4">'Subclasses should implement this method.'</span><span class="s1">)</span>

  <span class="s2">def </span><span class="s1">_to_xla_op_sharding(self</span><span class="s2">, </span><span class="s1">num_dimensions: int) -&gt; xc.OpSharding:</span>
    <span class="s2">raise </span><span class="s1">NotImplementedError(</span><span class="s4">'Subclasses should implement this method.'</span><span class="s1">)</span>

  <span class="s0">#############################################################################</span>
  <span class="s0"># Default implementations below that all subclasses will inherit.</span>

  <span class="s1">@functools.lru_cache(maxsize=</span><span class="s5">4096</span><span class="s1">)</span>
  <span class="s2">def </span><span class="s1">devices_indices_map(self</span><span class="s2">, </span><span class="s1">global_shape: Shape) -&gt; Mapping[Device</span><span class="s2">, </span><span class="s1">Index]:</span>
    <span class="s1">op_sharding = self._to_xla_op_sharding(len(global_shape))</span>
    <span class="s1">gspmd_sharding = GSPMDSharding(self._device_assignment</span><span class="s2">, </span><span class="s1">op_sharding)</span>
    <span class="s2">return </span><span class="s1">gspmd_sharding.devices_indices_map(global_shape)</span>

  <span class="s1">@functools.cached_property</span>
  <span class="s2">def </span><span class="s1">_addressable_device_assignment(self) -&gt; XLADeviceAssignment:</span>
    <span class="s2">return </span><span class="s1">[d </span><span class="s2">for </span><span class="s1">d </span><span class="s2">in </span><span class="s1">self._device_assignment</span>
            <span class="s2">if </span><span class="s1">d.process_index == d.client.process_index()]</span>

  <span class="s1">@functools.lru_cache(maxsize=</span><span class="s5">4096</span><span class="s1">)</span>
  <span class="s2">def </span><span class="s1">shard_shape(self</span><span class="s2">, </span><span class="s1">global_shape: Shape) -&gt; Shape:</span>
    <span class="s1">op_sharding = cast(xc.OpSharding</span><span class="s2">, </span><span class="s1">self._to_xla_op_sharding(len(global_shape)))</span>
    <span class="s2">if </span><span class="s1">pxla.is_op_sharding_replicated(op_sharding):</span>
      <span class="s2">return </span><span class="s1">global_shape</span>
    <span class="s1">partitions</span><span class="s2">, </span><span class="s1">_ = pxla.get_num_ways_dim_sharded(op_sharding)</span>
    <span class="s2">assert </span><span class="s1">len(partitions) == len(global_shape)</span><span class="s2">, </span><span class="s1">(len(partitions)</span><span class="s2">, </span><span class="s1">len(global_shape))</span>
    <span class="s1">out = []</span>
    <span class="s2">for </span><span class="s1">dim</span><span class="s2">, </span><span class="s1">(s</span><span class="s2">, </span><span class="s1">p) </span><span class="s2">in </span><span class="s1">enumerate(safe_zip(global_shape</span><span class="s2">, </span><span class="s1">partitions)):</span>
      <span class="s1">quotient</span><span class="s2">, </span><span class="s1">remainder = divmod(s</span><span class="s2">, </span><span class="s1">p)</span>
      <span class="s2">if </span><span class="s1">remainder != </span><span class="s5">0</span><span class="s1">:</span>
        <span class="s2">raise </span><span class="s1">ValueError(</span>
            <span class="s4">f&quot;Sharding </span><span class="s2">{</span><span class="s1">self</span><span class="s2">} </span><span class="s4">implies that array axis </span><span class="s2">{</span><span class="s1">dim</span><span class="s2">} </span><span class="s4">is partitioned &quot;</span>
            <span class="s4">f&quot;</span><span class="s2">{</span><span class="s1">p</span><span class="s2">} </span><span class="s4">times, but the dimension size is </span><span class="s2">{</span><span class="s1">s</span><span class="s2">} </span><span class="s4">&quot;</span>
            <span class="s4">f&quot;(full shape: </span><span class="s2">{</span><span class="s1">global_shape</span><span class="s2">}</span><span class="s4">, &quot;</span>
            <span class="s4">f&quot;per-dimension tiling factors: </span><span class="s2">{</span><span class="s1">partitions</span><span class="s2">} </span><span class="s4">should evenly divide &quot;</span>
            <span class="s4">&quot;the shape)&quot;</span><span class="s1">)</span>
      <span class="s1">out.append(quotient)</span>
    <span class="s2">return </span><span class="s1">tuple(out)</span>

  <span class="s2">def </span><span class="s1">is_equivalent_to(self: XLACompatibleSharding</span><span class="s2">,  </span><span class="s0"># type: ignore</span>
                       <span class="s1">other: XLACompatibleSharding</span><span class="s2">, </span><span class="s1">ndim: int) -&gt; bool:</span>
    <span class="s2">try</span><span class="s1">:</span>
      <span class="s2">return </span><span class="s1">(pxla.are_op_shardings_equal(self._to_xla_op_sharding(ndim)</span><span class="s2">,</span>
                                          <span class="s1">other._to_xla_op_sharding(ndim)) </span><span class="s2">and</span>
              <span class="s1">self._device_assignment == other._device_assignment)</span>
    <span class="s0"># NotImplementedError is raised by PmapSharding because it can't lower</span>
    <span class="s0"># to OpSharding. So if `other` is a PmapSharding, default to a strict</span>
    <span class="s0"># equality check.</span>
    <span class="s2">except </span><span class="s1">NotImplementedError:</span>
      <span class="s2">return </span><span class="s1">self == other</span>


<span class="s1">@functools.lru_cache()</span>
<span class="s2">def </span><span class="s1">_check_mesh_resource_axis(mesh</span><span class="s2">, </span><span class="s1">parsed_pspec):</span>
  <span class="s2">try</span><span class="s1">:</span>
    <span class="s1">[mesh.shape[r] </span><span class="s2">for </span><span class="s1">p </span><span class="s2">in </span><span class="s1">parsed_pspec </span><span class="s2">if </span><span class="s1">p </span><span class="s2">is not None</span>
     <span class="s2">for </span><span class="s1">r </span><span class="s2">in </span><span class="s1">p]</span>
  <span class="s2">except </span><span class="s1">KeyError </span><span class="s2">as </span><span class="s1">e:</span>
    <span class="s2">raise </span><span class="s1">ValueError(</span><span class="s4">f&quot;Resource axis: </span><span class="s2">{</span><span class="s1">e.args[</span><span class="s5">0</span><span class="s1">]</span><span class="s2">} </span><span class="s4">of </span><span class="s2">{</span><span class="s1">parsed_pspec.user_spec</span><span class="s2">} </span><span class="s4">is &quot;</span>
                     <span class="s4">&quot;undefined.&quot;</span><span class="s1">) </span><span class="s2">from None</span>


<span class="s2">def </span><span class="s1">hashed_index(x) -&gt; int:</span>
  <span class="s0"># This works for both `pjit`/`xmap` indices and `pmap` indices (which might</span>
  <span class="s0"># have an integer instead of a slice).</span>
  <span class="s2">assert </span><span class="s1">all(v.step </span><span class="s2">is None for </span><span class="s1">v </span><span class="s2">in </span><span class="s1">x </span><span class="s2">if </span><span class="s1">isinstance(v</span><span class="s2">, </span><span class="s1">slice))</span>
  <span class="s2">return </span><span class="s1">hash(tuple((v.start</span><span class="s2">, </span><span class="s1">v.stop) </span><span class="s2">if </span><span class="s1">isinstance(v</span><span class="s2">, </span><span class="s1">slice) </span><span class="s2">else </span><span class="s1">v </span><span class="s2">for </span><span class="s1">v </span><span class="s2">in </span><span class="s1">x))</span>


<span class="s1">@functools.lru_cache(maxsize=</span><span class="s5">4096</span><span class="s1">)</span>
<span class="s2">def </span><span class="s1">device_replica_id_map(sharding</span><span class="s2">, </span><span class="s1">global_shape: Shape) -&gt; Mapping[Device</span><span class="s2">, </span><span class="s1">int]:</span>
  <span class="s2">try</span><span class="s1">:</span>
    <span class="s1">device_indices_map_fn = sharding.devices_indices_map</span>
  <span class="s2">except </span><span class="s1">AttributeError:</span>
    <span class="s2">raise </span><span class="s1">ValueError(</span>
        <span class="s4">f'Cannot calculate replica ids from sharding: </span><span class="s2">{</span><span class="s1">sharding</span><span class="s2">}</span><span class="s4">. Please '</span>
        <span class="s4">'create a device to index mapping for your sharding from which replica '</span>
        <span class="s4">'ids will be calculated.'</span><span class="s1">) </span><span class="s2">from None</span>

  <span class="s1">index_to_replica: Dict[int</span><span class="s2">, </span><span class="s1">int] = Counter()</span>
  <span class="s1">out = {}</span>
  <span class="s2">for </span><span class="s1">device</span><span class="s2">, </span><span class="s1">index </span><span class="s2">in </span><span class="s1">device_indices_map_fn(global_shape).items():</span>
    <span class="s1">h_index = hashed_index(index)</span>
    <span class="s1">replica_id = index_to_replica[h_index]</span>
    <span class="s1">index_to_replica[h_index] += </span><span class="s5">1</span>
    <span class="s1">out[device] = replica_id</span>
  <span class="s2">return </span><span class="s1">out</span>


<span class="s2">class </span><span class="s1">_UnconstrainedPartitionSingleton:</span>

  <span class="s2">def </span><span class="s1">__str__(self):</span>
    <span class="s2">return </span><span class="s4">&quot;UNCONSTRAINED&quot;</span>


<span class="s0"># Unconstrained sentinel value for PartitionSpec, representing a dimension for</span>
<span class="s0"># which the user wants XLA to assign the best partitioning.</span>
<span class="s0"># TODO(yashkatariya): May rename to AUTO.</span>
<span class="s1">_UNCONSTRAINED_PARTITION = _UnconstrainedPartitionSingleton()</span>


<span class="s2">class </span><span class="s1">PartitionSpec(tuple):</span>
  <span class="s3">&quot;&quot;&quot;Tuple describing how to partition tensor into mesh . 
 
  Each element is either None, string or a tuple of strings. 
  See``NamedSharding`` class for more details. 
 
  We create a separate class for this so JAX's pytree utilities can distinguish 
  it from a tuple that should be treated as a pytree. 
  &quot;&quot;&quot;</span>

  <span class="s0"># A sentinel value representing a dim is unconstrained.</span>
  <span class="s1">UNCONSTRAINED = _UNCONSTRAINED_PARTITION</span>

  <span class="s2">def </span><span class="s1">__init__(self</span><span class="s2">, </span><span class="s1">*partitions):</span>
    <span class="s2">pass</span>

  <span class="s2">def </span><span class="s1">__new__(cls</span><span class="s2">, </span><span class="s1">*partitions):</span>
    <span class="s2">return </span><span class="s1">tuple.__new__(PartitionSpec</span><span class="s2">, </span><span class="s1">partitions)</span>

  <span class="s2">def </span><span class="s1">__repr__(self):</span>
    <span class="s2">return </span><span class="s4">&quot;PartitionSpec%s&quot; </span><span class="s1">% tuple.__repr__(self)</span>

  <span class="s2">def </span><span class="s1">__reduce__(self):</span>
    <span class="s2">return </span><span class="s1">(PartitionSpec</span><span class="s2">, </span><span class="s1">tuple(self))</span>


<span class="s1">@use_cpp_class(xc.NamedSharding)</span>
<span class="s2">class </span><span class="s1">NamedSharding(XLACompatibleSharding):</span>
  <span class="s3">r&quot;&quot;&quot;NamedSharding is a way to express ``Sharding``\s using named axes. 
 
  ``Mesh`` and ``PartitionSpec`` can be used to express a ``Sharding`` with a name. 
 
  ``Mesh`` is a NumPy array of JAX devices in a multi-dimensional grid, 
  where each axis of the mesh has a name, e.g. 'x' or 'y'. Another name for 
  ``Mesh`` is &quot;logical mesh&quot;. 
 
  ``PartitionSpec`` is a tuple, whose elements can be a ``None``, 
  a mesh axis or a tuple of mesh axes. Each element describes how an input 
  dimension is partitioned across zero or more mesh dimensions. For example, 
  PartitionSpec('x', 'y') is a PartitionSpec where the first dimension of data 
  is sharded across ``x`` axis of the mesh, and the second dimension is sharded 
  across ``y`` axis of the mesh. 
 
  The Distributed arrays and automatic parallelization 
  (https://jax.readthedocs.io/en/latest/notebooks/Distributed_arrays_and_automatic_parallelization.html#namedsharding-gives-a-way-to-express-shardings-with-names) 
  goes into more details and has diagrams to help explain the concept about 
  ``Mesh`` and ``PartitionSpec``. 
 
  Args: 
    mesh: A ``jax.sharding.Mesh`` object. 
    spec: A ``jax.sharding.PartitionSpec`` object. 
 
  Example: 
 
    &gt;&gt;&gt; from jax.sharding import Mesh 
    &gt;&gt;&gt; from jax.sharding import PartitionSpec as P 
    &gt;&gt;&gt; mesh = Mesh(np.array(jax.devices()).reshape(2, 4), ('x', 'y')) 
    &gt;&gt;&gt; spec = P('x', 'y') 
    &gt;&gt;&gt; named_sharding = jax.sharding.NamedSharding(mesh, spec) 
  &quot;&quot;&quot;</span>

  <span class="s1">mesh: mesh_lib.Mesh</span>
  <span class="s1">spec: PartitionSpec</span>
  <span class="s1">_parsed_pspec: Optional[Any]</span>

  <span class="s1">@use_cpp_method()</span>
  <span class="s2">def </span><span class="s1">__init__(</span>
      <span class="s1">self</span><span class="s2">, </span><span class="s1">mesh: mesh_lib.Mesh</span><span class="s2">, </span><span class="s1">spec: PartitionSpec</span><span class="s2">, </span><span class="s1">_parsed_pspec = </span><span class="s2">None</span><span class="s1">):</span>

    <span class="s1">self.mesh = mesh</span>
    <span class="s1">self.spec = spec</span>
    <span class="s1">self._parsed_pspec = _parsed_pspec</span>
    <span class="s1">self._preprocess()</span>

  <span class="s2">def </span><span class="s1">__reduce__(self):</span>
    <span class="s2">return </span><span class="s1">type(self)</span><span class="s2">, </span><span class="s1">(self.mesh</span><span class="s2">, </span><span class="s1">self.spec)</span>

  <span class="s2">def </span><span class="s1">_preprocess(self):</span>
    <span class="s0"># This split exists because you can pass `_parsed_pspec` that has been</span>
    <span class="s0"># modified from the original. For example: Adding extra dimension to</span>
    <span class="s0"># axis_resources for vmap handlers. In such cases you need to preserve the</span>
    <span class="s0"># `sync` attribute of parsed pspecs.</span>
    <span class="s0"># PartitionSpec is inferred from the parsed pspec in this case.</span>
    <span class="s0"># TODO(yaskatariya): Remove this and replace this with a normalized</span>
    <span class="s0"># representation of Parsed Pspec</span>
    <span class="s2">if </span><span class="s1">self._parsed_pspec </span><span class="s2">is None</span><span class="s1">:</span>
      <span class="s2">from </span><span class="s1">jax.experimental </span><span class="s2">import </span><span class="s1">pjit</span>
      <span class="s1">self._parsed_pspec</span><span class="s2">, </span><span class="s1">_</span><span class="s2">, </span><span class="s1">_ = pjit._prepare_axis_resources(</span>
          <span class="s1">self.spec</span><span class="s2">, </span><span class="s4">&quot;NamedSharding spec&quot;</span><span class="s1">)</span>

    <span class="s1">_check_mesh_resource_axis(self.mesh</span><span class="s2">, </span><span class="s1">self._parsed_pspec)</span>

  <span class="s2">def </span><span class="s1">__repr__(self):</span>
    <span class="s2">return </span><span class="s4">f'NamedSharding(mesh=</span><span class="s2">{</span><span class="s1">dict(self.mesh.shape)</span><span class="s2">}</span><span class="s4">, spec=</span><span class="s2">{</span><span class="s1">self.spec</span><span class="s2">}</span><span class="s4">)'</span>

  <span class="s2">def </span><span class="s1">__hash__(self):</span>
    <span class="s2">if not </span><span class="s1">hasattr(self</span><span class="s2">, </span><span class="s4">'_hash'</span><span class="s1">):</span>
      <span class="s1">self._hash = hash((self.mesh</span><span class="s2">, </span><span class="s1">self._parsed_pspec))</span>
    <span class="s2">return </span><span class="s1">self._hash</span>

  <span class="s2">def </span><span class="s1">__eq__(self</span><span class="s2">, </span><span class="s1">other):</span>
    <span class="s2">if not </span><span class="s1">isinstance(other</span><span class="s2">, </span><span class="s1">NamedSharding):</span>
      <span class="s2">return False</span>
    <span class="s2">if </span><span class="s1">id(self) == id(other):</span>
      <span class="s2">return True</span>
    <span class="s2">if </span><span class="s1">id(self.mesh) == id(other.mesh) </span><span class="s2">and </span><span class="s1">self._parsed_pspec == other._parsed_pspec:</span>
      <span class="s2">return True</span>
    <span class="s2">return </span><span class="s1">self.mesh == other.mesh </span><span class="s2">and </span><span class="s1">self._parsed_pspec == other._parsed_pspec</span>

  <span class="s2">def </span><span class="s1">is_compatible_aval(self</span><span class="s2">, </span><span class="s1">aval_shape: Shape):</span>
    <span class="s2">assert </span><span class="s1">self._parsed_pspec </span><span class="s2">is not None</span>
    <span class="s2">if </span><span class="s1">len(aval_shape) &lt; len(self._parsed_pspec):</span>
      <span class="s2">raise </span><span class="s1">ValueError(</span>
          <span class="s4">f&quot;Sharding </span><span class="s2">{</span><span class="s1">self</span><span class="s2">} </span><span class="s4">is only valid for values of rank at least &quot;</span>
          <span class="s4">f&quot;</span><span class="s2">{</span><span class="s1">len(self._parsed_pspec)</span><span class="s2">}</span><span class="s4">, but was applied to a value of rank &quot;</span>
          <span class="s4">f&quot;</span><span class="s2">{</span><span class="s1">len(aval_shape)</span><span class="s2">}</span><span class="s4">&quot;</span><span class="s1">)</span>

  <span class="s1">@classmethod</span>
  <span class="s2">def </span><span class="s1">_from_parsed_pspec(cls</span><span class="s2">, </span><span class="s1">mesh</span><span class="s2">, </span><span class="s1">parsed_pspec):</span>
    <span class="s2">return </span><span class="s1">cls(mesh</span><span class="s2">, </span><span class="s1">parsed_pspec.get_partition_spec()</span><span class="s2">, </span><span class="s1">parsed_pspec)</span>

  <span class="s1">@functools.cached_property</span>
  <span class="s2">def </span><span class="s1">device_set(self) -&gt; Set[Device]:</span>
    <span class="s2">return </span><span class="s1">set(self.mesh.devices.flat)</span>

  <span class="s1">@functools.cached_property</span>
  <span class="s2">def </span><span class="s1">_device_assignment(self) -&gt; XLADeviceAssignment:</span>
    <span class="s2">return </span><span class="s1">list(self.mesh.devices.flat)</span>

  <span class="s1">@functools.lru_cache(maxsize=</span><span class="s5">4096</span><span class="s1">)</span>
  <span class="s2">def </span><span class="s1">_to_xla_op_sharding(</span>
      <span class="s1">self</span><span class="s2">,</span>
      <span class="s1">num_dimensions: int</span><span class="s2">,</span>
      <span class="s1">axis_ctx: Optional[Union[mlir.SPMDAxisContext</span><span class="s2">, </span><span class="s1">mlir.ShardingContext]] = </span><span class="s2">None</span>
  <span class="s1">) -&gt; xc.OpSharding:</span>
    <span class="s2">from </span><span class="s1">jax.experimental.pjit </span><span class="s2">import </span><span class="s1">get_array_mapping</span>
    <span class="s2">assert </span><span class="s1">self._parsed_pspec </span><span class="s2">is not None</span>
    <span class="s1">array_mapping = get_array_mapping(self._parsed_pspec)</span>
    <span class="s0"># TODO(yashkatariya): Move away from sharding spec in NamedSharding</span>
    <span class="s0"># since we don't really need sharding spec.</span>
    <span class="s1">sharding_spec = pxla.new_mesh_sharding_specs(</span>
        <span class="s1">self.mesh.shape</span><span class="s2">, </span><span class="s1">self.mesh.axis_names)(num_dimensions</span><span class="s2">, </span><span class="s1">array_mapping)</span>
    <span class="s0"># Used in `with_sharding_constraint`.</span>
    <span class="s1">special_axes = {}</span>
    <span class="s0"># Manual axes is only used with xmap.</span>
    <span class="s2">if </span><span class="s1">axis_ctx </span><span class="s2">is not None and </span><span class="s1">isinstance(axis_ctx</span><span class="s2">, </span><span class="s1">mlir.SPMDAxisContext):</span>
      <span class="s1">axis_names = self.mesh.axis_names</span>
      <span class="s0"># Ignore type because mypy doesn't recognize the `hasattr` check above.</span>
      <span class="s2">for </span><span class="s1">manual_axis </span><span class="s2">in </span><span class="s1">axis_ctx.manual_axes:  </span><span class="s0"># type: ignore</span>
        <span class="s1">special_axes[axis_names.index(manual_axis)] = xc.OpSharding.Type.MANUAL</span>
    <span class="s2">return </span><span class="s1">sharding_spec.sharding_proto(special_axes=special_axes)</span>


<span class="s1">@functools.lru_cache()</span>
<span class="s2">def </span><span class="s1">get_replicated_op_sharding():</span>
  <span class="s1">proto = xc.OpSharding()</span>
  <span class="s1">proto.type = xc.OpSharding.Type.REPLICATED</span>
  <span class="s2">return </span><span class="s1">proto</span>


<span class="s1">@use_cpp_class(xc.SingleDeviceSharding)</span>
<span class="s2">class </span><span class="s1">SingleDeviceSharding(XLACompatibleSharding):</span>
  <span class="s3">&quot;&quot;&quot;A subclass of ``XLACompatibleSharding`` that places its data on a single device. 
 
  Args: 
    device: A single :py:class:`Device`. 
 
  Example: 
 
    &gt;&gt;&gt; single_device_sharding = jax.sharding.SingleDeviceSharding( 
    ...     jax.devices()[0]) 
  &quot;&quot;&quot;</span>

  <span class="s1">_device: Device</span>

  <span class="s1">@use_cpp_method()</span>
  <span class="s2">def </span><span class="s1">__init__(self</span><span class="s2">, </span><span class="s1">device: Device):</span>
    <span class="s1">self._device = device</span>

  <span class="s2">def </span><span class="s1">__reduce__(self):</span>
    <span class="s2">return </span><span class="s1">type(self)</span><span class="s2">, </span><span class="s1">(self._device</span><span class="s2">,</span><span class="s1">)</span>

  <span class="s2">def </span><span class="s1">__repr__(self):</span>
    <span class="s2">return </span><span class="s4">f&quot;SingleDeviceSharding(device=</span><span class="s2">{</span><span class="s1">repr(self._device)</span><span class="s2">}</span><span class="s4">)&quot;</span>

  <span class="s2">def </span><span class="s1">__hash__(self):</span>
    <span class="s2">return </span><span class="s1">hash(self._device)</span>

  <span class="s2">def </span><span class="s1">__eq__(self</span><span class="s2">, </span><span class="s1">other):</span>
    <span class="s2">if not </span><span class="s1">isinstance(other</span><span class="s2">, </span><span class="s1">SingleDeviceSharding):</span>
      <span class="s2">return False</span>
    <span class="s2">if </span><span class="s1">id(self) == id(other):</span>
      <span class="s2">return True</span>
    <span class="s2">return </span><span class="s1">self._device == other._device</span>

  <span class="s1">@property</span>
  <span class="s2">def </span><span class="s1">device_set(self) -&gt; Set[Device]:</span>
    <span class="s2">return </span><span class="s1">{self._device}</span>

  <span class="s2">def </span><span class="s1">devices_indices_map(self</span><span class="s2">, </span><span class="s1">global_shape: Shape) -&gt; Mapping[Device</span><span class="s2">, </span><span class="s1">Index]:  </span><span class="s0"># type: ignore</span>
    <span class="s2">return </span><span class="s1">{self._device: (slice(</span><span class="s2">None</span><span class="s1">)</span><span class="s2">,</span><span class="s1">) * len(global_shape)}</span>

  <span class="s1">@property</span>
  <span class="s2">def </span><span class="s1">_device_assignment(self) -&gt; XLADeviceAssignment:</span>
    <span class="s2">return </span><span class="s1">[self._device]</span>

  <span class="s2">def </span><span class="s1">_to_xla_op_sharding(self</span><span class="s2">, </span><span class="s1">num_dimensions: int) -&gt; xc.OpSharding:</span>
    <span class="s2">return </span><span class="s1">get_replicated_op_sharding()</span>


<span class="s1">@use_cpp_class(xc.PmapSharding)</span>
<span class="s2">class </span><span class="s1">PmapSharding(XLACompatibleSharding):</span>
  <span class="s1">devices: np.ndarray</span>
  <span class="s1">sharding_spec: pxla.ShardingSpec</span>

  <span class="s1">@use_cpp_method()</span>
  <span class="s2">def </span><span class="s1">__init__(self</span><span class="s2">, </span><span class="s1">devices: Union[Sequence[Device]</span><span class="s2">, </span><span class="s1">np.ndarray]</span><span class="s2">,</span>
               <span class="s1">sharding_spec: pxla.ShardingSpec):</span>
    <span class="s1">self.devices = np.asarray(devices)</span>
    <span class="s0"># The sharding spec should be pmap's sharding spec.</span>
    <span class="s1">self.sharding_spec = sharding_spec</span>

  <span class="s2">def </span><span class="s1">__reduce__(self):</span>
    <span class="s2">return </span><span class="s1">type(self)</span><span class="s2">, </span><span class="s1">(self.devices</span><span class="s2">, </span><span class="s1">self.sharding_spec)</span>

  <span class="s2">def </span><span class="s1">__eq__(self</span><span class="s2">, </span><span class="s1">other):</span>
    <span class="s2">if not </span><span class="s1">isinstance(other</span><span class="s2">, </span><span class="s1">PmapSharding):</span>
      <span class="s2">return False</span>
    <span class="s2">if </span><span class="s1">id(self) == id(other):</span>
      <span class="s2">return True</span>
    <span class="s2">return </span><span class="s1">(self.sharding_spec == other.sharding_spec </span><span class="s2">and</span>
            <span class="s1">np.array_equal(self.devices</span><span class="s2">, </span><span class="s1">other.devices))</span>

  <span class="s2">def </span><span class="s1">__hash__(self):</span>
    <span class="s2">if not </span><span class="s1">hasattr(self</span><span class="s2">, </span><span class="s4">'_hash'</span><span class="s1">):</span>
      <span class="s1">self._hash = hash((tuple(self.devices.flat)</span><span class="s2">, </span><span class="s1">self.sharding_spec))</span>
    <span class="s2">return </span><span class="s1">self._hash</span>

  <span class="s2">def </span><span class="s1">__str__(self):</span>
    <span class="s1">device_ids = [d.id </span><span class="s2">for </span><span class="s1">d </span><span class="s2">in </span><span class="s1">self.devices.flat]</span>
    <span class="s2">return </span><span class="s1">(</span><span class="s4">f'PmapSharding(sharding_spec=</span><span class="s2">{</span><span class="s1">self.sharding_spec</span><span class="s2">}</span><span class="s4">, '</span>
            <span class="s4">f'</span><span class="s2">{</span><span class="s1">device_ids=</span><span class="s2">}</span><span class="s4">, '</span>
            <span class="s4">f'device_platform=</span><span class="s2">{</span><span class="s1">self.devices.flat[</span><span class="s5">0</span><span class="s1">].platform.upper()</span><span class="s2">}</span><span class="s4">, '</span>
            <span class="s4">f'device_shape=</span><span class="s2">{</span><span class="s1">self.devices.shape</span><span class="s2">}</span><span class="s4">)'</span><span class="s1">)</span>

  <span class="s2">def </span><span class="s1">__repr__(self):</span>
    <span class="s2">return </span><span class="s1">(</span><span class="s4">f'PmapSharding(sharding_spec=</span><span class="s2">{</span><span class="s1">self.sharding_spec</span><span class="s2">}</span><span class="s4">, '</span>
            <span class="s4">f'devices=</span><span class="s2">{</span><span class="s1">self.devices</span><span class="s2">}</span><span class="s4">)'</span><span class="s1">)</span>

  <span class="s2">def </span><span class="s1">is_equivalent_to(self: PmapSharding</span><span class="s2">, </span><span class="s1">other: PmapSharding</span><span class="s2">,  </span><span class="s0"># type: ignore</span>
                       <span class="s1">ndim: int) -&gt; bool:</span>
    <span class="s2">return </span><span class="s1">self == other</span>

  <span class="s0"># TODO(yashkatariya): Expose `sharded_dim_size` in the API if required.</span>
  <span class="s1">@classmethod</span>
  <span class="s2">def </span><span class="s1">default(cls</span><span class="s2">, </span><span class="s1">shape: Shape</span><span class="s2">, </span><span class="s1">sharded_dim: int = </span><span class="s5">0</span><span class="s1">) -&gt; PmapSharding:</span>
    <span class="s3">&quot;&quot;&quot;Creates a `PmapSharding` which matches the implicit device order used by 
    `pmap`. 
 
    Args: 
      shape: The shape of the input array. 
      sharded_dim: Dimension the input array is sharded on. Defaults to 0. 
    &quot;&quot;&quot;</span>
    <span class="s0"># The dtype doesn't matter here. Its only used for creating the</span>
    <span class="s0"># sharding_spec.</span>
    <span class="s1">aval = core.ShapedArray(shape</span><span class="s2">, </span><span class="s1">np.int32)</span>
    <span class="s1">sharding_spec = pxla._create_pmap_sharding_spec(aval</span><span class="s2">, </span><span class="s1">sharded_dim)</span>

    <span class="s1">num_ways_sharded = </span><span class="s2">None</span>
    <span class="s2">for </span><span class="s1">s </span><span class="s2">in </span><span class="s1">sharding_spec.sharding:</span>
      <span class="s2">if </span><span class="s1">isinstance(s</span><span class="s2">, </span><span class="s1">pxla.Unstacked):</span>
        <span class="s1">num_ways_sharded = s.size</span>
    <span class="s2">if </span><span class="s1">num_ways_sharded </span><span class="s2">is None</span><span class="s1">:</span>
      <span class="s2">raise </span><span class="s1">NotImplementedError(</span>
          <span class="s4">'`None` to sharded_dim is not supported. Please file a jax '</span>
          <span class="s4">'issue if you need this feature.'</span><span class="s1">)</span>

    <span class="s1">pmap_devices: np.ndarray = np.array(jax.local_devices()[:num_ways_sharded])</span>
    <span class="s2">return </span><span class="s1">cls(pmap_devices</span><span class="s2">, </span><span class="s1">sharding_spec)</span>

  <span class="s1">@functools.cached_property</span>
  <span class="s2">def </span><span class="s1">device_set(self) -&gt; Set[Device]:</span>
    <span class="s2">return </span><span class="s1">set(self.devices.flat)</span>

  <span class="s1">@functools.lru_cache(maxsize=</span><span class="s5">4096</span><span class="s1">)</span>
  <span class="s2">def </span><span class="s1">devices_indices_map(self</span><span class="s2">, </span><span class="s1">global_shape: Shape) -&gt; Mapping[Device</span><span class="s2">, </span><span class="s1">Index]:</span>
    <span class="s1">indices = pxla.spec_to_indices(global_shape</span><span class="s2">, </span><span class="s1">self.sharding_spec)</span>
    <span class="s2">return </span><span class="s1">dict(safe_zip(self.devices.flat</span><span class="s2">, </span><span class="s1">indices))  </span><span class="s0"># type: ignore[arg-type]</span>

  <span class="s1">@functools.cached_property</span>
  <span class="s2">def </span><span class="s1">_device_assignment(self) -&gt; XLADeviceAssignment:</span>
    <span class="s2">return </span><span class="s1">list(self.devices.flat)</span>

  <span class="s2">def </span><span class="s1">_to_xla_op_sharding(self</span><span class="s2">, </span><span class="s1">num_dimensions: int) -&gt; xc.OpSharding:</span>
    <span class="s2">raise </span><span class="s1">NotImplementedError(</span><span class="s4">&quot;pmap doesn't use OpSharding.&quot;</span><span class="s1">)</span>

  <span class="s1">@functools.lru_cache(maxsize=</span><span class="s5">4096</span><span class="s1">)</span>
  <span class="s2">def </span><span class="s1">shard_shape(self</span><span class="s2">, </span><span class="s1">global_shape: Shape) -&gt; Shape:</span>
    <span class="s1">sharded_dim = </span><span class="s2">None</span>
    <span class="s1">sharded_dim_size = </span><span class="s2">None</span>
    <span class="s2">for </span><span class="s1">i</span><span class="s2">, </span><span class="s1">s </span><span class="s2">in </span><span class="s1">enumerate(self.sharding_spec.sharding):</span>
      <span class="s2">if </span><span class="s1">isinstance(s</span><span class="s2">, </span><span class="s1">pxla.Unstacked):</span>
        <span class="s1">sharded_dim = i</span>
        <span class="s1">sharded_dim_size = s.size</span>
        <span class="s2">break</span>
    <span class="s2">if </span><span class="s1">sharded_dim </span><span class="s2">is None</span><span class="s1">:</span>
      <span class="s2">return </span><span class="s1">global_shape</span>
    <span class="s2">if </span><span class="s1">global_shape[sharded_dim] != sharded_dim_size:</span>
      <span class="s2">raise </span><span class="s1">ValueError(</span>
          <span class="s4">f'The sharded dimension must be equal to the number of '</span>
          <span class="s4">f'devices passed to PmapSharding. Got sharded dimension </span><span class="s2">{</span><span class="s1">sharded_dim</span><span class="s2">} </span><span class="s4">'</span>
          <span class="s4">f'with value </span><span class="s2">{</span><span class="s1">global_shape[sharded_dim]</span><span class="s2">} </span><span class="s4">in shape </span><span class="s2">{</span><span class="s1">global_shape</span><span class="s2">} </span><span class="s4">and '</span>
          <span class="s4">f'the number of devices=</span><span class="s2">{</span><span class="s1">len(self._device_assignment)</span><span class="s2">}</span><span class="s4">'</span><span class="s1">)</span>
    <span class="s2">return </span><span class="s1">global_shape[:sharded_dim] + global_shape[sharded_dim+</span><span class="s5">1</span><span class="s1">:]</span>


<span class="s2">class </span><span class="s1">PositionalSharding(XLACompatibleSharding):</span>
  <span class="s1">_devices: List[xc.Device]</span>
  <span class="s1">_ids: np.ndarray  </span><span class="s0"># dtype DeviceIdSet</span>

  <span class="s2">def </span><span class="s1">__init__(self</span><span class="s2">, </span><span class="s1">devices: Union[Sequence[xc.Device]</span><span class="s2">, </span><span class="s1">np.ndarray]):</span>
    <span class="s2">if not </span><span class="s1">isinstance(devices</span><span class="s2">, </span><span class="s1">np.ndarray):</span>
      <span class="s1">devices = np.array(devices</span><span class="s2">, </span><span class="s1">dtype=</span><span class="s4">'object'</span><span class="s1">)</span>
    <span class="s2">if not </span><span class="s1">devices.size:</span>
      <span class="s2">raise </span><span class="s1">ValueError(</span><span class="s4">f&quot;</span><span class="s2">{</span><span class="s1">self.__class__.__name__</span><span class="s2">}</span><span class="s4">.__init__ requires at least &quot;</span>
                       <span class="s4">f&quot;one device, got </span><span class="s2">{</span><span class="s1">devices</span><span class="s2">}</span><span class="s4">&quot;</span><span class="s1">)</span>
    <span class="s1">self._devices = list(devices.flat)</span>
    <span class="s1">name = self._devices[</span><span class="s5">0</span><span class="s1">].platform.upper()</span>
    <span class="s1">self._ids = np.array([DeviceIdSet(name</span><span class="s2">, </span><span class="s1">i) </span><span class="s2">for </span><span class="s1">i </span><span class="s2">in </span><span class="s1">range(devices.size)]</span><span class="s2">,</span>
                         <span class="s1">dtype=</span><span class="s4">'object'</span><span class="s1">).reshape(devices.shape)</span>

  <span class="s1">shape = property(op.attrgetter(</span><span class="s4">'_ids.shape'</span><span class="s1">))</span>
  <span class="s1">ndim = property(op.attrgetter(</span><span class="s4">'_ids.ndim'</span><span class="s1">))</span>

  <span class="s2">def </span><span class="s1">__repr__(self) -&gt; str:</span>
    <span class="s1">cls_name = self.__class__.__name__</span>
    <span class="s1">ids = self._ids.copy()</span>
    <span class="s1">platform_name = self._devices[</span><span class="s5">0</span><span class="s1">].platform.upper()</span>
    <span class="s2">for </span><span class="s1">idx</span><span class="s2">, </span><span class="s1">x </span><span class="s2">in </span><span class="s1">np.ndenumerate(ids):</span>
      <span class="s1">ids[idx] = DeviceIdSet(platform_name</span><span class="s2">, </span><span class="s1">*(self._devices[i].id </span><span class="s2">for </span><span class="s1">i </span><span class="s2">in </span><span class="s1">x))</span>
    <span class="s1">body = np.array2string(ids</span><span class="s2">, </span><span class="s1">prefix=cls_name + </span><span class="s4">'('</span><span class="s2">, </span><span class="s1">suffix=</span><span class="s4">')'</span><span class="s2">,</span>
                           <span class="s1">max_line_width=</span><span class="s5">100</span><span class="s1">)</span>
    <span class="s2">return </span><span class="s4">f'</span><span class="s2">{</span><span class="s1">cls_name</span><span class="s2">}</span><span class="s4">(</span><span class="s2">{</span><span class="s1">body</span><span class="s2">}</span><span class="s4">)'</span>

  <span class="s2">def </span><span class="s1">reshape(self</span><span class="s2">, </span><span class="s1">*shape):</span>
    <span class="s2">return </span><span class="s1">self.remake(self._devices</span><span class="s2">, </span><span class="s1">self._ids.reshape(*shape))</span>

  <span class="s2">def </span><span class="s1">transpose(self</span><span class="s2">, </span><span class="s1">*axes):</span>
    <span class="s2">return </span><span class="s1">self.remake(self._devices</span><span class="s2">, </span><span class="s1">self._ids.transpose(*axes))</span>
  <span class="s1">T = property(transpose)</span>

  <span class="s2">def </span><span class="s1">replicate(self</span><span class="s2">, </span><span class="s1">axis=</span><span class="s2">None, </span><span class="s1">keepdims=</span><span class="s2">True</span><span class="s1">):</span>
    <span class="s1">new_ids = self._ids.sum(axis=axis</span><span class="s2">, </span><span class="s1">keepdims=keepdims)  </span><span class="s0"># union</span>
    <span class="s2">return </span><span class="s1">self.remake(self._devices</span><span class="s2">, </span><span class="s1">new_ids)</span>

  <span class="s1">@classmethod</span>
  <span class="s2">def </span><span class="s1">remake(</span>
      <span class="s1">cls</span><span class="s2">, </span><span class="s1">devices: List[xc.Device]</span><span class="s2">, </span><span class="s1">ids: np.ndarray) -&gt; PositionalSharding:</span>
    <span class="s1">self = cls.__new__(cls)</span>
    <span class="s1">self._devices = devices</span>
    <span class="s1">self._ids = ids</span>
    <span class="s2">return </span><span class="s1">self</span>

  <span class="s0"># Hashable</span>

  <span class="s2">def </span><span class="s1">__hash__(self) -&gt; int:</span>
    <span class="s2">return </span><span class="s1">id(self._devices)</span>

  <span class="s2">def </span><span class="s1">__eq__(self</span><span class="s2">, </span><span class="s1">other) -&gt; bool:</span>
    <span class="s2">return </span><span class="s1">(isinstance(other</span><span class="s2">, </span><span class="s1">PositionalSharding) </span><span class="s2">and</span>
            <span class="s1">id(self._devices) == id(other._devices) </span><span class="s2">and</span>
            <span class="s1">bool(np.all(self._ids == other._ids)))</span>

  <span class="s0"># Sharding interface</span>

  <span class="s1">@functools.cached_property</span>
  <span class="s2">def </span><span class="s1">device_set(self) -&gt; set[xc.Device]:</span>
    <span class="s2">return </span><span class="s1">set(self._devices)</span>

  <span class="s0"># XLACompatibleSharding interface</span>

  <span class="s1">@functools.lru_cache(maxsize=</span><span class="s5">4096</span><span class="s1">)</span>
  <span class="s2">def </span><span class="s1">_to_xla_op_sharding(self</span><span class="s2">, </span><span class="s1">num_dimensions: int</span><span class="s2">, </span><span class="s1">axis_ctx=</span><span class="s2">None</span><span class="s1">):</span>
    <span class="s2">assert </span><span class="s1">axis_ctx </span><span class="s2">is None</span>

    <span class="s1">pbuf = xc.OpSharding()</span>
    <span class="s2">if </span><span class="s1">self.shape == (</span><span class="s5">1</span><span class="s2">,</span><span class="s1">) * self.ndim:</span>
      <span class="s1">pbuf.type = xc.OpSharding.Type.REPLICATED</span>
      <span class="s2">return </span><span class="s1">pbuf</span>

    <span class="s1">shape = self.shape[self.ndim - num_dimensions:]  </span><span class="s0"># 'rank promotion' of val</span>
    <span class="s1">set_size</span><span class="s2">, </span><span class="s1">= {len(device_set) </span><span class="s2">for </span><span class="s1">device_set </span><span class="s2">in </span><span class="s1">self._ids.flat}</span>
    <span class="s1">pbuf.type = xc.OpSharding.Type.OTHER</span>
    <span class="s2">if </span><span class="s1">set_size &gt; </span><span class="s5">1</span><span class="s1">:</span>
      <span class="s1">pbuf.last_tile_dims = [xc.OpSharding.Type.REPLICATED]</span>
      <span class="s1">pbuf.tile_assignment_dimensions = (*shape</span><span class="s2">, </span><span class="s1">set_size)</span>
    <span class="s2">else</span><span class="s1">:</span>
      <span class="s1">pbuf.tile_assignment_dimensions = shape</span>
    <span class="s1">pbuf.tile_assignment_devices = [i </span><span class="s2">for </span><span class="s1">ids </span><span class="s2">in </span><span class="s1">self._ids.flat </span><span class="s2">for </span><span class="s1">i </span><span class="s2">in </span><span class="s1">ids]</span>
    <span class="s2">return </span><span class="s1">pbuf</span>

  <span class="s1">@property</span>
  <span class="s2">def </span><span class="s1">_device_assignment(self) -&gt; list[xc.Device]:</span>
    <span class="s2">return </span><span class="s1">self._devices</span>

<span class="s2">class </span><span class="s1">DeviceIdSet:</span>
  <span class="s1">_name: str</span>
  <span class="s1">_ids: FrozenSet[int]</span>
  <span class="s2">def </span><span class="s1">__init__(self</span><span class="s2">, </span><span class="s1">name</span><span class="s2">, </span><span class="s1">*ids):</span>
    <span class="s1">self._name = name</span>
    <span class="s1">self._ids = frozenset(ids)</span>

  <span class="s2">def </span><span class="s1">__iter__(self):</span>
    <span class="s2">return </span><span class="s1">iter(sorted(self._ids))</span>

  <span class="s2">def </span><span class="s1">__add__(self</span><span class="s2">, </span><span class="s1">other) -&gt; DeviceIdSet:</span>
    <span class="s2">assert </span><span class="s1">isinstance(other</span><span class="s2">, </span><span class="s1">DeviceIdSet)</span>
    <span class="s2">return </span><span class="s1">DeviceIdSet(self._name</span><span class="s2">, </span><span class="s1">*(self._ids | other._ids))</span>

  <span class="s2">def </span><span class="s1">__len__(self) -&gt; int:</span>
    <span class="s2">return </span><span class="s1">len(self._ids)</span>

  <span class="s2">def </span><span class="s1">__repr__(self) -&gt; str:</span>
    <span class="s1">ids = </span><span class="s4">', '</span><span class="s1">.join(safe_map(str</span><span class="s2">, </span><span class="s1">sorted(self._ids)))</span>
    <span class="s2">return </span><span class="s4">f'</span><span class="s2">{{{</span><span class="s1">self._name</span><span class="s2">} {</span><span class="s1">ids</span><span class="s2">}}}</span><span class="s4">'</span>

  <span class="s2">def </span><span class="s1">__hash__(self) -&gt; int:</span>
    <span class="s2">return </span><span class="s1">hash((self._name</span><span class="s2">, </span><span class="s1">self._ids))</span>

  <span class="s2">def </span><span class="s1">__eq__(self</span><span class="s2">, </span><span class="s1">other) -&gt; bool:</span>
    <span class="s2">return </span><span class="s1">(isinstance(other</span><span class="s2">, </span><span class="s1">DeviceIdSet) </span><span class="s2">and </span><span class="s1">self._name == other._name </span><span class="s2">and</span>
            <span class="s1">self._ids == other._ids)</span>


<span class="s1">@use_cpp_class(xc.GSPMDSharding)</span>
<span class="s2">class </span><span class="s1">GSPMDSharding(XLACompatibleSharding):</span>
  <span class="s1">_devices: Tuple[Device</span><span class="s2">, </span><span class="s1">...]</span>
  <span class="s1">_op_sharding: xc.OpSharding</span>

  <span class="s1">@use_cpp_method()</span>
  <span class="s2">def </span><span class="s1">__init__(self</span><span class="s2">, </span><span class="s1">devices: Sequence[Device]</span><span class="s2">, </span><span class="s1">op_sharding: xc.OpSharding):</span>
    <span class="s1">self._devices = tuple(devices)</span>
    <span class="s1">self._op_sharding = op_sharding</span>

  <span class="s2">def </span><span class="s1">__reduce__(self):</span>
    <span class="s2">return </span><span class="s1">type(self)</span><span class="s2">, </span><span class="s1">(self._devices</span><span class="s2">, </span><span class="s1">self._op_sharding)</span>

  <span class="s1">@functools.cached_property</span>
  <span class="s2">def </span><span class="s1">_op_sharding_hash(self):</span>
    <span class="s2">return </span><span class="s1">hash(xc.HloSharding.from_proto(self._op_sharding))</span>

  <span class="s2">def </span><span class="s1">__eq__(self</span><span class="s2">, </span><span class="s1">other):</span>
    <span class="s2">if not </span><span class="s1">isinstance(other</span><span class="s2">, </span><span class="s1">GSPMDSharding):</span>
      <span class="s2">return False</span>
    <span class="s2">if </span><span class="s1">id(self) == id(other):</span>
      <span class="s2">return True</span>
    <span class="s2">return </span><span class="s1">(pxla.are_op_shardings_equal(self._op_sharding</span><span class="s2">, </span><span class="s1">other._op_sharding) </span><span class="s2">and</span>
            <span class="s1">self._devices == other._devices)</span>

  <span class="s2">def </span><span class="s1">__hash__(self):</span>
    <span class="s2">if not </span><span class="s1">hasattr(self</span><span class="s2">, </span><span class="s4">'_hash'</span><span class="s1">):</span>
      <span class="s1">self._hash = hash((self._devices</span><span class="s2">, </span><span class="s1">self._op_sharding_hash))</span>
    <span class="s2">return </span><span class="s1">self._hash</span>

  <span class="s2">def </span><span class="s1">__repr__(self):</span>
    <span class="s2">return </span><span class="s4">f'GSPMDSharding(</span><span class="s2">{</span><span class="s1">repr(xc.HloSharding.from_proto(self._op_sharding))</span><span class="s2">}</span><span class="s4">)'</span>

  <span class="s2">def </span><span class="s1">is_compatible_aval(self</span><span class="s2">, </span><span class="s1">aval_shape: Shape):</span>
    <span class="s1">num_ways_dim_sharded</span><span class="s2">, </span><span class="s1">_ = pxla.get_num_ways_dim_sharded(self._op_sharding)</span>
    <span class="s2">if </span><span class="s1">len(aval_shape) &lt; len(num_ways_dim_sharded):</span>
      <span class="s2">raise </span><span class="s1">ValueError(</span>
          <span class="s4">f&quot;Sharding </span><span class="s2">{</span><span class="s1">self</span><span class="s2">} </span><span class="s4">is only valid for values of rank at least &quot;</span>
          <span class="s4">f&quot;</span><span class="s2">{</span><span class="s1">len(num_ways_dim_sharded)</span><span class="s2">}</span><span class="s4">, but was applied to a value of rank &quot;</span>
          <span class="s4">f&quot;</span><span class="s2">{</span><span class="s1">len(aval_shape)</span><span class="s2">}</span><span class="s4">&quot;</span><span class="s1">)</span>

  <span class="s1">@functools.cached_property</span>
  <span class="s2">def </span><span class="s1">device_set(self) -&gt; Set[Device]:</span>
    <span class="s2">return </span><span class="s1">set(self._devices)</span>

  <span class="s1">@functools.lru_cache(maxsize=</span><span class="s5">4096</span><span class="s1">)</span>
  <span class="s2">def </span><span class="s1">devices_indices_map(self</span><span class="s2">, </span><span class="s1">global_shape: Shape) -&gt; Mapping[Device</span><span class="s2">, </span><span class="s1">Index]:</span>
    <span class="s1">indices = pxla.op_sharding_to_indices(self._op_sharding</span><span class="s2">, </span><span class="s1">global_shape</span><span class="s2">,</span>
                                          <span class="s1">len(self._devices))</span>
    <span class="s2">return </span><span class="s1">dict(safe_zip(self._devices</span><span class="s2">, </span><span class="s1">indices))</span>

  <span class="s1">@property</span>
  <span class="s2">def </span><span class="s1">_device_assignment(self) -&gt; XLADeviceAssignment:</span>
    <span class="s2">return </span><span class="s1">list(self._devices)</span>

  <span class="s2">def </span><span class="s1">_to_xla_op_sharding(self</span><span class="s2">, </span><span class="s1">num_dimensions: int) -&gt; xc.OpSharding:</span>
    <span class="s2">return </span><span class="s1">self._op_sharding</span>

  <span class="s1">@classmethod</span>
  <span class="s2">def </span><span class="s1">get_replicated(cls</span><span class="s2">, </span><span class="s1">device_assignment):</span>
    <span class="s1">proto = get_replicated_op_sharding()</span>
    <span class="s2">return </span><span class="s1">cls(device_assignment</span><span class="s2">, </span><span class="s1">proto)</span>
</pre>
</body>
</html>